{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17257/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAIhCAYAAACfVbSSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA76UlEQVR4nO3deXxU1f3/8fckkAlLEtaEICHEpTWCGkxc2PzhQiwFxLqAiCwCFkwAWaqQakWhEkGLtGJQZJfFSAFBpWgqZbFCiRHBihYVJEGJEUSCCAmZub8/KPl2SEAyzpzLzLyej8d9PMzNnXM/M6J+fJ9zzzgsy7IEAAAAvwuzuwAAAIBQQeMFAABgCI0XAACAITReAAAAhtB4AQAAGELjBQAAYAiNFwAAgCE0XgAAAIbQeAEAABhC4wV4Yf78+XI4HJVHrVq1FB8fr7vvvlufffaZbXU9/vjjcjgctt3/dAUFBcrMzNTll1+uqKgoxcXF6eabb9a6deuqXDtw4ECPz7RevXpq1aqVbr31Vs2bN09lZWU1vv+YMWPkcDjUvXt3X7wdAPjZaLyAn2HevHnavHmz/v73v2v48OFavXq1OnbsqEOHDtld2nlh6dKl2rp1qwYNGqRVq1Zp9uzZcjqduummm7Rw4cIq19epU0ebN2/W5s2b9cYbb2jixImqV6+e7r//fqWmpmrfvn3nfO8TJ05o0aJFkqS1a9fqq6++8tn7AgCvWQBqbN68eZYkKz8/3+P8E088YUmy5s6da0tdEyZMsM6nf6y/+eabKucqKiqsK664wrrooos8zg8YMMCqV69eteO89dZbVu3ata1rr732nO+9bNkyS5LVrVs3S5L15JNPntPrysvLrRMnTlT7u6NHj57z/QGgOiRegA+lpaVJkr755pvKc8ePH9fYsWOVkpKimJgYNWrUSO3atdOqVauqvN7hcGj48OF6+eWXlZycrLp16+rKK6/UG2+8UeXaN998UykpKXI6nUpKStIzzzxTbU3Hjx9XVlaWkpKSFBERoQsuuECZmZn6/vvvPa5r1aqVunfvrjfeeENt27ZVnTp1lJycXHnv+fPnKzk5WfXq1dM111yj999//yc/j9jY2CrnwsPDlZqaqqKiop98/Snp6em6//779a9//UsbN248p9fMmTNHERERmjdvnhISEjRv3jxZluVxzfr16+VwOPTyyy9r7NixuuCCC+R0OvX5559r4MCBql+/vj766COlp6crKipKN910kyQpLy9PPXv2VIsWLRQZGamLL75YQ4cO1YEDByrH3rRpkxwOh5YuXVqltoULF8rhcCg/P/+cPwMAwYHGC/ChPXv2SJJ+8YtfVJ4rKyvTd999p9/97nd67bXXtHTpUnXs2FG33357tdNtb775pmbMmKGJEydq+fLlatSokX7zm99o9+7dlde888476tmzp6KiovTKK6/o6aef1quvvqp58+Z5jGVZlm677TY988wz6tevn958802NGTNGCxYs0I033lhl3dT27duVlZWlcePGacWKFYqJidHtt9+uCRMmaPbs2Zo8ebIWL16sw4cPq3v37jp27FiNP6OKigpt2rRJrVu3rtHrbr31Vkk6p8Zr3759evvtt9WzZ081bdpUAwYM0Oeff37G12ZlZamwsFAvvPCCXn/99cqGsby8XLfeeqtuvPFGrVq1Sk888YQk6YsvvlC7du00c+ZMvf3223rsscf0r3/9Sx07dtSJEyckSZ06dVLbtm31/PPPV7nfjBkzdPXVV+vqq6+u0WcAIAjYHbkBgejUVOOWLVusEydOWEeOHLHWrl1rNWvWzLr++uvPOFVlWSen2k6cOGENHjzYatu2rcfvJFlxcXFWaWlp5bni4mIrLCzMys7Orjx37bXXWs2bN7eOHTtWea60tNRq1KiRx1Tj2rVrLUnW1KlTPe6Tm5trSbJmzZpVeS4xMdGqU6eOtW/fvspzH374oSXJio+P95hme+211yxJ1urVq8/l4/LwyCOPWJKs1157zeP82aYaLcuyPvnkE0uS9cADD/zkPSZOnGhJstauXWtZlmXt3r3bcjgcVr9+/Tyu+8c//mFJsq6//voqYwwYMOCcpo3dbrd14sQJa+/evZYka9WqVZW/O/XnZNu2bZXntm7dakmyFixY8JPvA0DwIfECfobrrrtOtWvXVlRUlH71q1+pYcOGWrVqlWrVquVx3bJly9ShQwfVr19ftWrVUu3atTVnzhx98sknVca84YYbFBUVVflzXFycYmNjtXfvXknS0aNHlZ+fr9tvv12RkZGV10VFRalHjx4eY516enDgwIEe5++66y7Vq1dP77zzjsf5lJQUXXDBBZU/JycnS5I6d+6sunXrVjl/qqZzNXv2bD355JMaO3asevbsWaPXWqdNE57tulPTi126dJEkJSUlqXPnzlq+fLlKS0urvOaOO+4443jV/a6kpETDhg1TQkJC5d/PxMRESfL4e9qnTx/FxsZ6pF7PPfecmjZtqt69e5/T+wEQXGi8gJ9h4cKFys/P17p16zR06FB98skn6tOnj8c1K1asUK9evXTBBRdo0aJF2rx5s/Lz8zVo0CAdP368ypiNGzeucs7pdFZO6x06dEhut1vNmjWrct3p5w4ePKhatWqpadOmHucdDoeaNWumgwcPepxv1KiRx88RERFnPV9d/Wcyb948DR06VL/97W/19NNPn/PrTjnV5DVv3vys161bt0579uzRXXfdpdLSUn3//ff6/vvv1atXL/3444/VrrmKj4+vdqy6desqOjra45zb7VZ6erpWrFihhx9+WO+88462bt2qLVu2SJLH9KvT6dTQoUO1ZMkSff/99/r222/16quvasiQIXI6nTV6/wCCQ62fvgTAmSQnJ1cuqL/hhhvkcrk0e/Zs/fWvf9Wdd94pSVq0aJGSkpKUm5vrsceWN/tSSVLDhg3lcDhUXFxc5Xenn2vcuLEqKir07bffejRflmWpuLjY2BqjefPmaciQIRowYIBeeOEFr/YaW716taST6dvZzJkzR5I0bdo0TZs2rdrfDx061OPcmeqp7vy///1vbd++XfPnz9eAAQMqz3/++efVjvHAAw/oqaee0ty5c3X8+HFVVFRo2LBhZ30PAIIXiRfgQ1OnTlXDhg312GOPye12Szr5H++IiAiP/4gXFxdX+1TjuTj1VOGKFSs8EqcjR47o9ddf97j21FN4p/azOmX58uU6evRo5e/9af78+RoyZIjuvfdezZ4926umKy8vT7Nnz1b79u3VsWPHM1536NAhrVy5Uh06dNA//vGPKkffvn2Vn5+vf//7316/n1P1n55Yvfjii9VeHx8fr7vuuks5OTl64YUX1KNHD7Vs2dLr+wMIbCRegA81bNhQWVlZevjhh7VkyRLde++96t69u1asWKGMjAzdeeedKioq0qRJkxQfH+/1LveTJk3Sr371K3Xp0kVjx46Vy+XSlClTVK9ePX333XeV13Xp0kW33HKLxo0bp9LSUnXo0EE7duzQhAkT1LZtW/Xr189Xb71ay5Yt0+DBg5WSkqKhQ4dq69atHr9v27atRwPjdrsrp+zKyspUWFiov/3tb3r11VeVnJysV1999az3W7x4sY4fP66RI0dWm4w1btxYixcv1pw5c/Tss8969Z4uvfRSXXTRRRo/frwsy1KjRo30+uuvKy8v74yvefDBB3XttddKUpUnTwGEGHvX9gOB6UwbqFqWZR07dsxq2bKldckll1gVFRWWZVnWU089ZbVq1cpyOp1WcnKy9dJLL1W72akkKzMzs8qYiYmJ1oABAzzOrV692rriiiusiIgIq2XLltZTTz1V7ZjHjh2zxo0bZyUmJlq1a9e24uPjrQceeMA6dOhQlXt069atyr2rq2nPnj2WJOvpp58+42dkWf/3ZOCZjj179pzx2jp16lgtW7a0evToYc2dO9cqKys7670sy7JSUlKs2NjYs1573XXXWU2aNLHKysoqn2pctmxZtbWf6SnLnTt3Wl26dLGioqKshg0bWnfddZdVWFhoSbImTJhQ7WtatWplJScn/+R7ABDcHJZ1jo8KAQC8smPHDl155ZV6/vnnlZGRYXc5AGxE4wUAfvLFF19o7969+v3vf6/CwkJ9/vnnHttyAAg9LK4HAD+ZNGmSunTpoh9++EHLli2j6QJA4gUAAGAKiRcAAIAhNF4AAACG0HgBAAAYEtAbqLrdbn399deKioryajdsAABCiWVZOnLkiJo3b66wMPPZy/Hjx1VeXu6XsSMiIhQZGemXsX0poBuvr7/+WgkJCXaXAQBAQCkqKlKLFi2M3vP48eNKSqyv4hKXX8Zv1qyZ9uzZc943XwHdeEVFRUmSWv3uDwpznt8f9OkunP2l3SV4pdGSY3aX4LWSm47YXYJXhn24y+4SvPK7DXfbXYL3AnQRRtjRcLtL8MollxfZXYLXil9LtLuEGnGVH9enCyZW/vfTpPLychWXuLS3oJWio3z7D1npEbcSU79UeXk5jZc/nZpeDHNGKvw8/6BPVysswu4SvFK7nn/+T8WEWo7adpfglbpRgfkf07A6gfXPpIdAbbxcgflnpVY9509fdJ4KjwjMP+d2Ls+pH+VQ/Sjf3t+twFluFNCNFwAACCwuyy2Xj3cQdVlu3w7oRwH6/3UAAACBh8QLAAAY45Ylt3wbefl6PH8i8QIAADCExAsAABjjllu+XpHl+xH9h8QLAADAEBIvAABgjMuy5LJ8uybL1+P5E4kXAACAISReAADAmFB/qpHGCwAAGOOWJVcIN15MNQIAABhC4gUAAIwJ9alGEi8AAABDSLwAAIAxbCcBAAAAI0i8AACAMe7/Hr4eM1DYnnjl5OQoKSlJkZGRSk1N1aZNm+wuCQAAwC9sbbxyc3M1atQoPfLII9q2bZs6deqkrl27qrCw0M6yAACAn7j+u4+Xr49AYWvjNW3aNA0ePFhDhgxRcnKypk+froSEBM2cOdPOsgAAgJ+4LP8cgcK2xqu8vFwFBQVKT0/3OJ+enq733nuv2teUlZWptLTU4wAAAAgUtjVeBw4ckMvlUlxcnMf5uLg4FRcXV/ua7OxsxcTEVB4JCQkmSgUAAD7i9tMRKGxfXO9wODx+tiyryrlTsrKydPjw4cqjqKjIRIkAAAA+Ydt2Ek2aNFF4eHiVdKukpKRKCnaK0+mU0+k0UR4AAPADtxxyqfqA5eeMGShsS7wiIiKUmpqqvLw8j/N5eXlq3769TVUBAAD4j60bqI4ZM0b9+vVTWlqa2rVrp1mzZqmwsFDDhg2zsywAAOAnbuvk4esxA4WtjVfv3r118OBBTZw4Ufv371ebNm20Zs0aJSYm2lkWAACAX9j+lUEZGRnKyMiwuwwAAGCAyw9rvHw9nj/Z3ngBAIDQEeqNl+3bSQAAAIQKEi8AAGCM23LIbfl4Owkfj+dPJF4AAACGkHgBAABjWOMFAAAAI0i8AACAMS6FyeXj3Mfl09H8i8QLAADAEBIvAABgjOWHpxqtAHqqkcYLAAAYw+J6AAAAGEHiBQAAjHFZYXJZPl5cb/l0OL8i8QIAADCExAsAABjjlkNuH+c+bgVO5EXiBQAAYEhQJF51ShwKjwicJxokqfzieLtL8Mr+8YH1Of+vsHe+tbsEr7isXXaX4JWWrwfun5XINQV2l+CVrM+22V2CVxZ828HuErx24qNjdpdQIxUVx+0ugaca7S4AAAAgVARF4gUAAAKDf55qDJw1XjReAADAmJOL6307Nejr8fyJqUYAAABDSLwAAIAxboXJxXYSAAAA8DcSLwAAYEyoL64n8QIAADCExAsAABjjVhhfGQQAAAD/I/ECAADGuCyHXJaPvzLIx+P5E40XAAAwxuWH7SRcTDUCAADgdCReAADAGLcVJrePt5Nws50EAAAATkfiBQAAjGGNFwAAAIwg8QIAAMa45fvtH9w+Hc2/SLwAAAAMIfECAADG+OcrgwInR6LxAgAAxrisMLl8vJ2Er8fzp8CpFAAAIMCReAEAAGPccsgtXy+uD5zvaiTxAgAAMITECwAAGMMaLwAAABhB4gUAAIzxz1cGBU6OFDiVAgAABDgSLwAAYIzbcsjt668M8vF4/kTiBQAAYAiJFwAAMMbthzVefGUQAABANdxWmNw+3v7B1+P5U+BUCgAAEOBIvAAAgDEuOeTy8Vf8+Ho8fyLxAgAAMITECwAAGMMaLwAAABhB4gUAAIxxyfdrslw+Hc2/SLwAAAAMIfECAADGsMYLAADAEJcV5pfDGzk5OUpKSlJkZKRSU1O1adOms16/ePFiXXnllapbt67i4+N133336eDBgzW6J40XAAAIObm5uRo1apQeeeQRbdu2TZ06dVLXrl1VWFhY7fXvvvuu+vfvr8GDB+vjjz/WsmXLlJ+fryFDhtTovjReAADAGEsOuX18WF4s1p82bZoGDx6sIUOGKDk5WdOnT1dCQoJmzpxZ7fVbtmxRq1atNHLkSCUlJaljx44aOnSo3n///Rrdl8YLAAAEhdLSUo+jrKys2uvKy8tVUFCg9PR0j/Pp6el67733qn1N+/bttW/fPq1Zs0aWZembb77RX//6V3Xr1q1GNdJ4AQAAY/y5xishIUExMTGVR3Z2drU1HDhwQC6XS3FxcR7n4+LiVFxcXO1r2rdvr8WLF6t3796KiIhQs2bN1KBBAz333HM1ev80XgAAICgUFRXp8OHDlUdWVtZZr3c4PKcoLcuqcu6UnTt3auTIkXrsscdUUFCgtWvXas+ePRo2bFiNagyK7SSOXHNMYXUtu8uokWPN6thdglfKmgTSNnWeku+uPnI+3z34ZF+7S/DKBZGB86W1p/tm+LV2l+CVR7KutrsErxxrHLh/Vh6e/YrdJdTIjz+4tPEqe2twWw65Ld/+PT81XnR0tKKjo3/y+iZNmig8PLxKulVSUlIlBTslOztbHTp00EMPPSRJuuKKK1SvXj116tRJf/zjHxUfH39OtZJ4AQCAkBIREaHU1FTl5eV5nM/Ly1P79u2rfc2PP/6osDDPtik8PFzSyaTsXAVF4gUAAAKDS2Fy+Tj38Wa8MWPGqF+/fkpLS1O7du00a9YsFRYWVk4dZmVl6auvvtLChQslST169ND999+vmTNn6pZbbtH+/fs1atQoXXPNNWrevPk535fGCwAAGOPPqcaa6N27tw4ePKiJEydq//79atOmjdasWaPExERJ0v79+z329Bo4cKCOHDmiGTNmaOzYsWrQoIFuvPFGTZkypUb3pfECAAAhKSMjQxkZGdX+bv78+VXOjRgxQiNGjPhZ96TxAgAAxrgVJrePpxp9PZ4/BU6lAAAAAY7ECwAAGOOyHHL5eI2Xr8fzJxIvAAAAQ0i8AACAMefLU412IfECAAAwhMQLAAAYY1lhclu+zX0sH4/nTzReAADAGJcccsnHi+t9PJ4/BU6LCAAAEOBIvAAAgDFuy/eL4d3n/h3VtiPxAgAAMITECwAAGOP2w+J6X4/nT4FTKQAAQIAj8QIAAMa45ZDbx08h+no8f7I18crOztbVV1+tqKgoxcbG6rbbbtN//vMfO0sCAADwG1sbrw0bNigzM1NbtmxRXl6eKioqlJ6erqNHj9pZFgAA8JNTX5Lt6yNQ2DrVuHbtWo+f582bp9jYWBUUFOj666+3qSoAAOAvob64/rxa43X48GFJUqNGjar9fVlZmcrKyip/Li0tNVIXAACAL5w3LaJlWRozZow6duyoNm3aVHtNdna2YmJiKo+EhATDVQIAgJ/DLYfclo8PFtfX3PDhw7Vjxw4tXbr0jNdkZWXp8OHDlUdRUZHBCgEAAH6e82KqccSIEVq9erU2btyoFi1anPE6p9Mpp9NpsDIAAOBLlh+2k7ACKPGytfGyLEsjRozQypUrtX79eiUlJdlZDgAAgF/Z2nhlZmZqyZIlWrVqlaKiolRcXCxJiomJUZ06dewsDQAA+MGpdVm+HjNQ2LrGa+bMmTp8+LA6d+6s+Pj4yiM3N9fOsgAAAPzC9qlGAAAQOtjHCwAAwBCmGgEAAGAEiRcAADDG7YftJNhAFQAAAFWQeAEAAGNY4wUAAAAjSLwAAIAxJF4AAAAwgsQLAAAYE+qJF40XAAAwJtQbL6YaAQAADCHxAgAAxljy/YangfTNzyReAAAAhpB4AQAAY1jjBQAAACNIvAAAgDGhnngFRePlKHHKEem0u4yaCaA/JP+r+YUH7C7Ba69ue93uErxy5foH7C7BKwfb1LG7BK81uq7Y7hK8su7yXLtL8EqPO4fYXYLXWtUOrH8nHq3ltruEkBcUjRcAAAgMJF4AAACGhHrjxeJ6AAAAQ0i8AACAMZblkOXjhMrX4/kTiRcAAIAhJF4AAMAYtxw+/8ogX4/nTyReAAAAhpB4AQAAY3iqEQAAAEaQeAEAAGN4qhEAAABGkHgBAABjQn2NF40XAAAwhqlGAAAAGEHiBQAAjLH8MNVI4gUAAIAqSLwAAIAxliTL8v2YgYLECwAAwBASLwAAYIxbDjn4kmwAAAD4G4kXAAAwJtT38aLxAgAAxrgthxwhvHM9U40AAACGkHgBAABjLMsP20kE0H4SJF4AAACGkHgBAABjQn1xPYkXAACAISReAADAGBIvAAAAGEHiBQAAjAn1fbxovAAAgDFsJwEAAAAjSLwAAIAxJxMvXy+u9+lwfkXiBQAAYAiJFwAAMIbtJAAAAGAEiRcAADDG+u/h6zEDBYkXAACAISReAADAmFBf40XjBQAAzAnxuUamGgEAAAwh8QIAAOb4YapRATTVSOIFAABCUk5OjpKSkhQZGanU1FRt2rTprNeXlZXpkUceUWJiopxOpy666CLNnTu3Rvck8QIAAMacL1+SnZubq1GjRiknJ0cdOnTQiy++qK5du2rnzp1q2bJlta/p1auXvvnmG82ZM0cXX3yxSkpKVFFRUaP70ngBAICQM23aNA0ePFhDhgyRJE2fPl1vvfWWZs6cqezs7CrXr127Vhs2bNDu3bvVqFEjSVKrVq1qfN+gaLzqXFiq8LpldpdRIzOuWGp3CV4ZNfUBu0vw2uVFI+wuwSuOCJfdJXhlZr8X7C7Ba/etH2R3CV65bfwAu0vwSunko3aX4LXB2/rbXUKNuH4sk/SUrTX4czuJ0tJSj/NOp1NOp7PK9eXl5SooKND48eM9zqenp+u9996r9h6rV69WWlqapk6dqpdffln16tXTrbfeqkmTJqlOnTrnXGtQNF4AAAAJCQkeP0+YMEGPP/54lesOHDggl8uluLg4j/NxcXEqLi6uduzdu3fr3XffVWRkpFauXKkDBw4oIyND3333XY3WedF4AQAAcyyH759C/O94RUVFio6OrjxdXdr1vxwOzzosy6py7hS32y2Hw6HFixcrJiZG0snpyjvvvFPPP//8OadeNF4AAMAYfy6uj46O9mi8zqRJkyYKDw+vkm6VlJRUScFOiY+P1wUXXFDZdElScnKyLMvSvn37dMkll5xTrWwnAQAAQkpERIRSU1OVl5fncT4vL0/t27ev9jUdOnTQ119/rR9++KHy3K5duxQWFqYWLVqc871pvAAAgDmWn44aGjNmjGbPnq25c+fqk08+0ejRo1VYWKhhw4ZJkrKystS///89PHHPPfeocePGuu+++7Rz505t3LhRDz30kAYNGsTiegAAgLPp3bu3Dh48qIkTJ2r//v1q06aN1qxZo8TEREnS/v37VVhYWHl9/fr1lZeXpxEjRigtLU2NGzdWr1699Mc//rFG96XxAgAAxvhzO4maysjIUEZGRrW/mz9/fpVzl156aZXpyZpiqhEAAMAQEi8AAGCWj59qDCQkXgAAAIaQeAEAAGPOpzVedqDxAgAA5ni5/cNPjhkgmGoEAAAwhMQLAAAY5Pjv4esxAwOJFwAAgCEkXgAAwBzWeAEAAMAEEi8AAGAOiRcAAABMOG8ar+zsbDkcDo0aNcruUgAAgL9YDv8cAeK8mGrMz8/XrFmzdMUVV9hdCgAA8CPLOnn4esxAYXvi9cMPP6hv37566aWX1LBhQ7vLAQAA8BvbG6/MzEx169ZNN998809eW1ZWptLSUo8DAAAEEMtPR4CwdarxlVde0QcffKD8/Pxzuj47O1tPPPGEn6sCAADwD9sSr6KiIj344INatGiRIiMjz+k1WVlZOnz4cOVRVFTk5yoBAIBPsbjeHgUFBSopKVFqamrlOZfLpY0bN2rGjBkqKytTeHi4x2ucTqecTqfpUgEAAHzCtsbrpptu0kcffeRx7r777tOll16qcePGVWm6AABA4HNYJw9fjxkobGu8oqKi1KZNG49z9erVU+PGjaucBwAACAY1XuO1YMECvfnmm5U/P/zww2rQoIHat2+vvXv3+rQ4AAAQZEL8qcYaN16TJ09WnTp1JEmbN2/WjBkzNHXqVDVp0kSjR4/+WcWsX79e06dP/1ljAACA8xiL62umqKhIF198sSTptdde05133qnf/va36tChgzp37uzr+gAAAIJGjROv+vXr6+DBg5Kkt99+u3Lj08jISB07dsy31QEAgOAS4lONNU68unTpoiFDhqht27batWuXunXrJkn6+OOP1apVK1/XBwAAEDRqnHg9//zzateunb799lstX75cjRs3lnRyX64+ffr4vEAAABBESLxqpkGDBpoxY0aV83yVDwAAwNmdU+O1Y8cOtWnTRmFhYdqxY8dZr73iiit8UhgAAAhC/kiogi3xSklJUXFxsWJjY5WSkiKHwyHL+r93eepnh8Mhl8vlt2IBAAAC2Tk1Xnv27FHTpk0r/xoAAMAr/th3K9j28UpMTKz2r0/3vykYAAAAPNX4qcZ+/frphx9+qHL+yy+/1PXXX++TogAAQHA69SXZvj4CRY0br507d+ryyy/XP//5z8pzCxYs0JVXXqm4uDifFgcAAIIM20nUzL/+9S89+uijuvHGGzV27Fh99tlnWrt2rf785z9r0KBB/qgRAAAgKNS48apVq5aeeuopOZ1OTZo0SbVq1dKGDRvUrl07f9QHAAAQNGo81XjixAmNHTtWU6ZMUVZWltq1a6ff/OY3WrNmjT/qAwAACBo1TrzS0tL0448/av369bruuutkWZamTp2q22+/XYMGDVJOTo4/6gQAAEHAId8vhg+czSS8bLz+8pe/qF69epJObp46btw43XLLLbr33nt9XuC56Hvh+4qsX+O3Yqvj7tp2l+CV+Nf32l2C1+K2NLC7BK9YtcPtLsEroxv0srsEr+3pOtvuErzT1e4CvHNLi1S7S/Ben6vtrqBGXOWB+d+eYFLjbmXOnDnVnk9JSVFBQcHPLggAAAQxNlD13rFjx3TixAmPc06n82cVBAAAEKxqvLj+6NGjGj58uGJjY1W/fn01bNjQ4wAAADijEN/Hq8aN18MPP6x169YpJydHTqdTs2fP1hNPPKHmzZtr4cKF/qgRAAAEixBvvGo81fj6669r4cKF6ty5swYNGqROnTrp4osvVmJiohYvXqy+ffv6o04AAICAV+PE67vvvlNSUpIkKTo6Wt99950kqWPHjtq4caNvqwMAAEGF72qsoQsvvFBffvmlJOmyyy7Tq6++KulkEtagQQNf1gYAABBUatx43Xfffdq+fbskKSsrq3Kt1+jRo/XQQw/5vEAAABBEWONVM6NHj6786xtuuEGffvqp3n//fV100UW68sorfVocAABAMPnZ2723bNlSLVu29EUtAAAg2PkjoQqgxKvGU40AAADwTmB9wSEAAAho/ngKMSifaty3b58/6wAAAKHg1Hc1+voIEOfceLVp00Yvv/yyP2sBAAAIaufceE2ePFmZmZm64447dPDgQX/WBAAAglWIbydxzo1XRkaGtm/frkOHDql169ZavXq1P+sCAAAIOjVaXJ+UlKR169ZpxowZuuOOO5ScnKxatTyH+OCDD3xaIAAACB6hvri+xk817t27V8uXL1ejRo3Us2fPKo0XAAAAqlejrumll17S2LFjdfPNN+vf//63mjZt6q+6AABAMArxDVTPufH61a9+pa1bt2rGjBnq37+/P2sCAAAISufceLlcLu3YsUMtWrTwZz0AACCY+WGNV1AmXnl5ef6sAwAAhIIQn2rkuxoBAAAM4ZFEAABgDokXAAAATCDxAgAAxoT6BqokXgAAAIbQeAEAABhC4wUAAGAIa7wAAIA5If5UI40XAAAwhsX1AAAAMILECwAAmBVACZWvkXgBAAAYQuIFAADMCfHF9SReAAAAhpB4AQAAY3iqEQAAAEaQeAEAAHNCfI0XjRcAADCGqUYAAAAYQeIFAADMCfGpRhIvAAAAQ0i8AACAOSReAAAAMIHGCwAAGHPqqUZfH97IyclRUlKSIiMjlZqaqk2bNp3T6/75z3+qVq1aSklJqfE9g2KqccNdl6pWmNPuMmok9/pb7C7BK4cy7a7Ae/WL7K7AO+VdSu0uwSvONxvaXYLX+sdeb3cJXjnYt5HdJXil7JamdpfgtW9uqLC7hBpxH6uQXrW7ivNDbm6uRo0apZycHHXo0EEvvviiunbtqp07d6ply5ZnfN3hw4fVv39/3XTTTfrmm29qfF8SLwAAYI7lp6OGpk2bpsGDB2vIkCFKTk7W9OnTlZCQoJkzZ571dUOHDtU999yjdu3a1fymovECAAAm+bHxKi0t9TjKysqqLaG8vFwFBQVKT0/3OJ+enq733nvvjKXPmzdPX3zxhSZMmODNO5dE4wUAAIJEQkKCYmJiKo/s7Oxqrztw4IBcLpfi4uI8zsfFxam4uLja13z22WcaP368Fi9erFq1vF+pFRRrvAAAQGDw51cGFRUVKTo6uvK803n29d8Oh8PjZ8uyqpyTJJfLpXvuuUdPPPGEfvGLX/ysWmm8AABAUIiOjvZovM6kSZMmCg8Pr5JulZSUVEnBJOnIkSN6//33tW3bNg0fPlyS5Ha7ZVmWatWqpbfffls33njjOdVI4wUAAMw5DzZQjYiIUGpqqvLy8vSb3/ym8nxeXp569uxZ5fro6Gh99NFHHudycnK0bt06/fWvf1VSUtI535vGCwAAhJwxY8aoX79+SktLU7t27TRr1iwVFhZq2LBhkqSsrCx99dVXWrhwocLCwtSmTRuP18fGxioyMrLK+Z9C4wUAAIzx5xqvmujdu7cOHjyoiRMnav/+/WrTpo3WrFmjxMRESdL+/ftVWFjo20JF4wUAAEJURkaGMjIyqv3d/Pnzz/raxx9/XI8//niN70njBQAAzDkP1njZicYLAACYE+KNFxuoAgAAGELiBQAAjHH89/D1mIGCxAsAAMAQEi8AAGAOa7wAAABgAokXAAAw5nzZQNUuJF4AAACG2N54ffXVV7r33nvVuHFj1a1bVykpKSooKLC7LAAA4A+Wn44AYetU46FDh9ShQwfdcMMN+tvf/qbY2Fh98cUXatCggZ1lAQAAfwqgRsnXbG28pkyZooSEBM2bN6/yXKtWrewrCAAAwI9snWpcvXq10tLSdNdddyk2NlZt27bVSy+9dMbry8rKVFpa6nEAAIDAcWpxva+PQGFr47V7927NnDlTl1xyid566y0NGzZMI0eO1MKFC6u9Pjs7WzExMZVHQkKC4YoBAAC8Z2vj5Xa7ddVVV2ny5Mlq27athg4dqvvvv18zZ86s9vqsrCwdPny48igqKjJcMQAA+FlCfHG9rY1XfHy8LrvsMo9zycnJKiwsrPZ6p9Op6OhojwMAACBQ2Lq4vkOHDvrPf/7jcW7Xrl1KTEy0qSIAAOBPbKBqo9GjR2vLli2aPHmyPv/8cy1ZskSzZs1SZmamnWUBAAD4ha2N19VXX62VK1dq6dKlatOmjSZNmqTp06erb9++dpYFAAD8JcTXeNn+XY3du3dX9+7d7S4DAADA72xvvAAAQOgI9TVeNF4AAMAcf0wNBlDjZfuXZAMAAIQKEi8AAGAOiRcAAABMIPECAADGhPriehIvAAAAQ0i8AACAOazxAgAAgAkkXgAAwBiHZclh+Tai8vV4/kTjBQAAzGGqEQAAACaQeAEAAGPYTgIAAABGkHgBAABzWOMFAAAAE4Ii8fry7hYKd0baXUaNxHT4xu4SvNLi2YZ2l+C1S5/8t90leGXL3LZ2l+CVXiP+bncJXlv6Uhe7S/BKs8ZH7C7BK3W//N7uErz2i6RjdpdQIxVHy7TP5hpY4wUAAAAjgiLxAgAAASLE13jReAEAAGOYagQAAIARJF4AAMCcEJ9qJPECAAAwhMQLAAAYFUhrsnyNxAsAAMAQEi8AAGCOZZ08fD1mgCDxAgAAMITECwAAGBPq+3jReAEAAHPYTgIAAAAmkHgBAABjHO6Th6/HDBQkXgAAAIaQeAEAAHNY4wUAAAATSLwAAIAxob6dBIkXAACAISReAADAnBD/yiAaLwAAYAxTjQAAADCCxAsAAJjDdhIAAAAwgcQLAAAYwxovAAAAGEHiBQAAzAnx7SRIvAAAAAwh8QIAAMaE+hovGi8AAGAO20kAAADABBIvAABgTKhPNZJ4AQAAGELiBQAAzHFbJw9fjxkgSLwAAAAMIfECAADm8FQjAAAATCDxAgAAxjjkh6cafTucX9F4AQAAc/iuRgAAAJhA4gUAAIxhA1UAAAAYQeIFAADMYTsJAAAAmEDiBQAAjHFYlhw+fgrR1+P5U1A0Xnf9ZoMi69e2u4waWTvx/9ldglesh4rtLsFrU+PX212CVzpEXmV3CV65r8GHdpfgtX9svs7uErxSmB5ldwleafRpXbtL8Fot1zd2l1AjFS6X3SWEvKBovAAAQIBw//fw9ZgBgjVeAADAmFNTjb4+vJGTk6OkpCRFRkYqNTVVmzZtOuO1K1asUJcuXdS0aVNFR0erXbt2euutt2p8TxovAAAQcnJzczVq1Cg98sgj2rZtmzp16qSuXbuqsLCw2us3btyoLl26aM2aNSooKNANN9ygHj16aNu2bTW6L1ONAADAnPNkO4lp06Zp8ODBGjJkiCRp+vTpeuuttzRz5kxlZ2dXuX769OkeP0+ePFmrVq3S66+/rrZt257zfUm8AABAUCgtLfU4ysrKqr2uvLxcBQUFSk9P9zifnp6u995775zu5Xa7deTIETVq1KhGNdJ4AQAAc059SbavD0kJCQmKiYmpPKpLriTpwIEDcrlciouL8zgfFxen4uJze3r/T3/6k44ePapevXrV6O0z1QgAAIJCUVGRoqOjK392Op1nvd7hcHj8bFlWlXPVWbp0qR5//HGtWrVKsbGxNaqRxgsAABjjzy/Jjo6O9mi8zqRJkyYKDw+vkm6VlJRUScFOl5ubq8GDB2vZsmW6+eaba1wrU40AACCkREREKDU1VXl5eR7n8/Ly1L59+zO+bunSpRo4cKCWLFmibt26eXVvEi8AAGDO/6zJ8umYNTRmzBj169dPaWlpateunWbNmqXCwkINGzZMkpSVlaWvvvpKCxculHSy6erfv7/+/Oc/67rrrqtMy+rUqaOYmJhzvi+NFwAACDm9e/fWwYMHNXHiRO3fv19t2rTRmjVrlJiYKEnav3+/x55eL774oioqKpSZmanMzMzK8wMGDND8+fPP+b40XgAAwBiH++Th6zG9kZGRoYyMjGp/d3oztX79eu9uchoaLwAAYM55MtVoFxbXAwAAGELiBQAAzDlPvjLILiReAAAAhpB4AQAAYxyWJYeP12T5ejx/IvECAAAwhMQLAACYw1ON9qmoqNCjjz6qpKQk1alTRxdeeKEmTpwot9vHG3wAAACcB2xNvKZMmaIXXnhBCxYsUOvWrfX+++/rvvvuU0xMjB588EE7SwMAAP5gSfJ1vhI4gZe9jdfmzZvVs2fPyi+abNWqlZYuXar333+/2uvLyspUVlZW+XNpaamROgEAgG+wuN5GHTt21DvvvKNdu3ZJkrZv3653331Xv/71r6u9Pjs7WzExMZVHQkKCyXIBAAB+FlsTr3Hjxunw4cO69NJLFR4eLpfLpSeffFJ9+vSp9vqsrCyNGTOm8ufS0lKaLwAAAoklPyyu9+1w/mRr45Wbm6tFixZpyZIlat26tT788EONGjVKzZs314ABA6pc73Q65XQ6bagUAADg57O18XrooYc0fvx43X333ZKkyy+/XHv37lV2dna1jRcAAAhwbCdhnx9//FFhYZ4lhIeHs50EAAAISrYmXj169NCTTz6pli1bqnXr1tq2bZumTZumQYMG2VkWAADwF7ckhx/GDBC2Nl7PPfec/vCHPygjI0MlJSVq3ry5hg4dqscee8zOsgAAAPzC1sYrKipK06dP1/Tp0+0sAwAAGBLq+3jxXY0AAMAcFtcDAADABBIvAABgDokXAAAATCDxAgAA5pB4AQAAwAQSLwAAYE6Ib6BK4gUAAGAIiRcAADCGDVQBAABMYXE9AAAATCDxAgAA5rgtyeHjhMpN4gUAAIDTkHgBAABzWOMFAAAAE0i8AACAQX5IvBQ4iVdQNF6rFvw/hTsj7S6jRsp/aXcF3rk4oszuErz2l++utLsEr1QE1h/tSu03Zdpdgtda1Q/MyQDn93ZX4J2ot3faXYLXPu2SbHcJNeI+dtzuEkJeUDReAAAgQIT4Gi8aLwAAYI7bks+nBtlOAgAAAKcj8QIAAOZY7pOHr8cMECReAAAAhpB4AQAAc0J8cT2JFwAAgCEkXgAAwByeagQAAIAJJF4AAMCcEF/jReMFAADMseSHxsu3w/kTU40AAACGkHgBAABzQnyqkcQLAADAEBIvAABgjtstycdf8ePmK4MAAABwGhIvAABgDmu8AAAAYAKJFwAAMCfEEy8aLwAAYA7f1QgAAAATSLwAAIAxluWWZfl2+wdfj+dPJF4AAACGkHgBAABzLMv3a7ICaHE9iRcAAIAhJF4AAMAcyw9PNZJ4AQAA4HQkXgAAwBy3W3L4+CnEAHqqkcYLAACYw1QjAAAATCDxAgAAxlhutywfTzWygSoAAACqIPECAADmsMYLAAAAJpB4AQAAc9yW5CDxAgAAgJ+ReAEAAHMsS5KvN1Al8QIAAMBpSLwAAIAxltuS5eM1XlYAJV40XgAAwBzLLd9PNbKBKgAAAE5D4gUAAIwJ9alGEi8AAABDSLwAAIA5Ib7GK6Abr1PRoqv8uM2V1Jwrwu4KvHPiaLndJXjteO0TdpfgFVdZ4P35liT3j4FZtyRVVATmZIArQP/xrLACtHBJ7mOB9efcffxkvXZOzVXohM+/qrFCgfPvd4cVSBOjp9m3b58SEhLsLgMAgIBSVFSkFi1aGL3n8ePHlZSUpOLiYr+M36xZM+3Zs0eRkZF+Gd9XArrxcrvd+vrrrxUVFSWHw+HTsUtLS5WQkKCioiJFR0f7dGxUj8/cLD5vs/i8zeMzr8qyLB05ckTNmzdXWJj5ZPf48eMqL/dPwhkREXHeN11SgE81hoWF+b1jj46O5h9Yw/jMzeLzNovP2zw+c08xMTG23TsyMjIgmiN/CsyFDAAAAAGIxgsAAMAQGq8zcDqdmjBhgpxOp92lhAw+c7P4vM3i8zaPzxzno4BeXA8AABBISLwAAAAMofECAAAwhMYLAADAEBovAAAAQ2i8ziAnJ0dJSUmKjIxUamqqNm3aZHdJQSk7O1tXX321oqKiFBsbq9tuu03/+c9/7C4rZGRnZ8vhcGjUqFF2lxLUvvrqK917771q3Lix6tatq5SUFBUUFNhdVlCqqKjQo48+qqSkJNWpU0cXXnihJk6cKLc7cL5EGcGNxqsaubm5GjVqlB555BFt27ZNnTp1UteuXVVYWGh3aUFnw4YNyszM1JYtW5SXl6eKigqlp6fr6NGjdpcW9PLz8zVr1ixdccUVdpcS1A4dOqQOHTqodu3a+tvf/qadO3fqT3/6kxo0aGB3aUFpypQpeuGFFzRjxgx98sknmjp1qp5++mk999xzdpcGSGI7iWpde+21uuqqqzRz5szKc8nJybrtttuUnZ1tY2XB79tvv1VsbKw2bNig66+/3u5ygtYPP/ygq666Sjk5OfrjH/+olJQUTZ8+3e6ygtL48eP1z3/+k9TckO7duysuLk5z5sypPHfHHXeobt26evnll22sDDiJxOs05eXlKigoUHp6usf59PR0vffeezZVFToOHz4sSWrUqJHNlQS3zMxMdevWTTfffLPdpQS91atXKy0tTXfddZdiY2PVtm1bvfTSS3aXFbQ6duyod955R7t27ZIkbd++Xe+++65+/etf21wZcFJAf0m2Pxw4cEAul0txcXEe5+Pi4lRcXGxTVaHBsiyNGTNGHTt2VJs2bewuJ2i98sor+uCDD5Sfn293KSFh9+7dmjlzpsaMGaPf//732rp1q0aOHCmn06n+/fvbXV7QGTdunA4fPqxLL71U4eHhcrlcevLJJ9WnTx+7SwMk0XidkcPh8PjZsqwq5+Bbw4cP144dO/Tuu+/aXUrQKioq0oMPPqi3335bkZGRdpcTEtxut9LS0jR58mRJUtu2bfXxxx9r5syZNF5+kJubq0WLFmnJkiVq3bq1PvzwQ40aNUrNmzfXgAED7C4PoPE6XZMmTRQeHl4l3SopKamSgsF3RowYodWrV2vjxo1q0aKF3eUErYKCApWUlCg1NbXynMvl0saNGzVjxgyVlZUpPDzcxgqDT3x8vC677DKPc8nJyVq+fLlNFQW3hx56SOPHj9fdd98tSbr88su1d+9eZWdn03jhvMAar9NEREQoNTVVeXl5Hufz8vLUvn17m6oKXpZlafjw4VqxYoXWrVunpKQku0sKajfddJM++ugjffjhh5VHWlqa+vbtqw8//JCmyw86dOhQZYuUXbt2KTEx0aaKgtuPP/6osDDP/7SFh4eznQTOGyRe1RgzZoz69euntLQ0tWvXTrNmzVJhYaGGDRtmd2lBJzMzU0uWLNGqVasUFRVVmTTGxMSoTp06NlcXfKKioqqsn6tXr54aN27Mujo/GT16tNq3b6/JkyerV69e2rp1q2bNmqVZs2bZXVpQ6tGjh5588km1bNlSrVu31rZt2zRt2jQNGjTI7tIASWwncUY5OTmaOnWq9u/frzZt2ujZZ59lewM/ONO6uXnz5mngwIFmiwlRnTt3ZjsJP3vjjTeUlZWlzz77TElJSRozZozuv/9+u8sKSkeOHNEf/vAHrVy5UiUlJWrevLn69Omjxx57TBEREXaXB9B4AQAAmMIaLwAAAENovAAAAAyh8QIAADCExgsAAMAQGi8AAABDaLwAAAAMofECAAAwhMYLAADAEBovALZzOBx67bXX7C4DAPyOxguAXC6X2rdvrzvuuMPj/OHDh5WQkKBHH33Ur/ffv3+/unbt6td7AMD5gK8MAiBJ+uyzz5SSkqJZs2apb9++kqT+/ftr+/btys/P53vuAMAHSLwASJIuueQSZWdna8SIEfr666+1atUqvfLKK1qwYMFZm65FixYpLS1NUVFRatasme655x6VlJRU/n7ixIlq3ry5Dh48WHnu1ltv1fXXXy+32y3Jc6qxvLxcw4cPV3x8vCIjI9WqVStlZ2f7500DgGEkXgAqWZalG2+8UeHh4froo480YsSIn5xmnDt3ruLj4/XLX/5SJSUlGj16tBo2bKg1a9ZIOjmN2alTJ8XFxWnlypV64YUXNH78eG3fvl2JiYmSTjZeK1eu1G233aZnnnlGf/nLX7R48WK1bNlSRUVFKioqUp8+ffz+/gHA32i8AHj49NNPlZycrMsvv1wffPCBatWqVaPX5+fn65prrtGRI0dUv359SdLu3buVkpKijIwMPffccx7TmZJn4zVy5Eh9/PHH+vvf/y6Hw+HT9wYAdmOqEYCHuXPnqm7dutqzZ4/27dv3k9dv27ZNPXv2VGJioqKiotS5c2dJUmFhYeU1F154oZ555hlNmTJFPXr08Gi6Tjdw4EB9+OGH+uUvf6mRI0fq7bff/tnvCQDOFzReACpt3rxZzz77rFatWqV27dpp8ODBOlsofvToUaWnp6t+/fpatGiR8vPztXLlSkkn12r9r40bNyo8PFxffvmlKioqzjjmVVddpT179mjSpEk6duyYevXqpTvvvNM3bxAAbEbjBUCSdOzYMQ0YMEBDhw7VzTffrNmzZys/P18vvvjiGV/z6aef6sCBA3rqqafUqVMnXXrppR4L60/Jzc3VihUrtH79ehUVFWnSpElnrSU6Olq9e/fWSy+9pNzcXC1fvlzffffdz36PAGA3Gi8AkqTx48fL7XZrypQpkqSWLVvqT3/6kx566CF9+eWX1b6mZcuWioiI0HPPPafdu3dr9erVVZqqffv26YEHHtCUKVPUsWNHzZ8/X9nZ2dqyZUu1Yz777LN65ZVX9Omnn2rXrl1atmyZmjVrpgYNGvjy7QKALWi8AGjDhg16/vnnNX/+fNWrV6/y/P3336/27dufccqxadOmmj9/vpYtW6bLLrtMTz31lJ555pnK31uWpYEDB+qaa67R8OHDJUldunTR8OHDde+99+qHH36oMmb9+vU1ZcoUpaWl6eqrr9aXX36pNWvWKCyMf10BCHw81QgAAGAI/wsJAABgCI0XAACAITReAAAAhtB4AQAAGELjBQAAYAiNFwAAgCE0XgAAAIbQeAEAABhC4wUAAGAIjRcAAIAhNF4AAACG/H9cueVffK2NggAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        # optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        # print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                outputs = outputs.sum(dim=0)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            outputs = outputs.sum(dim=0)\n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "\n",
    "\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"0\",\n",
    "#                 single_step = False, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 0.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = True,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = False, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[],\n",
    "#                 scale_exp=[[-10,-10],[-10,-10],[-9,-9]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jcrebimi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251103_171708-jcrebimi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jcrebimi' target=\"_blank\">balmy-sweep-15</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jcrebimi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jcrebimi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': False, 'unique_name': '20251103_171715_418', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 1e-05, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 1e-05\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch-0   lr=['0.0000100'], tr/val_loss:  1.808980/  1.608424, val:  35.83%, val_best:  35.83%, tr:  38.46%, tr_best:  38.46%, epoch time: 70.13 seconds, 1.17 minutes\n",
      "epoch-1   lr=['0.0000100'], tr/val_loss:  1.143965/  1.327648, val:  45.00%, val_best:  45.00%, tr:  55.23%, tr_best:  55.23%, epoch time: 68.99 seconds, 1.15 minutes\n",
      "epoch-2   lr=['0.0000100'], tr/val_loss:  0.986759/  1.218249, val:  51.25%, val_best:  51.25%, tr:  60.66%, tr_best:  60.66%, epoch time: 71.27 seconds, 1.19 minutes\n",
      "epoch-3   lr=['0.0000100'], tr/val_loss:  0.896185/  1.128671, val:  56.25%, val_best:  56.25%, tr:  63.17%, tr_best:  63.17%, epoch time: 71.59 seconds, 1.19 minutes\n",
      "epoch-4   lr=['0.0000100'], tr/val_loss:  0.833248/  1.066529, val:  59.17%, val_best:  59.17%, tr:  65.94%, tr_best:  65.94%, epoch time: 71.42 seconds, 1.19 minutes\n",
      "epoch-5   lr=['0.0000100'], tr/val_loss:  0.784797/  1.037368, val:  59.58%, val_best:  59.58%, tr:  67.47%, tr_best:  67.47%, epoch time: 71.65 seconds, 1.19 minutes\n",
      "epoch-6   lr=['0.0000100'], tr/val_loss:  0.742514/  1.022131, val:  57.92%, val_best:  59.58%, tr:  68.85%, tr_best:  68.85%, epoch time: 71.82 seconds, 1.20 minutes\n",
      "epoch-7   lr=['0.0000100'], tr/val_loss:  0.712993/  0.978507, val:  59.58%, val_best:  59.58%, tr:  70.51%, tr_best:  70.51%, epoch time: 71.54 seconds, 1.19 minutes\n",
      "epoch-8   lr=['0.0000100'], tr/val_loss:  0.681852/  0.958709, val:  62.08%, val_best:  62.08%, tr:  71.62%, tr_best:  71.62%, epoch time: 71.51 seconds, 1.19 minutes\n",
      "epoch-9   lr=['0.0000100'], tr/val_loss:  0.660346/  0.933829, val:  62.50%, val_best:  62.50%, tr:  72.16%, tr_best:  72.16%, epoch time: 71.51 seconds, 1.19 minutes\n",
      "epoch-10  lr=['0.0000100'], tr/val_loss:  0.636113/  0.930306, val:  60.42%, val_best:  62.50%, tr:  73.35%, tr_best:  73.35%, epoch time: 71.72 seconds, 1.20 minutes\n",
      "epoch-11  lr=['0.0000100'], tr/val_loss:  0.617702/  0.905087, val:  61.25%, val_best:  62.50%, tr:  75.14%, tr_best:  75.14%, epoch time: 71.82 seconds, 1.20 minutes\n",
      "epoch-12  lr=['0.0000100'], tr/val_loss:  0.603000/  0.884332, val:  60.42%, val_best:  62.50%, tr:  75.34%, tr_best:  75.34%, epoch time: 71.32 seconds, 1.19 minutes\n",
      "epoch-13  lr=['0.0000100'], tr/val_loss:  0.584697/  0.876390, val:  64.17%, val_best:  64.17%, tr:  76.01%, tr_best:  76.01%, epoch time: 71.75 seconds, 1.20 minutes\n",
      "epoch-14  lr=['0.0000100'], tr/val_loss:  0.568850/  0.861100, val:  63.75%, val_best:  64.17%, tr:  76.56%, tr_best:  76.56%, epoch time: 71.33 seconds, 1.19 minutes\n",
      "epoch-15  lr=['0.0000100'], tr/val_loss:  0.555970/  0.859128, val:  63.33%, val_best:  64.17%, tr:  77.41%, tr_best:  77.41%, epoch time: 71.54 seconds, 1.19 minutes\n",
      "epoch-16  lr=['0.0000100'], tr/val_loss:  0.541046/  0.838533, val:  67.92%, val_best:  67.92%, tr:  78.04%, tr_best:  78.04%, epoch time: 71.80 seconds, 1.20 minutes\n",
      "epoch-17  lr=['0.0000100'], tr/val_loss:  0.531622/  0.834081, val:  66.67%, val_best:  67.92%, tr:  78.63%, tr_best:  78.63%, epoch time: 72.00 seconds, 1.20 minutes\n",
      "epoch-18  lr=['0.0000100'], tr/val_loss:  0.521775/  0.838115, val:  65.83%, val_best:  67.92%, tr:  78.97%, tr_best:  78.97%, epoch time: 71.51 seconds, 1.19 minutes\n",
      "epoch-19  lr=['0.0000100'], tr/val_loss:  0.511788/  0.830594, val:  65.83%, val_best:  67.92%, tr:  79.67%, tr_best:  79.67%, epoch time: 71.86 seconds, 1.20 minutes\n",
      "epoch-20  lr=['0.0000100'], tr/val_loss:  0.501444/  0.824760, val:  65.42%, val_best:  67.92%, tr:  79.42%, tr_best:  79.67%, epoch time: 71.90 seconds, 1.20 minutes\n",
      "epoch-21  lr=['0.0000100'], tr/val_loss:  0.487966/  0.840663, val:  65.83%, val_best:  67.92%, tr:  80.93%, tr_best:  80.93%, epoch time: 71.43 seconds, 1.19 minutes\n",
      "epoch-22  lr=['0.0000100'], tr/val_loss:  0.480479/  0.790141, val:  67.50%, val_best:  67.92%, tr:  81.11%, tr_best:  81.11%, epoch time: 71.65 seconds, 1.19 minutes\n",
      "epoch-23  lr=['0.0000100'], tr/val_loss:  0.471816/  0.807797, val:  65.00%, val_best:  67.92%, tr:  81.33%, tr_best:  81.33%, epoch time: 71.97 seconds, 1.20 minutes\n",
      "epoch-24  lr=['0.0000100'], tr/val_loss:  0.461081/  0.796499, val:  65.42%, val_best:  67.92%, tr:  82.24%, tr_best:  82.24%, epoch time: 71.47 seconds, 1.19 minutes\n",
      "epoch-25  lr=['0.0000100'], tr/val_loss:  0.456470/  0.792858, val:  67.92%, val_best:  67.92%, tr:  82.10%, tr_best:  82.24%, epoch time: 71.77 seconds, 1.20 minutes\n",
      "epoch-26  lr=['0.0000100'], tr/val_loss:  0.446958/  0.800523, val:  65.00%, val_best:  67.92%, tr:  82.89%, tr_best:  82.89%, epoch time: 71.58 seconds, 1.19 minutes\n",
      "epoch-27  lr=['0.0000100'], tr/val_loss:  0.440896/  0.793527, val:  65.00%, val_best:  67.92%, tr:  82.75%, tr_best:  82.89%, epoch time: 71.42 seconds, 1.19 minutes\n",
      "epoch-28  lr=['0.0000100'], tr/val_loss:  0.435618/  0.789269, val:  67.92%, val_best:  67.92%, tr:  83.00%, tr_best:  83.00%, epoch time: 71.36 seconds, 1.19 minutes\n",
      "epoch-29  lr=['0.0000100'], tr/val_loss:  0.428358/  0.787927, val:  68.75%, val_best:  68.75%, tr:  83.09%, tr_best:  83.09%, epoch time: 71.73 seconds, 1.20 minutes\n",
      "epoch-30  lr=['0.0000100'], tr/val_loss:  0.418898/  0.790108, val:  70.00%, val_best:  70.00%, tr:  84.31%, tr_best:  84.31%, epoch time: 71.58 seconds, 1.19 minutes\n",
      "epoch-31  lr=['0.0000100'], tr/val_loss:  0.412852/  0.767995, val:  69.17%, val_best:  70.00%, tr:  84.29%, tr_best:  84.31%, epoch time: 71.54 seconds, 1.19 minutes\n",
      "epoch-32  lr=['0.0000100'], tr/val_loss:  0.407765/  0.773203, val:  70.00%, val_best:  70.00%, tr:  84.63%, tr_best:  84.63%, epoch time: 72.07 seconds, 1.20 minutes\n",
      "epoch-33  lr=['0.0000100'], tr/val_loss:  0.400523/  0.765565, val:  71.25%, val_best:  71.25%, tr:  85.01%, tr_best:  85.01%, epoch time: 71.91 seconds, 1.20 minutes\n",
      "epoch-34  lr=['0.0000100'], tr/val_loss:  0.393965/  0.765199, val:  71.67%, val_best:  71.67%, tr:  85.64%, tr_best:  85.64%, epoch time: 71.80 seconds, 1.20 minutes\n",
      "epoch-35  lr=['0.0000100'], tr/val_loss:  0.388257/  0.765009, val:  71.25%, val_best:  71.67%, tr:  85.69%, tr_best:  85.69%, epoch time: 71.79 seconds, 1.20 minutes\n",
      "epoch-36  lr=['0.0000100'], tr/val_loss:  0.383087/  0.777799, val:  69.58%, val_best:  71.67%, tr:  85.64%, tr_best:  85.69%, epoch time: 71.57 seconds, 1.19 minutes\n",
      "epoch-37  lr=['0.0000100'], tr/val_loss:  0.375345/  0.750491, val:  70.42%, val_best:  71.67%, tr:  86.09%, tr_best:  86.09%, epoch time: 70.90 seconds, 1.18 minutes\n",
      "epoch-38  lr=['0.0000100'], tr/val_loss:  0.372890/  0.767928, val:  71.25%, val_best:  71.67%, tr:  86.36%, tr_best:  86.36%, epoch time: 71.65 seconds, 1.19 minutes\n",
      "epoch-39  lr=['0.0000100'], tr/val_loss:  0.367013/  0.777051, val:  71.67%, val_best:  71.67%, tr:  86.34%, tr_best:  86.36%, epoch time: 71.40 seconds, 1.19 minutes\n",
      "epoch-40  lr=['0.0000100'], tr/val_loss:  0.361960/  0.771240, val:  70.83%, val_best:  71.67%, tr:  86.90%, tr_best:  86.90%, epoch time: 70.75 seconds, 1.18 minutes\n",
      "epoch-41  lr=['0.0000100'], tr/val_loss:  0.354560/  0.759866, val:  73.33%, val_best:  73.33%, tr:  87.20%, tr_best:  87.20%, epoch time: 71.35 seconds, 1.19 minutes\n",
      "epoch-42  lr=['0.0000100'], tr/val_loss:  0.349930/  0.767414, val:  73.33%, val_best:  73.33%, tr:  86.95%, tr_best:  87.20%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "epoch-43  lr=['0.0000100'], tr/val_loss:  0.348002/  0.769996, val:  71.67%, val_best:  73.33%, tr:  87.35%, tr_best:  87.35%, epoch time: 71.67 seconds, 1.19 minutes\n",
      "epoch-44  lr=['0.0000100'], tr/val_loss:  0.339729/  0.761910, val:  73.33%, val_best:  73.33%, tr:  87.78%, tr_best:  87.78%, epoch time: 71.83 seconds, 1.20 minutes\n",
      "epoch-45  lr=['0.0000100'], tr/val_loss:  0.336161/  0.750887, val:  72.08%, val_best:  73.33%, tr:  88.19%, tr_best:  88.19%, epoch time: 71.50 seconds, 1.19 minutes\n",
      "epoch-46  lr=['0.0000100'], tr/val_loss:  0.330963/  0.765678, val:  72.08%, val_best:  73.33%, tr:  88.01%, tr_best:  88.19%, epoch time: 71.01 seconds, 1.18 minutes\n",
      "epoch-47  lr=['0.0000100'], tr/val_loss:  0.323942/  0.755181, val:  73.33%, val_best:  73.33%, tr:  88.53%, tr_best:  88.53%, epoch time: 71.89 seconds, 1.20 minutes\n",
      "epoch-48  lr=['0.0000100'], tr/val_loss:  0.321490/  0.755437, val:  73.33%, val_best:  73.33%, tr:  89.09%, tr_best:  89.09%, epoch time: 71.53 seconds, 1.19 minutes\n",
      "epoch-49  lr=['0.0000100'], tr/val_loss:  0.317751/  0.756470, val:  72.50%, val_best:  73.33%, tr:  88.89%, tr_best:  89.09%, epoch time: 71.56 seconds, 1.19 minutes\n",
      "epoch-50  lr=['0.0000100'], tr/val_loss:  0.314461/  0.747311, val:  72.50%, val_best:  73.33%, tr:  89.36%, tr_best:  89.36%, epoch time: 71.48 seconds, 1.19 minutes\n",
      "epoch-51  lr=['0.0000100'], tr/val_loss:  0.310079/  0.750418, val:  70.00%, val_best:  73.33%, tr:  89.54%, tr_best:  89.54%, epoch time: 71.61 seconds, 1.19 minutes\n",
      "epoch-52  lr=['0.0000100'], tr/val_loss:  0.304876/  0.755786, val:  71.67%, val_best:  73.33%, tr:  89.38%, tr_best:  89.54%, epoch time: 71.19 seconds, 1.19 minutes\n",
      "epoch-53  lr=['0.0000100'], tr/val_loss:  0.300824/  0.729988, val:  70.83%, val_best:  73.33%, tr:  90.01%, tr_best:  90.01%, epoch time: 71.53 seconds, 1.19 minutes\n",
      "epoch-54  lr=['0.0000100'], tr/val_loss:  0.300663/  0.745037, val:  72.08%, val_best:  73.33%, tr:  89.86%, tr_best:  90.01%, epoch time: 71.39 seconds, 1.19 minutes\n",
      "epoch-55  lr=['0.0000100'], tr/val_loss:  0.297586/  0.729469, val:  74.17%, val_best:  74.17%, tr:  89.92%, tr_best:  90.01%, epoch time: 71.28 seconds, 1.19 minutes\n",
      "epoch-56  lr=['0.0000100'], tr/val_loss:  0.296158/  0.745069, val:  73.33%, val_best:  74.17%, tr:  89.54%, tr_best:  90.01%, epoch time: 71.67 seconds, 1.19 minutes\n",
      "epoch-57  lr=['0.0000100'], tr/val_loss:  0.291223/  0.768739, val:  68.75%, val_best:  74.17%, tr:  90.33%, tr_best:  90.33%, epoch time: 71.46 seconds, 1.19 minutes\n",
      "epoch-58  lr=['0.0000100'], tr/val_loss:  0.287057/  0.761981, val:  72.50%, val_best:  74.17%, tr:  90.42%, tr_best:  90.42%, epoch time: 71.39 seconds, 1.19 minutes\n",
      "epoch-59  lr=['0.0000100'], tr/val_loss:  0.283354/  0.742209, val:  74.17%, val_best:  74.17%, tr:  90.73%, tr_best:  90.73%, epoch time: 71.77 seconds, 1.20 minutes\n",
      "epoch-60  lr=['0.0000100'], tr/val_loss:  0.280991/  0.766423, val:  70.83%, val_best:  74.17%, tr:  90.62%, tr_best:  90.73%, epoch time: 71.83 seconds, 1.20 minutes\n",
      "epoch-61  lr=['0.0000100'], tr/val_loss:  0.278712/  0.745541, val:  74.58%, val_best:  74.58%, tr:  91.21%, tr_best:  91.21%, epoch time: 71.65 seconds, 1.19 minutes\n",
      "epoch-62  lr=['0.0000100'], tr/val_loss:  0.274911/  0.743409, val:  75.83%, val_best:  75.83%, tr:  90.64%, tr_best:  91.21%, epoch time: 71.93 seconds, 1.20 minutes\n",
      "epoch-63  lr=['0.0000100'], tr/val_loss:  0.270908/  0.729774, val:  74.17%, val_best:  75.83%, tr:  91.23%, tr_best:  91.23%, epoch time: 71.59 seconds, 1.19 minutes\n",
      "epoch-64  lr=['0.0000100'], tr/val_loss:  0.268501/  0.733554, val:  73.75%, val_best:  75.83%, tr:  91.37%, tr_best:  91.37%, epoch time: 71.22 seconds, 1.19 minutes\n",
      "epoch-65  lr=['0.0000100'], tr/val_loss:  0.267468/  0.757074, val:  70.00%, val_best:  75.83%, tr:  91.41%, tr_best:  91.41%, epoch time: 71.34 seconds, 1.19 minutes\n",
      "epoch-66  lr=['0.0000100'], tr/val_loss:  0.264302/  0.735772, val:  72.92%, val_best:  75.83%, tr:  91.64%, tr_best:  91.64%, epoch time: 71.50 seconds, 1.19 minutes\n",
      "epoch-67  lr=['0.0000100'], tr/val_loss:  0.260688/  0.747095, val:  72.92%, val_best:  75.83%, tr:  91.52%, tr_best:  91.64%, epoch time: 71.29 seconds, 1.19 minutes\n",
      "epoch-68  lr=['0.0000100'], tr/val_loss:  0.257884/  0.746082, val:  74.58%, val_best:  75.83%, tr:  91.95%, tr_best:  91.95%, epoch time: 71.89 seconds, 1.20 minutes\n",
      "epoch-69  lr=['0.0000100'], tr/val_loss:  0.254353/  0.718020, val:  75.00%, val_best:  75.83%, tr:  92.04%, tr_best:  92.04%, epoch time: 71.33 seconds, 1.19 minutes\n",
      "epoch-70  lr=['0.0000100'], tr/val_loss:  0.252559/  0.722024, val:  75.00%, val_best:  75.83%, tr:  92.29%, tr_best:  92.29%, epoch time: 71.39 seconds, 1.19 minutes\n",
      "epoch-71  lr=['0.0000100'], tr/val_loss:  0.250064/  0.722638, val:  75.42%, val_best:  75.83%, tr:  91.93%, tr_best:  92.29%, epoch time: 71.61 seconds, 1.19 minutes\n",
      "epoch-72  lr=['0.0000100'], tr/val_loss:  0.247785/  0.726330, val:  73.33%, val_best:  75.83%, tr:  92.13%, tr_best:  92.29%, epoch time: 71.18 seconds, 1.19 minutes\n",
      "epoch-73  lr=['0.0000100'], tr/val_loss:  0.243769/  0.721481, val:  77.92%, val_best:  77.92%, tr:  92.20%, tr_best:  92.29%, epoch time: 70.06 seconds, 1.17 minutes\n",
      "epoch-74  lr=['0.0000100'], tr/val_loss:  0.242633/  0.722452, val:  75.00%, val_best:  77.92%, tr:  92.54%, tr_best:  92.54%, epoch time: 71.83 seconds, 1.20 minutes\n",
      "epoch-75  lr=['0.0000100'], tr/val_loss:  0.238456/  0.728772, val:  73.75%, val_best:  77.92%, tr:  92.97%, tr_best:  92.97%, epoch time: 71.14 seconds, 1.19 minutes\n",
      "epoch-76  lr=['0.0000100'], tr/val_loss:  0.235436/  0.731639, val:  74.17%, val_best:  77.92%, tr:  92.76%, tr_best:  92.97%, epoch time: 71.49 seconds, 1.19 minutes\n",
      "epoch-77  lr=['0.0000100'], tr/val_loss:  0.234677/  0.709482, val:  75.83%, val_best:  77.92%, tr:  92.90%, tr_best:  92.97%, epoch time: 71.67 seconds, 1.19 minutes\n",
      "epoch-78  lr=['0.0000100'], tr/val_loss:  0.231680/  0.727947, val:  72.50%, val_best:  77.92%, tr:  92.74%, tr_best:  92.97%, epoch time: 71.32 seconds, 1.19 minutes\n",
      "epoch-79  lr=['0.0000100'], tr/val_loss:  0.227569/  0.718852, val:  75.42%, val_best:  77.92%, tr:  93.39%, tr_best:  93.39%, epoch time: 71.32 seconds, 1.19 minutes\n",
      "epoch-80  lr=['0.0000100'], tr/val_loss:  0.224133/  0.710771, val:  75.83%, val_best:  77.92%, tr:  93.44%, tr_best:  93.44%, epoch time: 71.34 seconds, 1.19 minutes\n",
      "epoch-81  lr=['0.0000100'], tr/val_loss:  0.222041/  0.710939, val:  75.83%, val_best:  77.92%, tr:  93.46%, tr_best:  93.46%, epoch time: 71.78 seconds, 1.20 minutes\n",
      "epoch-82  lr=['0.0000100'], tr/val_loss:  0.217893/  0.707024, val:  75.42%, val_best:  77.92%, tr:  93.67%, tr_best:  93.67%, epoch time: 71.40 seconds, 1.19 minutes\n",
      "epoch-83  lr=['0.0000100'], tr/val_loss:  0.215840/  0.710267, val:  77.08%, val_best:  77.92%, tr:  93.80%, tr_best:  93.80%, epoch time: 71.27 seconds, 1.19 minutes\n",
      "epoch-84  lr=['0.0000100'], tr/val_loss:  0.216234/  0.713475, val:  77.92%, val_best:  77.92%, tr:  93.67%, tr_best:  93.80%, epoch time: 71.85 seconds, 1.20 minutes\n",
      "epoch-85  lr=['0.0000100'], tr/val_loss:  0.213002/  0.712793, val:  77.50%, val_best:  77.92%, tr:  93.89%, tr_best:  93.89%, epoch time: 71.16 seconds, 1.19 minutes\n",
      "epoch-86  lr=['0.0000100'], tr/val_loss:  0.210675/  0.698912, val:  78.33%, val_best:  78.33%, tr:  93.87%, tr_best:  93.89%, epoch time: 71.18 seconds, 1.19 minutes\n",
      "epoch-87  lr=['0.0000100'], tr/val_loss:  0.209917/  0.712600, val:  77.92%, val_best:  78.33%, tr:  94.05%, tr_best:  94.05%, epoch time: 71.19 seconds, 1.19 minutes\n",
      "epoch-88  lr=['0.0000100'], tr/val_loss:  0.207463/  0.719057, val:  79.58%, val_best:  79.58%, tr:  94.12%, tr_best:  94.12%, epoch time: 71.67 seconds, 1.19 minutes\n",
      "epoch-89  lr=['0.0000100'], tr/val_loss:  0.204362/  0.728143, val:  77.92%, val_best:  79.58%, tr:  94.09%, tr_best:  94.12%, epoch time: 71.79 seconds, 1.20 minutes\n",
      "epoch-90  lr=['0.0000100'], tr/val_loss:  0.200861/  0.731979, val:  76.67%, val_best:  79.58%, tr:  94.25%, tr_best:  94.25%, epoch time: 71.20 seconds, 1.19 minutes\n",
      "epoch-91  lr=['0.0000100'], tr/val_loss:  0.200829/  0.719564, val:  78.75%, val_best:  79.58%, tr:  94.43%, tr_best:  94.43%, epoch time: 71.69 seconds, 1.19 minutes\n",
      "epoch-92  lr=['0.0000100'], tr/val_loss:  0.197632/  0.731450, val:  77.50%, val_best:  79.58%, tr:  94.07%, tr_best:  94.43%, epoch time: 71.15 seconds, 1.19 minutes\n",
      "epoch-93  lr=['0.0000100'], tr/val_loss:  0.195154/  0.731497, val:  76.67%, val_best:  79.58%, tr:  94.63%, tr_best:  94.63%, epoch time: 70.88 seconds, 1.18 minutes\n",
      "epoch-94  lr=['0.0000100'], tr/val_loss:  0.192030/  0.734288, val:  77.50%, val_best:  79.58%, tr:  94.66%, tr_best:  94.66%, epoch time: 71.67 seconds, 1.19 minutes\n",
      "epoch-95  lr=['0.0000100'], tr/val_loss:  0.191033/  0.733227, val:  75.83%, val_best:  79.58%, tr:  94.45%, tr_best:  94.66%, epoch time: 71.88 seconds, 1.20 minutes\n",
      "epoch-96  lr=['0.0000100'], tr/val_loss:  0.188240/  0.734724, val:  74.58%, val_best:  79.58%, tr:  94.75%, tr_best:  94.75%, epoch time: 71.78 seconds, 1.20 minutes\n",
      "epoch-97  lr=['0.0000100'], tr/val_loss:  0.187755/  0.744192, val:  75.00%, val_best:  79.58%, tr:  94.79%, tr_best:  94.79%, epoch time: 71.49 seconds, 1.19 minutes\n",
      "epoch-98  lr=['0.0000100'], tr/val_loss:  0.183362/  0.723449, val:  75.83%, val_best:  79.58%, tr:  95.36%, tr_best:  95.36%, epoch time: 71.90 seconds, 1.20 minutes\n",
      "epoch-99  lr=['0.0000100'], tr/val_loss:  0.181906/  0.718577, val:  76.67%, val_best:  79.58%, tr:  95.09%, tr_best:  95.36%, epoch time: 71.70 seconds, 1.20 minutes\n",
      "epoch-100 lr=['0.0000100'], tr/val_loss:  0.182513/  0.708337, val:  77.08%, val_best:  79.58%, tr:  94.97%, tr_best:  95.36%, epoch time: 71.29 seconds, 1.19 minutes\n",
      "epoch-101 lr=['0.0000100'], tr/val_loss:  0.179058/  0.720611, val:  75.83%, val_best:  79.58%, tr:  95.29%, tr_best:  95.36%, epoch time: 71.54 seconds, 1.19 minutes\n",
      "epoch-102 lr=['0.0000100'], tr/val_loss:  0.175499/  0.732186, val:  76.25%, val_best:  79.58%, tr:  95.63%, tr_best:  95.63%, epoch time: 70.96 seconds, 1.18 minutes\n",
      "epoch-103 lr=['0.0000100'], tr/val_loss:  0.173304/  0.702972, val:  78.75%, val_best:  79.58%, tr:  95.60%, tr_best:  95.63%, epoch time: 70.69 seconds, 1.18 minutes\n",
      "epoch-104 lr=['0.0000100'], tr/val_loss:  0.169665/  0.712943, val:  77.08%, val_best:  79.58%, tr:  95.76%, tr_best:  95.76%, epoch time: 71.07 seconds, 1.18 minutes\n",
      "epoch-105 lr=['0.0000100'], tr/val_loss:  0.171507/  0.702818, val:  79.17%, val_best:  79.58%, tr:  95.72%, tr_best:  95.76%, epoch time: 70.77 seconds, 1.18 minutes\n",
      "epoch-106 lr=['0.0000100'], tr/val_loss:  0.167959/  0.710456, val:  78.75%, val_best:  79.58%, tr:  95.72%, tr_best:  95.76%, epoch time: 71.39 seconds, 1.19 minutes\n",
      "epoch-107 lr=['0.0000100'], tr/val_loss:  0.167983/  0.699702, val:  79.17%, val_best:  79.58%, tr:  95.90%, tr_best:  95.90%, epoch time: 71.32 seconds, 1.19 minutes\n",
      "epoch-108 lr=['0.0000100'], tr/val_loss:  0.165140/  0.704091, val:  78.75%, val_best:  79.58%, tr:  95.65%, tr_best:  95.90%, epoch time: 71.20 seconds, 1.19 minutes\n",
      "epoch-109 lr=['0.0000100'], tr/val_loss:  0.164693/  0.696186, val:  79.58%, val_best:  79.58%, tr:  95.58%, tr_best:  95.90%, epoch time: 71.09 seconds, 1.18 minutes\n",
      "epoch-110 lr=['0.0000100'], tr/val_loss:  0.162799/  0.695069, val:  78.33%, val_best:  79.58%, tr:  96.06%, tr_best:  96.06%, epoch time: 70.94 seconds, 1.18 minutes\n",
      "epoch-111 lr=['0.0000100'], tr/val_loss:  0.160679/  0.697292, val:  79.17%, val_best:  79.58%, tr:  95.96%, tr_best:  96.06%, epoch time: 71.03 seconds, 1.18 minutes\n",
      "epoch-112 lr=['0.0000100'], tr/val_loss:  0.159716/  0.691205, val:  79.58%, val_best:  79.58%, tr:  96.08%, tr_best:  96.08%, epoch time: 71.09 seconds, 1.18 minutes\n",
      "epoch-113 lr=['0.0000100'], tr/val_loss:  0.157522/  0.710598, val:  77.50%, val_best:  79.58%, tr:  95.92%, tr_best:  96.08%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-114 lr=['0.0000100'], tr/val_loss:  0.157395/  0.702980, val:  78.33%, val_best:  79.58%, tr:  95.90%, tr_best:  96.08%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "epoch-115 lr=['0.0000100'], tr/val_loss:  0.155360/  0.711214, val:  77.50%, val_best:  79.58%, tr:  96.24%, tr_best:  96.24%, epoch time: 70.74 seconds, 1.18 minutes\n",
      "epoch-116 lr=['0.0000100'], tr/val_loss:  0.152696/  0.709993, val:  78.33%, val_best:  79.58%, tr:  96.08%, tr_best:  96.24%, epoch time: 71.25 seconds, 1.19 minutes\n",
      "epoch-117 lr=['0.0000100'], tr/val_loss:  0.152141/  0.702888, val:  77.92%, val_best:  79.58%, tr:  96.39%, tr_best:  96.39%, epoch time: 71.96 seconds, 1.20 minutes\n",
      "epoch-118 lr=['0.0000100'], tr/val_loss:  0.148994/  0.701546, val:  78.75%, val_best:  79.58%, tr:  96.35%, tr_best:  96.39%, epoch time: 71.93 seconds, 1.20 minutes\n",
      "epoch-119 lr=['0.0000100'], tr/val_loss:  0.149867/  0.693956, val:  78.75%, val_best:  79.58%, tr:  96.17%, tr_best:  96.39%, epoch time: 71.53 seconds, 1.19 minutes\n",
      "epoch-120 lr=['0.0000100'], tr/val_loss:  0.147805/  0.698275, val:  79.58%, val_best:  79.58%, tr:  96.60%, tr_best:  96.60%, epoch time: 70.75 seconds, 1.18 minutes\n",
      "epoch-121 lr=['0.0000100'], tr/val_loss:  0.146738/  0.701904, val:  78.75%, val_best:  79.58%, tr:  96.78%, tr_best:  96.78%, epoch time: 71.94 seconds, 1.20 minutes\n",
      "epoch-122 lr=['0.0000100'], tr/val_loss:  0.146344/  0.699902, val:  78.75%, val_best:  79.58%, tr:  96.19%, tr_best:  96.78%, epoch time: 71.34 seconds, 1.19 minutes\n",
      "epoch-123 lr=['0.0000100'], tr/val_loss:  0.144651/  0.706607, val:  79.58%, val_best:  79.58%, tr:  96.37%, tr_best:  96.78%, epoch time: 70.91 seconds, 1.18 minutes\n",
      "epoch-124 lr=['0.0000100'], tr/val_loss:  0.142588/  0.706083, val:  79.58%, val_best:  79.58%, tr:  96.44%, tr_best:  96.78%, epoch time: 71.56 seconds, 1.19 minutes\n",
      "epoch-125 lr=['0.0000100'], tr/val_loss:  0.142942/  0.699459, val:  80.42%, val_best:  80.42%, tr:  96.73%, tr_best:  96.78%, epoch time: 71.29 seconds, 1.19 minutes\n",
      "epoch-126 lr=['0.0000100'], tr/val_loss:  0.138417/  0.716697, val:  78.75%, val_best:  80.42%, tr:  96.60%, tr_best:  96.78%, epoch time: 70.76 seconds, 1.18 minutes\n",
      "epoch-127 lr=['0.0000100'], tr/val_loss:  0.138900/  0.714925, val:  80.00%, val_best:  80.42%, tr:  96.73%, tr_best:  96.78%, epoch time: 70.69 seconds, 1.18 minutes\n",
      "epoch-128 lr=['0.0000100'], tr/val_loss:  0.138448/  0.725545, val:  78.33%, val_best:  80.42%, tr:  96.51%, tr_best:  96.78%, epoch time: 71.69 seconds, 1.19 minutes\n",
      "epoch-129 lr=['0.0000100'], tr/val_loss:  0.135282/  0.725057, val:  78.75%, val_best:  80.42%, tr:  96.87%, tr_best:  96.87%, epoch time: 69.79 seconds, 1.16 minutes\n",
      "epoch-130 lr=['0.0000100'], tr/val_loss:  0.134385/  0.743923, val:  78.33%, val_best:  80.42%, tr:  96.60%, tr_best:  96.87%, epoch time: 69.72 seconds, 1.16 minutes\n",
      "epoch-131 lr=['0.0000100'], tr/val_loss:  0.134512/  0.727277, val:  79.17%, val_best:  80.42%, tr:  96.87%, tr_best:  96.87%, epoch time: 70.62 seconds, 1.18 minutes\n",
      "epoch-132 lr=['0.0000100'], tr/val_loss:  0.131858/  0.728256, val:  79.58%, val_best:  80.42%, tr:  97.20%, tr_best:  97.20%, epoch time: 71.91 seconds, 1.20 minutes\n",
      "epoch-133 lr=['0.0000100'], tr/val_loss:  0.130284/  0.726031, val:  77.92%, val_best:  80.42%, tr:  96.91%, tr_best:  97.20%, epoch time: 70.93 seconds, 1.18 minutes\n",
      "epoch-134 lr=['0.0000100'], tr/val_loss:  0.128601/  0.721216, val:  79.17%, val_best:  80.42%, tr:  97.27%, tr_best:  97.27%, epoch time: 71.52 seconds, 1.19 minutes\n",
      "epoch-135 lr=['0.0000100'], tr/val_loss:  0.126743/  0.730345, val:  77.92%, val_best:  80.42%, tr:  97.18%, tr_best:  97.27%, epoch time: 71.38 seconds, 1.19 minutes\n",
      "epoch-136 lr=['0.0000100'], tr/val_loss:  0.125029/  0.718037, val:  79.17%, val_best:  80.42%, tr:  97.07%, tr_best:  97.27%, epoch time: 71.38 seconds, 1.19 minutes\n",
      "epoch-137 lr=['0.0000100'], tr/val_loss:  0.124458/  0.732504, val:  78.33%, val_best:  80.42%, tr:  97.07%, tr_best:  97.27%, epoch time: 71.41 seconds, 1.19 minutes\n",
      "epoch-138 lr=['0.0000100'], tr/val_loss:  0.122321/  0.718549, val:  77.92%, val_best:  80.42%, tr:  97.41%, tr_best:  97.41%, epoch time: 71.94 seconds, 1.20 minutes\n",
      "epoch-139 lr=['0.0000100'], tr/val_loss:  0.121045/  0.738003, val:  78.75%, val_best:  80.42%, tr:  97.41%, tr_best:  97.41%, epoch time: 71.75 seconds, 1.20 minutes\n",
      "epoch-140 lr=['0.0000100'], tr/val_loss:  0.121132/  0.739376, val:  79.17%, val_best:  80.42%, tr:  97.43%, tr_best:  97.43%, epoch time: 71.53 seconds, 1.19 minutes\n",
      "epoch-141 lr=['0.0000100'], tr/val_loss:  0.119792/  0.724468, val:  79.17%, val_best:  80.42%, tr:  97.52%, tr_best:  97.52%, epoch time: 71.34 seconds, 1.19 minutes\n",
      "epoch-142 lr=['0.0000100'], tr/val_loss:  0.118882/  0.729684, val:  80.00%, val_best:  80.42%, tr:  97.39%, tr_best:  97.52%, epoch time: 71.37 seconds, 1.19 minutes\n",
      "epoch-143 lr=['0.0000100'], tr/val_loss:  0.116852/  0.735224, val:  77.92%, val_best:  80.42%, tr:  97.54%, tr_best:  97.54%, epoch time: 71.90 seconds, 1.20 minutes\n",
      "epoch-144 lr=['0.0000100'], tr/val_loss:  0.115789/  0.720056, val:  80.42%, val_best:  80.42%, tr:  97.61%, tr_best:  97.61%, epoch time: 71.77 seconds, 1.20 minutes\n",
      "epoch-145 lr=['0.0000100'], tr/val_loss:  0.115122/  0.726449, val:  79.58%, val_best:  80.42%, tr:  97.61%, tr_best:  97.61%, epoch time: 71.53 seconds, 1.19 minutes\n",
      "epoch-146 lr=['0.0000100'], tr/val_loss:  0.114634/  0.735187, val:  79.58%, val_best:  80.42%, tr:  97.43%, tr_best:  97.61%, epoch time: 70.99 seconds, 1.18 minutes\n",
      "epoch-147 lr=['0.0000100'], tr/val_loss:  0.113190/  0.734833, val:  79.58%, val_best:  80.42%, tr:  97.75%, tr_best:  97.75%, epoch time: 71.71 seconds, 1.20 minutes\n",
      "epoch-148 lr=['0.0000100'], tr/val_loss:  0.111858/  0.734341, val:  79.58%, val_best:  80.42%, tr:  97.79%, tr_best:  97.79%, epoch time: 71.40 seconds, 1.19 minutes\n",
      "epoch-149 lr=['0.0000100'], tr/val_loss:  0.112374/  0.732033, val:  78.33%, val_best:  80.42%, tr:  97.68%, tr_best:  97.79%, epoch time: 70.22 seconds, 1.17 minutes\n",
      "epoch-150 lr=['0.0000100'], tr/val_loss:  0.109858/  0.730819, val:  79.58%, val_best:  80.42%, tr:  97.75%, tr_best:  97.79%, epoch time: 71.52 seconds, 1.19 minutes\n",
      "epoch-151 lr=['0.0000100'], tr/val_loss:  0.109849/  0.743102, val:  78.75%, val_best:  80.42%, tr:  97.59%, tr_best:  97.79%, epoch time: 72.03 seconds, 1.20 minutes\n",
      "epoch-152 lr=['0.0000100'], tr/val_loss:  0.110014/  0.735113, val:  78.33%, val_best:  80.42%, tr:  97.75%, tr_best:  97.79%, epoch time: 71.40 seconds, 1.19 minutes\n",
      "epoch-153 lr=['0.0000100'], tr/val_loss:  0.107341/  0.738612, val:  80.00%, val_best:  80.42%, tr:  97.79%, tr_best:  97.79%, epoch time: 71.14 seconds, 1.19 minutes\n",
      "epoch-154 lr=['0.0000100'], tr/val_loss:  0.106277/  0.740079, val:  80.00%, val_best:  80.42%, tr:  97.97%, tr_best:  97.97%, epoch time: 71.39 seconds, 1.19 minutes\n",
      "epoch-155 lr=['0.0000100'], tr/val_loss:  0.106110/  0.734666, val:  78.75%, val_best:  80.42%, tr:  97.86%, tr_best:  97.97%, epoch time: 71.72 seconds, 1.20 minutes\n",
      "epoch-156 lr=['0.0000100'], tr/val_loss:  0.103698/  0.740831, val:  78.75%, val_best:  80.42%, tr:  98.08%, tr_best:  98.08%, epoch time: 71.27 seconds, 1.19 minutes\n",
      "epoch-157 lr=['0.0000100'], tr/val_loss:  0.103086/  0.749577, val:  78.75%, val_best:  80.42%, tr:  97.95%, tr_best:  98.08%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-158 lr=['0.0000100'], tr/val_loss:  0.102238/  0.748815, val:  77.92%, val_best:  80.42%, tr:  97.97%, tr_best:  98.08%, epoch time: 71.43 seconds, 1.19 minutes\n",
      "epoch-159 lr=['0.0000100'], tr/val_loss:  0.101870/  0.759093, val:  77.92%, val_best:  80.42%, tr:  97.99%, tr_best:  98.08%, epoch time: 71.45 seconds, 1.19 minutes\n",
      "epoch-160 lr=['0.0000100'], tr/val_loss:  0.101312/  0.749893, val:  77.50%, val_best:  80.42%, tr:  98.04%, tr_best:  98.08%, epoch time: 71.68 seconds, 1.19 minutes\n",
      "epoch-161 lr=['0.0000100'], tr/val_loss:  0.098969/  0.742757, val:  79.17%, val_best:  80.42%, tr:  97.90%, tr_best:  98.08%, epoch time: 71.70 seconds, 1.19 minutes\n",
      "epoch-162 lr=['0.0000100'], tr/val_loss:  0.098435/  0.749308, val:  77.92%, val_best:  80.42%, tr:  97.97%, tr_best:  98.08%, epoch time: 71.31 seconds, 1.19 minutes\n",
      "epoch-163 lr=['0.0000100'], tr/val_loss:  0.097809/  0.751150, val:  78.33%, val_best:  80.42%, tr:  98.04%, tr_best:  98.08%, epoch time: 71.01 seconds, 1.18 minutes\n",
      "epoch-164 lr=['0.0000100'], tr/val_loss:  0.097107/  0.766211, val:  77.50%, val_best:  80.42%, tr:  98.22%, tr_best:  98.22%, epoch time: 72.00 seconds, 1.20 minutes\n",
      "epoch-165 lr=['0.0000100'], tr/val_loss:  0.096348/  0.748091, val:  78.33%, val_best:  80.42%, tr:  97.97%, tr_best:  98.22%, epoch time: 71.96 seconds, 1.20 minutes\n",
      "epoch-166 lr=['0.0000100'], tr/val_loss:  0.095702/  0.758833, val:  79.58%, val_best:  80.42%, tr:  98.33%, tr_best:  98.33%, epoch time: 71.84 seconds, 1.20 minutes\n",
      "epoch-167 lr=['0.0000100'], tr/val_loss:  0.094692/  0.776568, val:  79.17%, val_best:  80.42%, tr:  98.38%, tr_best:  98.38%, epoch time: 71.02 seconds, 1.18 minutes\n",
      "epoch-168 lr=['0.0000100'], tr/val_loss:  0.093256/  0.773465, val:  79.17%, val_best:  80.42%, tr:  98.40%, tr_best:  98.40%, epoch time: 71.24 seconds, 1.19 minutes\n",
      "epoch-169 lr=['0.0000100'], tr/val_loss:  0.093561/  0.753205, val:  79.58%, val_best:  80.42%, tr:  98.62%, tr_best:  98.62%, epoch time: 71.24 seconds, 1.19 minutes\n",
      "epoch-170 lr=['0.0000100'], tr/val_loss:  0.090694/  0.763881, val:  79.17%, val_best:  80.42%, tr:  98.51%, tr_best:  98.62%, epoch time: 71.34 seconds, 1.19 minutes\n",
      "epoch-171 lr=['0.0000100'], tr/val_loss:  0.091134/  0.772298, val:  78.33%, val_best:  80.42%, tr:  98.51%, tr_best:  98.62%, epoch time: 71.05 seconds, 1.18 minutes\n",
      "epoch-172 lr=['0.0000100'], tr/val_loss:  0.089745/  0.752959, val:  78.75%, val_best:  80.42%, tr:  98.65%, tr_best:  98.65%, epoch time: 71.19 seconds, 1.19 minutes\n",
      "epoch-173 lr=['0.0000100'], tr/val_loss:  0.089408/  0.768646, val:  78.75%, val_best:  80.42%, tr:  98.51%, tr_best:  98.65%, epoch time: 71.42 seconds, 1.19 minutes\n",
      "epoch-174 lr=['0.0000100'], tr/val_loss:  0.088206/  0.764772, val:  80.00%, val_best:  80.42%, tr:  98.60%, tr_best:  98.65%, epoch time: 71.51 seconds, 1.19 minutes\n",
      "epoch-175 lr=['0.0000100'], tr/val_loss:  0.088545/  0.767848, val:  78.75%, val_best:  80.42%, tr:  98.53%, tr_best:  98.65%, epoch time: 71.60 seconds, 1.19 minutes\n",
      "epoch-176 lr=['0.0000100'], tr/val_loss:  0.087155/  0.767990, val:  77.92%, val_best:  80.42%, tr:  98.56%, tr_best:  98.65%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-177 lr=['0.0000100'], tr/val_loss:  0.086764/  0.764931, val:  79.58%, val_best:  80.42%, tr:  98.67%, tr_best:  98.67%, epoch time: 71.65 seconds, 1.19 minutes\n",
      "epoch-178 lr=['0.0000100'], tr/val_loss:  0.084508/  0.766487, val:  78.75%, val_best:  80.42%, tr:  98.72%, tr_best:  98.72%, epoch time: 71.72 seconds, 1.20 minutes\n",
      "epoch-179 lr=['0.0000100'], tr/val_loss:  0.084497/  0.751412, val:  80.42%, val_best:  80.42%, tr:  98.60%, tr_best:  98.72%, epoch time: 70.93 seconds, 1.18 minutes\n",
      "epoch-180 lr=['0.0000100'], tr/val_loss:  0.084477/  0.746243, val:  79.58%, val_best:  80.42%, tr:  98.74%, tr_best:  98.74%, epoch time: 71.60 seconds, 1.19 minutes\n",
      "epoch-181 lr=['0.0000100'], tr/val_loss:  0.082637/  0.747505, val:  80.00%, val_best:  80.42%, tr:  98.76%, tr_best:  98.76%, epoch time: 71.89 seconds, 1.20 minutes\n",
      "epoch-182 lr=['0.0000100'], tr/val_loss:  0.082759/  0.744229, val:  80.00%, val_best:  80.42%, tr:  98.62%, tr_best:  98.76%, epoch time: 71.22 seconds, 1.19 minutes\n",
      "epoch-183 lr=['0.0000100'], tr/val_loss:  0.080990/  0.734915, val:  80.42%, val_best:  80.42%, tr:  98.74%, tr_best:  98.76%, epoch time: 71.67 seconds, 1.19 minutes\n",
      "epoch-184 lr=['0.0000100'], tr/val_loss:  0.080683/  0.736741, val:  80.00%, val_best:  80.42%, tr:  98.85%, tr_best:  98.85%, epoch time: 71.49 seconds, 1.19 minutes\n",
      "epoch-185 lr=['0.0000100'], tr/val_loss:  0.079758/  0.726244, val:  80.00%, val_best:  80.42%, tr:  98.83%, tr_best:  98.85%, epoch time: 71.85 seconds, 1.20 minutes\n",
      "epoch-186 lr=['0.0000100'], tr/val_loss:  0.078876/  0.751321, val:  79.58%, val_best:  80.42%, tr:  98.83%, tr_best:  98.85%, epoch time: 71.48 seconds, 1.19 minutes\n",
      "epoch-187 lr=['0.0000100'], tr/val_loss:  0.079296/  0.740644, val:  80.00%, val_best:  80.42%, tr:  98.94%, tr_best:  98.94%, epoch time: 71.42 seconds, 1.19 minutes\n",
      "epoch-188 lr=['0.0000100'], tr/val_loss:  0.076699/  0.751739, val:  79.58%, val_best:  80.42%, tr:  99.03%, tr_best:  99.03%, epoch time: 72.15 seconds, 1.20 minutes\n",
      "epoch-189 lr=['0.0000100'], tr/val_loss:  0.077040/  0.739423, val:  80.83%, val_best:  80.83%, tr:  99.08%, tr_best:  99.08%, epoch time: 71.36 seconds, 1.19 minutes\n",
      "epoch-190 lr=['0.0000100'], tr/val_loss:  0.075539/  0.757771, val:  80.00%, val_best:  80.83%, tr:  99.01%, tr_best:  99.08%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "epoch-191 lr=['0.0000100'], tr/val_loss:  0.074965/  0.758114, val:  80.00%, val_best:  80.83%, tr:  99.12%, tr_best:  99.12%, epoch time: 71.58 seconds, 1.19 minutes\n",
      "epoch-192 lr=['0.0000100'], tr/val_loss:  0.074733/  0.732636, val:  80.00%, val_best:  80.83%, tr:  99.03%, tr_best:  99.12%, epoch time: 71.46 seconds, 1.19 minutes\n",
      "epoch-193 lr=['0.0000100'], tr/val_loss:  0.075222/  0.742681, val:  80.42%, val_best:  80.83%, tr:  98.96%, tr_best:  99.12%, epoch time: 71.28 seconds, 1.19 minutes\n",
      "epoch-194 lr=['0.0000100'], tr/val_loss:  0.073810/  0.755148, val:  80.83%, val_best:  80.83%, tr:  99.21%, tr_best:  99.21%, epoch time: 72.72 seconds, 1.21 minutes\n",
      "epoch-195 lr=['0.0000100'], tr/val_loss:  0.073375/  0.747400, val:  80.00%, val_best:  80.83%, tr:  99.05%, tr_best:  99.21%, epoch time: 71.53 seconds, 1.19 minutes\n",
      "epoch-196 lr=['0.0000100'], tr/val_loss:  0.072084/  0.741749, val:  79.17%, val_best:  80.83%, tr:  99.21%, tr_best:  99.21%, epoch time: 72.07 seconds, 1.20 minutes\n",
      "epoch-197 lr=['0.0000100'], tr/val_loss:  0.071709/  0.756767, val:  80.42%, val_best:  80.83%, tr:  99.17%, tr_best:  99.21%, epoch time: 71.37 seconds, 1.19 minutes\n",
      "epoch-198 lr=['0.0000100'], tr/val_loss:  0.071794/  0.742253, val:  80.00%, val_best:  80.83%, tr:  99.03%, tr_best:  99.21%, epoch time: 71.58 seconds, 1.19 minutes\n",
      "epoch-199 lr=['0.0000100'], tr/val_loss:  0.071740/  0.755689, val:  80.00%, val_best:  80.83%, tr:  99.10%, tr_best:  99.21%, epoch time: 71.61 seconds, 1.19 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14143587e2b74745a52c207ffc846dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99098</td></tr><tr><td>tr_epoch_loss</td><td>0.07174</td></tr><tr><td>val_acc_best</td><td>0.80833</td></tr><tr><td>val_acc_now</td><td>0.8</td></tr><tr><td>val_loss</td><td>0.75569</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">balmy-sweep-15</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jcrebimi' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jcrebimi</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251103_171708-jcrebimi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nng761ya with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251103_211655-nng761ya</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nng761ya' target=\"_blank\">lunar-sweep-12</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nng761ya' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nng761ya</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': False, 'unique_name': '20251103_211703_331', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = deea846692c028030dda4288c5a62e02\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d18ea47e42908fd1fd969b7f53daa77d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d927659af8cbcce93fcdf7890586c422\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a7b1cb019bae1c7ff9ca5108a2227c78\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d32e74b7370101199dbd6e963d91b37\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 34d12bca5b3860bcf37e9e43ba148aff\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2ffe8c11d12fcf65fcbccccceb17dc92\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 6b6e0cb38311b5e12527b1f1f47e8bef\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 38e4136667b4877be986f5db6c6531e4\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 596467cdf6688b976b4a9bef67ce1458\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=1, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.186626/  1.726376, val:  31.67%, val_best:  31.67%, tr:  13.95%, tr_best:  13.95%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  1.171197/  1.065886, val:  54.17%, val_best:  54.17%, tr:  52.30%, tr_best:  52.30%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  0.810594/  0.872814, val:  60.42%, val_best:  60.42%, tr:  63.93%, tr_best:  63.93%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  0.675223/  0.908305, val:  63.75%, val_best:  63.75%, tr:  68.42%, tr_best:  68.42%, epoch time: 74.01 seconds, 1.23 minutes\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  0.579543/  0.791602, val:  67.08%, val_best:  67.08%, tr:  73.02%, tr_best:  73.02%, epoch time: 74.85 seconds, 1.25 minutes\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  0.520860/  0.768353, val:  67.92%, val_best:  67.92%, tr:  75.23%, tr_best:  75.23%, epoch time: 74.12 seconds, 1.24 minutes\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  0.474938/  0.809960, val:  67.92%, val_best:  67.92%, tr:  77.50%, tr_best:  77.50%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  0.431648/  0.856442, val:  66.67%, val_best:  67.92%, tr:  79.98%, tr_best:  79.98%, epoch time: 74.19 seconds, 1.24 minutes\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  0.397493/  0.828721, val:  71.25%, val_best:  71.25%, tr:  81.99%, tr_best:  81.99%, epoch time: 74.52 seconds, 1.24 minutes\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.358194/  0.858851, val:  69.17%, val_best:  71.25%, tr:  84.33%, tr_best:  84.33%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.323862/  0.773138, val:  72.50%, val_best:  72.50%, tr:  85.66%, tr_best:  85.66%, epoch time: 73.92 seconds, 1.23 minutes\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.294967/  0.819295, val:  72.50%, val_best:  72.50%, tr:  87.53%, tr_best:  87.53%, epoch time: 73.92 seconds, 1.23 minutes\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.261609/  0.835116, val:  69.58%, val_best:  72.50%, tr:  88.93%, tr_best:  88.93%, epoch time: 74.36 seconds, 1.24 minutes\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.231506/  0.959296, val:  70.42%, val_best:  72.50%, tr:  90.96%, tr_best:  90.96%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.207488/  0.847444, val:  75.42%, val_best:  75.42%, tr:  91.46%, tr_best:  91.46%, epoch time: 74.09 seconds, 1.23 minutes\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.178364/  0.882338, val:  74.17%, val_best:  75.42%, tr:  93.35%, tr_best:  93.35%, epoch time: 74.23 seconds, 1.24 minutes\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.152704/  0.834342, val:  75.00%, val_best:  75.42%, tr:  94.07%, tr_best:  94.07%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.139205/  0.936455, val:  72.92%, val_best:  75.42%, tr:  94.86%, tr_best:  94.86%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.115380/  0.890956, val:  76.25%, val_best:  76.25%, tr:  95.85%, tr_best:  95.85%, epoch time: 74.38 seconds, 1.24 minutes\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.099944/  0.878355, val:  74.58%, val_best:  76.25%, tr:  96.69%, tr_best:  96.69%, epoch time: 74.48 seconds, 1.24 minutes\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.080134/  0.948022, val:  75.42%, val_best:  76.25%, tr:  97.41%, tr_best:  97.41%, epoch time: 74.56 seconds, 1.24 minutes\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.073689/  0.860716, val:  78.33%, val_best:  78.33%, tr:  97.61%, tr_best:  97.61%, epoch time: 74.10 seconds, 1.24 minutes\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.059894/  0.984899, val:  75.83%, val_best:  78.33%, tr:  98.11%, tr_best:  98.11%, epoch time: 74.40 seconds, 1.24 minutes\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.050180/  0.885659, val:  77.08%, val_best:  78.33%, tr:  98.62%, tr_best:  98.62%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.060604/  1.635631, val:  68.75%, val_best:  78.33%, tr:  98.08%, tr_best:  98.62%, epoch time: 73.74 seconds, 1.23 minutes\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.045608/  0.982462, val:  76.67%, val_best:  78.33%, tr:  98.81%, tr_best:  98.81%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.034540/  1.016005, val:  77.50%, val_best:  78.33%, tr:  98.99%, tr_best:  98.99%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.027722/  1.055030, val:  76.67%, val_best:  78.33%, tr:  99.32%, tr_best:  99.32%, epoch time: 74.27 seconds, 1.24 minutes\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.033059/  1.064474, val:  76.67%, val_best:  78.33%, tr:  99.14%, tr_best:  99.32%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.024577/  1.064487, val:  74.58%, val_best:  78.33%, tr:  99.41%, tr_best:  99.41%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.021875/  0.984590, val:  77.50%, val_best:  78.33%, tr:  99.57%, tr_best:  99.57%, epoch time: 73.90 seconds, 1.23 minutes\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.019803/  1.101310, val:  76.25%, val_best:  78.33%, tr:  99.59%, tr_best:  99.59%, epoch time: 73.90 seconds, 1.23 minutes\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.012381/  0.991693, val:  77.92%, val_best:  78.33%, tr:  99.82%, tr_best:  99.82%, epoch time: 74.13 seconds, 1.24 minutes\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.013365/  0.975127, val:  79.17%, val_best:  79.17%, tr:  99.82%, tr_best:  99.82%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.010371/  1.065378, val:  76.67%, val_best:  79.17%, tr:  99.89%, tr_best:  99.89%, epoch time: 74.02 seconds, 1.23 minutes\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.013921/  1.070916, val:  77.50%, val_best:  79.17%, tr:  99.80%, tr_best:  99.89%, epoch time: 74.28 seconds, 1.24 minutes\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.008583/  1.162722, val:  76.67%, val_best:  79.17%, tr:  99.91%, tr_best:  99.91%, epoch time: 74.33 seconds, 1.24 minutes\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.008448/  1.130590, val:  75.83%, val_best:  79.17%, tr:  99.95%, tr_best:  99.95%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.009352/  1.107930, val:  77.92%, val_best:  79.17%, tr:  99.89%, tr_best:  99.95%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.007428/  1.132747, val:  76.67%, val_best:  79.17%, tr:  99.93%, tr_best:  99.95%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.008061/  1.097356, val:  77.92%, val_best:  79.17%, tr:  99.91%, tr_best:  99.95%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.006900/  1.140557, val:  75.83%, val_best:  79.17%, tr:  99.93%, tr_best:  99.95%, epoch time: 74.17 seconds, 1.24 minutes\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.005276/  1.174102, val:  77.92%, val_best:  79.17%, tr:  99.98%, tr_best:  99.98%, epoch time: 73.95 seconds, 1.23 minutes\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.004266/  1.166683, val:  76.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.004372/  1.112980, val:  79.17%, val_best:  79.17%, tr:  99.95%, tr_best: 100.00%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.004032/  1.154927, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.28 seconds, 1.24 minutes\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.003628/  1.177795, val:  77.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.96 seconds, 1.23 minutes\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.003214/  1.143493, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.94 seconds, 1.23 minutes\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.002804/  1.209608, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.08 seconds, 1.23 minutes\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.003041/  1.181743, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.98 seconds, 1.23 minutes\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.003232/  1.160039, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.003321/  1.200833, val:  77.92%, val_best:  79.17%, tr:  99.95%, tr_best: 100.00%, epoch time: 74.37 seconds, 1.24 minutes\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.002762/  1.099622, val:  77.92%, val_best:  79.17%, tr:  99.98%, tr_best: 100.00%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.002596/  1.092578, val:  78.33%, val_best:  79.17%, tr:  99.98%, tr_best: 100.00%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.002986/  1.161027, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.002245/  1.102073, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.04 seconds, 1.23 minutes\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.002047/  1.150768, val:  78.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.33 seconds, 1.24 minutes\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.001774/  1.123107, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.001675/  1.148842, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.04 seconds, 1.23 minutes\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.001753/  1.115923, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.90 seconds, 1.23 minutes\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.001585/  1.184236, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.001736/  1.115297, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.05 seconds, 1.23 minutes\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.001724/  1.117408, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.001728/  1.164444, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.001768/  1.142606, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.001639/  1.166329, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.001662/  1.139110, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.001444/  1.139700, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.001490/  1.090287, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.96 seconds, 1.23 minutes\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.001407/  1.163959, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.00 seconds, 1.23 minutes\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.001418/  1.175268, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.001308/  1.117819, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.001298/  1.184688, val:  78.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.87 seconds, 1.23 minutes\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.001303/  1.224721, val:  77.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.35 seconds, 1.24 minutes\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.001140/  1.167886, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.001104/  1.167747, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.90 seconds, 1.23 minutes\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.001119/  1.146126, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.001224/  1.143932, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.001219/  1.163902, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.001168/  1.167662, val:  79.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.06 seconds, 1.23 minutes\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.001065/  1.173472, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.001062/  1.170451, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.001131/  1.232145, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.11 seconds, 1.24 minutes\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.001050/  1.221925, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.87 seconds, 1.23 minutes\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.001132/  1.232586, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.86 seconds, 1.23 minutes\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.000961/  1.210938, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.000957/  1.223548, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.000982/  1.204478, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.000957/  1.213343, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.001013/  1.262093, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.45 seconds, 1.22 minutes\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.000883/  1.230784, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.000837/  1.247722, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.000870/  1.279136, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.07 seconds, 1.23 minutes\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.000803/  1.268646, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.08 seconds, 1.22 minutes\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.000861/  1.279557, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.000813/  1.238690, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.000863/  1.292663, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.000846/  1.274049, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.000847/  1.243937, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.000773/  1.258486, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  0.000765/  1.255385, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.50 seconds, 1.22 minutes\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  0.000768/  1.266828, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  0.000771/  1.271082, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.68 seconds, 1.23 minutes\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  0.000787/  1.287447, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  0.000740/  1.313203, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  0.001619/  1.314515, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  0.001310/  1.374800, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  0.001197/  1.300639, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  0.000770/  1.328630, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  0.000716/  1.283216, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  0.000671/  1.285309, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.83 seconds, 1.25 minutes\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  0.000654/  1.297364, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  0.000627/  1.330602, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  0.000618/  1.343316, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  0.000657/  1.337310, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  0.000649/  1.328565, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  0.000666/  1.369296, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  0.000593/  1.332560, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  0.000604/  1.360515, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  0.000583/  1.334131, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  0.000557/  1.356246, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  0.000542/  1.343913, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  0.000574/  1.360827, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  0.000596/  1.349616, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.96 seconds, 1.23 minutes\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  0.000550/  1.379344, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  0.000556/  1.349158, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  0.000519/  1.332057, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.30 seconds, 1.22 minutes\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  0.000524/  1.342191, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  0.000527/  1.357502, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  0.000510/  1.389917, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.17 seconds, 1.22 minutes\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  0.000520/  1.324147, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  0.000510/  1.408708, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.48 seconds, 1.22 minutes\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  0.000494/  1.385563, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  0.000478/  1.364648, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.14 seconds, 1.22 minutes\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  0.000486/  1.395200, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  0.000492/  1.391204, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  0.000475/  1.350926, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.29 seconds, 1.20 minutes\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  0.000462/  1.382894, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  0.000475/  1.388209, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  0.000438/  1.397570, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.17 seconds, 1.22 minutes\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  0.000443/  1.380303, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  0.000439/  1.390957, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.98 seconds, 1.23 minutes\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  0.000449/  1.419201, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  0.000449/  1.383840, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  0.000421/  1.412784, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  0.000433/  1.419272, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.22 minutes\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  0.000437/  1.389021, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  0.000444/  1.439464, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.66 seconds, 1.21 minutes\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  0.000409/  1.416355, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  0.000436/  1.406951, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  0.000418/  1.375136, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.17 seconds, 1.22 minutes\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  0.000401/  1.387032, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.84 seconds, 1.23 minutes\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  0.000415/  1.398458, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.59 seconds, 1.23 minutes\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  0.000395/  1.356554, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  0.000388/  1.371585, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  0.000389/  1.375075, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  0.000359/  1.386400, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  0.000362/  1.399351, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.68 seconds, 1.23 minutes\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  0.000377/  1.373860, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  0.000370/  1.407147, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  0.000375/  1.394337, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.89 seconds, 1.23 minutes\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  0.000362/  1.376695, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  0.000345/  1.350539, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  0.000346/  1.367914, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  0.000352/  1.425017, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  0.000342/  1.410468, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  0.000336/  1.418766, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  0.000337/  1.451312, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  0.000325/  1.424365, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  0.000338/  1.373009, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  0.000333/  1.402063, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  0.000330/  1.417447, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  0.000322/  1.415466, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  0.000328/  1.408041, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  0.000317/  1.392101, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  0.000318/  1.383998, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  0.000333/  1.404038, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.34 seconds, 1.22 minutes\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  0.000322/  1.369164, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  0.000308/  1.393858, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  0.000296/  1.399582, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  0.000302/  1.410051, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  0.000297/  1.399237, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  0.000288/  1.392828, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  0.000298/  1.377941, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  0.000292/  1.401419, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  0.000297/  1.414943, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  0.000295/  1.419938, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  0.000301/  1.417719, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  0.000317/  1.388293, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  0.000324/  1.417113, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.68 seconds, 1.23 minutes\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  0.000316/  1.410400, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  0.000333/  1.402306, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.59 seconds, 1.23 minutes\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  0.000316/  1.399685, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  0.000292/  1.404379, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  0.000299/  1.409873, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  0.000294/  1.408702, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.71 seconds, 1.21 minutes\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  0.000297/  1.397225, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  0.000293/  1.431398, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  0.000291/  1.429481, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  0.000304/  1.446758, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.60 seconds, 1.23 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7844024816d487a8099455bf7129472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.0003</td></tr><tr><td>val_acc_best</td><td>0.80417</td></tr><tr><td>val_acc_now</td><td>0.77083</td></tr><tr><td>val_loss</td><td>1.44676</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lunar-sweep-12</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nng761ya' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nng761ya</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251103_211655-nng761ya/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hgdeyrd6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251104_012419-hgdeyrd6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hgdeyrd6' target=\"_blank\">confused-sweep-21</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hgdeyrd6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hgdeyrd6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': False, 'unique_name': '20251104_012428_937', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 1e-05, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 18, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f1351bdee3d35c47af449525e007adf4\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 713fc8490e508dff15bed30e755e5ae6\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 4e1f85d1c0ff71c6a0df58e4542628f1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 626851829f1152228185abed5f6dca5e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 3c558307d41237628ef99b773b1c784f\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 81e44cafc682693b4b2086c7f81d224b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = b36895cc577f68764ec3fb2cd889299e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = efeaf39262f61e3ee121af192099af65\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a15007b6dd509fde13c404b80435835e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 6fa31eb5ad96d639190daf0921d987e2\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=2, surrogate=sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=2, surrogate=sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 1e-05\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch-0   lr=['0.0000100'], tr/val_loss:  2.123862/  1.978212, val:  27.92%, val_best:  27.92%, tr:  24.55%, tr_best:  24.55%, epoch time: 67.77 seconds, 1.13 minutes\n",
      "epoch-1   lr=['0.0000100'], tr/val_loss:  1.804770/  1.821688, val:  31.67%, val_best:  31.67%, tr:  34.22%, tr_best:  34.22%, epoch time: 67.60 seconds, 1.13 minutes\n",
      "epoch-2   lr=['0.0000100'], tr/val_loss:  1.658137/  1.699831, val:  34.58%, val_best:  34.58%, tr:  39.04%, tr_best:  39.04%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "epoch-3   lr=['0.0000100'], tr/val_loss:  1.548633/  1.634862, val:  36.25%, val_best:  36.25%, tr:  42.20%, tr_best:  42.20%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "epoch-4   lr=['0.0000100'], tr/val_loss:  1.464614/  1.558543, val:  38.33%, val_best:  38.33%, tr:  44.70%, tr_best:  44.70%, epoch time: 67.80 seconds, 1.13 minutes\n",
      "epoch-5   lr=['0.0000100'], tr/val_loss:  1.393354/  1.496902, val:  42.92%, val_best:  42.92%, tr:  47.68%, tr_best:  47.68%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "epoch-6   lr=['0.0000100'], tr/val_loss:  1.335780/  1.446651, val:  42.50%, val_best:  42.92%, tr:  50.45%, tr_best:  50.45%, epoch time: 69.25 seconds, 1.15 minutes\n",
      "epoch-7   lr=['0.0000100'], tr/val_loss:  1.289678/  1.412635, val:  45.00%, val_best:  45.00%, tr:  51.94%, tr_best:  51.94%, epoch time: 67.37 seconds, 1.12 minutes\n",
      "epoch-8   lr=['0.0000100'], tr/val_loss:  1.250608/  1.368721, val:  47.08%, val_best:  47.08%, tr:  53.25%, tr_best:  53.25%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "epoch-9   lr=['0.0000100'], tr/val_loss:  1.216016/  1.315039, val:  47.08%, val_best:  47.08%, tr:  54.33%, tr_best:  54.33%, epoch time: 67.12 seconds, 1.12 minutes\n",
      "epoch-10  lr=['0.0000100'], tr/val_loss:  1.184366/  1.292789, val:  50.42%, val_best:  50.42%, tr:  54.96%, tr_best:  54.96%, epoch time: 68.21 seconds, 1.14 minutes\n",
      "epoch-11  lr=['0.0000100'], tr/val_loss:  1.153322/  1.283703, val:  48.75%, val_best:  50.42%, tr:  56.04%, tr_best:  56.04%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "epoch-12  lr=['0.0000100'], tr/val_loss:  1.130905/  1.264916, val:  49.58%, val_best:  50.42%, tr:  57.21%, tr_best:  57.21%, epoch time: 67.38 seconds, 1.12 minutes\n",
      "epoch-13  lr=['0.0000100'], tr/val_loss:  1.111033/  1.240976, val:  51.25%, val_best:  51.25%, tr:  57.71%, tr_best:  57.71%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "epoch-14  lr=['0.0000100'], tr/val_loss:  1.091414/  1.204671, val:  54.17%, val_best:  54.17%, tr:  57.71%, tr_best:  57.71%, epoch time: 67.09 seconds, 1.12 minutes\n",
      "epoch-15  lr=['0.0000100'], tr/val_loss:  1.071468/  1.211311, val:  52.50%, val_best:  54.17%, tr:  58.70%, tr_best:  58.70%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "epoch-16  lr=['0.0000100'], tr/val_loss:  1.053072/  1.194406, val:  53.33%, val_best:  54.17%, tr:  59.69%, tr_best:  59.69%, epoch time: 67.35 seconds, 1.12 minutes\n",
      "epoch-17  lr=['0.0000100'], tr/val_loss:  1.036993/  1.183227, val:  52.92%, val_best:  54.17%, tr:  60.14%, tr_best:  60.14%, epoch time: 68.00 seconds, 1.13 minutes\n",
      "epoch-18  lr=['0.0000100'], tr/val_loss:  1.021945/  1.170367, val:  54.58%, val_best:  54.58%, tr:  60.96%, tr_best:  60.96%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "epoch-19  lr=['0.0000100'], tr/val_loss:  1.008803/  1.163996, val:  55.00%, val_best:  55.00%, tr:  61.90%, tr_best:  61.90%, epoch time: 67.18 seconds, 1.12 minutes\n",
      "epoch-20  lr=['0.0000100'], tr/val_loss:  0.995004/  1.138216, val:  55.83%, val_best:  55.83%, tr:  62.17%, tr_best:  62.17%, epoch time: 67.24 seconds, 1.12 minutes\n",
      "epoch-21  lr=['0.0000100'], tr/val_loss:  0.980789/  1.140487, val:  57.08%, val_best:  57.08%, tr:  62.62%, tr_best:  62.62%, epoch time: 67.86 seconds, 1.13 minutes\n",
      "epoch-22  lr=['0.0000100'], tr/val_loss:  0.969133/  1.115302, val:  59.17%, val_best:  59.17%, tr:  62.98%, tr_best:  62.98%, epoch time: 67.79 seconds, 1.13 minutes\n",
      "epoch-23  lr=['0.0000100'], tr/val_loss:  0.955556/  1.107146, val:  56.67%, val_best:  59.17%, tr:  64.13%, tr_best:  64.13%, epoch time: 67.60 seconds, 1.13 minutes\n",
      "epoch-24  lr=['0.0000100'], tr/val_loss:  0.945006/  1.108608, val:  55.42%, val_best:  59.17%, tr:  64.65%, tr_best:  64.65%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "epoch-25  lr=['0.0000100'], tr/val_loss:  0.936055/  1.098320, val:  59.17%, val_best:  59.17%, tr:  65.71%, tr_best:  65.71%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "epoch-26  lr=['0.0000100'], tr/val_loss:  0.927993/  1.078023, val:  60.00%, val_best:  60.00%, tr:  65.98%, tr_best:  65.98%, epoch time: 68.74 seconds, 1.15 minutes\n",
      "epoch-27  lr=['0.0000100'], tr/val_loss:  0.917163/  1.079741, val:  59.58%, val_best:  60.00%, tr:  66.43%, tr_best:  66.43%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "epoch-28  lr=['0.0000100'], tr/val_loss:  0.911475/  1.065229, val:  58.75%, val_best:  60.00%, tr:  65.98%, tr_best:  66.43%, epoch time: 67.77 seconds, 1.13 minutes\n",
      "epoch-29  lr=['0.0000100'], tr/val_loss:  0.901718/  1.061998, val:  59.58%, val_best:  60.00%, tr:  66.37%, tr_best:  66.43%, epoch time: 67.33 seconds, 1.12 minutes\n",
      "epoch-30  lr=['0.0000100'], tr/val_loss:  0.891531/  1.058184, val:  61.25%, val_best:  61.25%, tr:  66.86%, tr_best:  66.86%, epoch time: 67.79 seconds, 1.13 minutes\n",
      "epoch-31  lr=['0.0000100'], tr/val_loss:  0.882224/  1.054296, val:  62.50%, val_best:  62.50%, tr:  67.38%, tr_best:  67.38%, epoch time: 67.49 seconds, 1.12 minutes\n",
      "epoch-32  lr=['0.0000100'], tr/val_loss:  0.874936/  1.045593, val:  62.50%, val_best:  62.50%, tr:  68.08%, tr_best:  68.08%, epoch time: 67.75 seconds, 1.13 minutes\n",
      "epoch-33  lr=['0.0000100'], tr/val_loss:  0.868147/  1.045237, val:  62.92%, val_best:  62.92%, tr:  68.85%, tr_best:  68.85%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "epoch-34  lr=['0.0000100'], tr/val_loss:  0.862331/  1.031252, val:  62.08%, val_best:  62.92%, tr:  68.42%, tr_best:  68.85%, epoch time: 67.61 seconds, 1.13 minutes\n",
      "epoch-35  lr=['0.0000100'], tr/val_loss:  0.852388/  1.030715, val:  61.67%, val_best:  62.92%, tr:  68.42%, tr_best:  68.85%, epoch time: 67.95 seconds, 1.13 minutes\n",
      "epoch-36  lr=['0.0000100'], tr/val_loss:  0.846345/  1.020420, val:  62.08%, val_best:  62.92%, tr:  69.27%, tr_best:  69.27%, epoch time: 67.82 seconds, 1.13 minutes\n",
      "epoch-37  lr=['0.0000100'], tr/val_loss:  0.838964/  1.019618, val:  61.67%, val_best:  62.92%, tr:  69.21%, tr_best:  69.27%, epoch time: 67.87 seconds, 1.13 minutes\n",
      "epoch-38  lr=['0.0000100'], tr/val_loss:  0.830565/  1.009167, val:  62.92%, val_best:  62.92%, tr:  69.45%, tr_best:  69.45%, epoch time: 67.56 seconds, 1.13 minutes\n",
      "epoch-39  lr=['0.0000100'], tr/val_loss:  0.822348/  0.996929, val:  61.25%, val_best:  62.92%, tr:  70.45%, tr_best:  70.45%, epoch time: 67.42 seconds, 1.12 minutes\n",
      "epoch-40  lr=['0.0000100'], tr/val_loss:  0.818276/  0.986749, val:  63.75%, val_best:  63.75%, tr:  70.49%, tr_best:  70.49%, epoch time: 68.07 seconds, 1.13 minutes\n",
      "epoch-41  lr=['0.0000100'], tr/val_loss:  0.812654/  0.982290, val:  62.08%, val_best:  63.75%, tr:  70.96%, tr_best:  70.96%, epoch time: 67.55 seconds, 1.13 minutes\n",
      "epoch-42  lr=['0.0000100'], tr/val_loss:  0.808071/  0.982106, val:  61.67%, val_best:  63.75%, tr:  70.76%, tr_best:  70.96%, epoch time: 67.79 seconds, 1.13 minutes\n",
      "epoch-43  lr=['0.0000100'], tr/val_loss:  0.802245/  0.971810, val:  64.17%, val_best:  64.17%, tr:  71.01%, tr_best:  71.01%, epoch time: 67.80 seconds, 1.13 minutes\n",
      "epoch-44  lr=['0.0000100'], tr/val_loss:  0.798381/  0.966937, val:  63.33%, val_best:  64.17%, tr:  71.33%, tr_best:  71.33%, epoch time: 67.73 seconds, 1.13 minutes\n",
      "epoch-45  lr=['0.0000100'], tr/val_loss:  0.791359/  0.975623, val:  63.33%, val_best:  64.17%, tr:  71.84%, tr_best:  71.84%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "epoch-46  lr=['0.0000100'], tr/val_loss:  0.785681/  0.968819, val:  65.00%, val_best:  65.00%, tr:  71.80%, tr_best:  71.84%, epoch time: 67.55 seconds, 1.13 minutes\n",
      "epoch-47  lr=['0.0000100'], tr/val_loss:  0.782683/  0.969262, val:  65.00%, val_best:  65.00%, tr:  72.00%, tr_best:  72.00%, epoch time: 67.70 seconds, 1.13 minutes\n",
      "epoch-48  lr=['0.0000100'], tr/val_loss:  0.776753/  0.965195, val:  65.00%, val_best:  65.00%, tr:  72.34%, tr_best:  72.34%, epoch time: 67.47 seconds, 1.12 minutes\n",
      "epoch-49  lr=['0.0000100'], tr/val_loss:  0.774042/  0.966662, val:  64.17%, val_best:  65.00%, tr:  72.00%, tr_best:  72.34%, epoch time: 67.98 seconds, 1.13 minutes\n",
      "epoch-50  lr=['0.0000100'], tr/val_loss:  0.767286/  0.957120, val:  63.33%, val_best:  65.00%, tr:  72.63%, tr_best:  72.63%, epoch time: 67.43 seconds, 1.12 minutes\n",
      "epoch-51  lr=['0.0000100'], tr/val_loss:  0.763944/  0.954720, val:  64.58%, val_best:  65.00%, tr:  72.54%, tr_best:  72.63%, epoch time: 67.50 seconds, 1.12 minutes\n",
      "epoch-52  lr=['0.0000100'], tr/val_loss:  0.757843/  0.953570, val:  63.33%, val_best:  65.00%, tr:  72.72%, tr_best:  72.72%, epoch time: 68.13 seconds, 1.14 minutes\n",
      "epoch-53  lr=['0.0000100'], tr/val_loss:  0.754279/  0.952124, val:  63.75%, val_best:  65.00%, tr:  73.08%, tr_best:  73.08%, epoch time: 67.04 seconds, 1.12 minutes\n",
      "epoch-54  lr=['0.0000100'], tr/val_loss:  0.748854/  0.961422, val:  64.58%, val_best:  65.00%, tr:  72.66%, tr_best:  73.08%, epoch time: 67.52 seconds, 1.13 minutes\n",
      "epoch-55  lr=['0.0000100'], tr/val_loss:  0.745609/  0.947557, val:  65.42%, val_best:  65.42%, tr:  72.68%, tr_best:  73.08%, epoch time: 67.45 seconds, 1.12 minutes\n",
      "epoch-56  lr=['0.0000100'], tr/val_loss:  0.740632/  0.940554, val:  65.83%, val_best:  65.83%, tr:  73.65%, tr_best:  73.65%, epoch time: 67.32 seconds, 1.12 minutes\n",
      "epoch-57  lr=['0.0000100'], tr/val_loss:  0.736768/  0.936186, val:  66.67%, val_best:  66.67%, tr:  74.01%, tr_best:  74.01%, epoch time: 67.78 seconds, 1.13 minutes\n",
      "epoch-58  lr=['0.0000100'], tr/val_loss:  0.731745/  0.933753, val:  66.67%, val_best:  66.67%, tr:  74.53%, tr_best:  74.53%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "epoch-59  lr=['0.0000100'], tr/val_loss:  0.727338/  0.927970, val:  67.08%, val_best:  67.08%, tr:  74.48%, tr_best:  74.53%, epoch time: 67.36 seconds, 1.12 minutes\n",
      "epoch-60  lr=['0.0000100'], tr/val_loss:  0.725015/  0.920253, val:  67.08%, val_best:  67.08%, tr:  74.08%, tr_best:  74.53%, epoch time: 67.52 seconds, 1.13 minutes\n",
      "epoch-61  lr=['0.0000100'], tr/val_loss:  0.721911/  0.917663, val:  67.50%, val_best:  67.50%, tr:  74.46%, tr_best:  74.53%, epoch time: 67.26 seconds, 1.12 minutes\n",
      "epoch-62  lr=['0.0000100'], tr/val_loss:  0.717716/  0.922134, val:  67.92%, val_best:  67.92%, tr:  74.75%, tr_best:  74.75%, epoch time: 67.65 seconds, 1.13 minutes\n",
      "epoch-63  lr=['0.0000100'], tr/val_loss:  0.716150/  0.916774, val:  68.75%, val_best:  68.75%, tr:  74.84%, tr_best:  74.84%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "epoch-64  lr=['0.0000100'], tr/val_loss:  0.711661/  0.915403, val:  68.33%, val_best:  68.75%, tr:  75.05%, tr_best:  75.05%, epoch time: 67.55 seconds, 1.13 minutes\n",
      "epoch-65  lr=['0.0000100'], tr/val_loss:  0.708621/  0.922399, val:  66.67%, val_best:  68.75%, tr:  75.09%, tr_best:  75.09%, epoch time: 68.15 seconds, 1.14 minutes\n",
      "epoch-66  lr=['0.0000100'], tr/val_loss:  0.704182/  0.923414, val:  67.92%, val_best:  68.75%, tr:  75.52%, tr_best:  75.52%, epoch time: 67.52 seconds, 1.13 minutes\n",
      "epoch-67  lr=['0.0000100'], tr/val_loss:  0.700846/  0.918659, val:  66.67%, val_best:  68.75%, tr:  75.43%, tr_best:  75.52%, epoch time: 67.77 seconds, 1.13 minutes\n",
      "epoch-68  lr=['0.0000100'], tr/val_loss:  0.697641/  0.918422, val:  68.75%, val_best:  68.75%, tr:  75.59%, tr_best:  75.59%, epoch time: 67.71 seconds, 1.13 minutes\n",
      "epoch-69  lr=['0.0000100'], tr/val_loss:  0.693949/  0.923039, val:  67.92%, val_best:  68.75%, tr:  75.36%, tr_best:  75.59%, epoch time: 67.99 seconds, 1.13 minutes\n",
      "epoch-70  lr=['0.0000100'], tr/val_loss:  0.691414/  0.910717, val:  67.92%, val_best:  68.75%, tr:  75.41%, tr_best:  75.59%, epoch time: 68.49 seconds, 1.14 minutes\n",
      "epoch-71  lr=['0.0000100'], tr/val_loss:  0.686326/  0.907319, val:  67.50%, val_best:  68.75%, tr:  76.28%, tr_best:  76.28%, epoch time: 67.38 seconds, 1.12 minutes\n",
      "epoch-72  lr=['0.0000100'], tr/val_loss:  0.683123/  0.911482, val:  67.50%, val_best:  68.75%, tr:  76.17%, tr_best:  76.28%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "epoch-73  lr=['0.0000100'], tr/val_loss:  0.680026/  0.914411, val:  66.25%, val_best:  68.75%, tr:  76.42%, tr_best:  76.42%, epoch time: 67.69 seconds, 1.13 minutes\n",
      "epoch-74  lr=['0.0000100'], tr/val_loss:  0.677402/  0.919217, val:  66.25%, val_best:  68.75%, tr:  76.71%, tr_best:  76.71%, epoch time: 67.76 seconds, 1.13 minutes\n",
      "epoch-75  lr=['0.0000100'], tr/val_loss:  0.675418/  0.919458, val:  67.08%, val_best:  68.75%, tr:  76.60%, tr_best:  76.71%, epoch time: 67.73 seconds, 1.13 minutes\n",
      "epoch-76  lr=['0.0000100'], tr/val_loss:  0.672149/  0.920871, val:  66.67%, val_best:  68.75%, tr:  76.44%, tr_best:  76.71%, epoch time: 67.34 seconds, 1.12 minutes\n",
      "epoch-77  lr=['0.0000100'], tr/val_loss:  0.669954/  0.919042, val:  66.25%, val_best:  68.75%, tr:  76.49%, tr_best:  76.71%, epoch time: 67.85 seconds, 1.13 minutes\n",
      "epoch-78  lr=['0.0000100'], tr/val_loss:  0.667348/  0.922170, val:  67.08%, val_best:  68.75%, tr:  76.78%, tr_best:  76.78%, epoch time: 67.36 seconds, 1.12 minutes\n",
      "epoch-79  lr=['0.0000100'], tr/val_loss:  0.662683/  0.920913, val:  65.83%, val_best:  68.75%, tr:  76.96%, tr_best:  76.96%, epoch time: 67.46 seconds, 1.12 minutes\n",
      "epoch-80  lr=['0.0000100'], tr/val_loss:  0.660130/  0.916150, val:  64.58%, val_best:  68.75%, tr:  77.07%, tr_best:  77.07%, epoch time: 67.33 seconds, 1.12 minutes\n",
      "epoch-81  lr=['0.0000100'], tr/val_loss:  0.655747/  0.914796, val:  64.17%, val_best:  68.75%, tr:  77.41%, tr_best:  77.41%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "epoch-82  lr=['0.0000100'], tr/val_loss:  0.653017/  0.904776, val:  64.17%, val_best:  68.75%, tr:  77.30%, tr_best:  77.41%, epoch time: 67.96 seconds, 1.13 minutes\n",
      "epoch-83  lr=['0.0000100'], tr/val_loss:  0.650200/  0.917497, val:  65.00%, val_best:  68.75%, tr:  77.30%, tr_best:  77.41%, epoch time: 67.85 seconds, 1.13 minutes\n",
      "epoch-84  lr=['0.0000100'], tr/val_loss:  0.649025/  0.908346, val:  65.83%, val_best:  68.75%, tr:  77.25%, tr_best:  77.41%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "epoch-85  lr=['0.0000100'], tr/val_loss:  0.647198/  0.904803, val:  66.25%, val_best:  68.75%, tr:  77.03%, tr_best:  77.41%, epoch time: 67.15 seconds, 1.12 minutes\n",
      "epoch-86  lr=['0.0000100'], tr/val_loss:  0.642679/  0.907124, val:  65.83%, val_best:  68.75%, tr:  77.46%, tr_best:  77.46%, epoch time: 67.98 seconds, 1.13 minutes\n",
      "epoch-87  lr=['0.0000100'], tr/val_loss:  0.639130/  0.903382, val:  66.25%, val_best:  68.75%, tr:  78.09%, tr_best:  78.09%, epoch time: 67.61 seconds, 1.13 minutes\n",
      "epoch-88  lr=['0.0000100'], tr/val_loss:  0.636082/  0.901080, val:  65.42%, val_best:  68.75%, tr:  77.95%, tr_best:  78.09%, epoch time: 67.31 seconds, 1.12 minutes\n",
      "epoch-89  lr=['0.0000100'], tr/val_loss:  0.632975/  0.897707, val:  66.25%, val_best:  68.75%, tr:  78.34%, tr_best:  78.34%, epoch time: 67.28 seconds, 1.12 minutes\n",
      "epoch-90  lr=['0.0000100'], tr/val_loss:  0.628483/  0.895664, val:  67.50%, val_best:  68.75%, tr:  78.27%, tr_best:  78.34%, epoch time: 67.52 seconds, 1.13 minutes\n",
      "epoch-91  lr=['0.0000100'], tr/val_loss:  0.628190/  0.896492, val:  66.25%, val_best:  68.75%, tr:  78.18%, tr_best:  78.34%, epoch time: 67.24 seconds, 1.12 minutes\n",
      "epoch-92  lr=['0.0000100'], tr/val_loss:  0.626297/  0.899777, val:  67.50%, val_best:  68.75%, tr:  78.34%, tr_best:  78.34%, epoch time: 67.72 seconds, 1.13 minutes\n",
      "epoch-93  lr=['0.0000100'], tr/val_loss:  0.622994/  0.903243, val:  67.08%, val_best:  68.75%, tr:  78.63%, tr_best:  78.63%, epoch time: 67.94 seconds, 1.13 minutes\n",
      "epoch-94  lr=['0.0000100'], tr/val_loss:  0.620869/  0.897481, val:  67.92%, val_best:  68.75%, tr:  78.88%, tr_best:  78.88%, epoch time: 67.71 seconds, 1.13 minutes\n",
      "epoch-95  lr=['0.0000100'], tr/val_loss:  0.619586/  0.896546, val:  67.50%, val_best:  68.75%, tr:  78.63%, tr_best:  78.88%, epoch time: 67.87 seconds, 1.13 minutes\n",
      "epoch-96  lr=['0.0000100'], tr/val_loss:  0.615961/  0.892432, val:  67.92%, val_best:  68.75%, tr:  78.85%, tr_best:  78.88%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "epoch-97  lr=['0.0000100'], tr/val_loss:  0.613556/  0.889311, val:  69.58%, val_best:  69.58%, tr:  79.01%, tr_best:  79.01%, epoch time: 67.90 seconds, 1.13 minutes\n",
      "epoch-98  lr=['0.0000100'], tr/val_loss:  0.610770/  0.887363, val:  67.50%, val_best:  69.58%, tr:  78.94%, tr_best:  79.01%, epoch time: 67.97 seconds, 1.13 minutes\n",
      "epoch-99  lr=['0.0000100'], tr/val_loss:  0.609436/  0.893789, val:  68.33%, val_best:  69.58%, tr:  78.94%, tr_best:  79.01%, epoch time: 67.48 seconds, 1.12 minutes\n",
      "epoch-100 lr=['0.0000100'], tr/val_loss:  0.607361/  0.881801, val:  70.42%, val_best:  70.42%, tr:  78.90%, tr_best:  79.01%, epoch time: 67.36 seconds, 1.12 minutes\n",
      "epoch-101 lr=['0.0000100'], tr/val_loss:  0.603434/  0.895099, val:  67.50%, val_best:  70.42%, tr:  79.44%, tr_best:  79.44%, epoch time: 66.96 seconds, 1.12 minutes\n",
      "epoch-102 lr=['0.0000100'], tr/val_loss:  0.601340/  0.891052, val:  68.75%, val_best:  70.42%, tr:  79.24%, tr_best:  79.44%, epoch time: 67.47 seconds, 1.12 minutes\n",
      "epoch-103 lr=['0.0000100'], tr/val_loss:  0.597596/  0.893347, val:  69.58%, val_best:  70.42%, tr:  79.24%, tr_best:  79.44%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "epoch-104 lr=['0.0000100'], tr/val_loss:  0.597663/  0.898482, val:  68.75%, val_best:  70.42%, tr:  79.40%, tr_best:  79.44%, epoch time: 68.25 seconds, 1.14 minutes\n",
      "epoch-105 lr=['0.0000100'], tr/val_loss:  0.593424/  0.895695, val:  69.58%, val_best:  70.42%, tr:  79.62%, tr_best:  79.62%, epoch time: 67.38 seconds, 1.12 minutes\n",
      "epoch-106 lr=['0.0000100'], tr/val_loss:  0.591953/  0.887388, val:  69.58%, val_best:  70.42%, tr:  79.69%, tr_best:  79.69%, epoch time: 67.60 seconds, 1.13 minutes\n",
      "epoch-107 lr=['0.0000100'], tr/val_loss:  0.591102/  0.889123, val:  70.00%, val_best:  70.42%, tr:  79.82%, tr_best:  79.82%, epoch time: 67.15 seconds, 1.12 minutes\n",
      "epoch-108 lr=['0.0000100'], tr/val_loss:  0.586726/  0.888974, val:  70.00%, val_best:  70.42%, tr:  79.60%, tr_best:  79.82%, epoch time: 68.15 seconds, 1.14 minutes\n",
      "epoch-109 lr=['0.0000100'], tr/val_loss:  0.585283/  0.886617, val:  69.17%, val_best:  70.42%, tr:  79.73%, tr_best:  79.82%, epoch time: 67.06 seconds, 1.12 minutes\n",
      "epoch-110 lr=['0.0000100'], tr/val_loss:  0.581322/  0.886090, val:  69.58%, val_best:  70.42%, tr:  79.96%, tr_best:  79.96%, epoch time: 67.55 seconds, 1.13 minutes\n",
      "epoch-111 lr=['0.0000100'], tr/val_loss:  0.580515/  0.885554, val:  70.83%, val_best:  70.83%, tr:  79.76%, tr_best:  79.96%, epoch time: 67.62 seconds, 1.13 minutes\n",
      "epoch-112 lr=['0.0000100'], tr/val_loss:  0.578547/  0.890559, val:  70.42%, val_best:  70.83%, tr:  79.42%, tr_best:  79.96%, epoch time: 67.37 seconds, 1.12 minutes\n",
      "epoch-113 lr=['0.0000100'], tr/val_loss:  0.575470/  0.888089, val:  70.42%, val_best:  70.83%, tr:  79.78%, tr_best:  79.96%, epoch time: 67.72 seconds, 1.13 minutes\n",
      "epoch-114 lr=['0.0000100'], tr/val_loss:  0.576019/  0.898452, val:  70.00%, val_best:  70.83%, tr:  79.82%, tr_best:  79.96%, epoch time: 67.52 seconds, 1.13 minutes\n",
      "epoch-115 lr=['0.0000100'], tr/val_loss:  0.574214/  0.898178, val:  70.83%, val_best:  70.83%, tr:  79.80%, tr_best:  79.96%, epoch time: 67.64 seconds, 1.13 minutes\n",
      "epoch-116 lr=['0.0000100'], tr/val_loss:  0.573999/  0.892342, val:  69.58%, val_best:  70.83%, tr:  79.85%, tr_best:  79.96%, epoch time: 67.80 seconds, 1.13 minutes\n",
      "epoch-117 lr=['0.0000100'], tr/val_loss:  0.570875/  0.880591, val:  69.58%, val_best:  70.83%, tr:  79.82%, tr_best:  79.96%, epoch time: 67.27 seconds, 1.12 minutes\n",
      "epoch-118 lr=['0.0000100'], tr/val_loss:  0.567285/  0.877474, val:  71.67%, val_best:  71.67%, tr:  80.12%, tr_best:  80.12%, epoch time: 67.74 seconds, 1.13 minutes\n",
      "epoch-119 lr=['0.0000100'], tr/val_loss:  0.566360/  0.881921, val:  70.83%, val_best:  71.67%, tr:  79.85%, tr_best:  80.12%, epoch time: 67.94 seconds, 1.13 minutes\n",
      "epoch-120 lr=['0.0000100'], tr/val_loss:  0.564669/  0.878760, val:  70.00%, val_best:  71.67%, tr:  80.16%, tr_best:  80.16%, epoch time: 67.56 seconds, 1.13 minutes\n",
      "epoch-121 lr=['0.0000100'], tr/val_loss:  0.561311/  0.870564, val:  70.00%, val_best:  71.67%, tr:  79.85%, tr_best:  80.16%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "epoch-122 lr=['0.0000100'], tr/val_loss:  0.560207/  0.875722, val:  70.83%, val_best:  71.67%, tr:  80.37%, tr_best:  80.37%, epoch time: 67.50 seconds, 1.12 minutes\n",
      "epoch-123 lr=['0.0000100'], tr/val_loss:  0.558671/  0.871850, val:  70.00%, val_best:  71.67%, tr:  79.85%, tr_best:  80.37%, epoch time: 67.87 seconds, 1.13 minutes\n",
      "epoch-124 lr=['0.0000100'], tr/val_loss:  0.556338/  0.884027, val:  67.92%, val_best:  71.67%, tr:  80.07%, tr_best:  80.37%, epoch time: 67.87 seconds, 1.13 minutes\n",
      "epoch-125 lr=['0.0000100'], tr/val_loss:  0.555279/  0.879990, val:  70.00%, val_best:  71.67%, tr:  80.09%, tr_best:  80.37%, epoch time: 67.46 seconds, 1.12 minutes\n",
      "epoch-126 lr=['0.0000100'], tr/val_loss:  0.553590/  0.878037, val:  69.17%, val_best:  71.67%, tr:  80.55%, tr_best:  80.55%, epoch time: 67.42 seconds, 1.12 minutes\n",
      "epoch-127 lr=['0.0000100'], tr/val_loss:  0.553131/  0.880962, val:  68.33%, val_best:  71.67%, tr:  80.50%, tr_best:  80.55%, epoch time: 67.75 seconds, 1.13 minutes\n",
      "epoch-128 lr=['0.0000100'], tr/val_loss:  0.551181/  0.881361, val:  67.92%, val_best:  71.67%, tr:  81.06%, tr_best:  81.06%, epoch time: 68.29 seconds, 1.14 minutes\n",
      "epoch-129 lr=['0.0000100'], tr/val_loss:  0.548096/  0.885103, val:  66.67%, val_best:  71.67%, tr:  81.29%, tr_best:  81.29%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "epoch-130 lr=['0.0000100'], tr/val_loss:  0.546384/  0.878608, val:  68.33%, val_best:  71.67%, tr:  81.36%, tr_best:  81.36%, epoch time: 67.12 seconds, 1.12 minutes\n",
      "epoch-131 lr=['0.0000100'], tr/val_loss:  0.545739/  0.872047, val:  67.08%, val_best:  71.67%, tr:  81.22%, tr_best:  81.36%, epoch time: 67.54 seconds, 1.13 minutes\n",
      "epoch-132 lr=['0.0000100'], tr/val_loss:  0.544400/  0.870378, val:  67.92%, val_best:  71.67%, tr:  81.27%, tr_best:  81.36%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "epoch-133 lr=['0.0000100'], tr/val_loss:  0.542958/  0.875041, val:  67.08%, val_best:  71.67%, tr:  81.29%, tr_best:  81.36%, epoch time: 67.85 seconds, 1.13 minutes\n",
      "epoch-134 lr=['0.0000100'], tr/val_loss:  0.541215/  0.870344, val:  69.17%, val_best:  71.67%, tr:  81.38%, tr_best:  81.38%, epoch time: 67.62 seconds, 1.13 minutes\n",
      "epoch-135 lr=['0.0000100'], tr/val_loss:  0.538304/  0.867709, val:  70.42%, val_best:  71.67%, tr:  81.27%, tr_best:  81.38%, epoch time: 67.30 seconds, 1.12 minutes\n",
      "epoch-136 lr=['0.0000100'], tr/val_loss:  0.537047/  0.872271, val:  69.17%, val_best:  71.67%, tr:  81.33%, tr_best:  81.38%, epoch time: 67.95 seconds, 1.13 minutes\n",
      "epoch-137 lr=['0.0000100'], tr/val_loss:  0.535815/  0.876944, val:  69.58%, val_best:  71.67%, tr:  81.33%, tr_best:  81.38%, epoch time: 67.91 seconds, 1.13 minutes\n",
      "epoch-138 lr=['0.0000100'], tr/val_loss:  0.534068/  0.873855, val:  70.00%, val_best:  71.67%, tr:  81.18%, tr_best:  81.38%, epoch time: 67.64 seconds, 1.13 minutes\n",
      "epoch-139 lr=['0.0000100'], tr/val_loss:  0.532915/  0.872247, val:  70.42%, val_best:  71.67%, tr:  81.49%, tr_best:  81.49%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "epoch-140 lr=['0.0000100'], tr/val_loss:  0.532611/  0.867325, val:  69.58%, val_best:  71.67%, tr:  81.04%, tr_best:  81.49%, epoch time: 67.75 seconds, 1.13 minutes\n",
      "epoch-141 lr=['0.0000100'], tr/val_loss:  0.528922/  0.865324, val:  69.58%, val_best:  71.67%, tr:  81.47%, tr_best:  81.49%, epoch time: 67.83 seconds, 1.13 minutes\n",
      "epoch-142 lr=['0.0000100'], tr/val_loss:  0.528510/  0.863929, val:  69.58%, val_best:  71.67%, tr:  81.79%, tr_best:  81.79%, epoch time: 67.80 seconds, 1.13 minutes\n",
      "epoch-143 lr=['0.0000100'], tr/val_loss:  0.526325/  0.877636, val:  69.17%, val_best:  71.67%, tr:  81.58%, tr_best:  81.79%, epoch time: 67.92 seconds, 1.13 minutes\n",
      "epoch-144 lr=['0.0000100'], tr/val_loss:  0.524611/  0.878117, val:  67.92%, val_best:  71.67%, tr:  81.63%, tr_best:  81.79%, epoch time: 67.72 seconds, 1.13 minutes\n",
      "epoch-145 lr=['0.0000100'], tr/val_loss:  0.522638/  0.876409, val:  68.75%, val_best:  71.67%, tr:  81.83%, tr_best:  81.83%, epoch time: 67.50 seconds, 1.12 minutes\n",
      "epoch-146 lr=['0.0000100'], tr/val_loss:  0.522326/  0.882997, val:  69.58%, val_best:  71.67%, tr:  81.72%, tr_best:  81.83%, epoch time: 67.39 seconds, 1.12 minutes\n",
      "epoch-147 lr=['0.0000100'], tr/val_loss:  0.519813/  0.884745, val:  68.33%, val_best:  71.67%, tr:  81.76%, tr_best:  81.83%, epoch time: 67.83 seconds, 1.13 minutes\n",
      "epoch-148 lr=['0.0000100'], tr/val_loss:  0.519158/  0.878336, val:  70.00%, val_best:  71.67%, tr:  81.61%, tr_best:  81.83%, epoch time: 67.22 seconds, 1.12 minutes\n",
      "epoch-149 lr=['0.0000100'], tr/val_loss:  0.518232/  0.877722, val:  69.58%, val_best:  71.67%, tr:  81.76%, tr_best:  81.83%, epoch time: 67.47 seconds, 1.12 minutes\n",
      "epoch-150 lr=['0.0000100'], tr/val_loss:  0.516909/  0.879620, val:  69.58%, val_best:  71.67%, tr:  81.63%, tr_best:  81.83%, epoch time: 67.52 seconds, 1.13 minutes\n",
      "epoch-151 lr=['0.0000100'], tr/val_loss:  0.514399/  0.874674, val:  69.17%, val_best:  71.67%, tr:  81.65%, tr_best:  81.83%, epoch time: 67.56 seconds, 1.13 minutes\n",
      "epoch-152 lr=['0.0000100'], tr/val_loss:  0.512959/  0.879145, val:  68.75%, val_best:  71.67%, tr:  81.81%, tr_best:  81.83%, epoch time: 68.00 seconds, 1.13 minutes\n",
      "epoch-153 lr=['0.0000100'], tr/val_loss:  0.511220/  0.882910, val:  68.33%, val_best:  71.67%, tr:  82.10%, tr_best:  82.10%, epoch time: 67.58 seconds, 1.13 minutes\n",
      "epoch-154 lr=['0.0000100'], tr/val_loss:  0.509485/  0.882452, val:  68.75%, val_best:  71.67%, tr:  82.15%, tr_best:  82.15%, epoch time: 67.50 seconds, 1.12 minutes\n",
      "epoch-155 lr=['0.0000100'], tr/val_loss:  0.509116/  0.879140, val:  71.67%, val_best:  71.67%, tr:  82.08%, tr_best:  82.15%, epoch time: 67.46 seconds, 1.12 minutes\n",
      "epoch-156 lr=['0.0000100'], tr/val_loss:  0.507065/  0.874687, val:  71.25%, val_best:  71.67%, tr:  82.15%, tr_best:  82.15%, epoch time: 67.35 seconds, 1.12 minutes\n",
      "epoch-157 lr=['0.0000100'], tr/val_loss:  0.505439/  0.884144, val:  70.83%, val_best:  71.67%, tr:  82.46%, tr_best:  82.46%, epoch time: 67.45 seconds, 1.12 minutes\n",
      "epoch-158 lr=['0.0000100'], tr/val_loss:  0.506712/  0.876533, val:  71.67%, val_best:  71.67%, tr:  82.39%, tr_best:  82.46%, epoch time: 67.82 seconds, 1.13 minutes\n",
      "epoch-159 lr=['0.0000100'], tr/val_loss:  0.505594/  0.882059, val:  68.75%, val_best:  71.67%, tr:  82.17%, tr_best:  82.46%, epoch time: 68.14 seconds, 1.14 minutes\n",
      "epoch-160 lr=['0.0000100'], tr/val_loss:  0.502890/  0.876441, val:  70.83%, val_best:  71.67%, tr:  82.57%, tr_best:  82.57%, epoch time: 67.85 seconds, 1.13 minutes\n",
      "epoch-161 lr=['0.0000100'], tr/val_loss:  0.500213/  0.877787, val:  70.00%, val_best:  71.67%, tr:  82.78%, tr_best:  82.78%, epoch time: 67.98 seconds, 1.13 minutes\n",
      "epoch-162 lr=['0.0000100'], tr/val_loss:  0.497755/  0.877193, val:  71.67%, val_best:  71.67%, tr:  82.66%, tr_best:  82.78%, epoch time: 67.30 seconds, 1.12 minutes\n",
      "epoch-163 lr=['0.0000100'], tr/val_loss:  0.496718/  0.882008, val:  69.58%, val_best:  71.67%, tr:  82.64%, tr_best:  82.78%, epoch time: 67.60 seconds, 1.13 minutes\n",
      "epoch-164 lr=['0.0000100'], tr/val_loss:  0.494927/  0.885902, val:  70.83%, val_best:  71.67%, tr:  82.98%, tr_best:  82.98%, epoch time: 67.45 seconds, 1.12 minutes\n",
      "epoch-165 lr=['0.0000100'], tr/val_loss:  0.494272/  0.876265, val:  72.08%, val_best:  72.08%, tr:  83.00%, tr_best:  83.00%, epoch time: 67.66 seconds, 1.13 minutes\n",
      "epoch-166 lr=['0.0000100'], tr/val_loss:  0.492376/  0.878970, val:  69.58%, val_best:  72.08%, tr:  83.30%, tr_best:  83.30%, epoch time: 67.79 seconds, 1.13 minutes\n",
      "epoch-167 lr=['0.0000100'], tr/val_loss:  0.489976/  0.876789, val:  70.42%, val_best:  72.08%, tr:  83.72%, tr_best:  83.72%, epoch time: 67.77 seconds, 1.13 minutes\n",
      "epoch-168 lr=['0.0000100'], tr/val_loss:  0.488021/  0.880999, val:  71.67%, val_best:  72.08%, tr:  83.48%, tr_best:  83.72%, epoch time: 67.46 seconds, 1.12 minutes\n",
      "epoch-169 lr=['0.0000100'], tr/val_loss:  0.487017/  0.870940, val:  72.08%, val_best:  72.08%, tr:  83.63%, tr_best:  83.72%, epoch time: 67.67 seconds, 1.13 minutes\n",
      "epoch-170 lr=['0.0000100'], tr/val_loss:  0.486642/  0.865307, val:  72.08%, val_best:  72.08%, tr:  83.90%, tr_best:  83.90%, epoch time: 67.79 seconds, 1.13 minutes\n",
      "epoch-171 lr=['0.0000100'], tr/val_loss:  0.485696/  0.866763, val:  71.67%, val_best:  72.08%, tr:  83.84%, tr_best:  83.90%, epoch time: 67.87 seconds, 1.13 minutes\n",
      "epoch-172 lr=['0.0000100'], tr/val_loss:  0.484638/  0.868595, val:  72.50%, val_best:  72.50%, tr:  83.66%, tr_best:  83.90%, epoch time: 67.79 seconds, 1.13 minutes\n",
      "epoch-173 lr=['0.0000100'], tr/val_loss:  0.482775/  0.859697, val:  73.33%, val_best:  73.33%, tr:  83.52%, tr_best:  83.90%, epoch time: 67.53 seconds, 1.13 minutes\n",
      "epoch-174 lr=['0.0000100'], tr/val_loss:  0.482313/  0.862642, val:  72.92%, val_best:  73.33%, tr:  83.27%, tr_best:  83.90%, epoch time: 68.24 seconds, 1.14 minutes\n",
      "epoch-175 lr=['0.0000100'], tr/val_loss:  0.480846/  0.858730, val:  73.33%, val_best:  73.33%, tr:  83.72%, tr_best:  83.90%, epoch time: 68.04 seconds, 1.13 minutes\n",
      "epoch-176 lr=['0.0000100'], tr/val_loss:  0.478918/  0.851266, val:  73.33%, val_best:  73.33%, tr:  83.81%, tr_best:  83.90%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "epoch-177 lr=['0.0000100'], tr/val_loss:  0.479186/  0.859591, val:  72.92%, val_best:  73.33%, tr:  83.57%, tr_best:  83.90%, epoch time: 67.86 seconds, 1.13 minutes\n",
      "epoch-178 lr=['0.0000100'], tr/val_loss:  0.478030/  0.862317, val:  71.67%, val_best:  73.33%, tr:  83.61%, tr_best:  83.90%, epoch time: 67.25 seconds, 1.12 minutes\n",
      "epoch-179 lr=['0.0000100'], tr/val_loss:  0.477648/  0.866020, val:  71.67%, val_best:  73.33%, tr:  83.95%, tr_best:  83.95%, epoch time: 68.00 seconds, 1.13 minutes\n",
      "epoch-180 lr=['0.0000100'], tr/val_loss:  0.475648/  0.859377, val:  72.08%, val_best:  73.33%, tr:  84.04%, tr_best:  84.04%, epoch time: 67.79 seconds, 1.13 minutes\n",
      "epoch-181 lr=['0.0000100'], tr/val_loss:  0.473660/  0.866564, val:  72.50%, val_best:  73.33%, tr:  84.29%, tr_best:  84.29%, epoch time: 67.20 seconds, 1.12 minutes\n",
      "epoch-182 lr=['0.0000100'], tr/val_loss:  0.473518/  0.867600, val:  71.25%, val_best:  73.33%, tr:  84.11%, tr_best:  84.29%, epoch time: 67.57 seconds, 1.13 minutes\n",
      "epoch-183 lr=['0.0000100'], tr/val_loss:  0.471605/  0.857813, val:  70.83%, val_best:  73.33%, tr:  84.22%, tr_best:  84.29%, epoch time: 67.83 seconds, 1.13 minutes\n",
      "epoch-184 lr=['0.0000100'], tr/val_loss:  0.468270/  0.865039, val:  71.25%, val_best:  73.33%, tr:  84.15%, tr_best:  84.29%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "epoch-185 lr=['0.0000100'], tr/val_loss:  0.467096/  0.866694, val:  71.25%, val_best:  73.33%, tr:  84.56%, tr_best:  84.56%, epoch time: 67.86 seconds, 1.13 minutes\n",
      "epoch-186 lr=['0.0000100'], tr/val_loss:  0.467616/  0.870695, val:  71.25%, val_best:  73.33%, tr:  83.99%, tr_best:  84.56%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "epoch-187 lr=['0.0000100'], tr/val_loss:  0.464928/  0.865104, val:  72.08%, val_best:  73.33%, tr:  84.13%, tr_best:  84.56%, epoch time: 67.54 seconds, 1.13 minutes\n",
      "epoch-188 lr=['0.0000100'], tr/val_loss:  0.463239/  0.863026, val:  72.08%, val_best:  73.33%, tr:  84.45%, tr_best:  84.56%, epoch time: 68.00 seconds, 1.13 minutes\n",
      "epoch-189 lr=['0.0000100'], tr/val_loss:  0.462161/  0.861163, val:  72.08%, val_best:  73.33%, tr:  84.08%, tr_best:  84.56%, epoch time: 67.62 seconds, 1.13 minutes\n",
      "epoch-190 lr=['0.0000100'], tr/val_loss:  0.461080/  0.859199, val:  72.92%, val_best:  73.33%, tr:  84.27%, tr_best:  84.56%, epoch time: 67.60 seconds, 1.13 minutes\n",
      "epoch-191 lr=['0.0000100'], tr/val_loss:  0.459237/  0.862589, val:  72.92%, val_best:  73.33%, tr:  84.49%, tr_best:  84.56%, epoch time: 67.93 seconds, 1.13 minutes\n",
      "epoch-192 lr=['0.0000100'], tr/val_loss:  0.460159/  0.859313, val:  72.92%, val_best:  73.33%, tr:  84.69%, tr_best:  84.69%, epoch time: 67.53 seconds, 1.13 minutes\n",
      "epoch-193 lr=['0.0000100'], tr/val_loss:  0.458440/  0.865636, val:  73.33%, val_best:  73.33%, tr:  84.17%, tr_best:  84.69%, epoch time: 67.27 seconds, 1.12 minutes\n",
      "epoch-194 lr=['0.0000100'], tr/val_loss:  0.458423/  0.864847, val:  72.50%, val_best:  73.33%, tr:  84.51%, tr_best:  84.69%, epoch time: 67.15 seconds, 1.12 minutes\n",
      "epoch-195 lr=['0.0000100'], tr/val_loss:  0.458447/  0.857161, val:  72.08%, val_best:  73.33%, tr:  84.33%, tr_best:  84.69%, epoch time: 67.50 seconds, 1.13 minutes\n",
      "epoch-196 lr=['0.0000100'], tr/val_loss:  0.455348/  0.856757, val:  72.92%, val_best:  73.33%, tr:  84.56%, tr_best:  84.69%, epoch time: 66.93 seconds, 1.12 minutes\n",
      "epoch-197 lr=['0.0000100'], tr/val_loss:  0.453703/  0.853196, val:  73.33%, val_best:  73.33%, tr:  84.49%, tr_best:  84.69%, epoch time: 67.33 seconds, 1.12 minutes\n",
      "epoch-198 lr=['0.0000100'], tr/val_loss:  0.454810/  0.855103, val:  73.33%, val_best:  73.33%, tr:  84.65%, tr_best:  84.69%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "epoch-199 lr=['0.0000100'], tr/val_loss:  0.451765/  0.853237, val:  73.33%, val_best:  73.33%, tr:  84.54%, tr_best:  84.69%, epoch time: 67.24 seconds, 1.12 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a823d97c488745268882a73af6ffcc63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.84536</td></tr><tr><td>tr_epoch_loss</td><td>0.45176</td></tr><tr><td>val_acc_best</td><td>0.73333</td></tr><tr><td>val_acc_now</td><td>0.73333</td></tr><tr><td>val_loss</td><td>0.85324</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">confused-sweep-21</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hgdeyrd6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hgdeyrd6</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251104_012419-hgdeyrd6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s1z4b7z5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251104_051137-s1z4b7z5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/s1z4b7z5' target=\"_blank\">decent-sweep-28</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/s1z4b7z5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/s1z4b7z5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': False, 'unique_name': '20251104_051146_704', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.005, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.005\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch-0   lr=['0.0050000'], tr/val_loss:  1.069997/  1.377572, val:  46.25%, val_best:  46.25%, tr:  56.90%, tr_best:  56.90%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "epoch-1   lr=['0.0050000'], tr/val_loss:  0.685947/  0.840029, val:  66.67%, val_best:  66.67%, tr:  69.48%, tr_best:  69.48%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "epoch-2   lr=['0.0050000'], tr/val_loss:  0.533250/  0.827692, val:  70.42%, val_best:  70.42%, tr:  76.92%, tr_best:  76.92%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "epoch-3   lr=['0.0050000'], tr/val_loss:  0.404177/  0.829844, val:  76.67%, val_best:  76.67%, tr:  83.43%, tr_best:  83.43%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "epoch-4   lr=['0.0050000'], tr/val_loss:  0.334131/  0.760710, val:  78.75%, val_best:  78.75%, tr:  87.53%, tr_best:  87.53%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "epoch-5   lr=['0.0050000'], tr/val_loss:  0.267518/  0.660348, val:  80.00%, val_best:  80.00%, tr:  89.86%, tr_best:  89.86%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "epoch-6   lr=['0.0050000'], tr/val_loss:  0.240032/  0.900891, val:  80.83%, val_best:  80.83%, tr:  91.19%, tr_best:  91.19%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "epoch-7   lr=['0.0050000'], tr/val_loss:  0.200194/  0.711291, val:  77.50%, val_best:  80.83%, tr:  92.70%, tr_best:  92.70%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "epoch-8   lr=['0.0050000'], tr/val_loss:  0.169038/  0.779952, val:  83.33%, val_best:  83.33%, tr:  94.50%, tr_best:  94.50%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "epoch-9   lr=['0.0050000'], tr/val_loss:  0.145978/  0.722498, val:  82.08%, val_best:  83.33%, tr:  94.97%, tr_best:  94.97%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "epoch-10  lr=['0.0050000'], tr/val_loss:  0.109104/  0.797027, val:  81.25%, val_best:  83.33%, tr:  96.15%, tr_best:  96.15%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "epoch-11  lr=['0.0050000'], tr/val_loss:  0.090873/  0.735700, val:  82.08%, val_best:  83.33%, tr:  96.78%, tr_best:  96.78%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "epoch-12  lr=['0.0050000'], tr/val_loss:  0.092569/  0.556443, val:  83.75%, val_best:  83.75%, tr:  96.96%, tr_best:  96.96%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "epoch-13  lr=['0.0050000'], tr/val_loss:  0.104183/  0.626819, val:  83.33%, val_best:  83.75%, tr:  96.69%, tr_best:  96.96%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "epoch-14  lr=['0.0050000'], tr/val_loss:  0.076835/  0.766264, val:  82.50%, val_best:  83.75%, tr:  97.29%, tr_best:  97.29%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "epoch-15  lr=['0.0050000'], tr/val_loss:  0.053623/  0.753582, val:  84.17%, val_best:  84.17%, tr:  98.08%, tr_best:  98.08%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "epoch-16  lr=['0.0050000'], tr/val_loss:  0.043418/  0.668203, val:  84.58%, val_best:  84.58%, tr:  98.81%, tr_best:  98.81%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "epoch-17  lr=['0.0050000'], tr/val_loss:  0.047594/  0.797732, val:  83.75%, val_best:  84.58%, tr:  98.51%, tr_best:  98.81%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "epoch-18  lr=['0.0050000'], tr/val_loss:  0.048844/  0.912459, val:  85.00%, val_best:  85.00%, tr:  98.40%, tr_best:  98.81%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "epoch-19  lr=['0.0050000'], tr/val_loss:  0.023836/  0.883774, val:  84.58%, val_best:  85.00%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "epoch-20  lr=['0.0050000'], tr/val_loss:  0.063205/  1.246175, val:  80.00%, val_best:  85.00%, tr:  97.95%, tr_best:  99.39%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "epoch-21  lr=['0.0050000'], tr/val_loss:  0.056663/  1.023421, val:  86.25%, val_best:  86.25%, tr:  98.40%, tr_best:  99.39%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "epoch-22  lr=['0.0050000'], tr/val_loss:  0.018756/  0.896519, val:  85.00%, val_best:  86.25%, tr:  99.55%, tr_best:  99.55%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "epoch-23  lr=['0.0050000'], tr/val_loss:  0.013485/  1.002720, val:  84.58%, val_best:  86.25%, tr:  99.64%, tr_best:  99.64%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "epoch-24  lr=['0.0050000'], tr/val_loss:  0.018546/  0.708905, val:  89.17%, val_best:  89.17%, tr:  99.46%, tr_best:  99.64%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "epoch-25  lr=['0.0050000'], tr/val_loss:  0.045148/  0.828729, val:  79.58%, val_best:  89.17%, tr:  98.58%, tr_best:  99.64%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "epoch-26  lr=['0.0050000'], tr/val_loss:  0.035445/  1.023193, val:  82.92%, val_best:  89.17%, tr:  98.76%, tr_best:  99.64%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "epoch-27  lr=['0.0050000'], tr/val_loss:  0.015342/  0.808545, val:  87.08%, val_best:  89.17%, tr:  99.35%, tr_best:  99.64%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "epoch-28  lr=['0.0050000'], tr/val_loss:  0.002940/  0.755388, val:  87.50%, val_best:  89.17%, tr:  99.93%, tr_best:  99.93%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "epoch-29  lr=['0.0050000'], tr/val_loss:  0.001029/  0.752153, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "epoch-30  lr=['0.0050000'], tr/val_loss:  0.000527/  0.788045, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "epoch-31  lr=['0.0050000'], tr/val_loss:  0.000458/  0.780551, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "epoch-32  lr=['0.0050000'], tr/val_loss:  0.000366/  0.780533, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "epoch-33  lr=['0.0050000'], tr/val_loss:  0.000310/  0.786002, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "epoch-34  lr=['0.0050000'], tr/val_loss:  0.000278/  0.815796, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "epoch-35  lr=['0.0050000'], tr/val_loss:  0.000276/  0.818090, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "epoch-36  lr=['0.0050000'], tr/val_loss:  0.000244/  0.830177, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "epoch-37  lr=['0.0050000'], tr/val_loss:  0.000230/  0.825886, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "epoch-38  lr=['0.0050000'], tr/val_loss:  0.000208/  0.825463, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "epoch-39  lr=['0.0050000'], tr/val_loss:  0.000197/  0.812889, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "epoch-40  lr=['0.0050000'], tr/val_loss:  0.000190/  0.798914, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "epoch-41  lr=['0.0050000'], tr/val_loss:  0.000179/  0.806804, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "epoch-42  lr=['0.0050000'], tr/val_loss:  0.000168/  0.796469, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "epoch-43  lr=['0.0050000'], tr/val_loss:  0.000164/  0.799729, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "epoch-44  lr=['0.0050000'], tr/val_loss:  0.000153/  0.803311, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "epoch-45  lr=['0.0050000'], tr/val_loss:  0.000147/  0.817819, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "epoch-46  lr=['0.0050000'], tr/val_loss:  0.000140/  0.810943, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "epoch-47  lr=['0.0050000'], tr/val_loss:  0.000137/  0.816923, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "epoch-48  lr=['0.0050000'], tr/val_loss:  0.000131/  0.825840, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "epoch-49  lr=['0.0050000'], tr/val_loss:  0.000128/  0.823994, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "epoch-50  lr=['0.0050000'], tr/val_loss:  0.000121/  0.834181, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "epoch-51  lr=['0.0050000'], tr/val_loss:  0.000119/  0.840990, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "epoch-52  lr=['0.0050000'], tr/val_loss:  0.000114/  0.831867, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "epoch-53  lr=['0.0050000'], tr/val_loss:  0.000110/  0.838325, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "epoch-54  lr=['0.0050000'], tr/val_loss:  0.000109/  0.831923, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "epoch-55  lr=['0.0050000'], tr/val_loss:  0.000105/  0.836748, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "epoch-56  lr=['0.0050000'], tr/val_loss:  0.000107/  0.835882, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "epoch-57  lr=['0.0050000'], tr/val_loss:  0.000100/  0.832888, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "epoch-58  lr=['0.0050000'], tr/val_loss:  0.000097/  0.836895, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "epoch-59  lr=['0.0050000'], tr/val_loss:  0.000095/  0.840813, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "epoch-60  lr=['0.0050000'], tr/val_loss:  0.000092/  0.844832, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "epoch-61  lr=['0.0050000'], tr/val_loss:  0.000089/  0.848545, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "epoch-62  lr=['0.0050000'], tr/val_loss:  0.000088/  0.853810, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "epoch-63  lr=['0.0050000'], tr/val_loss:  0.000085/  0.856133, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "epoch-64  lr=['0.0050000'], tr/val_loss:  0.000084/  0.850024, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "epoch-65  lr=['0.0050000'], tr/val_loss:  0.000083/  0.854470, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "epoch-66  lr=['0.0050000'], tr/val_loss:  0.000081/  0.859935, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "epoch-67  lr=['0.0050000'], tr/val_loss:  0.000080/  0.861501, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "epoch-68  lr=['0.0050000'], tr/val_loss:  0.000077/  0.866131, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "epoch-69  lr=['0.0050000'], tr/val_loss:  0.000077/  0.867734, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "epoch-70  lr=['0.0050000'], tr/val_loss:  0.000077/  0.869749, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "epoch-71  lr=['0.0050000'], tr/val_loss:  0.000074/  0.863001, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "epoch-72  lr=['0.0050000'], tr/val_loss:  0.000074/  0.852685, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "epoch-73  lr=['0.0050000'], tr/val_loss:  0.000072/  0.860139, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "epoch-74  lr=['0.0050000'], tr/val_loss:  0.000073/  0.863549, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "epoch-75  lr=['0.0050000'], tr/val_loss:  0.000070/  0.870353, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "epoch-76  lr=['0.0050000'], tr/val_loss:  0.000068/  0.871618, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "epoch-77  lr=['0.0050000'], tr/val_loss:  0.000066/  0.875250, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "epoch-78  lr=['0.0050000'], tr/val_loss:  0.000064/  0.879052, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "epoch-79  lr=['0.0050000'], tr/val_loss:  0.000063/  0.878181, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "epoch-80  lr=['0.0050000'], tr/val_loss:  0.000063/  0.881596, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "epoch-81  lr=['0.0050000'], tr/val_loss:  0.000061/  0.881630, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "epoch-82  lr=['0.0050000'], tr/val_loss:  0.000060/  0.889751, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "epoch-83  lr=['0.0050000'], tr/val_loss:  0.000058/  0.893142, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "epoch-84  lr=['0.0050000'], tr/val_loss:  0.000057/  0.892482, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "epoch-85  lr=['0.0050000'], tr/val_loss:  0.000055/  0.887071, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "epoch-86  lr=['0.0050000'], tr/val_loss:  0.000054/  0.887086, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "epoch-87  lr=['0.0050000'], tr/val_loss:  0.000053/  0.888749, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "epoch-88  lr=['0.0050000'], tr/val_loss:  0.000054/  0.884848, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "epoch-89  lr=['0.0050000'], tr/val_loss:  0.000053/  0.891791, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "epoch-90  lr=['0.0050000'], tr/val_loss:  0.000052/  0.895167, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "epoch-91  lr=['0.0050000'], tr/val_loss:  0.000050/  0.904217, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "epoch-92  lr=['0.0050000'], tr/val_loss:  0.000049/  0.900939, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "epoch-93  lr=['0.0050000'], tr/val_loss:  0.000048/  0.903155, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "epoch-94  lr=['0.0050000'], tr/val_loss:  0.000048/  0.906809, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "epoch-95  lr=['0.0050000'], tr/val_loss:  0.000047/  0.907886, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "epoch-96  lr=['0.0050000'], tr/val_loss:  0.000046/  0.907073, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "epoch-97  lr=['0.0050000'], tr/val_loss:  0.000045/  0.906754, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "epoch-98  lr=['0.0050000'], tr/val_loss:  0.000045/  0.906592, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "epoch-99  lr=['0.0050000'], tr/val_loss:  0.000046/  0.906162, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "epoch-100 lr=['0.0050000'], tr/val_loss:  0.000044/  0.904039, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "epoch-101 lr=['0.0050000'], tr/val_loss:  0.000044/  0.905388, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "epoch-102 lr=['0.0050000'], tr/val_loss:  0.000043/  0.907326, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "epoch-103 lr=['0.0050000'], tr/val_loss:  0.000042/  0.909461, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "epoch-104 lr=['0.0050000'], tr/val_loss:  0.000042/  0.911379, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "epoch-105 lr=['0.0050000'], tr/val_loss:  0.000041/  0.909205, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "epoch-106 lr=['0.0050000'], tr/val_loss:  0.000041/  0.908743, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "epoch-107 lr=['0.0050000'], tr/val_loss:  0.000040/  0.905590, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "epoch-108 lr=['0.0050000'], tr/val_loss:  0.000040/  0.910590, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "epoch-109 lr=['0.0050000'], tr/val_loss:  0.000039/  0.905528, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "epoch-110 lr=['0.0050000'], tr/val_loss:  0.000039/  0.904316, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "epoch-111 lr=['0.0050000'], tr/val_loss:  0.000038/  0.904132, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "epoch-112 lr=['0.0050000'], tr/val_loss:  0.000038/  0.906401, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "epoch-113 lr=['0.0050000'], tr/val_loss:  0.000037/  0.908088, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "epoch-114 lr=['0.0050000'], tr/val_loss:  0.000036/  0.911675, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "epoch-115 lr=['0.0050000'], tr/val_loss:  0.000036/  0.912154, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "epoch-116 lr=['0.0050000'], tr/val_loss:  0.000036/  0.911891, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "epoch-117 lr=['0.0050000'], tr/val_loss:  0.000035/  0.916871, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "epoch-118 lr=['0.0050000'], tr/val_loss:  0.000035/  0.916951, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "epoch-119 lr=['0.0050000'], tr/val_loss:  0.000035/  0.918399, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "epoch-120 lr=['0.0050000'], tr/val_loss:  0.000035/  0.912223, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "epoch-121 lr=['0.0050000'], tr/val_loss:  0.000034/  0.912462, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "epoch-122 lr=['0.0050000'], tr/val_loss:  0.000034/  0.915571, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "epoch-123 lr=['0.0050000'], tr/val_loss:  0.000034/  0.915026, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "epoch-124 lr=['0.0050000'], tr/val_loss:  0.000034/  0.913544, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "epoch-125 lr=['0.0050000'], tr/val_loss:  0.000034/  0.913797, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "epoch-126 lr=['0.0050000'], tr/val_loss:  0.000033/  0.910698, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "epoch-127 lr=['0.0050000'], tr/val_loss:  0.000033/  0.909083, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "epoch-128 lr=['0.0050000'], tr/val_loss:  0.000033/  0.909050, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "epoch-129 lr=['0.0050000'], tr/val_loss:  0.000032/  0.910128, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "epoch-130 lr=['0.0050000'], tr/val_loss:  0.000031/  0.911077, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "epoch-131 lr=['0.0050000'], tr/val_loss:  0.000032/  0.913194, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "epoch-132 lr=['0.0050000'], tr/val_loss:  0.000031/  0.913931, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "epoch-133 lr=['0.0050000'], tr/val_loss:  0.000031/  0.915370, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "epoch-134 lr=['0.0050000'], tr/val_loss:  0.000031/  0.913448, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "epoch-135 lr=['0.0050000'], tr/val_loss:  0.000030/  0.926078, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "epoch-136 lr=['0.0050000'], tr/val_loss:  0.000030/  0.927817, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "epoch-137 lr=['0.0050000'], tr/val_loss:  0.000030/  0.930016, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "epoch-138 lr=['0.0050000'], tr/val_loss:  0.000029/  0.927196, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "epoch-139 lr=['0.0050000'], tr/val_loss:  0.000029/  0.929429, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "epoch-140 lr=['0.0050000'], tr/val_loss:  0.000029/  0.931487, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "epoch-141 lr=['0.0050000'], tr/val_loss:  0.000029/  0.926002, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "epoch-142 lr=['0.0050000'], tr/val_loss:  0.000029/  0.927320, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "epoch-143 lr=['0.0050000'], tr/val_loss:  0.000028/  0.927798, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "epoch-144 lr=['0.0050000'], tr/val_loss:  0.000028/  0.928109, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "epoch-145 lr=['0.0050000'], tr/val_loss:  0.000028/  0.925400, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "epoch-146 lr=['0.0050000'], tr/val_loss:  0.000028/  0.923195, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "epoch-147 lr=['0.0050000'], tr/val_loss:  0.000027/  0.923493, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "epoch-148 lr=['0.0050000'], tr/val_loss:  0.000028/  0.923680, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "epoch-149 lr=['0.0050000'], tr/val_loss:  0.000027/  0.927679, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "epoch-150 lr=['0.0050000'], tr/val_loss:  0.000027/  0.921314, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "epoch-151 lr=['0.0050000'], tr/val_loss:  0.000027/  0.920447, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "epoch-152 lr=['0.0050000'], tr/val_loss:  0.000027/  0.921502, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "epoch-153 lr=['0.0050000'], tr/val_loss:  0.000027/  0.921758, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "epoch-154 lr=['0.0050000'], tr/val_loss:  0.000026/  0.922222, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "epoch-155 lr=['0.0050000'], tr/val_loss:  0.000026/  0.922007, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "epoch-156 lr=['0.0050000'], tr/val_loss:  0.000026/  0.922730, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "epoch-157 lr=['0.0050000'], tr/val_loss:  0.000026/  0.923039, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "epoch-158 lr=['0.0050000'], tr/val_loss:  0.000026/  0.924610, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "epoch-159 lr=['0.0050000'], tr/val_loss:  0.000026/  0.923786, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "epoch-160 lr=['0.0050000'], tr/val_loss:  0.000025/  0.928419, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "epoch-161 lr=['0.0050000'], tr/val_loss:  0.000026/  0.935855, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "epoch-162 lr=['0.0050000'], tr/val_loss:  0.000025/  0.935356, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "epoch-163 lr=['0.0050000'], tr/val_loss:  0.000025/  0.938089, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "epoch-164 lr=['0.0050000'], tr/val_loss:  0.000025/  0.942785, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "epoch-165 lr=['0.0050000'], tr/val_loss:  0.000025/  0.942581, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "epoch-166 lr=['0.0050000'], tr/val_loss:  0.000024/  0.947525, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "epoch-167 lr=['0.0050000'], tr/val_loss:  0.000024/  0.943370, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "epoch-168 lr=['0.0050000'], tr/val_loss:  0.000023/  0.938938, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "epoch-169 lr=['0.0050000'], tr/val_loss:  0.000023/  0.944732, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "epoch-170 lr=['0.0050000'], tr/val_loss:  0.000024/  0.945606, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "epoch-171 lr=['0.0050000'], tr/val_loss:  0.000023/  0.946275, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "epoch-172 lr=['0.0050000'], tr/val_loss:  0.000023/  0.947183, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "epoch-173 lr=['0.0050000'], tr/val_loss:  0.000023/  0.949099, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "epoch-174 lr=['0.0050000'], tr/val_loss:  0.000023/  0.949482, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "epoch-175 lr=['0.0050000'], tr/val_loss:  0.000023/  0.955524, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "epoch-176 lr=['0.0050000'], tr/val_loss:  0.000023/  0.957093, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "epoch-177 lr=['0.0050000'], tr/val_loss:  0.000023/  0.959201, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "epoch-178 lr=['0.0050000'], tr/val_loss:  0.000022/  0.964601, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "epoch-179 lr=['0.0050000'], tr/val_loss:  0.000022/  0.963947, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "epoch-180 lr=['0.0050000'], tr/val_loss:  0.000022/  0.968823, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "epoch-181 lr=['0.0050000'], tr/val_loss:  0.000022/  0.971623, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "epoch-182 lr=['0.0050000'], tr/val_loss:  0.000022/  0.971170, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "epoch-183 lr=['0.0050000'], tr/val_loss:  0.000022/  0.970713, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "epoch-184 lr=['0.0050000'], tr/val_loss:  0.000022/  0.972573, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "epoch-185 lr=['0.0050000'], tr/val_loss:  0.000021/  0.970605, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "epoch-186 lr=['0.0050000'], tr/val_loss:  0.000021/  0.974913, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "epoch-187 lr=['0.0050000'], tr/val_loss:  0.000021/  0.975498, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "epoch-188 lr=['0.0050000'], tr/val_loss:  0.000021/  0.974592, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "epoch-189 lr=['0.0050000'], tr/val_loss:  0.000021/  0.979286, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "epoch-190 lr=['0.0050000'], tr/val_loss:  0.000021/  0.983430, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "epoch-191 lr=['0.0050000'], tr/val_loss:  0.000021/  0.980902, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "epoch-192 lr=['0.0050000'], tr/val_loss:  0.000021/  0.981642, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.66 seconds, 1.34 minutes\n",
      "epoch-193 lr=['0.0050000'], tr/val_loss:  0.000021/  0.983154, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "epoch-194 lr=['0.0050000'], tr/val_loss:  0.000021/  0.981239, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "epoch-195 lr=['0.0050000'], tr/val_loss:  0.000020/  0.984707, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "epoch-196 lr=['0.0050000'], tr/val_loss:  0.000020/  0.986918, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "epoch-197 lr=['0.0050000'], tr/val_loss:  0.000021/  0.983657, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "epoch-198 lr=['0.0050000'], tr/val_loss:  0.000020/  0.983323, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "epoch-199 lr=['0.0050000'], tr/val_loss:  0.000020/  0.984716, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a72d2cdc4a4161957a560db64af604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>2e-05</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.87917</td></tr><tr><td>val_loss</td><td>0.98472</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">decent-sweep-28</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/s1z4b7z5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/s1z4b7z5</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251104_051137-s1z4b7z5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1v9k5l1w with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251104_093300-1v9k5l1w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1v9k5l1w' target=\"_blank\">efficient-sweep-37</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1v9k5l1w' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1v9k5l1w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': False, 'unique_name': '20251104_093310_321', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2f9a9628b37638eec5575c56720d39aa\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 1bc451b987afbd0db0a2f6b2c2c70fe4\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2abefffa648dc088cd2271ffcb1b6e55\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 5226d1c3186ad61eaa496b197b310eda\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c9806e87ebb2f3e9bef81d2853a3474d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 907d9a80c5ee9829717b10e2d193d91f\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 00724b775a394b1c1571d17d55948856\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 164d32966f661e30c5d288c2082a1218\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 30567250fc090f206bd7595aa67d7018\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f6f2bd2ff320f82075b17bdd8cca0a0f\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  0.925916/  0.989112, val:  57.92%, val_best:  57.92%, tr:  62.94%, tr_best:  62.94%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  0.564514/  0.839511, val:  67.08%, val_best:  67.08%, tr:  74.30%, tr_best:  74.30%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  0.457205/  0.914295, val:  66.25%, val_best:  67.08%, tr:  79.37%, tr_best:  79.37%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  0.367471/  0.974441, val:  69.17%, val_best:  69.17%, tr:  83.59%, tr_best:  83.59%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  0.299134/  0.842812, val:  73.33%, val_best:  73.33%, tr:  87.98%, tr_best:  87.98%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  0.228124/  0.810202, val:  78.33%, val_best:  78.33%, tr:  90.78%, tr_best:  90.78%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  0.171199/  1.040794, val:  75.83%, val_best:  78.33%, tr:  93.55%, tr_best:  93.55%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  0.123832/  0.919402, val:  80.00%, val_best:  80.00%, tr:  95.33%, tr_best:  95.33%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  0.102477/  0.901585, val:  82.08%, val_best:  82.08%, tr:  96.46%, tr_best:  96.46%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.085199/  0.906907, val:  80.83%, val_best:  82.08%, tr:  97.20%, tr_best:  97.20%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.062914/  0.878551, val:  80.00%, val_best:  82.08%, tr:  97.81%, tr_best:  97.81%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.042985/  0.907380, val:  80.42%, val_best:  82.08%, tr:  98.49%, tr_best:  98.49%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.044134/  0.858873, val:  81.25%, val_best:  82.08%, tr:  98.69%, tr_best:  98.69%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.033988/  0.887197, val:  82.08%, val_best:  82.08%, tr:  99.03%, tr_best:  99.03%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.016824/  0.848758, val:  82.92%, val_best:  82.92%, tr:  99.71%, tr_best:  99.71%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.020463/  0.880680, val:  81.67%, val_best:  82.92%, tr:  99.41%, tr_best:  99.71%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.011179/  0.911499, val:  82.50%, val_best:  82.92%, tr:  99.77%, tr_best:  99.77%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.018846/  0.970308, val:  84.58%, val_best:  84.58%, tr:  99.53%, tr_best:  99.77%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.006927/  1.016064, val:  81.25%, val_best:  84.58%, tr:  99.93%, tr_best:  99.93%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.009004/  0.908445, val:  84.17%, val_best:  84.58%, tr:  99.82%, tr_best:  99.93%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.002705/  0.964928, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.002159/  0.991225, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.001851/  0.984848, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.001712/  1.037720, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.002839/  0.984156, val:  82.50%, val_best:  84.58%, tr:  99.93%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.002449/  1.087482, val:  81.25%, val_best:  84.58%, tr:  99.98%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.001387/  1.052662, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.001387/  1.076954, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.001125/  1.049812, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.000989/  1.070185, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.000944/  1.084848, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.000843/  1.075572, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.000786/  1.072009, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.000764/  1.077109, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.000759/  1.074469, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.000745/  1.083884, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.000695/  1.070741, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.000704/  1.098731, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.000668/  1.079400, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.000638/  1.099084, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.000651/  1.101724, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.000565/  1.082752, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.000583/  1.080978, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.000554/  1.083423, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.000546/  1.070734, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.000526/  1.095405, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.000537/  1.075469, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.000512/  1.065303, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.000497/  1.073717, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.000477/  1.073063, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.000475/  1.071153, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.000477/  1.084615, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.000461/  1.092132, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.000453/  1.091658, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.000428/  1.090979, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.000430/  1.088903, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.000432/  1.119860, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.000412/  1.108744, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.000392/  1.119068, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.000411/  1.086371, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.000367/  1.115743, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.000355/  1.111230, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.000352/  1.110189, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.000343/  1.126572, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.000326/  1.099525, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.000325/  1.127308, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.000317/  1.126537, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.000315/  1.124497, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.000312/  1.129022, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.000312/  1.121912, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.000313/  1.104666, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.000302/  1.145758, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.000315/  1.129984, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.000292/  1.117856, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.000291/  1.108469, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.000276/  1.142034, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.000284/  1.135539, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.000275/  1.129767, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.000285/  1.129388, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.000285/  1.127149, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.000268/  1.139256, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.000267/  1.142634, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.000261/  1.143604, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.000261/  1.145788, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.000256/  1.153609, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.000251/  1.147538, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.000245/  1.151936, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.000245/  1.159992, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.000249/  1.165671, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.000239/  1.154286, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.000242/  1.159652, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.000219/  1.150835, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.000222/  1.161028, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.000217/  1.160401, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.000215/  1.145707, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.000217/  1.133220, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.000211/  1.145074, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.000208/  1.168690, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.000206/  1.154872, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.000209/  1.158372, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  0.000206/  1.160845, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  0.000206/  1.175320, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  0.000200/  1.177529, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  0.000194/  1.160442, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  0.000195/  1.171480, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  0.000192/  1.160633, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  0.000191/  1.168923, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  0.000184/  1.172482, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  0.000181/  1.159167, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  0.000188/  1.166462, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  0.000181/  1.157347, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  0.000179/  1.160476, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  0.000179/  1.164130, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  0.000180/  1.163738, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  0.000176/  1.170277, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  0.000178/  1.172800, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  0.000174/  1.174371, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  0.000174/  1.177548, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  0.000171/  1.172793, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  0.000166/  1.182350, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  0.000171/  1.195230, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  0.000162/  1.193746, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  0.000157/  1.203869, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  0.000159/  1.207623, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  0.000162/  1.201953, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  0.000151/  1.202169, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  0.000152/  1.201984, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  0.000157/  1.204235, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  0.000148/  1.203972, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  0.000151/  1.206005, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  0.000147/  1.206256, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  0.000142/  1.203088, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  0.000150/  1.212792, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  0.000146/  1.205981, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  0.000144/  1.209204, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  0.000143/  1.208702, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  0.000138/  1.214094, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  0.000139/  1.206343, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  0.000136/  1.204289, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  0.000136/  1.206771, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  0.000136/  1.204602, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  0.000128/  1.204136, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  0.000129/  1.212791, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  0.000129/  1.221011, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  0.000129/  1.217100, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  0.000128/  1.212934, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  0.000126/  1.212594, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  0.000128/  1.211055, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  0.000123/  1.214929, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  0.000125/  1.209974, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  0.000123/  1.222705, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  0.000121/  1.229767, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  0.000124/  1.225098, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  0.000120/  1.214464, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  0.000122/  1.216065, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  0.000123/  1.227916, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  0.000121/  1.222302, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  0.000116/  1.233493, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  0.000119/  1.231097, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  0.000117/  1.224512, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  0.000118/  1.227853, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  0.000115/  1.225354, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  0.000116/  1.219580, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  0.000115/  1.224200, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  0.000115/  1.222216, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  0.000110/  1.218784, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  0.000112/  1.216247, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  0.000110/  1.215008, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  0.000113/  1.197083, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  0.000108/  1.208363, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  0.000109/  1.203363, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  0.000107/  1.205297, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  0.000104/  1.201306, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  0.000106/  1.196617, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  0.000106/  1.190443, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  0.000104/  1.192703, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  0.000107/  1.192520, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  0.000104/  1.208005, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  0.000104/  1.191951, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  0.000102/  1.206007, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  0.000101/  1.208416, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  0.000099/  1.211450, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  0.000101/  1.224804, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  0.000099/  1.220546, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  0.000101/  1.219698, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  0.000098/  1.226756, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  0.000099/  1.214048, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  0.000099/  1.211152, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  0.000096/  1.208756, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  0.000094/  1.211749, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  0.000096/  1.216634, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  0.000094/  1.225428, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  0.000093/  1.218760, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  0.000094/  1.217380, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  0.000093/  1.212799, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  0.000091/  1.214711, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  0.000093/  1.216545, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  0.000092/  1.211005, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  0.000091/  1.215503, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  0.000094/  1.218483, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1078fdca1d914635ad907be3f93c96d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>9e-05</td></tr><tr><td>val_acc_best</td><td>0.84583</td></tr><tr><td>val_acc_now</td><td>0.825</td></tr><tr><td>val_loss</td><td>1.21848</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-sweep-37</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1v9k5l1w' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1v9k5l1w</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251104_093300-1v9k5l1w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x7h69sau with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251104_135515-x7h69sau</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x7h69sau' target=\"_blank\">firm-sweep-46</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x7h69sau' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x7h69sau</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': False, 'unique_name': '20251104_135525_255', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.005, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 18, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f1351bdee3d35c47af449525e007adf4\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 713fc8490e508dff15bed30e755e5ae6\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 4e1f85d1c0ff71c6a0df58e4542628f1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 626851829f1152228185abed5f6dca5e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 3c558307d41237628ef99b773b1c784f\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 81e44cafc682693b4b2086c7f81d224b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = b36895cc577f68764ec3fb2cd889299e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = efeaf39262f61e3ee121af192099af65\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a15007b6dd509fde13c404b80435835e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 6fa31eb5ad96d639190daf0921d987e2\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=0, sg_width=8, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=0, sg_width=8, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.005\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch-0   lr=['0.0050000'], tr/val_loss:  1.016397/  1.104702, val:  55.42%, val_best:  55.42%, tr:  58.05%, tr_best:  58.05%, epoch time: 71.07 seconds, 1.18 minutes\n",
      "epoch-1   lr=['0.0050000'], tr/val_loss:  0.686732/  0.802856, val:  68.33%, val_best:  68.33%, tr:  71.30%, tr_best:  71.30%, epoch time: 71.10 seconds, 1.18 minutes\n",
      "epoch-2   lr=['0.0050000'], tr/val_loss:  0.499140/  0.783709, val:  72.50%, val_best:  72.50%, tr:  79.85%, tr_best:  79.85%, epoch time: 71.85 seconds, 1.20 minutes\n",
      "epoch-3   lr=['0.0050000'], tr/val_loss:  0.383677/  0.768364, val:  81.25%, val_best:  81.25%, tr:  86.14%, tr_best:  86.14%, epoch time: 71.58 seconds, 1.19 minutes\n",
      "epoch-4   lr=['0.0050000'], tr/val_loss:  0.299819/  0.848269, val:  76.25%, val_best:  81.25%, tr:  89.18%, tr_best:  89.18%, epoch time: 71.60 seconds, 1.19 minutes\n",
      "epoch-5   lr=['0.0050000'], tr/val_loss:  0.252461/  0.778122, val:  81.67%, val_best:  81.67%, tr:  90.40%, tr_best:  90.40%, epoch time: 71.42 seconds, 1.19 minutes\n",
      "epoch-6   lr=['0.0050000'], tr/val_loss:  0.225165/  0.736244, val:  80.42%, val_best:  81.67%, tr:  92.20%, tr_best:  92.20%, epoch time: 70.95 seconds, 1.18 minutes\n",
      "epoch-7   lr=['0.0050000'], tr/val_loss:  0.201501/  0.985231, val:  81.67%, val_best:  81.67%, tr:  93.26%, tr_best:  93.26%, epoch time: 71.76 seconds, 1.20 minutes\n",
      "epoch-8   lr=['0.0050000'], tr/val_loss:  0.154942/  0.719555, val:  85.42%, val_best:  85.42%, tr:  95.13%, tr_best:  95.13%, epoch time: 71.11 seconds, 1.19 minutes\n",
      "epoch-9   lr=['0.0050000'], tr/val_loss:  0.113310/  0.739496, val:  84.58%, val_best:  85.42%, tr:  96.51%, tr_best:  96.51%, epoch time: 71.08 seconds, 1.18 minutes\n",
      "epoch-10  lr=['0.0050000'], tr/val_loss:  0.119440/  0.834022, val:  82.92%, val_best:  85.42%, tr:  95.72%, tr_best:  96.51%, epoch time: 71.35 seconds, 1.19 minutes\n",
      "epoch-11  lr=['0.0050000'], tr/val_loss:  0.128848/  0.794832, val:  80.42%, val_best:  85.42%, tr:  95.76%, tr_best:  96.51%, epoch time: 70.99 seconds, 1.18 minutes\n",
      "epoch-12  lr=['0.0050000'], tr/val_loss:  0.110109/  0.917824, val:  77.50%, val_best:  85.42%, tr:  96.42%, tr_best:  96.51%, epoch time: 70.86 seconds, 1.18 minutes\n",
      "epoch-13  lr=['0.0050000'], tr/val_loss:  0.119054/  0.547910, val:  89.17%, val_best:  89.17%, tr:  95.72%, tr_best:  96.51%, epoch time: 71.19 seconds, 1.19 minutes\n",
      "epoch-14  lr=['0.0050000'], tr/val_loss:  0.077853/  0.562517, val:  87.08%, val_best:  89.17%, tr:  97.61%, tr_best:  97.61%, epoch time: 70.72 seconds, 1.18 minutes\n",
      "epoch-15  lr=['0.0050000'], tr/val_loss:  0.089517/  0.663572, val:  87.92%, val_best:  89.17%, tr:  96.98%, tr_best:  97.61%, epoch time: 71.53 seconds, 1.19 minutes\n",
      "epoch-16  lr=['0.0050000'], tr/val_loss:  0.070817/  0.780109, val:  88.33%, val_best:  89.17%, tr:  97.50%, tr_best:  97.61%, epoch time: 70.54 seconds, 1.18 minutes\n",
      "epoch-17  lr=['0.0050000'], tr/val_loss:  0.061013/  0.684637, val:  88.33%, val_best:  89.17%, tr:  98.04%, tr_best:  98.04%, epoch time: 71.35 seconds, 1.19 minutes\n",
      "epoch-18  lr=['0.0050000'], tr/val_loss:  0.067887/  0.831091, val:  83.75%, val_best:  89.17%, tr:  97.97%, tr_best:  98.04%, epoch time: 70.84 seconds, 1.18 minutes\n",
      "epoch-19  lr=['0.0050000'], tr/val_loss:  0.058160/  0.970906, val:  83.33%, val_best:  89.17%, tr:  98.22%, tr_best:  98.22%, epoch time: 71.03 seconds, 1.18 minutes\n",
      "epoch-20  lr=['0.0050000'], tr/val_loss:  0.033741/  0.925129, val:  84.17%, val_best:  89.17%, tr:  98.92%, tr_best:  98.92%, epoch time: 71.46 seconds, 1.19 minutes\n",
      "epoch-21  lr=['0.0050000'], tr/val_loss:  0.023990/  0.959790, val:  85.42%, val_best:  89.17%, tr:  99.26%, tr_best:  99.26%, epoch time: 71.04 seconds, 1.18 minutes\n",
      "epoch-22  lr=['0.0050000'], tr/val_loss:  0.012397/  0.791267, val:  86.25%, val_best:  89.17%, tr:  99.71%, tr_best:  99.71%, epoch time: 71.32 seconds, 1.19 minutes\n",
      "epoch-23  lr=['0.0050000'], tr/val_loss:  0.010278/  0.866004, val:  87.50%, val_best:  89.17%, tr:  99.64%, tr_best:  99.71%, epoch time: 71.51 seconds, 1.19 minutes\n",
      "epoch-24  lr=['0.0050000'], tr/val_loss:  0.005177/  0.865310, val:  87.08%, val_best:  89.17%, tr:  99.89%, tr_best:  99.89%, epoch time: 70.74 seconds, 1.18 minutes\n",
      "epoch-25  lr=['0.0050000'], tr/val_loss:  0.001010/  0.943292, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "epoch-26  lr=['0.0050000'], tr/val_loss:  0.000603/  0.919357, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.65 seconds, 1.18 minutes\n",
      "epoch-27  lr=['0.0050000'], tr/val_loss:  0.000526/  0.928109, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.06 seconds, 1.18 minutes\n",
      "epoch-28  lr=['0.0050000'], tr/val_loss:  0.000317/  0.904883, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.74 seconds, 1.18 minutes\n",
      "epoch-29  lr=['0.0050000'], tr/val_loss:  0.000272/  0.897374, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.84 seconds, 1.18 minutes\n",
      "epoch-30  lr=['0.0050000'], tr/val_loss:  0.000252/  0.922653, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.82 seconds, 1.18 minutes\n",
      "epoch-31  lr=['0.0050000'], tr/val_loss:  0.000238/  0.910860, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.25 seconds, 1.19 minutes\n",
      "epoch-32  lr=['0.0050000'], tr/val_loss:  0.000210/  0.922993, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.88 seconds, 1.18 minutes\n",
      "epoch-33  lr=['0.0050000'], tr/val_loss:  0.000194/  0.931750, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.57 seconds, 1.19 minutes\n",
      "epoch-34  lr=['0.0050000'], tr/val_loss:  0.000183/  0.939517, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.55 seconds, 1.18 minutes\n",
      "epoch-35  lr=['0.0050000'], tr/val_loss:  0.000176/  0.933950, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.86 seconds, 1.18 minutes\n",
      "epoch-36  lr=['0.0050000'], tr/val_loss:  0.000171/  0.932180, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.26 seconds, 1.19 minutes\n",
      "epoch-37  lr=['0.0050000'], tr/val_loss:  0.000166/  0.934533, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.84 seconds, 1.18 minutes\n",
      "epoch-38  lr=['0.0050000'], tr/val_loss:  0.000159/  0.931203, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.61 seconds, 1.18 minutes\n",
      "epoch-39  lr=['0.0050000'], tr/val_loss:  0.000157/  0.946600, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.77 seconds, 1.18 minutes\n",
      "epoch-40  lr=['0.0050000'], tr/val_loss:  0.000131/  0.953463, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.98 seconds, 1.18 minutes\n",
      "epoch-41  lr=['0.0050000'], tr/val_loss:  0.000127/  0.954700, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.30 seconds, 1.19 minutes\n",
      "epoch-42  lr=['0.0050000'], tr/val_loss:  0.000121/  0.967956, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.31 seconds, 1.19 minutes\n",
      "epoch-43  lr=['0.0050000'], tr/val_loss:  0.000116/  0.964916, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.49 seconds, 1.19 minutes\n",
      "epoch-44  lr=['0.0050000'], tr/val_loss:  0.000113/  0.968059, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.02 seconds, 1.18 minutes\n",
      "epoch-45  lr=['0.0050000'], tr/val_loss:  0.000110/  0.971179, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.51 seconds, 1.19 minutes\n",
      "epoch-46  lr=['0.0050000'], tr/val_loss:  0.000106/  0.965092, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.21 seconds, 1.19 minutes\n",
      "epoch-47  lr=['0.0050000'], tr/val_loss:  0.000103/  0.977389, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.98 seconds, 1.18 minutes\n",
      "epoch-48  lr=['0.0050000'], tr/val_loss:  0.000098/  0.970664, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.40 seconds, 1.19 minutes\n",
      "epoch-49  lr=['0.0050000'], tr/val_loss:  0.000096/  0.972613, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.59 seconds, 1.19 minutes\n",
      "epoch-50  lr=['0.0050000'], tr/val_loss:  0.000097/  0.965779, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.97 seconds, 1.18 minutes\n",
      "epoch-51  lr=['0.0050000'], tr/val_loss:  0.000091/  0.963702, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.79 seconds, 1.18 minutes\n",
      "epoch-52  lr=['0.0050000'], tr/val_loss:  0.000090/  0.968223, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.03 seconds, 1.18 minutes\n",
      "epoch-53  lr=['0.0050000'], tr/val_loss:  0.000088/  0.968161, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.27 seconds, 1.19 minutes\n",
      "epoch-54  lr=['0.0050000'], tr/val_loss:  0.000087/  0.966007, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.18 seconds, 1.19 minutes\n",
      "epoch-55  lr=['0.0050000'], tr/val_loss:  0.000084/  0.969035, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.58 seconds, 1.19 minutes\n",
      "epoch-56  lr=['0.0050000'], tr/val_loss:  0.000081/  0.976977, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.60 seconds, 1.19 minutes\n",
      "epoch-57  lr=['0.0050000'], tr/val_loss:  0.000080/  0.979725, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.87 seconds, 1.18 minutes\n",
      "epoch-58  lr=['0.0050000'], tr/val_loss:  0.000077/  0.982595, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.81 seconds, 1.18 minutes\n",
      "epoch-59  lr=['0.0050000'], tr/val_loss:  0.000068/  0.987312, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.79 seconds, 1.20 minutes\n",
      "epoch-60  lr=['0.0050000'], tr/val_loss:  0.000066/  0.986799, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.20 seconds, 1.19 minutes\n",
      "epoch-61  lr=['0.0050000'], tr/val_loss:  0.000064/  0.987208, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.30 seconds, 1.19 minutes\n",
      "epoch-62  lr=['0.0050000'], tr/val_loss:  0.000063/  0.979548, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.37 seconds, 1.19 minutes\n",
      "epoch-63  lr=['0.0050000'], tr/val_loss:  0.000063/  0.981701, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.90 seconds, 1.20 minutes\n",
      "epoch-64  lr=['0.0050000'], tr/val_loss:  0.000060/  0.980693, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.95 seconds, 1.18 minutes\n",
      "epoch-65  lr=['0.0050000'], tr/val_loss:  0.000060/  0.987090, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.02 seconds, 1.18 minutes\n",
      "epoch-66  lr=['0.0050000'], tr/val_loss:  0.000058/  0.987946, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.11 seconds, 1.19 minutes\n",
      "epoch-67  lr=['0.0050000'], tr/val_loss:  0.000059/  0.981233, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.44 seconds, 1.19 minutes\n",
      "epoch-68  lr=['0.0050000'], tr/val_loss:  0.000057/  0.984503, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.35 seconds, 1.19 minutes\n",
      "epoch-69  lr=['0.0050000'], tr/val_loss:  0.000056/  0.987270, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.97 seconds, 1.18 minutes\n",
      "epoch-70  lr=['0.0050000'], tr/val_loss:  0.000056/  0.987985, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.41 seconds, 1.19 minutes\n",
      "epoch-71  lr=['0.0050000'], tr/val_loss:  0.000055/  0.992669, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.13 seconds, 1.19 minutes\n",
      "epoch-72  lr=['0.0050000'], tr/val_loss:  0.000052/  0.988702, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.85 seconds, 1.18 minutes\n",
      "epoch-73  lr=['0.0050000'], tr/val_loss:  0.000053/  0.983990, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.44 seconds, 1.19 minutes\n",
      "epoch-74  lr=['0.0050000'], tr/val_loss:  0.000052/  0.989895, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.07 seconds, 1.18 minutes\n",
      "epoch-75  lr=['0.0050000'], tr/val_loss:  0.000051/  0.990729, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.35 seconds, 1.19 minutes\n",
      "epoch-76  lr=['0.0050000'], tr/val_loss:  0.000050/  0.995246, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.94 seconds, 1.18 minutes\n",
      "epoch-77  lr=['0.0050000'], tr/val_loss:  0.000049/  0.988609, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.74 seconds, 1.18 minutes\n",
      "epoch-78  lr=['0.0050000'], tr/val_loss:  0.000049/  0.987581, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.09 seconds, 1.18 minutes\n",
      "epoch-79  lr=['0.0050000'], tr/val_loss:  0.000048/  0.984274, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.12 seconds, 1.19 minutes\n",
      "epoch-80  lr=['0.0050000'], tr/val_loss:  0.000048/  0.979757, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.98 seconds, 1.18 minutes\n",
      "epoch-81  lr=['0.0050000'], tr/val_loss:  0.000047/  0.981242, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.09 seconds, 1.18 minutes\n",
      "epoch-82  lr=['0.0050000'], tr/val_loss:  0.000047/  0.978637, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.04 seconds, 1.18 minutes\n",
      "epoch-83  lr=['0.0050000'], tr/val_loss:  0.000046/  0.977170, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.46 seconds, 1.19 minutes\n",
      "epoch-84  lr=['0.0050000'], tr/val_loss:  0.000046/  0.974292, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.12 seconds, 1.19 minutes\n",
      "epoch-85  lr=['0.0050000'], tr/val_loss:  0.000040/  0.976557, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.23 seconds, 1.19 minutes\n",
      "epoch-86  lr=['0.0050000'], tr/val_loss:  0.000039/  0.979383, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.95 seconds, 1.18 minutes\n",
      "epoch-87  lr=['0.0050000'], tr/val_loss:  0.000039/  0.982575, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.13 seconds, 1.19 minutes\n",
      "epoch-88  lr=['0.0050000'], tr/val_loss:  0.000038/  0.985013, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "epoch-89  lr=['0.0050000'], tr/val_loss:  0.000038/  0.984543, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "epoch-90  lr=['0.0050000'], tr/val_loss:  0.000037/  0.997586, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.11 seconds, 1.19 minutes\n",
      "epoch-91  lr=['0.0050000'], tr/val_loss:  0.000037/  0.997702, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-92  lr=['0.0050000'], tr/val_loss:  0.000036/  1.003158, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.96 seconds, 1.20 minutes\n",
      "epoch-93  lr=['0.0050000'], tr/val_loss:  0.000036/  1.000919, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.51 seconds, 1.19 minutes\n",
      "epoch-94  lr=['0.0050000'], tr/val_loss:  0.000036/  0.995320, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.05 seconds, 1.18 minutes\n",
      "epoch-95  lr=['0.0050000'], tr/val_loss:  0.000035/  1.002083, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.26 seconds, 1.19 minutes\n",
      "epoch-96  lr=['0.0050000'], tr/val_loss:  0.000034/  1.004893, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.37 seconds, 1.19 minutes\n",
      "epoch-97  lr=['0.0050000'], tr/val_loss:  0.000035/  1.008222, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.19 seconds, 1.19 minutes\n",
      "epoch-98  lr=['0.0050000'], tr/val_loss:  0.000034/  1.010278, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.00 seconds, 1.18 minutes\n",
      "epoch-99  lr=['0.0050000'], tr/val_loss:  0.000035/  1.006499, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.24 seconds, 1.19 minutes\n",
      "epoch-100 lr=['0.0050000'], tr/val_loss:  0.000033/  1.006678, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.34 seconds, 1.19 minutes\n",
      "epoch-101 lr=['0.0050000'], tr/val_loss:  0.000033/  1.008693, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.35 seconds, 1.19 minutes\n",
      "epoch-102 lr=['0.0050000'], tr/val_loss:  0.000034/  1.002481, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.12 seconds, 1.19 minutes\n",
      "epoch-103 lr=['0.0050000'], tr/val_loss:  0.000034/  1.005053, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.25 seconds, 1.19 minutes\n",
      "epoch-104 lr=['0.0050000'], tr/val_loss:  0.000032/  1.001199, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.53 seconds, 1.19 minutes\n",
      "epoch-105 lr=['0.0050000'], tr/val_loss:  0.000032/  0.996835, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "epoch-106 lr=['0.0050000'], tr/val_loss:  0.000031/  0.994536, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.71 seconds, 1.20 minutes\n",
      "epoch-107 lr=['0.0050000'], tr/val_loss:  0.000031/  1.001527, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.68 seconds, 1.18 minutes\n",
      "epoch-108 lr=['0.0050000'], tr/val_loss:  0.000031/  1.004794, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.35 seconds, 1.19 minutes\n",
      "epoch-109 lr=['0.0050000'], tr/val_loss:  0.000031/  1.001846, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.05 seconds, 1.18 minutes\n",
      "epoch-110 lr=['0.0050000'], tr/val_loss:  0.000031/  0.991250, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.08 seconds, 1.18 minutes\n",
      "epoch-111 lr=['0.0050000'], tr/val_loss:  0.000029/  0.990366, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.98 seconds, 1.18 minutes\n",
      "epoch-112 lr=['0.0050000'], tr/val_loss:  0.000029/  0.990129, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.40 seconds, 1.19 minutes\n",
      "epoch-113 lr=['0.0050000'], tr/val_loss:  0.000029/  0.989389, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.32 seconds, 1.19 minutes\n",
      "epoch-114 lr=['0.0050000'], tr/val_loss:  0.000029/  0.989467, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.00 seconds, 1.18 minutes\n",
      "epoch-115 lr=['0.0050000'], tr/val_loss:  0.000028/  0.990591, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.35 seconds, 1.19 minutes\n",
      "epoch-116 lr=['0.0050000'], tr/val_loss:  0.000028/  0.995399, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.66 seconds, 1.18 minutes\n",
      "epoch-117 lr=['0.0050000'], tr/val_loss:  0.000029/  0.994132, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.55 seconds, 1.18 minutes\n",
      "epoch-118 lr=['0.0050000'], tr/val_loss:  0.000028/  0.993445, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.74 seconds, 1.18 minutes\n",
      "epoch-119 lr=['0.0050000'], tr/val_loss:  0.000028/  0.996422, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.90 seconds, 1.18 minutes\n",
      "epoch-120 lr=['0.0050000'], tr/val_loss:  0.000027/  0.998320, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.23 seconds, 1.19 minutes\n",
      "epoch-121 lr=['0.0050000'], tr/val_loss:  0.000026/  1.003487, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.08 seconds, 1.18 minutes\n",
      "epoch-122 lr=['0.0050000'], tr/val_loss:  0.000027/  1.002097, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.75 seconds, 1.18 minutes\n",
      "epoch-123 lr=['0.0050000'], tr/val_loss:  0.000027/  1.002438, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.89 seconds, 1.18 minutes\n",
      "epoch-124 lr=['0.0050000'], tr/val_loss:  0.000026/  1.002871, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.48 seconds, 1.19 minutes\n",
      "epoch-125 lr=['0.0050000'], tr/val_loss:  0.000026/  1.003956, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.46 seconds, 1.19 minutes\n",
      "epoch-126 lr=['0.0050000'], tr/val_loss:  0.000026/  1.005028, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.57 seconds, 1.18 minutes\n",
      "epoch-127 lr=['0.0050000'], tr/val_loss:  0.000026/  1.005146, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "epoch-128 lr=['0.0050000'], tr/val_loss:  0.000026/  1.006862, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.18 seconds, 1.19 minutes\n",
      "epoch-129 lr=['0.0050000'], tr/val_loss:  0.000026/  1.009396, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.13 seconds, 1.19 minutes\n",
      "epoch-130 lr=['0.0050000'], tr/val_loss:  0.000025/  1.011073, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.21 seconds, 1.19 minutes\n",
      "epoch-131 lr=['0.0050000'], tr/val_loss:  0.000025/  1.013163, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.84 seconds, 1.18 minutes\n",
      "epoch-132 lr=['0.0050000'], tr/val_loss:  0.000024/  1.013350, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.84 seconds, 1.18 minutes\n",
      "epoch-133 lr=['0.0050000'], tr/val_loss:  0.000024/  1.013135, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.83 seconds, 1.18 minutes\n",
      "epoch-134 lr=['0.0050000'], tr/val_loss:  0.000024/  1.016735, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-135 lr=['0.0050000'], tr/val_loss:  0.000024/  1.017267, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.84 seconds, 1.18 minutes\n",
      "epoch-136 lr=['0.0050000'], tr/val_loss:  0.000024/  1.016367, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.08 seconds, 1.20 minutes\n",
      "epoch-137 lr=['0.0050000'], tr/val_loss:  0.000023/  1.018640, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.38 seconds, 1.19 minutes\n",
      "epoch-138 lr=['0.0050000'], tr/val_loss:  0.000023/  1.017806, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.07 seconds, 1.18 minutes\n",
      "epoch-139 lr=['0.0050000'], tr/val_loss:  0.000022/  1.017898, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "epoch-140 lr=['0.0050000'], tr/val_loss:  0.000023/  1.018080, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.61 seconds, 1.19 minutes\n",
      "epoch-141 lr=['0.0050000'], tr/val_loss:  0.000022/  1.020267, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "epoch-142 lr=['0.0050000'], tr/val_loss:  0.000022/  1.023738, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.28 seconds, 1.19 minutes\n",
      "epoch-143 lr=['0.0050000'], tr/val_loss:  0.000022/  1.019975, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.15 seconds, 1.19 minutes\n",
      "epoch-144 lr=['0.0050000'], tr/val_loss:  0.000022/  1.021714, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.36 seconds, 1.19 minutes\n",
      "epoch-145 lr=['0.0050000'], tr/val_loss:  0.000022/  1.022698, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.07 seconds, 1.18 minutes\n",
      "epoch-146 lr=['0.0050000'], tr/val_loss:  0.000022/  1.022949, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.46 seconds, 1.19 minutes\n",
      "epoch-147 lr=['0.0050000'], tr/val_loss:  0.000021/  1.018077, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.96 seconds, 1.18 minutes\n",
      "epoch-148 lr=['0.0050000'], tr/val_loss:  0.000022/  1.018711, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "epoch-149 lr=['0.0050000'], tr/val_loss:  0.000021/  1.012370, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.59 seconds, 1.19 minutes\n",
      "epoch-150 lr=['0.0050000'], tr/val_loss:  0.000021/  1.013602, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.15 seconds, 1.20 minutes\n",
      "epoch-151 lr=['0.0050000'], tr/val_loss:  0.000021/  1.022981, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.40 seconds, 1.19 minutes\n",
      "epoch-152 lr=['0.0050000'], tr/val_loss:  0.000021/  1.027062, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.63 seconds, 1.19 minutes\n",
      "epoch-153 lr=['0.0050000'], tr/val_loss:  0.000021/  1.027624, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.56 seconds, 1.19 minutes\n",
      "epoch-154 lr=['0.0050000'], tr/val_loss:  0.000021/  1.028510, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.31 seconds, 1.19 minutes\n",
      "epoch-155 lr=['0.0050000'], tr/val_loss:  0.000020/  1.029918, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.16 seconds, 1.20 minutes\n",
      "epoch-156 lr=['0.0050000'], tr/val_loss:  0.000020/  1.031449, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.51 seconds, 1.19 minutes\n",
      "epoch-157 lr=['0.0050000'], tr/val_loss:  0.000020/  1.034720, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.11 seconds, 1.19 minutes\n",
      "epoch-158 lr=['0.0050000'], tr/val_loss:  0.000020/  1.043193, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.16 seconds, 1.19 minutes\n",
      "epoch-159 lr=['0.0050000'], tr/val_loss:  0.000020/  1.043306, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "epoch-160 lr=['0.0050000'], tr/val_loss:  0.000020/  1.044591, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.37 seconds, 1.19 minutes\n",
      "epoch-161 lr=['0.0050000'], tr/val_loss:  0.000020/  1.045146, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.22 seconds, 1.19 minutes\n",
      "epoch-162 lr=['0.0050000'], tr/val_loss:  0.000020/  1.040640, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.36 seconds, 1.19 minutes\n",
      "epoch-163 lr=['0.0050000'], tr/val_loss:  0.000019/  1.040348, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.12 seconds, 1.19 minutes\n",
      "epoch-164 lr=['0.0050000'], tr/val_loss:  0.000019/  1.040352, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.23 seconds, 1.19 minutes\n",
      "epoch-165 lr=['0.0050000'], tr/val_loss:  0.000019/  1.036238, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.04 seconds, 1.18 minutes\n",
      "epoch-166 lr=['0.0050000'], tr/val_loss:  0.000019/  1.037467, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.25 seconds, 1.19 minutes\n",
      "epoch-167 lr=['0.0050000'], tr/val_loss:  0.000019/  1.038964, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.15 seconds, 1.20 minutes\n",
      "epoch-168 lr=['0.0050000'], tr/val_loss:  0.000019/  1.039993, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.35 seconds, 1.19 minutes\n",
      "epoch-169 lr=['0.0050000'], tr/val_loss:  0.000019/  1.045594, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "epoch-170 lr=['0.0050000'], tr/val_loss:  0.000019/  1.042371, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.51 seconds, 1.19 minutes\n",
      "epoch-171 lr=['0.0050000'], tr/val_loss:  0.000019/  1.042587, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.43 seconds, 1.19 minutes\n",
      "epoch-172 lr=['0.0050000'], tr/val_loss:  0.000019/  1.042205, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.89 seconds, 1.18 minutes\n",
      "epoch-173 lr=['0.0050000'], tr/val_loss:  0.000018/  1.039555, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.53 seconds, 1.19 minutes\n",
      "epoch-174 lr=['0.0050000'], tr/val_loss:  0.000019/  1.039428, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.20 seconds, 1.19 minutes\n",
      "epoch-175 lr=['0.0050000'], tr/val_loss:  0.000018/  1.040477, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.95 seconds, 1.18 minutes\n",
      "epoch-176 lr=['0.0050000'], tr/val_loss:  0.000018/  1.041532, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.26 seconds, 1.19 minutes\n",
      "epoch-177 lr=['0.0050000'], tr/val_loss:  0.000018/  1.042244, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.77 seconds, 1.18 minutes\n",
      "epoch-178 lr=['0.0050000'], tr/val_loss:  0.000018/  1.041545, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.08 seconds, 1.18 minutes\n",
      "epoch-179 lr=['0.0050000'], tr/val_loss:  0.000018/  1.041084, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.76 seconds, 1.20 minutes\n",
      "epoch-180 lr=['0.0050000'], tr/val_loss:  0.000016/  1.045594, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.08 seconds, 1.17 minutes\n",
      "epoch-181 lr=['0.0050000'], tr/val_loss:  0.000016/  1.047556, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.40 seconds, 1.17 minutes\n",
      "epoch-182 lr=['0.0050000'], tr/val_loss:  0.000016/  1.048699, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.01 seconds, 1.18 minutes\n",
      "epoch-183 lr=['0.0050000'], tr/val_loss:  0.000016/  1.051712, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.11 seconds, 1.19 minutes\n",
      "epoch-184 lr=['0.0050000'], tr/val_loss:  0.000016/  1.047461, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.56 seconds, 1.18 minutes\n",
      "epoch-185 lr=['0.0050000'], tr/val_loss:  0.000016/  1.049265, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.91 seconds, 1.18 minutes\n",
      "epoch-186 lr=['0.0050000'], tr/val_loss:  0.000016/  1.049498, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "epoch-187 lr=['0.0050000'], tr/val_loss:  0.000015/  1.052283, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.23 seconds, 1.19 minutes\n",
      "epoch-188 lr=['0.0050000'], tr/val_loss:  0.000015/  1.050993, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.53 seconds, 1.18 minutes\n",
      "epoch-189 lr=['0.0050000'], tr/val_loss:  0.000014/  1.054564, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.46 seconds, 1.17 minutes\n",
      "epoch-190 lr=['0.0050000'], tr/val_loss:  0.000014/  1.054018, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.63 seconds, 1.18 minutes\n",
      "epoch-191 lr=['0.0050000'], tr/val_loss:  0.000014/  1.054198, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.33 seconds, 1.17 minutes\n",
      "epoch-192 lr=['0.0050000'], tr/val_loss:  0.000014/  1.058913, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.63 seconds, 1.18 minutes\n",
      "epoch-193 lr=['0.0050000'], tr/val_loss:  0.000014/  1.056394, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.21 seconds, 1.19 minutes\n",
      "epoch-194 lr=['0.0050000'], tr/val_loss:  0.000014/  1.060834, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.69 seconds, 1.19 minutes\n",
      "epoch-195 lr=['0.0050000'], tr/val_loss:  0.000014/  1.062566, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.08 seconds, 1.18 minutes\n",
      "epoch-196 lr=['0.0050000'], tr/val_loss:  0.000014/  1.062081, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.85 seconds, 1.18 minutes\n",
      "epoch-197 lr=['0.0050000'], tr/val_loss:  0.000013/  1.062597, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.83 seconds, 1.18 minutes\n",
      "epoch-198 lr=['0.0050000'], tr/val_loss:  0.000013/  1.061423, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.36 seconds, 1.19 minutes\n",
      "epoch-199 lr=['0.0050000'], tr/val_loss:  0.000013/  1.060047, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.87 seconds, 1.18 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7f05e3146f478ba2a67600a2545897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÜ‚ñÖ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÜ‚ñÖ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1e-05</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>1.06005</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">firm-sweep-46</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x7h69sau' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/x7h69sau</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251104_135515-x7h69sau/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ud2jhtb8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251104_175418-ud2jhtb8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ud2jhtb8' target=\"_blank\">bright-sweep-55</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ud2jhtb8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ud2jhtb8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': False, 'unique_name': '20251104_175427_848', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.005, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.005\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch-0   lr=['0.0050000'], tr/val_loss:  1.109409/  1.260434, val:  52.50%, val_best:  52.50%, tr:  56.33%, tr_best:  56.33%, epoch time: 71.48 seconds, 1.19 minutes\n",
      "epoch-1   lr=['0.0050000'], tr/val_loss:  0.759032/  1.040714, val:  57.92%, val_best:  57.92%, tr:  66.64%, tr_best:  66.64%, epoch time: 71.97 seconds, 1.20 minutes\n",
      "epoch-2   lr=['0.0050000'], tr/val_loss:  0.596298/  0.909211, val:  65.00%, val_best:  65.00%, tr:  73.47%, tr_best:  73.47%, epoch time: 71.78 seconds, 1.20 minutes\n",
      "epoch-3   lr=['0.0050000'], tr/val_loss:  0.503366/  0.798549, val:  76.25%, val_best:  76.25%, tr:  78.27%, tr_best:  78.27%, epoch time: 70.91 seconds, 1.18 minutes\n",
      "epoch-4   lr=['0.0050000'], tr/val_loss:  0.425871/  0.996578, val:  67.50%, val_best:  76.25%, tr:  83.77%, tr_best:  83.77%, epoch time: 71.82 seconds, 1.20 minutes\n",
      "epoch-5   lr=['0.0050000'], tr/val_loss:  0.318587/  0.594027, val:  83.33%, val_best:  83.33%, tr:  87.67%, tr_best:  87.67%, epoch time: 71.08 seconds, 1.18 minutes\n",
      "epoch-6   lr=['0.0050000'], tr/val_loss:  0.249354/  0.677130, val:  82.92%, val_best:  83.33%, tr:  90.67%, tr_best:  90.67%, epoch time: 71.30 seconds, 1.19 minutes\n",
      "epoch-7   lr=['0.0050000'], tr/val_loss:  0.193248/  0.658372, val:  84.17%, val_best:  84.17%, tr:  93.01%, tr_best:  93.01%, epoch time: 72.00 seconds, 1.20 minutes\n",
      "epoch-8   lr=['0.0050000'], tr/val_loss:  0.161211/  0.735762, val:  85.83%, val_best:  85.83%, tr:  94.48%, tr_best:  94.48%, epoch time: 72.11 seconds, 1.20 minutes\n",
      "epoch-9   lr=['0.0050000'], tr/val_loss:  0.127179/  0.726500, val:  82.08%, val_best:  85.83%, tr:  95.90%, tr_best:  95.90%, epoch time: 70.98 seconds, 1.18 minutes\n",
      "epoch-10  lr=['0.0050000'], tr/val_loss:  0.134317/  0.804747, val:  81.25%, val_best:  85.83%, tr:  95.96%, tr_best:  95.96%, epoch time: 71.03 seconds, 1.18 minutes\n",
      "epoch-11  lr=['0.0050000'], tr/val_loss:  0.112936/  1.059947, val:  76.25%, val_best:  85.83%, tr:  96.15%, tr_best:  96.15%, epoch time: 70.83 seconds, 1.18 minutes\n",
      "epoch-12  lr=['0.0050000'], tr/val_loss:  0.109159/  0.902318, val:  82.50%, val_best:  85.83%, tr:  96.78%, tr_best:  96.78%, epoch time: 71.10 seconds, 1.18 minutes\n",
      "epoch-13  lr=['0.0050000'], tr/val_loss:  0.076094/  0.748871, val:  83.33%, val_best:  85.83%, tr:  97.88%, tr_best:  97.88%, epoch time: 70.57 seconds, 1.18 minutes\n",
      "epoch-14  lr=['0.0050000'], tr/val_loss:  0.054420/  0.674798, val:  88.33%, val_best:  88.33%, tr:  98.42%, tr_best:  98.42%, epoch time: 70.87 seconds, 1.18 minutes\n",
      "epoch-15  lr=['0.0050000'], tr/val_loss:  0.024918/  0.943582, val:  84.17%, val_best:  88.33%, tr:  99.41%, tr_best:  99.41%, epoch time: 70.23 seconds, 1.17 minutes\n",
      "epoch-16  lr=['0.0050000'], tr/val_loss:  0.013350/  0.877620, val:  86.67%, val_best:  88.33%, tr:  99.75%, tr_best:  99.75%, epoch time: 70.24 seconds, 1.17 minutes\n",
      "epoch-17  lr=['0.0050000'], tr/val_loss:  0.015653/  0.678735, val:  87.50%, val_best:  88.33%, tr:  99.59%, tr_best:  99.75%, epoch time: 70.94 seconds, 1.18 minutes\n",
      "epoch-18  lr=['0.0050000'], tr/val_loss:  0.004484/  0.866870, val:  86.25%, val_best:  88.33%, tr:  99.89%, tr_best:  99.89%, epoch time: 70.82 seconds, 1.18 minutes\n",
      "epoch-19  lr=['0.0050000'], tr/val_loss:  0.000981/  0.910120, val:  85.42%, val_best:  88.33%, tr:  99.98%, tr_best:  99.98%, epoch time: 70.64 seconds, 1.18 minutes\n",
      "epoch-20  lr=['0.0050000'], tr/val_loss:  0.004390/  0.922306, val:  85.83%, val_best:  88.33%, tr:  99.89%, tr_best:  99.98%, epoch time: 70.40 seconds, 1.17 minutes\n",
      "epoch-21  lr=['0.0050000'], tr/val_loss:  0.001139/  0.896402, val:  85.42%, val_best:  88.33%, tr:  99.98%, tr_best:  99.98%, epoch time: 70.50 seconds, 1.17 minutes\n",
      "epoch-22  lr=['0.0050000'], tr/val_loss:  0.000843/  0.937305, val:  85.42%, val_best:  88.33%, tr:  99.98%, tr_best:  99.98%, epoch time: 70.94 seconds, 1.18 minutes\n",
      "epoch-23  lr=['0.0050000'], tr/val_loss:  0.000759/  0.919964, val:  85.42%, val_best:  88.33%, tr:  99.98%, tr_best:  99.98%, epoch time: 70.86 seconds, 1.18 minutes\n",
      "epoch-24  lr=['0.0050000'], tr/val_loss:  0.000448/  0.938666, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.03 seconds, 1.18 minutes\n",
      "epoch-25  lr=['0.0050000'], tr/val_loss:  0.000308/  0.929716, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.22 seconds, 1.19 minutes\n",
      "epoch-26  lr=['0.0050000'], tr/val_loss:  0.000242/  0.948368, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.84 seconds, 1.18 minutes\n",
      "epoch-27  lr=['0.0050000'], tr/val_loss:  0.000214/  0.956804, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.97 seconds, 1.18 minutes\n",
      "epoch-28  lr=['0.0050000'], tr/val_loss:  0.000205/  0.957044, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.96 seconds, 1.18 minutes\n",
      "epoch-29  lr=['0.0050000'], tr/val_loss:  0.000178/  0.963404, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.83 seconds, 1.18 minutes\n",
      "epoch-30  lr=['0.0050000'], tr/val_loss:  0.000168/  0.994320, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.83 seconds, 1.18 minutes\n",
      "epoch-31  lr=['0.0050000'], tr/val_loss:  0.000153/  0.979422, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.67 seconds, 1.18 minutes\n",
      "epoch-32  lr=['0.0050000'], tr/val_loss:  0.000149/  0.984885, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.97 seconds, 1.18 minutes\n",
      "epoch-33  lr=['0.0050000'], tr/val_loss:  0.000138/  0.993816, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.37 seconds, 1.19 minutes\n",
      "epoch-34  lr=['0.0050000'], tr/val_loss:  0.000133/  0.994961, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.86 seconds, 1.18 minutes\n",
      "epoch-35  lr=['0.0050000'], tr/val_loss:  0.000123/  1.001275, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.21 seconds, 1.19 minutes\n",
      "epoch-36  lr=['0.0050000'], tr/val_loss:  0.000114/  1.005818, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.34 seconds, 1.19 minutes\n",
      "epoch-37  lr=['0.0050000'], tr/val_loss:  0.000113/  1.015266, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.73 seconds, 1.18 minutes\n",
      "epoch-38  lr=['0.0050000'], tr/val_loss:  0.000107/  1.013858, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.15 seconds, 1.19 minutes\n",
      "epoch-39  lr=['0.0050000'], tr/val_loss:  0.000105/  1.012096, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.51 seconds, 1.19 minutes\n",
      "epoch-40  lr=['0.0050000'], tr/val_loss:  0.000101/  1.018967, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.65 seconds, 1.18 minutes\n",
      "epoch-41  lr=['0.0050000'], tr/val_loss:  0.000096/  1.018003, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.15 seconds, 1.19 minutes\n",
      "epoch-42  lr=['0.0050000'], tr/val_loss:  0.000091/  1.015819, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.00 seconds, 1.18 minutes\n",
      "epoch-43  lr=['0.0050000'], tr/val_loss:  0.000088/  1.025036, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.84 seconds, 1.18 minutes\n",
      "epoch-44  lr=['0.0050000'], tr/val_loss:  0.000086/  1.031977, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.75 seconds, 1.18 minutes\n",
      "epoch-45  lr=['0.0050000'], tr/val_loss:  0.000085/  1.032363, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.86 seconds, 1.18 minutes\n",
      "epoch-46  lr=['0.0050000'], tr/val_loss:  0.000080/  1.031885, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.05 seconds, 1.18 minutes\n",
      "epoch-47  lr=['0.0050000'], tr/val_loss:  0.000080/  1.034332, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.13 seconds, 1.19 minutes\n",
      "epoch-48  lr=['0.0050000'], tr/val_loss:  0.000078/  1.045486, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.87 seconds, 1.18 minutes\n",
      "epoch-49  lr=['0.0050000'], tr/val_loss:  0.000075/  1.048515, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.57 seconds, 1.19 minutes\n",
      "epoch-50  lr=['0.0050000'], tr/val_loss:  0.000071/  1.048705, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.71 seconds, 1.18 minutes\n",
      "epoch-51  lr=['0.0050000'], tr/val_loss:  0.000070/  1.043632, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.77 seconds, 1.18 minutes\n",
      "epoch-52  lr=['0.0050000'], tr/val_loss:  0.000070/  1.048893, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.56 seconds, 1.19 minutes\n",
      "epoch-53  lr=['0.0050000'], tr/val_loss:  0.000067/  1.054698, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.13 seconds, 1.19 minutes\n",
      "epoch-54  lr=['0.0050000'], tr/val_loss:  0.000064/  1.050894, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.18 seconds, 1.19 minutes\n",
      "epoch-55  lr=['0.0050000'], tr/val_loss:  0.000065/  1.052094, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "epoch-56  lr=['0.0050000'], tr/val_loss:  0.000061/  1.058673, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.15 seconds, 1.19 minutes\n",
      "epoch-57  lr=['0.0050000'], tr/val_loss:  0.000060/  1.056175, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "epoch-58  lr=['0.0050000'], tr/val_loss:  0.000060/  1.057905, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.95 seconds, 1.18 minutes\n",
      "epoch-59  lr=['0.0050000'], tr/val_loss:  0.000060/  1.062553, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.10 seconds, 1.18 minutes\n",
      "epoch-60  lr=['0.0050000'], tr/val_loss:  0.000058/  1.065339, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.09 seconds, 1.18 minutes\n",
      "epoch-61  lr=['0.0050000'], tr/val_loss:  0.000057/  1.065360, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.86 seconds, 1.18 minutes\n",
      "epoch-62  lr=['0.0050000'], tr/val_loss:  0.000057/  1.064509, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.83 seconds, 1.18 minutes\n",
      "epoch-63  lr=['0.0050000'], tr/val_loss:  0.000057/  1.069088, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.24 seconds, 1.19 minutes\n",
      "epoch-64  lr=['0.0050000'], tr/val_loss:  0.000055/  1.076353, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "epoch-65  lr=['0.0050000'], tr/val_loss:  0.000053/  1.078050, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "epoch-66  lr=['0.0050000'], tr/val_loss:  0.000051/  1.078243, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-67  lr=['0.0050000'], tr/val_loss:  0.000050/  1.083402, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.87 seconds, 1.18 minutes\n",
      "epoch-68  lr=['0.0050000'], tr/val_loss:  0.000051/  1.081595, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.10 seconds, 1.18 minutes\n",
      "epoch-69  lr=['0.0050000'], tr/val_loss:  0.000049/  1.080315, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.00 seconds, 1.18 minutes\n",
      "epoch-70  lr=['0.0050000'], tr/val_loss:  0.000049/  1.075925, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.67 seconds, 1.18 minutes\n",
      "epoch-71  lr=['0.0050000'], tr/val_loss:  0.000048/  1.074610, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.86 seconds, 1.18 minutes\n",
      "epoch-72  lr=['0.0050000'], tr/val_loss:  0.000046/  1.080076, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.95 seconds, 1.18 minutes\n",
      "epoch-73  lr=['0.0050000'], tr/val_loss:  0.000046/  1.077067, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "epoch-74  lr=['0.0050000'], tr/val_loss:  0.000045/  1.074490, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.04 seconds, 1.18 minutes\n",
      "epoch-75  lr=['0.0050000'], tr/val_loss:  0.000044/  1.072968, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.55 seconds, 1.19 minutes\n",
      "epoch-76  lr=['0.0050000'], tr/val_loss:  0.000043/  1.073842, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.13 seconds, 1.19 minutes\n",
      "epoch-77  lr=['0.0050000'], tr/val_loss:  0.000042/  1.081631, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.61 seconds, 1.18 minutes\n",
      "epoch-78  lr=['0.0050000'], tr/val_loss:  0.000041/  1.075064, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.53 seconds, 1.19 minutes\n",
      "epoch-79  lr=['0.0050000'], tr/val_loss:  0.000041/  1.071560, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.13 seconds, 1.19 minutes\n",
      "epoch-80  lr=['0.0050000'], tr/val_loss:  0.000040/  1.073762, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.13 seconds, 1.19 minutes\n",
      "epoch-81  lr=['0.0050000'], tr/val_loss:  0.000041/  1.067850, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.56 seconds, 1.18 minutes\n",
      "epoch-82  lr=['0.0050000'], tr/val_loss:  0.000039/  1.067909, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.13 seconds, 1.19 minutes\n",
      "epoch-83  lr=['0.0050000'], tr/val_loss:  0.000039/  1.066504, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.10 seconds, 1.18 minutes\n",
      "epoch-84  lr=['0.0050000'], tr/val_loss:  0.000039/  1.067716, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-85  lr=['0.0050000'], tr/val_loss:  0.000039/  1.079548, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.04 seconds, 1.18 minutes\n",
      "epoch-86  lr=['0.0050000'], tr/val_loss:  0.000038/  1.079169, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.77 seconds, 1.18 minutes\n",
      "epoch-87  lr=['0.0050000'], tr/val_loss:  0.000038/  1.085907, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.86 seconds, 1.18 minutes\n",
      "epoch-88  lr=['0.0050000'], tr/val_loss:  0.000037/  1.085772, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.54 seconds, 1.18 minutes\n",
      "epoch-89  lr=['0.0050000'], tr/val_loss:  0.000037/  1.084936, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.67 seconds, 1.18 minutes\n",
      "epoch-90  lr=['0.0050000'], tr/val_loss:  0.000037/  1.084505, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.21 seconds, 1.19 minutes\n",
      "epoch-91  lr=['0.0050000'], tr/val_loss:  0.000036/  1.083913, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.65 seconds, 1.18 minutes\n",
      "epoch-92  lr=['0.0050000'], tr/val_loss:  0.000037/  1.080213, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.56 seconds, 1.18 minutes\n",
      "epoch-93  lr=['0.0050000'], tr/val_loss:  0.000036/  1.079369, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.50 seconds, 1.19 minutes\n",
      "epoch-94  lr=['0.0050000'], tr/val_loss:  0.000036/  1.081950, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.18 seconds, 1.19 minutes\n",
      "epoch-95  lr=['0.0050000'], tr/val_loss:  0.000036/  1.079493, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.25 seconds, 1.19 minutes\n",
      "epoch-96  lr=['0.0050000'], tr/val_loss:  0.000035/  1.083470, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.86 seconds, 1.18 minutes\n",
      "epoch-97  lr=['0.0050000'], tr/val_loss:  0.000034/  1.082108, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.00 seconds, 1.18 minutes\n",
      "epoch-98  lr=['0.0050000'], tr/val_loss:  0.000034/  1.085589, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.00 seconds, 1.18 minutes\n",
      "epoch-99  lr=['0.0050000'], tr/val_loss:  0.000033/  1.086305, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.84 seconds, 1.18 minutes\n",
      "epoch-100 lr=['0.0050000'], tr/val_loss:  0.000032/  1.086365, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.59 seconds, 1.18 minutes\n",
      "epoch-101 lr=['0.0050000'], tr/val_loss:  0.000032/  1.086784, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.51 seconds, 1.19 minutes\n",
      "epoch-102 lr=['0.0050000'], tr/val_loss:  0.000032/  1.088895, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.69 seconds, 1.18 minutes\n",
      "epoch-103 lr=['0.0050000'], tr/val_loss:  0.000032/  1.091332, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.30 seconds, 1.19 minutes\n",
      "epoch-104 lr=['0.0050000'], tr/val_loss:  0.000031/  1.097161, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.85 seconds, 1.18 minutes\n",
      "epoch-105 lr=['0.0050000'], tr/val_loss:  0.000031/  1.099678, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.14 seconds, 1.19 minutes\n",
      "epoch-106 lr=['0.0050000'], tr/val_loss:  0.000030/  1.091229, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.96 seconds, 1.18 minutes\n",
      "epoch-107 lr=['0.0050000'], tr/val_loss:  0.000031/  1.090708, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.71 seconds, 1.18 minutes\n",
      "epoch-108 lr=['0.0050000'], tr/val_loss:  0.000030/  1.090545, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.89 seconds, 1.18 minutes\n",
      "epoch-109 lr=['0.0050000'], tr/val_loss:  0.000029/  1.095686, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.97 seconds, 1.18 minutes\n",
      "epoch-110 lr=['0.0050000'], tr/val_loss:  0.000029/  1.098652, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.02 seconds, 1.18 minutes\n",
      "epoch-111 lr=['0.0050000'], tr/val_loss:  0.000029/  1.098349, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.13 seconds, 1.19 minutes\n",
      "epoch-112 lr=['0.0050000'], tr/val_loss:  0.000028/  1.092838, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.09 seconds, 1.18 minutes\n",
      "epoch-113 lr=['0.0050000'], tr/val_loss:  0.000028/  1.095078, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.98 seconds, 1.18 minutes\n",
      "epoch-114 lr=['0.0050000'], tr/val_loss:  0.000028/  1.093240, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.04 seconds, 1.18 minutes\n",
      "epoch-115 lr=['0.0050000'], tr/val_loss:  0.000027/  1.099160, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "epoch-116 lr=['0.0050000'], tr/val_loss:  0.000027/  1.101557, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.08 seconds, 1.18 minutes\n",
      "epoch-117 lr=['0.0050000'], tr/val_loss:  0.000027/  1.102291, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.01 seconds, 1.18 minutes\n",
      "epoch-118 lr=['0.0050000'], tr/val_loss:  0.000027/  1.098740, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.63 seconds, 1.18 minutes\n",
      "epoch-119 lr=['0.0050000'], tr/val_loss:  0.000027/  1.094247, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-120 lr=['0.0050000'], tr/val_loss:  0.000026/  1.093676, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.02 seconds, 1.18 minutes\n",
      "epoch-121 lr=['0.0050000'], tr/val_loss:  0.000026/  1.096164, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.72 seconds, 1.18 minutes\n",
      "epoch-122 lr=['0.0050000'], tr/val_loss:  0.000026/  1.097140, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.68 seconds, 1.18 minutes\n",
      "epoch-123 lr=['0.0050000'], tr/val_loss:  0.000026/  1.099430, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.53 seconds, 1.18 minutes\n",
      "epoch-124 lr=['0.0050000'], tr/val_loss:  0.000025/  1.103149, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.72 seconds, 1.18 minutes\n",
      "epoch-125 lr=['0.0050000'], tr/val_loss:  0.000025/  1.116319, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.78 seconds, 1.18 minutes\n",
      "epoch-126 lr=['0.0050000'], tr/val_loss:  0.000024/  1.119484, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.59 seconds, 1.18 minutes\n",
      "epoch-127 lr=['0.0050000'], tr/val_loss:  0.000024/  1.115973, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.84 seconds, 1.18 minutes\n",
      "epoch-128 lr=['0.0050000'], tr/val_loss:  0.000024/  1.117280, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.31 seconds, 1.19 minutes\n",
      "epoch-129 lr=['0.0050000'], tr/val_loss:  0.000024/  1.114699, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "epoch-130 lr=['0.0050000'], tr/val_loss:  0.000024/  1.113571, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.32 seconds, 1.19 minutes\n",
      "epoch-131 lr=['0.0050000'], tr/val_loss:  0.000023/  1.121621, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.31 seconds, 1.19 minutes\n",
      "epoch-132 lr=['0.0050000'], tr/val_loss:  0.000023/  1.124495, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.27 seconds, 1.19 minutes\n",
      "epoch-133 lr=['0.0050000'], tr/val_loss:  0.000023/  1.124288, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.74 seconds, 1.18 minutes\n",
      "epoch-134 lr=['0.0050000'], tr/val_loss:  0.000023/  1.126027, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.21 seconds, 1.19 minutes\n",
      "epoch-135 lr=['0.0050000'], tr/val_loss:  0.000023/  1.127663, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.42 seconds, 1.19 minutes\n",
      "epoch-136 lr=['0.0050000'], tr/val_loss:  0.000023/  1.130118, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.65 seconds, 1.18 minutes\n",
      "epoch-137 lr=['0.0050000'], tr/val_loss:  0.000023/  1.131451, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.18 seconds, 1.19 minutes\n",
      "epoch-138 lr=['0.0050000'], tr/val_loss:  0.000023/  1.126890, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.56 seconds, 1.18 minutes\n",
      "epoch-139 lr=['0.0050000'], tr/val_loss:  0.000023/  1.128964, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.96 seconds, 1.18 minutes\n",
      "epoch-140 lr=['0.0050000'], tr/val_loss:  0.000023/  1.129944, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.15 seconds, 1.19 minutes\n",
      "epoch-141 lr=['0.0050000'], tr/val_loss:  0.000023/  1.129718, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "epoch-142 lr=['0.0050000'], tr/val_loss:  0.000023/  1.135312, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.03 seconds, 1.18 minutes\n",
      "epoch-143 lr=['0.0050000'], tr/val_loss:  0.000023/  1.132062, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.23 seconds, 1.19 minutes\n",
      "epoch-144 lr=['0.0050000'], tr/val_loss:  0.000022/  1.129841, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "epoch-145 lr=['0.0050000'], tr/val_loss:  0.000022/  1.131185, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "epoch-146 lr=['0.0050000'], tr/val_loss:  0.000022/  1.129565, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.72 seconds, 1.18 minutes\n",
      "epoch-147 lr=['0.0050000'], tr/val_loss:  0.000021/  1.130415, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.86 seconds, 1.18 minutes\n",
      "epoch-148 lr=['0.0050000'], tr/val_loss:  0.000022/  1.129923, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.00 seconds, 1.18 minutes\n",
      "epoch-149 lr=['0.0050000'], tr/val_loss:  0.000021/  1.136866, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.14 seconds, 1.19 minutes\n",
      "epoch-150 lr=['0.0050000'], tr/val_loss:  0.000021/  1.133554, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.66 seconds, 1.18 minutes\n",
      "epoch-151 lr=['0.0050000'], tr/val_loss:  0.000021/  1.131742, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.82 seconds, 1.18 minutes\n",
      "epoch-152 lr=['0.0050000'], tr/val_loss:  0.000021/  1.130929, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.94 seconds, 1.18 minutes\n",
      "epoch-153 lr=['0.0050000'], tr/val_loss:  0.000021/  1.136377, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.06 seconds, 1.18 minutes\n",
      "epoch-154 lr=['0.0050000'], tr/val_loss:  0.000021/  1.131536, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.08 seconds, 1.18 minutes\n",
      "epoch-155 lr=['0.0050000'], tr/val_loss:  0.000021/  1.129962, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.42 seconds, 1.17 minutes\n",
      "epoch-156 lr=['0.0050000'], tr/val_loss:  0.000020/  1.133101, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.68 seconds, 1.19 minutes\n",
      "epoch-157 lr=['0.0050000'], tr/val_loss:  0.000020/  1.137545, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.97 seconds, 1.18 minutes\n",
      "epoch-158 lr=['0.0050000'], tr/val_loss:  0.000020/  1.141196, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.13 seconds, 1.19 minutes\n",
      "epoch-159 lr=['0.0050000'], tr/val_loss:  0.000020/  1.135321, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.78 seconds, 1.18 minutes\n",
      "epoch-160 lr=['0.0050000'], tr/val_loss:  0.000020/  1.135430, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.16 seconds, 1.19 minutes\n",
      "epoch-161 lr=['0.0050000'], tr/val_loss:  0.000020/  1.135293, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.61 seconds, 1.18 minutes\n",
      "epoch-162 lr=['0.0050000'], tr/val_loss:  0.000019/  1.135966, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.62 seconds, 1.18 minutes\n",
      "epoch-163 lr=['0.0050000'], tr/val_loss:  0.000020/  1.135382, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.95 seconds, 1.18 minutes\n",
      "epoch-164 lr=['0.0050000'], tr/val_loss:  0.000019/  1.137299, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.98 seconds, 1.18 minutes\n",
      "epoch-165 lr=['0.0050000'], tr/val_loss:  0.000019/  1.132856, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.57 seconds, 1.18 minutes\n",
      "epoch-166 lr=['0.0050000'], tr/val_loss:  0.000019/  1.131504, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.64 seconds, 1.18 minutes\n",
      "epoch-167 lr=['0.0050000'], tr/val_loss:  0.000019/  1.133187, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.96 seconds, 1.18 minutes\n",
      "epoch-168 lr=['0.0050000'], tr/val_loss:  0.000019/  1.130605, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.95 seconds, 1.18 minutes\n",
      "epoch-169 lr=['0.0050000'], tr/val_loss:  0.000019/  1.130898, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.82 seconds, 1.18 minutes\n",
      "epoch-170 lr=['0.0050000'], tr/val_loss:  0.000019/  1.125344, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.03 seconds, 1.18 minutes\n",
      "epoch-171 lr=['0.0050000'], tr/val_loss:  0.000019/  1.125531, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-172 lr=['0.0050000'], tr/val_loss:  0.000018/  1.123931, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.02 seconds, 1.18 minutes\n",
      "epoch-173 lr=['0.0050000'], tr/val_loss:  0.000018/  1.124828, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.78 seconds, 1.18 minutes\n",
      "epoch-174 lr=['0.0050000'], tr/val_loss:  0.000018/  1.128223, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.50 seconds, 1.19 minutes\n",
      "epoch-175 lr=['0.0050000'], tr/val_loss:  0.000018/  1.129090, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.00 seconds, 1.18 minutes\n",
      "epoch-176 lr=['0.0050000'], tr/val_loss:  0.000018/  1.131797, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.35 seconds, 1.17 minutes\n",
      "epoch-177 lr=['0.0050000'], tr/val_loss:  0.000018/  1.135174, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.25 seconds, 1.19 minutes\n",
      "epoch-178 lr=['0.0050000'], tr/val_loss:  0.000018/  1.135919, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.90 seconds, 1.18 minutes\n",
      "epoch-179 lr=['0.0050000'], tr/val_loss:  0.000018/  1.134864, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.24 seconds, 1.19 minutes\n",
      "epoch-180 lr=['0.0050000'], tr/val_loss:  0.000018/  1.135727, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.34 seconds, 1.19 minutes\n",
      "epoch-181 lr=['0.0050000'], tr/val_loss:  0.000017/  1.136226, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.12 seconds, 1.19 minutes\n",
      "epoch-182 lr=['0.0050000'], tr/val_loss:  0.000017/  1.138639, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.75 seconds, 1.18 minutes\n",
      "epoch-183 lr=['0.0050000'], tr/val_loss:  0.000017/  1.137443, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.43 seconds, 1.17 minutes\n",
      "epoch-184 lr=['0.0050000'], tr/val_loss:  0.000017/  1.139930, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "epoch-185 lr=['0.0050000'], tr/val_loss:  0.000017/  1.141959, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.12 seconds, 1.19 minutes\n",
      "epoch-186 lr=['0.0050000'], tr/val_loss:  0.000016/  1.142745, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.37 seconds, 1.19 minutes\n",
      "epoch-187 lr=['0.0050000'], tr/val_loss:  0.000016/  1.145304, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.68 seconds, 1.18 minutes\n",
      "epoch-188 lr=['0.0050000'], tr/val_loss:  0.000016/  1.144215, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.34 seconds, 1.19 minutes\n",
      "epoch-189 lr=['0.0050000'], tr/val_loss:  0.000016/  1.145362, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.16 seconds, 1.19 minutes\n",
      "epoch-190 lr=['0.0050000'], tr/val_loss:  0.000016/  1.141750, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "epoch-191 lr=['0.0050000'], tr/val_loss:  0.000016/  1.142097, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.91 seconds, 1.18 minutes\n",
      "epoch-192 lr=['0.0050000'], tr/val_loss:  0.000016/  1.142041, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.11 seconds, 1.19 minutes\n",
      "epoch-193 lr=['0.0050000'], tr/val_loss:  0.000016/  1.144515, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.30 seconds, 1.19 minutes\n",
      "epoch-194 lr=['0.0050000'], tr/val_loss:  0.000016/  1.146778, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.39 seconds, 1.19 minutes\n",
      "epoch-195 lr=['0.0050000'], tr/val_loss:  0.000016/  1.147348, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.76 seconds, 1.18 minutes\n",
      "epoch-196 lr=['0.0050000'], tr/val_loss:  0.000016/  1.146964, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.57 seconds, 1.19 minutes\n",
      "epoch-197 lr=['0.0050000'], tr/val_loss:  0.000016/  1.147477, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.95 seconds, 1.18 minutes\n",
      "epoch-198 lr=['0.0050000'], tr/val_loss:  0.000015/  1.149549, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.31 seconds, 1.19 minutes\n",
      "epoch-199 lr=['0.0050000'], tr/val_loss:  0.000015/  1.150445, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.02 seconds, 1.18 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453e371605e74372b68577aa08524548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñá‚ñÅ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>2e-05</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.85833</td></tr><tr><td>val_loss</td><td>1.15045</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bright-sweep-55</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ud2jhtb8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ud2jhtb8</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251104_175418-ud2jhtb8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2mw26pzb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251104_215239-2mw26pzb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2mw26pzb' target=\"_blank\">likely-sweep-63</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2mw26pzb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2mw26pzb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': False, 'unique_name': '20251104_215249_044', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.005, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 18, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f1351bdee3d35c47af449525e007adf4\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 713fc8490e508dff15bed30e755e5ae6\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 4e1f85d1c0ff71c6a0df58e4542628f1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 626851829f1152228185abed5f6dca5e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 3c558307d41237628ef99b773b1c784f\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 81e44cafc682693b4b2086c7f81d224b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = b36895cc577f68764ec3fb2cd889299e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = efeaf39262f61e3ee121af192099af65\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a15007b6dd509fde13c404b80435835e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 6fa31eb5ad96d639190daf0921d987e2\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.005\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch-0   lr=['0.0050000'], tr/val_loss:  1.004355/  1.278955, val:  55.00%, val_best:  55.00%, tr:  62.24%, tr_best:  62.24%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "epoch-1   lr=['0.0050000'], tr/val_loss:  0.617859/  0.818328, val:  68.33%, val_best:  68.33%, tr:  75.32%, tr_best:  75.32%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "epoch-2   lr=['0.0050000'], tr/val_loss:  0.447137/  0.795807, val:  77.50%, val_best:  77.50%, tr:  83.57%, tr_best:  83.57%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "epoch-3   lr=['0.0050000'], tr/val_loss:  0.294444/  0.858662, val:  75.83%, val_best:  77.50%, tr:  89.77%, tr_best:  89.77%, epoch time: 72.85 seconds, 1.21 minutes\n",
      "epoch-4   lr=['0.0050000'], tr/val_loss:  0.199697/  1.305166, val:  73.75%, val_best:  77.50%, tr:  93.28%, tr_best:  93.28%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "epoch-5   lr=['0.0050000'], tr/val_loss:  0.164154/  0.802501, val:  82.92%, val_best:  82.92%, tr:  94.39%, tr_best:  94.39%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "epoch-6   lr=['0.0050000'], tr/val_loss:  0.140813/  0.793576, val:  83.33%, val_best:  83.33%, tr:  95.67%, tr_best:  95.67%, epoch time: 73.55 seconds, 1.23 minutes\n",
      "epoch-7   lr=['0.0050000'], tr/val_loss:  0.083878/  0.882963, val:  83.33%, val_best:  83.33%, tr:  97.25%, tr_best:  97.25%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "epoch-8   lr=['0.0050000'], tr/val_loss:  0.084098/  0.864326, val:  84.17%, val_best:  84.17%, tr:  97.20%, tr_best:  97.25%, epoch time: 73.59 seconds, 1.23 minutes\n",
      "epoch-9   lr=['0.0050000'], tr/val_loss:  0.072113/  0.837204, val:  78.33%, val_best:  84.17%, tr:  97.59%, tr_best:  97.59%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "epoch-10  lr=['0.0050000'], tr/val_loss:  0.078305/  0.743612, val:  87.08%, val_best:  87.08%, tr:  97.77%, tr_best:  97.77%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "epoch-11  lr=['0.0050000'], tr/val_loss:  0.040389/  0.855403, val:  85.83%, val_best:  87.08%, tr:  98.72%, tr_best:  98.72%, epoch time: 73.19 seconds, 1.22 minutes\n",
      "epoch-12  lr=['0.0050000'], tr/val_loss:  0.025425/  1.025429, val:  85.00%, val_best:  87.08%, tr:  99.30%, tr_best:  99.30%, epoch time: 73.48 seconds, 1.22 minutes\n",
      "epoch-13  lr=['0.0050000'], tr/val_loss:  0.028341/  1.071935, val:  84.58%, val_best:  87.08%, tr:  99.12%, tr_best:  99.30%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "epoch-14  lr=['0.0050000'], tr/val_loss:  0.012946/  1.166912, val:  83.75%, val_best:  87.08%, tr:  99.82%, tr_best:  99.82%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "epoch-15  lr=['0.0050000'], tr/val_loss:  0.011340/  0.961710, val:  82.92%, val_best:  87.08%, tr:  99.64%, tr_best:  99.82%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "epoch-16  lr=['0.0050000'], tr/val_loss:  0.002800/  1.023525, val:  85.00%, val_best:  87.08%, tr:  99.95%, tr_best:  99.95%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "epoch-17  lr=['0.0050000'], tr/val_loss:  0.001469/  1.059613, val:  85.00%, val_best:  87.08%, tr:  99.98%, tr_best:  99.98%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "epoch-18  lr=['0.0050000'], tr/val_loss:  0.000941/  1.096207, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "epoch-19  lr=['0.0050000'], tr/val_loss:  0.000697/  1.090991, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "epoch-20  lr=['0.0050000'], tr/val_loss:  0.000558/  1.099322, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "epoch-21  lr=['0.0050000'], tr/val_loss:  0.000502/  1.118408, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "epoch-22  lr=['0.0050000'], tr/val_loss:  0.000478/  1.148352, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "epoch-23  lr=['0.0050000'], tr/val_loss:  0.000425/  1.147652, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "epoch-24  lr=['0.0050000'], tr/val_loss:  0.000327/  1.147916, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "epoch-25  lr=['0.0050000'], tr/val_loss:  0.000327/  1.158009, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "epoch-26  lr=['0.0050000'], tr/val_loss:  0.000288/  1.163146, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "epoch-27  lr=['0.0050000'], tr/val_loss:  0.000263/  1.145487, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "epoch-28  lr=['0.0050000'], tr/val_loss:  0.000239/  1.160463, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "epoch-29  lr=['0.0050000'], tr/val_loss:  0.000230/  1.161625, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "epoch-30  lr=['0.0050000'], tr/val_loss:  0.000221/  1.174396, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "epoch-31  lr=['0.0050000'], tr/val_loss:  0.000201/  1.175792, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "epoch-32  lr=['0.0050000'], tr/val_loss:  0.000192/  1.179037, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "epoch-33  lr=['0.0050000'], tr/val_loss:  0.000181/  1.194281, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "epoch-34  lr=['0.0050000'], tr/val_loss:  0.000173/  1.203204, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "epoch-35  lr=['0.0050000'], tr/val_loss:  0.000162/  1.214156, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.98 seconds, 1.23 minutes\n",
      "epoch-36  lr=['0.0050000'], tr/val_loss:  0.000166/  1.207952, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.30 seconds, 1.22 minutes\n",
      "epoch-37  lr=['0.0050000'], tr/val_loss:  0.000162/  1.208429, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "epoch-38  lr=['0.0050000'], tr/val_loss:  0.000152/  1.202297, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "epoch-39  lr=['0.0050000'], tr/val_loss:  0.000149/  1.207972, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "epoch-40  lr=['0.0050000'], tr/val_loss:  0.000144/  1.228047, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "epoch-41  lr=['0.0050000'], tr/val_loss:  0.000141/  1.214273, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "epoch-42  lr=['0.0050000'], tr/val_loss:  0.000138/  1.207720, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.50 seconds, 1.23 minutes\n",
      "epoch-43  lr=['0.0050000'], tr/val_loss:  0.000128/  1.209769, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "epoch-44  lr=['0.0050000'], tr/val_loss:  0.000120/  1.206291, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.47 seconds, 1.21 minutes\n",
      "epoch-45  lr=['0.0050000'], tr/val_loss:  0.000120/  1.210623, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "epoch-46  lr=['0.0050000'], tr/val_loss:  0.000116/  1.222438, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.14 seconds, 1.22 minutes\n",
      "epoch-47  lr=['0.0050000'], tr/val_loss:  0.000120/  1.221250, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "epoch-48  lr=['0.0050000'], tr/val_loss:  0.000118/  1.218927, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "epoch-49  lr=['0.0050000'], tr/val_loss:  0.000111/  1.218885, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "epoch-50  lr=['0.0050000'], tr/val_loss:  0.000106/  1.211198, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.93 seconds, 1.23 minutes\n",
      "epoch-51  lr=['0.0050000'], tr/val_loss:  0.000108/  1.212717, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "epoch-52  lr=['0.0050000'], tr/val_loss:  0.000104/  1.221941, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "epoch-53  lr=['0.0050000'], tr/val_loss:  0.000102/  1.226160, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "epoch-54  lr=['0.0050000'], tr/val_loss:  0.000100/  1.212718, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "epoch-55  lr=['0.0050000'], tr/val_loss:  0.000098/  1.208728, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "epoch-56  lr=['0.0050000'], tr/val_loss:  0.000091/  1.208804, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "epoch-57  lr=['0.0050000'], tr/val_loss:  0.000092/  1.206586, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.19 seconds, 1.22 minutes\n",
      "epoch-58  lr=['0.0050000'], tr/val_loss:  0.000092/  1.220065, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.00 seconds, 1.22 minutes\n",
      "epoch-59  lr=['0.0050000'], tr/val_loss:  0.000092/  1.218028, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "epoch-60  lr=['0.0050000'], tr/val_loss:  0.000087/  1.204833, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.17 seconds, 1.22 minutes\n",
      "epoch-61  lr=['0.0050000'], tr/val_loss:  0.000090/  1.211352, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "epoch-62  lr=['0.0050000'], tr/val_loss:  0.000090/  1.220404, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.89 seconds, 1.21 minutes\n",
      "epoch-63  lr=['0.0050000'], tr/val_loss:  0.000088/  1.227577, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.55 seconds, 1.23 minutes\n",
      "epoch-64  lr=['0.0050000'], tr/val_loss:  0.000085/  1.225256, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "epoch-65  lr=['0.0050000'], tr/val_loss:  0.000082/  1.226524, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "epoch-66  lr=['0.0050000'], tr/val_loss:  0.000081/  1.223272, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "epoch-67  lr=['0.0050000'], tr/val_loss:  0.000081/  1.233059, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "epoch-68  lr=['0.0050000'], tr/val_loss:  0.000078/  1.225596, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "epoch-69  lr=['0.0050000'], tr/val_loss:  0.000076/  1.215281, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "epoch-70  lr=['0.0050000'], tr/val_loss:  0.000076/  1.230168, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "epoch-71  lr=['0.0050000'], tr/val_loss:  0.000075/  1.213396, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "epoch-72  lr=['0.0050000'], tr/val_loss:  0.000072/  1.215289, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.14 seconds, 1.22 minutes\n",
      "epoch-73  lr=['0.0050000'], tr/val_loss:  0.000073/  1.217305, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "epoch-74  lr=['0.0050000'], tr/val_loss:  0.000070/  1.217193, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.14 seconds, 1.22 minutes\n",
      "epoch-75  lr=['0.0050000'], tr/val_loss:  0.000069/  1.226559, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "epoch-76  lr=['0.0050000'], tr/val_loss:  0.000068/  1.232770, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "epoch-77  lr=['0.0050000'], tr/val_loss:  0.000068/  1.226855, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "epoch-78  lr=['0.0050000'], tr/val_loss:  0.000066/  1.236657, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "epoch-79  lr=['0.0050000'], tr/val_loss:  0.000064/  1.243680, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.33 seconds, 1.24 minutes\n",
      "epoch-80  lr=['0.0050000'], tr/val_loss:  0.000063/  1.249027, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "epoch-81  lr=['0.0050000'], tr/val_loss:  0.000062/  1.249194, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "epoch-82  lr=['0.0050000'], tr/val_loss:  0.000060/  1.259512, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "epoch-83  lr=['0.0050000'], tr/val_loss:  0.000061/  1.262930, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "epoch-84  lr=['0.0050000'], tr/val_loss:  0.000059/  1.262103, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "epoch-85  lr=['0.0050000'], tr/val_loss:  0.000060/  1.261884, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.55 seconds, 1.23 minutes\n",
      "epoch-86  lr=['0.0050000'], tr/val_loss:  0.000058/  1.262554, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "epoch-87  lr=['0.0050000'], tr/val_loss:  0.000055/  1.256879, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "epoch-88  lr=['0.0050000'], tr/val_loss:  0.000056/  1.243458, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "epoch-89  lr=['0.0050000'], tr/val_loss:  0.000055/  1.241338, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "epoch-90  lr=['0.0050000'], tr/val_loss:  0.000055/  1.243745, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "epoch-91  lr=['0.0050000'], tr/val_loss:  0.000054/  1.243114, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "epoch-92  lr=['0.0050000'], tr/val_loss:  0.000055/  1.248865, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "epoch-93  lr=['0.0050000'], tr/val_loss:  0.000052/  1.251347, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "epoch-94  lr=['0.0050000'], tr/val_loss:  0.000053/  1.253726, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "epoch-95  lr=['0.0050000'], tr/val_loss:  0.000052/  1.251551, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "epoch-96  lr=['0.0050000'], tr/val_loss:  0.000052/  1.242908, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "epoch-97  lr=['0.0050000'], tr/val_loss:  0.000051/  1.248919, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "epoch-98  lr=['0.0050000'], tr/val_loss:  0.000050/  1.248572, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "epoch-99  lr=['0.0050000'], tr/val_loss:  0.000050/  1.244119, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "epoch-100 lr=['0.0050000'], tr/val_loss:  0.000050/  1.246780, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "epoch-101 lr=['0.0050000'], tr/val_loss:  0.000052/  1.249048, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "epoch-102 lr=['0.0050000'], tr/val_loss:  0.000052/  1.243047, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.45 seconds, 1.22 minutes\n",
      "epoch-103 lr=['0.0050000'], tr/val_loss:  0.000050/  1.236017, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "epoch-104 lr=['0.0050000'], tr/val_loss:  0.000051/  1.247273, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "epoch-105 lr=['0.0050000'], tr/val_loss:  0.000050/  1.253168, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.89 seconds, 1.21 minutes\n",
      "epoch-106 lr=['0.0050000'], tr/val_loss:  0.000049/  1.255218, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.01 seconds, 1.22 minutes\n",
      "epoch-107 lr=['0.0050000'], tr/val_loss:  0.000048/  1.247910, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "epoch-108 lr=['0.0050000'], tr/val_loss:  0.000048/  1.253492, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "epoch-109 lr=['0.0050000'], tr/val_loss:  0.000048/  1.244219, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "epoch-110 lr=['0.0050000'], tr/val_loss:  0.000048/  1.242000, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "epoch-111 lr=['0.0050000'], tr/val_loss:  0.000046/  1.236701, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "epoch-112 lr=['0.0050000'], tr/val_loss:  0.000047/  1.244922, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.81 seconds, 1.23 minutes\n",
      "epoch-113 lr=['0.0050000'], tr/val_loss:  0.000046/  1.241659, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "epoch-114 lr=['0.0050000'], tr/val_loss:  0.000046/  1.238431, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.68 seconds, 1.21 minutes\n",
      "epoch-115 lr=['0.0050000'], tr/val_loss:  0.000044/  1.247397, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "epoch-116 lr=['0.0050000'], tr/val_loss:  0.000045/  1.245516, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "epoch-117 lr=['0.0050000'], tr/val_loss:  0.000044/  1.246361, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "epoch-118 lr=['0.0050000'], tr/val_loss:  0.000044/  1.250461, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "epoch-119 lr=['0.0050000'], tr/val_loss:  0.000043/  1.248350, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.19 seconds, 1.22 minutes\n",
      "epoch-120 lr=['0.0050000'], tr/val_loss:  0.000044/  1.247994, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "epoch-121 lr=['0.0050000'], tr/val_loss:  0.000043/  1.240536, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.88 seconds, 1.21 minutes\n",
      "epoch-122 lr=['0.0050000'], tr/val_loss:  0.000044/  1.240071, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "epoch-123 lr=['0.0050000'], tr/val_loss:  0.000041/  1.235388, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "epoch-124 lr=['0.0050000'], tr/val_loss:  0.000041/  1.249516, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "epoch-125 lr=['0.0050000'], tr/val_loss:  0.000041/  1.249810, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.74 seconds, 1.23 minutes\n",
      "epoch-126 lr=['0.0050000'], tr/val_loss:  0.000041/  1.246827, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "epoch-127 lr=['0.0050000'], tr/val_loss:  0.000039/  1.254674, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "epoch-128 lr=['0.0050000'], tr/val_loss:  0.000039/  1.257758, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.45 seconds, 1.22 minutes\n",
      "epoch-129 lr=['0.0050000'], tr/val_loss:  0.000039/  1.259569, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "epoch-130 lr=['0.0050000'], tr/val_loss:  0.000038/  1.261120, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.48 seconds, 1.22 minutes\n",
      "epoch-131 lr=['0.0050000'], tr/val_loss:  0.000039/  1.255841, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "epoch-132 lr=['0.0050000'], tr/val_loss:  0.000038/  1.256055, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "epoch-133 lr=['0.0050000'], tr/val_loss:  0.000037/  1.256906, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "epoch-134 lr=['0.0050000'], tr/val_loss:  0.000037/  1.258288, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "epoch-135 lr=['0.0050000'], tr/val_loss:  0.000036/  1.248796, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "epoch-136 lr=['0.0050000'], tr/val_loss:  0.000036/  1.254773, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "epoch-137 lr=['0.0050000'], tr/val_loss:  0.000035/  1.256165, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "epoch-138 lr=['0.0050000'], tr/val_loss:  0.000035/  1.260101, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "epoch-139 lr=['0.0050000'], tr/val_loss:  0.000035/  1.254819, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "epoch-140 lr=['0.0050000'], tr/val_loss:  0.000034/  1.260767, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "epoch-141 lr=['0.0050000'], tr/val_loss:  0.000034/  1.259057, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "epoch-142 lr=['0.0050000'], tr/val_loss:  0.000034/  1.257170, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "epoch-143 lr=['0.0050000'], tr/val_loss:  0.000033/  1.252531, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.48 seconds, 1.22 minutes\n",
      "epoch-144 lr=['0.0050000'], tr/val_loss:  0.000033/  1.247093, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "epoch-145 lr=['0.0050000'], tr/val_loss:  0.000033/  1.246923, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.25 seconds, 1.24 minutes\n",
      "epoch-146 lr=['0.0050000'], tr/val_loss:  0.000033/  1.240848, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.95 seconds, 1.23 minutes\n",
      "epoch-147 lr=['0.0050000'], tr/val_loss:  0.000033/  1.244024, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.59 seconds, 1.23 minutes\n",
      "epoch-148 lr=['0.0050000'], tr/val_loss:  0.000032/  1.240264, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "epoch-149 lr=['0.0050000'], tr/val_loss:  0.000032/  1.241435, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "epoch-150 lr=['0.0050000'], tr/val_loss:  0.000032/  1.242470, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "epoch-151 lr=['0.0050000'], tr/val_loss:  0.000031/  1.239398, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "epoch-152 lr=['0.0050000'], tr/val_loss:  0.000032/  1.239796, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.50 seconds, 1.23 minutes\n",
      "epoch-153 lr=['0.0050000'], tr/val_loss:  0.000032/  1.249382, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.46 seconds, 1.24 minutes\n",
      "epoch-154 lr=['0.0050000'], tr/val_loss:  0.000031/  1.250453, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "epoch-155 lr=['0.0050000'], tr/val_loss:  0.000031/  1.244341, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "epoch-156 lr=['0.0050000'], tr/val_loss:  0.000031/  1.239169, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.30 seconds, 1.22 minutes\n",
      "epoch-157 lr=['0.0050000'], tr/val_loss:  0.000031/  1.234974, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "epoch-158 lr=['0.0050000'], tr/val_loss:  0.000031/  1.235477, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.67 seconds, 1.23 minutes\n",
      "epoch-159 lr=['0.0050000'], tr/val_loss:  0.000031/  1.238791, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "epoch-160 lr=['0.0050000'], tr/val_loss:  0.000031/  1.234424, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.42 seconds, 1.24 minutes\n",
      "epoch-161 lr=['0.0050000'], tr/val_loss:  0.000030/  1.233945, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.55 seconds, 1.23 minutes\n",
      "epoch-162 lr=['0.0050000'], tr/val_loss:  0.000031/  1.229742, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "epoch-163 lr=['0.0050000'], tr/val_loss:  0.000031/  1.229807, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "epoch-164 lr=['0.0050000'], tr/val_loss:  0.000030/  1.227538, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "epoch-165 lr=['0.0050000'], tr/val_loss:  0.000030/  1.226641, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "epoch-166 lr=['0.0050000'], tr/val_loss:  0.000030/  1.231356, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "epoch-167 lr=['0.0050000'], tr/val_loss:  0.000030/  1.221402, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "epoch-168 lr=['0.0050000'], tr/val_loss:  0.000030/  1.225448, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "epoch-169 lr=['0.0050000'], tr/val_loss:  0.000030/  1.224071, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "epoch-170 lr=['0.0050000'], tr/val_loss:  0.000029/  1.221449, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.17 seconds, 1.22 minutes\n",
      "epoch-171 lr=['0.0050000'], tr/val_loss:  0.000030/  1.216942, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "epoch-172 lr=['0.0050000'], tr/val_loss:  0.000029/  1.222405, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.50 seconds, 1.22 minutes\n",
      "epoch-173 lr=['0.0050000'], tr/val_loss:  0.000029/  1.222903, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.60 seconds, 1.21 minutes\n",
      "epoch-174 lr=['0.0050000'], tr/val_loss:  0.000030/  1.224515, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "epoch-175 lr=['0.0050000'], tr/val_loss:  0.000029/  1.218502, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.11 seconds, 1.24 minutes\n",
      "epoch-176 lr=['0.0050000'], tr/val_loss:  0.000029/  1.209243, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "epoch-177 lr=['0.0050000'], tr/val_loss:  0.000029/  1.213269, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "epoch-178 lr=['0.0050000'], tr/val_loss:  0.000029/  1.213722, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "epoch-179 lr=['0.0050000'], tr/val_loss:  0.000029/  1.221486, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "epoch-180 lr=['0.0050000'], tr/val_loss:  0.000029/  1.222046, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "epoch-181 lr=['0.0050000'], tr/val_loss:  0.000028/  1.223438, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "epoch-182 lr=['0.0050000'], tr/val_loss:  0.000028/  1.225313, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.22 seconds, 1.20 minutes\n",
      "epoch-183 lr=['0.0050000'], tr/val_loss:  0.000028/  1.226117, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "epoch-184 lr=['0.0050000'], tr/val_loss:  0.000027/  1.220879, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.19 seconds, 1.22 minutes\n",
      "epoch-185 lr=['0.0050000'], tr/val_loss:  0.000027/  1.221287, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "epoch-186 lr=['0.0050000'], tr/val_loss:  0.000027/  1.221038, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.78 seconds, 1.21 minutes\n",
      "epoch-187 lr=['0.0050000'], tr/val_loss:  0.000026/  1.220536, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.50 seconds, 1.23 minutes\n",
      "epoch-188 lr=['0.0050000'], tr/val_loss:  0.000026/  1.221341, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "epoch-189 lr=['0.0050000'], tr/val_loss:  0.000026/  1.224078, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "epoch-190 lr=['0.0050000'], tr/val_loss:  0.000025/  1.222097, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.14 seconds, 1.22 minutes\n",
      "epoch-191 lr=['0.0050000'], tr/val_loss:  0.000026/  1.222598, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "epoch-192 lr=['0.0050000'], tr/val_loss:  0.000025/  1.224807, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "epoch-193 lr=['0.0050000'], tr/val_loss:  0.000025/  1.226501, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.21 minutes\n",
      "epoch-194 lr=['0.0050000'], tr/val_loss:  0.000025/  1.226078, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "epoch-195 lr=['0.0050000'], tr/val_loss:  0.000025/  1.235545, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "epoch-196 lr=['0.0050000'], tr/val_loss:  0.000024/  1.239925, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "epoch-197 lr=['0.0050000'], tr/val_loss:  0.000025/  1.244163, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "epoch-198 lr=['0.0050000'], tr/val_loss:  0.000025/  1.238116, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.74 seconds, 1.23 minutes\n",
      "epoch-199 lr=['0.0050000'], tr/val_loss:  0.000024/  1.231293, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6cbbe90b3046ebbf95a5cf733579fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>2e-05</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>1.23129</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">likely-sweep-63</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2mw26pzb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2mw26pzb</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251104_215239-2mw26pzb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hdzgxsil with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251105_015837-hdzgxsil</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hdzgxsil' target=\"_blank\">laced-sweep-71</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hdzgxsil' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hdzgxsil</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': False, 'unique_name': '20251105_015845_977', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.005, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 18, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f1351bdee3d35c47af449525e007adf4\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 713fc8490e508dff15bed30e755e5ae6\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 4e1f85d1c0ff71c6a0df58e4542628f1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 626851829f1152228185abed5f6dca5e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 3c558307d41237628ef99b773b1c784f\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 81e44cafc682693b4b2086c7f81d224b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = b36895cc577f68764ec3fb2cd889299e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = efeaf39262f61e3ee121af192099af65\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a15007b6dd509fde13c404b80435835e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 6fa31eb5ad96d639190daf0921d987e2\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=0, sg_width=4, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.005\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch-0   lr=['0.0050000'], tr/val_loss:  1.043511/  1.220115, val:  52.08%, val_best:  52.08%, tr:  59.54%, tr_best:  59.54%, epoch time: 70.60 seconds, 1.18 minutes\n",
      "epoch-1   lr=['0.0050000'], tr/val_loss:  0.679516/  0.835070, val:  64.58%, val_best:  64.58%, tr:  72.11%, tr_best:  72.11%, epoch time: 70.97 seconds, 1.18 minutes\n",
      "epoch-2   lr=['0.0050000'], tr/val_loss:  0.496110/  1.058631, val:  71.25%, val_best:  71.25%, tr:  81.06%, tr_best:  81.06%, epoch time: 70.75 seconds, 1.18 minutes\n",
      "epoch-3   lr=['0.0050000'], tr/val_loss:  0.372842/  0.763005, val:  76.25%, val_best:  76.25%, tr:  85.82%, tr_best:  85.82%, epoch time: 70.90 seconds, 1.18 minutes\n",
      "epoch-4   lr=['0.0050000'], tr/val_loss:  0.263625/  0.934837, val:  75.00%, val_best:  76.25%, tr:  90.85%, tr_best:  90.85%, epoch time: 70.67 seconds, 1.18 minutes\n",
      "epoch-5   lr=['0.0050000'], tr/val_loss:  0.222994/  0.749039, val:  82.92%, val_best:  82.92%, tr:  92.20%, tr_best:  92.20%, epoch time: 70.36 seconds, 1.17 minutes\n",
      "epoch-6   lr=['0.0050000'], tr/val_loss:  0.170585/  0.662655, val:  87.08%, val_best:  87.08%, tr:  94.57%, tr_best:  94.57%, epoch time: 70.01 seconds, 1.17 minutes\n",
      "epoch-7   lr=['0.0050000'], tr/val_loss:  0.153818/  0.803674, val:  80.83%, val_best:  87.08%, tr:  94.88%, tr_best:  94.88%, epoch time: 70.61 seconds, 1.18 minutes\n",
      "epoch-8   lr=['0.0050000'], tr/val_loss:  0.113855/  1.108603, val:  79.58%, val_best:  87.08%, tr:  96.57%, tr_best:  96.57%, epoch time: 70.34 seconds, 1.17 minutes\n",
      "epoch-9   lr=['0.0050000'], tr/val_loss:  0.112507/  1.053447, val:  77.50%, val_best:  87.08%, tr:  96.10%, tr_best:  96.57%, epoch time: 70.33 seconds, 1.17 minutes\n",
      "epoch-10  lr=['0.0050000'], tr/val_loss:  0.069517/  0.894315, val:  82.50%, val_best:  87.08%, tr:  97.72%, tr_best:  97.72%, epoch time: 71.12 seconds, 1.19 minutes\n",
      "epoch-11  lr=['0.0050000'], tr/val_loss:  0.067641/  1.065984, val:  78.75%, val_best:  87.08%, tr:  97.88%, tr_best:  97.88%, epoch time: 70.31 seconds, 1.17 minutes\n",
      "epoch-12  lr=['0.0050000'], tr/val_loss:  0.057105/  1.018360, val:  82.08%, val_best:  87.08%, tr:  98.40%, tr_best:  98.40%, epoch time: 70.48 seconds, 1.17 minutes\n",
      "epoch-13  lr=['0.0050000'], tr/val_loss:  0.061082/  0.963812, val:  84.58%, val_best:  87.08%, tr:  97.77%, tr_best:  98.40%, epoch time: 70.22 seconds, 1.17 minutes\n",
      "epoch-14  lr=['0.0050000'], tr/val_loss:  0.059299/  0.787312, val:  84.58%, val_best:  87.08%, tr:  98.20%, tr_best:  98.40%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "epoch-15  lr=['0.0050000'], tr/val_loss:  0.062515/  1.145097, val:  76.25%, val_best:  87.08%, tr:  97.99%, tr_best:  98.40%, epoch time: 70.75 seconds, 1.18 minutes\n",
      "epoch-16  lr=['0.0050000'], tr/val_loss:  0.025252/  1.026765, val:  80.00%, val_best:  87.08%, tr:  99.30%, tr_best:  99.30%, epoch time: 70.66 seconds, 1.18 minutes\n",
      "epoch-17  lr=['0.0050000'], tr/val_loss:  0.016535/  0.897358, val:  83.75%, val_best:  87.08%, tr:  99.50%, tr_best:  99.50%, epoch time: 71.05 seconds, 1.18 minutes\n",
      "epoch-18  lr=['0.0050000'], tr/val_loss:  0.013905/  0.985201, val:  83.33%, val_best:  87.08%, tr:  99.57%, tr_best:  99.57%, epoch time: 71.05 seconds, 1.18 minutes\n",
      "epoch-19  lr=['0.0050000'], tr/val_loss:  0.048685/  0.874428, val:  83.75%, val_best:  87.08%, tr:  98.78%, tr_best:  99.57%, epoch time: 70.71 seconds, 1.18 minutes\n",
      "epoch-20  lr=['0.0050000'], tr/val_loss:  0.014201/  0.903457, val:  82.50%, val_best:  87.08%, tr:  99.68%, tr_best:  99.68%, epoch time: 70.72 seconds, 1.18 minutes\n",
      "epoch-21  lr=['0.0050000'], tr/val_loss:  0.015530/  0.986744, val:  82.08%, val_best:  87.08%, tr:  99.66%, tr_best:  99.68%, epoch time: 71.06 seconds, 1.18 minutes\n",
      "epoch-22  lr=['0.0050000'], tr/val_loss:  0.011111/  0.947695, val:  82.92%, val_best:  87.08%, tr:  99.75%, tr_best:  99.75%, epoch time: 70.96 seconds, 1.18 minutes\n",
      "epoch-23  lr=['0.0050000'], tr/val_loss:  0.002625/  0.939561, val:  85.00%, val_best:  87.08%, tr:  99.91%, tr_best:  99.91%, epoch time: 70.77 seconds, 1.18 minutes\n",
      "epoch-24  lr=['0.0050000'], tr/val_loss:  0.000673/  0.952961, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.78 seconds, 1.18 minutes\n",
      "epoch-25  lr=['0.0050000'], tr/val_loss:  0.000264/  0.959457, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.90 seconds, 1.18 minutes\n",
      "epoch-26  lr=['0.0050000'], tr/val_loss:  0.000212/  0.936534, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.75 seconds, 1.18 minutes\n",
      "epoch-27  lr=['0.0050000'], tr/val_loss:  0.000177/  0.953801, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.12 seconds, 1.19 minutes\n",
      "epoch-28  lr=['0.0050000'], tr/val_loss:  0.000161/  0.952552, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.37 seconds, 1.19 minutes\n",
      "epoch-29  lr=['0.0050000'], tr/val_loss:  0.000132/  0.961533, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "epoch-30  lr=['0.0050000'], tr/val_loss:  0.000112/  0.966821, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.30 seconds, 1.19 minutes\n",
      "epoch-31  lr=['0.0050000'], tr/val_loss:  0.000103/  0.981698, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.98 seconds, 1.18 minutes\n",
      "epoch-32  lr=['0.0050000'], tr/val_loss:  0.000100/  0.984944, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.31 seconds, 1.19 minutes\n",
      "epoch-33  lr=['0.0050000'], tr/val_loss:  0.000093/  0.992115, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.36 seconds, 1.19 minutes\n",
      "epoch-34  lr=['0.0050000'], tr/val_loss:  0.000087/  0.999630, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.17 seconds, 1.17 minutes\n",
      "epoch-35  lr=['0.0050000'], tr/val_loss:  0.000082/  0.989382, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.06 seconds, 1.17 minutes\n",
      "epoch-36  lr=['0.0050000'], tr/val_loss:  0.000076/  0.996607, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.31 seconds, 1.19 minutes\n",
      "epoch-37  lr=['0.0050000'], tr/val_loss:  0.000074/  0.992397, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.96 seconds, 1.18 minutes\n",
      "epoch-38  lr=['0.0050000'], tr/val_loss:  0.000071/  0.994727, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.10 seconds, 1.19 minutes\n",
      "epoch-39  lr=['0.0050000'], tr/val_loss:  0.000068/  0.994890, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.11 seconds, 1.19 minutes\n",
      "epoch-40  lr=['0.0050000'], tr/val_loss:  0.000067/  0.994090, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-41  lr=['0.0050000'], tr/val_loss:  0.000065/  0.983501, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.72 seconds, 1.18 minutes\n",
      "epoch-42  lr=['0.0050000'], tr/val_loss:  0.000061/  0.989534, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.54 seconds, 1.18 minutes\n",
      "epoch-43  lr=['0.0050000'], tr/val_loss:  0.000061/  0.989032, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.29 seconds, 1.19 minutes\n",
      "epoch-44  lr=['0.0050000'], tr/val_loss:  0.000057/  0.993450, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.70 seconds, 1.19 minutes\n",
      "epoch-45  lr=['0.0050000'], tr/val_loss:  0.000057/  0.996837, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.93 seconds, 1.18 minutes\n",
      "epoch-46  lr=['0.0050000'], tr/val_loss:  0.000054/  0.995224, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-47  lr=['0.0050000'], tr/val_loss:  0.000052/  0.992420, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.81 seconds, 1.18 minutes\n",
      "epoch-48  lr=['0.0050000'], tr/val_loss:  0.000051/  0.993943, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "epoch-49  lr=['0.0050000'], tr/val_loss:  0.000048/  1.003971, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.98 seconds, 1.18 minutes\n",
      "epoch-50  lr=['0.0050000'], tr/val_loss:  0.000045/  1.008202, val:  84.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.20 seconds, 1.19 minutes\n",
      "epoch-51  lr=['0.0050000'], tr/val_loss:  0.000045/  1.003759, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.03 seconds, 1.18 minutes\n",
      "epoch-52  lr=['0.0050000'], tr/val_loss:  0.000044/  1.000958, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.28 seconds, 1.19 minutes\n",
      "epoch-53  lr=['0.0050000'], tr/val_loss:  0.000042/  1.009969, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "epoch-54  lr=['0.0050000'], tr/val_loss:  0.000041/  1.009660, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.16 seconds, 1.19 minutes\n",
      "epoch-55  lr=['0.0050000'], tr/val_loss:  0.000040/  1.015448, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.17 seconds, 1.20 minutes\n",
      "epoch-56  lr=['0.0050000'], tr/val_loss:  0.000040/  1.020066, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.57 seconds, 1.19 minutes\n",
      "epoch-57  lr=['0.0050000'], tr/val_loss:  0.000039/  1.016707, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.83 seconds, 1.18 minutes\n",
      "epoch-58  lr=['0.0050000'], tr/val_loss:  0.000038/  1.017699, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-59  lr=['0.0050000'], tr/val_loss:  0.000038/  1.016824, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.72 seconds, 1.18 minutes\n",
      "epoch-60  lr=['0.0050000'], tr/val_loss:  0.000036/  1.021705, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.23 seconds, 1.19 minutes\n",
      "epoch-61  lr=['0.0050000'], tr/val_loss:  0.000035/  1.021020, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.04 seconds, 1.18 minutes\n",
      "epoch-62  lr=['0.0050000'], tr/val_loss:  0.000034/  1.023266, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.02 seconds, 1.18 minutes\n",
      "epoch-63  lr=['0.0050000'], tr/val_loss:  0.000034/  1.024457, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.78 seconds, 1.18 minutes\n",
      "epoch-64  lr=['0.0050000'], tr/val_loss:  0.000033/  1.022805, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.01 seconds, 1.18 minutes\n",
      "epoch-65  lr=['0.0050000'], tr/val_loss:  0.000032/  1.019925, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.15 seconds, 1.19 minutes\n",
      "epoch-66  lr=['0.0050000'], tr/val_loss:  0.000032/  1.025598, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.73 seconds, 1.20 minutes\n",
      "epoch-67  lr=['0.0050000'], tr/val_loss:  0.000032/  1.025431, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.01 seconds, 1.18 minutes\n",
      "epoch-68  lr=['0.0050000'], tr/val_loss:  0.000030/  1.027279, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.42 seconds, 1.19 minutes\n",
      "epoch-69  lr=['0.0050000'], tr/val_loss:  0.000030/  1.025092, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.46 seconds, 1.19 minutes\n",
      "epoch-70  lr=['0.0050000'], tr/val_loss:  0.000029/  1.029604, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.12 seconds, 1.19 minutes\n",
      "epoch-71  lr=['0.0050000'], tr/val_loss:  0.000029/  1.030607, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.72 seconds, 1.18 minutes\n",
      "epoch-72  lr=['0.0050000'], tr/val_loss:  0.000029/  1.029093, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.13 seconds, 1.19 minutes\n",
      "epoch-73  lr=['0.0050000'], tr/val_loss:  0.000028/  1.031996, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.61 seconds, 1.19 minutes\n",
      "epoch-74  lr=['0.0050000'], tr/val_loss:  0.000027/  1.031465, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.18 seconds, 1.19 minutes\n",
      "epoch-75  lr=['0.0050000'], tr/val_loss:  0.000026/  1.034887, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.58 seconds, 1.18 minutes\n",
      "epoch-76  lr=['0.0050000'], tr/val_loss:  0.000026/  1.037066, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.54 seconds, 1.18 minutes\n",
      "epoch-77  lr=['0.0050000'], tr/val_loss:  0.000025/  1.042886, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.88 seconds, 1.18 minutes\n",
      "epoch-78  lr=['0.0050000'], tr/val_loss:  0.000025/  1.043030, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.91 seconds, 1.18 minutes\n",
      "epoch-79  lr=['0.0050000'], tr/val_loss:  0.000025/  1.043700, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.83 seconds, 1.18 minutes\n",
      "epoch-80  lr=['0.0050000'], tr/val_loss:  0.000025/  1.044654, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.49 seconds, 1.19 minutes\n",
      "epoch-81  lr=['0.0050000'], tr/val_loss:  0.000024/  1.047482, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.39 seconds, 1.19 minutes\n",
      "epoch-82  lr=['0.0050000'], tr/val_loss:  0.000023/  1.043712, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.25 seconds, 1.19 minutes\n",
      "epoch-83  lr=['0.0050000'], tr/val_loss:  0.000023/  1.049278, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-84  lr=['0.0050000'], tr/val_loss:  0.000023/  1.050801, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.28 seconds, 1.19 minutes\n",
      "epoch-85  lr=['0.0050000'], tr/val_loss:  0.000022/  1.057861, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.13 seconds, 1.19 minutes\n",
      "epoch-86  lr=['0.0050000'], tr/val_loss:  0.000022/  1.065249, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.10 seconds, 1.19 minutes\n",
      "epoch-87  lr=['0.0050000'], tr/val_loss:  0.000022/  1.063305, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.14 seconds, 1.19 minutes\n",
      "epoch-88  lr=['0.0050000'], tr/val_loss:  0.000021/  1.060302, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.34 seconds, 1.19 minutes\n",
      "epoch-89  lr=['0.0050000'], tr/val_loss:  0.000022/  1.064670, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.39 seconds, 1.19 minutes\n",
      "epoch-90  lr=['0.0050000'], tr/val_loss:  0.000021/  1.066798, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-91  lr=['0.0050000'], tr/val_loss:  0.000022/  1.056149, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.48 seconds, 1.19 minutes\n",
      "epoch-92  lr=['0.0050000'], tr/val_loss:  0.000021/  1.055386, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.89 seconds, 1.18 minutes\n",
      "epoch-93  lr=['0.0050000'], tr/val_loss:  0.000021/  1.054987, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.41 seconds, 1.19 minutes\n",
      "epoch-94  lr=['0.0050000'], tr/val_loss:  0.000020/  1.056691, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.26 seconds, 1.19 minutes\n",
      "epoch-95  lr=['0.0050000'], tr/val_loss:  0.000020/  1.057516, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.76 seconds, 1.18 minutes\n",
      "epoch-96  lr=['0.0050000'], tr/val_loss:  0.000019/  1.055441, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.33 seconds, 1.19 minutes\n",
      "epoch-97  lr=['0.0050000'], tr/val_loss:  0.000019/  1.055419, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.87 seconds, 1.18 minutes\n",
      "epoch-98  lr=['0.0050000'], tr/val_loss:  0.000019/  1.060073, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "epoch-99  lr=['0.0050000'], tr/val_loss:  0.000018/  1.063807, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.95 seconds, 1.18 minutes\n",
      "epoch-100 lr=['0.0050000'], tr/val_loss:  0.000018/  1.064783, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.94 seconds, 1.18 minutes\n",
      "epoch-101 lr=['0.0050000'], tr/val_loss:  0.000018/  1.067142, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.75 seconds, 1.18 minutes\n",
      "epoch-102 lr=['0.0050000'], tr/val_loss:  0.000018/  1.064340, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.72 seconds, 1.18 minutes\n",
      "epoch-103 lr=['0.0050000'], tr/val_loss:  0.000018/  1.062932, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.15 seconds, 1.19 minutes\n",
      "epoch-104 lr=['0.0050000'], tr/val_loss:  0.000017/  1.066424, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.16 seconds, 1.19 minutes\n",
      "epoch-105 lr=['0.0050000'], tr/val_loss:  0.000017/  1.068864, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.56 seconds, 1.18 minutes\n",
      "epoch-106 lr=['0.0050000'], tr/val_loss:  0.000017/  1.070956, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.08 seconds, 1.18 minutes\n",
      "epoch-107 lr=['0.0050000'], tr/val_loss:  0.000017/  1.072949, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.97 seconds, 1.18 minutes\n",
      "epoch-108 lr=['0.0050000'], tr/val_loss:  0.000017/  1.072859, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.82 seconds, 1.18 minutes\n",
      "epoch-109 lr=['0.0050000'], tr/val_loss:  0.000017/  1.078469, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.14 seconds, 1.19 minutes\n",
      "epoch-110 lr=['0.0050000'], tr/val_loss:  0.000016/  1.074702, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.44 seconds, 1.19 minutes\n",
      "epoch-111 lr=['0.0050000'], tr/val_loss:  0.000016/  1.075822, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.39 seconds, 1.19 minutes\n",
      "epoch-112 lr=['0.0050000'], tr/val_loss:  0.000016/  1.076082, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.22 seconds, 1.19 minutes\n",
      "epoch-113 lr=['0.0050000'], tr/val_loss:  0.000016/  1.075170, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.94 seconds, 1.18 minutes\n",
      "epoch-114 lr=['0.0050000'], tr/val_loss:  0.000016/  1.073438, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.95 seconds, 1.18 minutes\n",
      "epoch-115 lr=['0.0050000'], tr/val_loss:  0.000016/  1.080806, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.52 seconds, 1.19 minutes\n",
      "epoch-116 lr=['0.0050000'], tr/val_loss:  0.000015/  1.083712, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.79 seconds, 1.20 minutes\n",
      "epoch-117 lr=['0.0050000'], tr/val_loss:  0.000016/  1.083575, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-118 lr=['0.0050000'], tr/val_loss:  0.000015/  1.083320, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.81 seconds, 1.18 minutes\n",
      "epoch-119 lr=['0.0050000'], tr/val_loss:  0.000016/  1.086599, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.32 seconds, 1.19 minutes\n",
      "epoch-120 lr=['0.0050000'], tr/val_loss:  0.000016/  1.083997, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.56 seconds, 1.19 minutes\n",
      "epoch-121 lr=['0.0050000'], tr/val_loss:  0.000016/  1.084716, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.36 seconds, 1.19 minutes\n",
      "epoch-122 lr=['0.0050000'], tr/val_loss:  0.000015/  1.081630, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.90 seconds, 1.18 minutes\n",
      "epoch-123 lr=['0.0050000'], tr/val_loss:  0.000015/  1.078411, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.99 seconds, 1.18 minutes\n",
      "epoch-124 lr=['0.0050000'], tr/val_loss:  0.000015/  1.077374, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.33 seconds, 1.19 minutes\n",
      "epoch-125 lr=['0.0050000'], tr/val_loss:  0.000015/  1.079517, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.99 seconds, 1.18 minutes\n",
      "epoch-126 lr=['0.0050000'], tr/val_loss:  0.000014/  1.079936, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.01 seconds, 1.18 minutes\n",
      "epoch-127 lr=['0.0050000'], tr/val_loss:  0.000014/  1.077847, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.95 seconds, 1.18 minutes\n",
      "epoch-128 lr=['0.0050000'], tr/val_loss:  0.000014/  1.082705, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.18 seconds, 1.19 minutes\n",
      "epoch-129 lr=['0.0050000'], tr/val_loss:  0.000014/  1.084342, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.52 seconds, 1.19 minutes\n",
      "epoch-130 lr=['0.0050000'], tr/val_loss:  0.000014/  1.085166, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.85 seconds, 1.18 minutes\n",
      "epoch-131 lr=['0.0050000'], tr/val_loss:  0.000014/  1.078686, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.21 seconds, 1.19 minutes\n",
      "epoch-132 lr=['0.0050000'], tr/val_loss:  0.000013/  1.079777, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.59 seconds, 1.19 minutes\n",
      "epoch-133 lr=['0.0050000'], tr/val_loss:  0.000013/  1.081455, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.19 seconds, 1.19 minutes\n",
      "epoch-134 lr=['0.0050000'], tr/val_loss:  0.000013/  1.080846, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.53 seconds, 1.19 minutes\n",
      "epoch-135 lr=['0.0050000'], tr/val_loss:  0.000013/  1.080107, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.66 seconds, 1.18 minutes\n",
      "epoch-136 lr=['0.0050000'], tr/val_loss:  0.000013/  1.082825, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.05 seconds, 1.18 minutes\n",
      "epoch-137 lr=['0.0050000'], tr/val_loss:  0.000013/  1.082400, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.33 seconds, 1.19 minutes\n",
      "epoch-138 lr=['0.0050000'], tr/val_loss:  0.000013/  1.087964, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.64 seconds, 1.18 minutes\n",
      "epoch-139 lr=['0.0050000'], tr/val_loss:  0.000012/  1.093341, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.59 seconds, 1.18 minutes\n",
      "epoch-140 lr=['0.0050000'], tr/val_loss:  0.000012/  1.093206, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.68 seconds, 1.19 minutes\n",
      "epoch-141 lr=['0.0050000'], tr/val_loss:  0.000012/  1.092148, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.05 seconds, 1.18 minutes\n",
      "epoch-142 lr=['0.0050000'], tr/val_loss:  0.000012/  1.093045, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.76 seconds, 1.18 minutes\n",
      "epoch-143 lr=['0.0050000'], tr/val_loss:  0.000012/  1.095366, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.28 seconds, 1.19 minutes\n",
      "epoch-144 lr=['0.0050000'], tr/val_loss:  0.000012/  1.097559, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.24 seconds, 1.19 minutes\n",
      "epoch-145 lr=['0.0050000'], tr/val_loss:  0.000012/  1.096584, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.64 seconds, 1.18 minutes\n",
      "epoch-146 lr=['0.0050000'], tr/val_loss:  0.000012/  1.092583, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.03 seconds, 1.18 minutes\n",
      "epoch-147 lr=['0.0050000'], tr/val_loss:  0.000012/  1.098097, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.61 seconds, 1.18 minutes\n",
      "epoch-148 lr=['0.0050000'], tr/val_loss:  0.000012/  1.090746, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.05 seconds, 1.18 minutes\n",
      "epoch-149 lr=['0.0050000'], tr/val_loss:  0.000012/  1.089277, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.77 seconds, 1.18 minutes\n",
      "epoch-150 lr=['0.0050000'], tr/val_loss:  0.000012/  1.090444, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.73 seconds, 1.18 minutes\n",
      "epoch-151 lr=['0.0050000'], tr/val_loss:  0.000012/  1.086383, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.89 seconds, 1.18 minutes\n",
      "epoch-152 lr=['0.0050000'], tr/val_loss:  0.000012/  1.086452, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.63 seconds, 1.18 minutes\n",
      "epoch-153 lr=['0.0050000'], tr/val_loss:  0.000012/  1.086530, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.02 seconds, 1.18 minutes\n",
      "epoch-154 lr=['0.0050000'], tr/val_loss:  0.000012/  1.087746, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.58 seconds, 1.18 minutes\n",
      "epoch-155 lr=['0.0050000'], tr/val_loss:  0.000012/  1.088292, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.03 seconds, 1.18 minutes\n",
      "epoch-156 lr=['0.0050000'], tr/val_loss:  0.000012/  1.087072, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.51 seconds, 1.18 minutes\n",
      "epoch-157 lr=['0.0050000'], tr/val_loss:  0.000012/  1.086720, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.82 seconds, 1.18 minutes\n",
      "epoch-158 lr=['0.0050000'], tr/val_loss:  0.000011/  1.087108, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.14 seconds, 1.19 minutes\n",
      "epoch-159 lr=['0.0050000'], tr/val_loss:  0.000011/  1.087247, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.02 seconds, 1.18 minutes\n",
      "epoch-160 lr=['0.0050000'], tr/val_loss:  0.000011/  1.088257, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.03 seconds, 1.18 minutes\n",
      "epoch-161 lr=['0.0050000'], tr/val_loss:  0.000011/  1.092287, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.82 seconds, 1.18 minutes\n",
      "epoch-162 lr=['0.0050000'], tr/val_loss:  0.000011/  1.092679, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.85 seconds, 1.18 minutes\n",
      "epoch-163 lr=['0.0050000'], tr/val_loss:  0.000011/  1.095174, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.23 seconds, 1.19 minutes\n",
      "epoch-164 lr=['0.0050000'], tr/val_loss:  0.000011/  1.100715, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.01 seconds, 1.18 minutes\n",
      "epoch-165 lr=['0.0050000'], tr/val_loss:  0.000011/  1.100137, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.85 seconds, 1.18 minutes\n",
      "epoch-166 lr=['0.0050000'], tr/val_loss:  0.000011/  1.100138, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.25 seconds, 1.19 minutes\n",
      "epoch-167 lr=['0.0050000'], tr/val_loss:  0.000011/  1.097552, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.87 seconds, 1.18 minutes\n",
      "epoch-168 lr=['0.0050000'], tr/val_loss:  0.000010/  1.097011, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.18 seconds, 1.19 minutes\n",
      "epoch-169 lr=['0.0050000'], tr/val_loss:  0.000010/  1.095606, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.23 seconds, 1.19 minutes\n",
      "epoch-170 lr=['0.0050000'], tr/val_loss:  0.000010/  1.096799, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.84 seconds, 1.18 minutes\n",
      "epoch-171 lr=['0.0050000'], tr/val_loss:  0.000010/  1.099551, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.11 seconds, 1.19 minutes\n",
      "epoch-172 lr=['0.0050000'], tr/val_loss:  0.000010/  1.096439, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.77 seconds, 1.18 minutes\n",
      "epoch-173 lr=['0.0050000'], tr/val_loss:  0.000010/  1.092455, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.99 seconds, 1.20 minutes\n",
      "epoch-174 lr=['0.0050000'], tr/val_loss:  0.000010/  1.091931, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.45 seconds, 1.17 minutes\n",
      "epoch-175 lr=['0.0050000'], tr/val_loss:  0.000010/  1.094070, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.58 seconds, 1.18 minutes\n",
      "epoch-176 lr=['0.0050000'], tr/val_loss:  0.000010/  1.091118, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.25 seconds, 1.19 minutes\n",
      "epoch-177 lr=['0.0050000'], tr/val_loss:  0.000010/  1.095883, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.88 seconds, 1.18 minutes\n",
      "epoch-178 lr=['0.0050000'], tr/val_loss:  0.000010/  1.097562, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.90 seconds, 1.20 minutes\n",
      "epoch-179 lr=['0.0050000'], tr/val_loss:  0.000010/  1.098974, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.77 seconds, 1.18 minutes\n",
      "epoch-180 lr=['0.0050000'], tr/val_loss:  0.000010/  1.096118, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.07 seconds, 1.18 minutes\n",
      "epoch-181 lr=['0.0050000'], tr/val_loss:  0.000010/  1.097049, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.26 seconds, 1.19 minutes\n",
      "epoch-182 lr=['0.0050000'], tr/val_loss:  0.000010/  1.097691, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.68 seconds, 1.18 minutes\n",
      "epoch-183 lr=['0.0050000'], tr/val_loss:  0.000010/  1.100806, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.04 seconds, 1.18 minutes\n",
      "epoch-184 lr=['0.0050000'], tr/val_loss:  0.000009/  1.102663, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-185 lr=['0.0050000'], tr/val_loss:  0.000009/  1.105596, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.86 seconds, 1.18 minutes\n",
      "epoch-186 lr=['0.0050000'], tr/val_loss:  0.000009/  1.107064, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.98 seconds, 1.18 minutes\n",
      "epoch-187 lr=['0.0050000'], tr/val_loss:  0.000009/  1.103554, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.64 seconds, 1.19 minutes\n",
      "epoch-188 lr=['0.0050000'], tr/val_loss:  0.000009/  1.104223, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.85 seconds, 1.18 minutes\n",
      "epoch-189 lr=['0.0050000'], tr/val_loss:  0.000009/  1.104451, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.84 seconds, 1.18 minutes\n",
      "epoch-190 lr=['0.0050000'], tr/val_loss:  0.000009/  1.108669, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.29 seconds, 1.19 minutes\n",
      "epoch-191 lr=['0.0050000'], tr/val_loss:  0.000009/  1.106629, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.85 seconds, 1.18 minutes\n",
      "epoch-192 lr=['0.0050000'], tr/val_loss:  0.000009/  1.105941, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "epoch-193 lr=['0.0050000'], tr/val_loss:  0.000009/  1.108033, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.58 seconds, 1.19 minutes\n",
      "epoch-194 lr=['0.0050000'], tr/val_loss:  0.000009/  1.107328, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.99 seconds, 1.18 minutes\n",
      "epoch-195 lr=['0.0050000'], tr/val_loss:  0.000009/  1.109255, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.39 seconds, 1.17 minutes\n",
      "epoch-196 lr=['0.0050000'], tr/val_loss:  0.000009/  1.112053, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.67 seconds, 1.18 minutes\n",
      "epoch-197 lr=['0.0050000'], tr/val_loss:  0.000009/  1.111235, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.53 seconds, 1.19 minutes\n",
      "epoch-198 lr=['0.0050000'], tr/val_loss:  0.000009/  1.110956, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.12 seconds, 1.19 minutes\n",
      "epoch-199 lr=['0.0050000'], tr/val_loss:  0.000009/  1.112537, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.71 seconds, 1.20 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3186f694dc534182b7f912259e916462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñá‚ñÖ‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñá‚ñÖ‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÉ‚ñÅ‚ñá‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1e-05</td></tr><tr><td>val_acc_best</td><td>0.87083</td></tr><tr><td>val_acc_now</td><td>0.86667</td></tr><tr><td>val_loss</td><td>1.11254</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">laced-sweep-71</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hdzgxsil' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/hdzgxsil</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251105_015837-hdzgxsil/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6qykvr7x with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251105_055657-6qykvr7x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6qykvr7x' target=\"_blank\">glowing-sweep-79</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6qykvr7x' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6qykvr7x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': False, 'unique_name': '20251105_055706_693', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.005, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 18, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f1351bdee3d35c47af449525e007adf4\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 713fc8490e508dff15bed30e755e5ae6\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 4e1f85d1c0ff71c6a0df58e4542628f1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 626851829f1152228185abed5f6dca5e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 3c558307d41237628ef99b773b1c784f\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 81e44cafc682693b4b2086c7f81d224b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = b36895cc577f68764ec3fb2cd889299e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = efeaf39262f61e3ee121af192099af65\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a15007b6dd509fde13c404b80435835e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 6fa31eb5ad96d639190daf0921d987e2\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=6, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=6, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.005\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch-0   lr=['0.0050000'], tr/val_loss:  1.030658/  1.045865, val:  59.58%, val_best:  59.58%, tr:  59.67%, tr_best:  59.67%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "epoch-1   lr=['0.0050000'], tr/val_loss:  0.672101/  0.929991, val:  69.58%, val_best:  69.58%, tr:  72.50%, tr_best:  72.50%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "epoch-2   lr=['0.0050000'], tr/val_loss:  0.476700/  0.775900, val:  77.50%, val_best:  77.50%, tr:  81.70%, tr_best:  81.70%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "epoch-3   lr=['0.0050000'], tr/val_loss:  0.308646/  0.816140, val:  75.83%, val_best:  77.50%, tr:  88.86%, tr_best:  88.86%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "epoch-4   lr=['0.0050000'], tr/val_loss:  0.250884/  0.842389, val:  77.92%, val_best:  77.92%, tr:  91.25%, tr_best:  91.25%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "epoch-5   lr=['0.0050000'], tr/val_loss:  0.207468/  0.669625, val:  82.50%, val_best:  82.50%, tr:  92.81%, tr_best:  92.81%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "epoch-6   lr=['0.0050000'], tr/val_loss:  0.137188/  0.692634, val:  85.42%, val_best:  85.42%, tr:  94.97%, tr_best:  94.97%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "epoch-7   lr=['0.0050000'], tr/val_loss:  0.105133/  0.882963, val:  80.42%, val_best:  85.42%, tr:  96.62%, tr_best:  96.62%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "epoch-8   lr=['0.0050000'], tr/val_loss:  0.106465/  0.858370, val:  80.83%, val_best:  85.42%, tr:  96.55%, tr_best:  96.62%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "epoch-9   lr=['0.0050000'], tr/val_loss:  0.092346/  0.771417, val:  81.67%, val_best:  85.42%, tr:  97.05%, tr_best:  97.05%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "epoch-10  lr=['0.0050000'], tr/val_loss:  0.076695/  0.945826, val:  81.67%, val_best:  85.42%, tr:  97.45%, tr_best:  97.45%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "epoch-11  lr=['0.0050000'], tr/val_loss:  0.035893/  0.874761, val:  83.75%, val_best:  85.42%, tr:  98.92%, tr_best:  98.92%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "epoch-12  lr=['0.0050000'], tr/val_loss:  0.042602/  0.939709, val:  83.33%, val_best:  85.42%, tr:  98.53%, tr_best:  98.92%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "epoch-13  lr=['0.0050000'], tr/val_loss:  0.040329/  0.791692, val:  84.58%, val_best:  85.42%, tr:  98.72%, tr_best:  98.92%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "epoch-14  lr=['0.0050000'], tr/val_loss:  0.043069/  0.810344, val:  82.50%, val_best:  85.42%, tr:  98.76%, tr_best:  98.92%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "epoch-15  lr=['0.0050000'], tr/val_loss:  0.068413/  0.828935, val:  84.17%, val_best:  85.42%, tr:  97.75%, tr_best:  98.92%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "epoch-16  lr=['0.0050000'], tr/val_loss:  0.042278/  1.292323, val:  82.92%, val_best:  85.42%, tr:  98.65%, tr_best:  98.92%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "epoch-17  lr=['0.0050000'], tr/val_loss:  0.033454/  0.996088, val:  86.25%, val_best:  86.25%, tr:  99.01%, tr_best:  99.01%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "epoch-18  lr=['0.0050000'], tr/val_loss:  0.017453/  0.926943, val:  85.42%, val_best:  86.25%, tr:  99.50%, tr_best:  99.50%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "epoch-19  lr=['0.0050000'], tr/val_loss:  0.008075/  1.040957, val:  85.00%, val_best:  86.25%, tr:  99.82%, tr_best:  99.82%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "epoch-20  lr=['0.0050000'], tr/val_loss:  0.002487/  1.065310, val:  85.42%, val_best:  86.25%, tr:  99.95%, tr_best:  99.95%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "epoch-21  lr=['0.0050000'], tr/val_loss:  0.007870/  1.121076, val:  83.75%, val_best:  86.25%, tr:  99.80%, tr_best:  99.95%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "epoch-22  lr=['0.0050000'], tr/val_loss:  0.003463/  0.953398, val:  85.00%, val_best:  86.25%, tr:  99.95%, tr_best:  99.95%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "epoch-23  lr=['0.0050000'], tr/val_loss:  0.000760/  0.956340, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "epoch-24  lr=['0.0050000'], tr/val_loss:  0.000471/  1.009376, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "epoch-25  lr=['0.0050000'], tr/val_loss:  0.000369/  1.008269, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "epoch-26  lr=['0.0050000'], tr/val_loss:  0.000311/  1.011981, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "epoch-27  lr=['0.0050000'], tr/val_loss:  0.000267/  1.032245, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "epoch-28  lr=['0.0050000'], tr/val_loss:  0.000253/  1.041083, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "epoch-29  lr=['0.0050000'], tr/val_loss:  0.000232/  1.039751, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "epoch-30  lr=['0.0050000'], tr/val_loss:  0.000213/  1.039180, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "epoch-31  lr=['0.0050000'], tr/val_loss:  0.000199/  1.038388, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "epoch-32  lr=['0.0050000'], tr/val_loss:  0.000185/  1.043333, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "epoch-33  lr=['0.0050000'], tr/val_loss:  0.000175/  1.048287, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "epoch-34  lr=['0.0050000'], tr/val_loss:  0.000168/  1.064263, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "epoch-35  lr=['0.0050000'], tr/val_loss:  0.000159/  1.079336, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "epoch-36  lr=['0.0050000'], tr/val_loss:  0.000155/  1.091243, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "epoch-37  lr=['0.0050000'], tr/val_loss:  0.000146/  1.091292, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "epoch-38  lr=['0.0050000'], tr/val_loss:  0.000140/  1.098514, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "epoch-39  lr=['0.0050000'], tr/val_loss:  0.000135/  1.099272, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "epoch-40  lr=['0.0050000'], tr/val_loss:  0.000130/  1.100910, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "epoch-41  lr=['0.0050000'], tr/val_loss:  0.000127/  1.108032, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "epoch-42  lr=['0.0050000'], tr/val_loss:  0.000121/  1.108069, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "epoch-43  lr=['0.0050000'], tr/val_loss:  0.000119/  1.108070, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "epoch-44  lr=['0.0050000'], tr/val_loss:  0.000111/  1.112576, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "epoch-45  lr=['0.0050000'], tr/val_loss:  0.000108/  1.117411, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "epoch-46  lr=['0.0050000'], tr/val_loss:  0.000107/  1.112231, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "epoch-47  lr=['0.0050000'], tr/val_loss:  0.000105/  1.116583, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "epoch-48  lr=['0.0050000'], tr/val_loss:  0.000102/  1.112383, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "epoch-49  lr=['0.0050000'], tr/val_loss:  0.000098/  1.110910, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "epoch-50  lr=['0.0050000'], tr/val_loss:  0.000095/  1.106442, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "epoch-51  lr=['0.0050000'], tr/val_loss:  0.000094/  1.114789, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "epoch-52  lr=['0.0050000'], tr/val_loss:  0.000090/  1.124691, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "epoch-53  lr=['0.0050000'], tr/val_loss:  0.000087/  1.130797, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "epoch-54  lr=['0.0050000'], tr/val_loss:  0.000083/  1.129816, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "epoch-55  lr=['0.0050000'], tr/val_loss:  0.000083/  1.132943, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "epoch-56  lr=['0.0050000'], tr/val_loss:  0.000079/  1.130772, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "epoch-57  lr=['0.0050000'], tr/val_loss:  0.000079/  1.136276, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "epoch-58  lr=['0.0050000'], tr/val_loss:  0.000077/  1.132664, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "epoch-59  lr=['0.0050000'], tr/val_loss:  0.000076/  1.134391, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "epoch-60  lr=['0.0050000'], tr/val_loss:  0.000075/  1.139641, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "epoch-61  lr=['0.0050000'], tr/val_loss:  0.000071/  1.145879, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "epoch-62  lr=['0.0050000'], tr/val_loss:  0.000070/  1.156499, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "epoch-63  lr=['0.0050000'], tr/val_loss:  0.000072/  1.159979, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "epoch-64  lr=['0.0050000'], tr/val_loss:  0.000068/  1.161922, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "epoch-65  lr=['0.0050000'], tr/val_loss:  0.000066/  1.166012, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "epoch-66  lr=['0.0050000'], tr/val_loss:  0.000068/  1.166773, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "epoch-67  lr=['0.0050000'], tr/val_loss:  0.000064/  1.159914, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "epoch-68  lr=['0.0050000'], tr/val_loss:  0.000064/  1.160666, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "epoch-69  lr=['0.0050000'], tr/val_loss:  0.000061/  1.166330, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "epoch-70  lr=['0.0050000'], tr/val_loss:  0.000061/  1.167909, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "epoch-71  lr=['0.0050000'], tr/val_loss:  0.000061/  1.171603, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "epoch-72  lr=['0.0050000'], tr/val_loss:  0.000059/  1.173047, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.24 seconds, 1.30 minutes\n",
      "epoch-73  lr=['0.0050000'], tr/val_loss:  0.000059/  1.179785, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "epoch-74  lr=['0.0050000'], tr/val_loss:  0.000057/  1.175081, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "epoch-75  lr=['0.0050000'], tr/val_loss:  0.000057/  1.175497, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "epoch-76  lr=['0.0050000'], tr/val_loss:  0.000056/  1.181148, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "epoch-77  lr=['0.0050000'], tr/val_loss:  0.000055/  1.184822, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "epoch-78  lr=['0.0050000'], tr/val_loss:  0.000055/  1.181968, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "epoch-79  lr=['0.0050000'], tr/val_loss:  0.000054/  1.184194, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "epoch-80  lr=['0.0050000'], tr/val_loss:  0.000054/  1.190514, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "epoch-81  lr=['0.0050000'], tr/val_loss:  0.000052/  1.189559, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "epoch-82  lr=['0.0050000'], tr/val_loss:  0.000053/  1.189201, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "epoch-83  lr=['0.0050000'], tr/val_loss:  0.000052/  1.195148, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "epoch-84  lr=['0.0050000'], tr/val_loss:  0.000051/  1.199639, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "epoch-85  lr=['0.0050000'], tr/val_loss:  0.000050/  1.195350, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "epoch-86  lr=['0.0050000'], tr/val_loss:  0.000050/  1.196846, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "epoch-87  lr=['0.0050000'], tr/val_loss:  0.000050/  1.200881, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "epoch-88  lr=['0.0050000'], tr/val_loss:  0.000049/  1.198477, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "epoch-89  lr=['0.0050000'], tr/val_loss:  0.000048/  1.201078, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "epoch-90  lr=['0.0050000'], tr/val_loss:  0.000048/  1.201489, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "epoch-91  lr=['0.0050000'], tr/val_loss:  0.000046/  1.215523, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "epoch-92  lr=['0.0050000'], tr/val_loss:  0.000046/  1.218204, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "epoch-93  lr=['0.0050000'], tr/val_loss:  0.000046/  1.215649, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "epoch-94  lr=['0.0050000'], tr/val_loss:  0.000046/  1.223040, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "epoch-95  lr=['0.0050000'], tr/val_loss:  0.000044/  1.221461, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "epoch-96  lr=['0.0050000'], tr/val_loss:  0.000043/  1.226660, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "epoch-97  lr=['0.0050000'], tr/val_loss:  0.000043/  1.225716, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "epoch-98  lr=['0.0050000'], tr/val_loss:  0.000043/  1.221689, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "epoch-99  lr=['0.0050000'], tr/val_loss:  0.000042/  1.227756, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "epoch-100 lr=['0.0050000'], tr/val_loss:  0.000042/  1.231711, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "epoch-101 lr=['0.0050000'], tr/val_loss:  0.000041/  1.229031, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "epoch-102 lr=['0.0050000'], tr/val_loss:  0.000041/  1.229061, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "epoch-103 lr=['0.0050000'], tr/val_loss:  0.000040/  1.232008, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "epoch-104 lr=['0.0050000'], tr/val_loss:  0.000040/  1.231592, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.31 seconds, 1.31 minutes\n",
      "epoch-105 lr=['0.0050000'], tr/val_loss:  0.000040/  1.228578, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "epoch-106 lr=['0.0050000'], tr/val_loss:  0.000039/  1.233925, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "epoch-107 lr=['0.0050000'], tr/val_loss:  0.000038/  1.233420, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "epoch-108 lr=['0.0050000'], tr/val_loss:  0.000039/  1.224695, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "epoch-109 lr=['0.0050000'], tr/val_loss:  0.000038/  1.219928, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "epoch-110 lr=['0.0050000'], tr/val_loss:  0.000038/  1.227131, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "epoch-111 lr=['0.0050000'], tr/val_loss:  0.000038/  1.228811, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "epoch-112 lr=['0.0050000'], tr/val_loss:  0.000038/  1.227915, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "epoch-113 lr=['0.0050000'], tr/val_loss:  0.000037/  1.230902, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "epoch-114 lr=['0.0050000'], tr/val_loss:  0.000037/  1.228224, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "epoch-115 lr=['0.0050000'], tr/val_loss:  0.000037/  1.233506, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "epoch-116 lr=['0.0050000'], tr/val_loss:  0.000037/  1.233286, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "epoch-117 lr=['0.0050000'], tr/val_loss:  0.000036/  1.237892, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "epoch-118 lr=['0.0050000'], tr/val_loss:  0.000036/  1.244300, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "epoch-119 lr=['0.0050000'], tr/val_loss:  0.000036/  1.245747, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "epoch-120 lr=['0.0050000'], tr/val_loss:  0.000035/  1.242394, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "epoch-121 lr=['0.0050000'], tr/val_loss:  0.000035/  1.239869, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "epoch-122 lr=['0.0050000'], tr/val_loss:  0.000034/  1.237394, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "epoch-123 lr=['0.0050000'], tr/val_loss:  0.000034/  1.238943, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "epoch-124 lr=['0.0050000'], tr/val_loss:  0.000034/  1.239337, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "epoch-125 lr=['0.0050000'], tr/val_loss:  0.000034/  1.246230, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "epoch-126 lr=['0.0050000'], tr/val_loss:  0.000033/  1.243619, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "epoch-127 lr=['0.0050000'], tr/val_loss:  0.000033/  1.238022, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "epoch-128 lr=['0.0050000'], tr/val_loss:  0.000033/  1.242259, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "epoch-129 lr=['0.0050000'], tr/val_loss:  0.000033/  1.241641, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "epoch-130 lr=['0.0050000'], tr/val_loss:  0.000033/  1.235474, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "epoch-131 lr=['0.0050000'], tr/val_loss:  0.000032/  1.231470, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "epoch-132 lr=['0.0050000'], tr/val_loss:  0.000032/  1.238840, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "epoch-133 lr=['0.0050000'], tr/val_loss:  0.000032/  1.241287, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "epoch-134 lr=['0.0050000'], tr/val_loss:  0.000032/  1.247157, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "epoch-135 lr=['0.0050000'], tr/val_loss:  0.000031/  1.248828, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "epoch-136 lr=['0.0050000'], tr/val_loss:  0.000031/  1.250793, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "epoch-137 lr=['0.0050000'], tr/val_loss:  0.000031/  1.255531, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "epoch-138 lr=['0.0050000'], tr/val_loss:  0.000031/  1.263144, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "epoch-139 lr=['0.0050000'], tr/val_loss:  0.000030/  1.264203, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "epoch-140 lr=['0.0050000'], tr/val_loss:  0.000030/  1.265133, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "epoch-141 lr=['0.0050000'], tr/val_loss:  0.000029/  1.264195, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "epoch-142 lr=['0.0050000'], tr/val_loss:  0.000029/  1.267089, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "epoch-143 lr=['0.0050000'], tr/val_loss:  0.000029/  1.264906, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "epoch-144 lr=['0.0050000'], tr/val_loss:  0.000029/  1.261660, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "epoch-145 lr=['0.0050000'], tr/val_loss:  0.000028/  1.261958, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "epoch-146 lr=['0.0050000'], tr/val_loss:  0.000028/  1.260079, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "epoch-147 lr=['0.0050000'], tr/val_loss:  0.000028/  1.261012, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "epoch-148 lr=['0.0050000'], tr/val_loss:  0.000028/  1.256605, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "epoch-149 lr=['0.0050000'], tr/val_loss:  0.000027/  1.254044, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "epoch-150 lr=['0.0050000'], tr/val_loss:  0.000028/  1.255520, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "epoch-151 lr=['0.0050000'], tr/val_loss:  0.000027/  1.258131, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "epoch-152 lr=['0.0050000'], tr/val_loss:  0.000027/  1.259026, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "epoch-153 lr=['0.0050000'], tr/val_loss:  0.000027/  1.259079, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "epoch-154 lr=['0.0050000'], tr/val_loss:  0.000027/  1.268703, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "epoch-155 lr=['0.0050000'], tr/val_loss:  0.000027/  1.267703, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "epoch-156 lr=['0.0050000'], tr/val_loss:  0.000027/  1.270785, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "epoch-157 lr=['0.0050000'], tr/val_loss:  0.000027/  1.277232, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "epoch-158 lr=['0.0050000'], tr/val_loss:  0.000027/  1.277539, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "epoch-159 lr=['0.0050000'], tr/val_loss:  0.000026/  1.280622, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "epoch-160 lr=['0.0050000'], tr/val_loss:  0.000026/  1.282230, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "epoch-161 lr=['0.0050000'], tr/val_loss:  0.000026/  1.281516, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "epoch-162 lr=['0.0050000'], tr/val_loss:  0.000026/  1.285759, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "epoch-163 lr=['0.0050000'], tr/val_loss:  0.000026/  1.288324, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "epoch-164 lr=['0.0050000'], tr/val_loss:  0.000025/  1.286767, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "epoch-165 lr=['0.0050000'], tr/val_loss:  0.000025/  1.289893, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "epoch-166 lr=['0.0050000'], tr/val_loss:  0.000025/  1.287309, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "epoch-167 lr=['0.0050000'], tr/val_loss:  0.000025/  1.286418, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "epoch-168 lr=['0.0050000'], tr/val_loss:  0.000024/  1.288359, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "epoch-169 lr=['0.0050000'], tr/val_loss:  0.000024/  1.290155, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "epoch-170 lr=['0.0050000'], tr/val_loss:  0.000024/  1.293240, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "epoch-171 lr=['0.0050000'], tr/val_loss:  0.000024/  1.290178, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "epoch-172 lr=['0.0050000'], tr/val_loss:  0.000024/  1.293115, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "epoch-173 lr=['0.0050000'], tr/val_loss:  0.000024/  1.294910, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "epoch-174 lr=['0.0050000'], tr/val_loss:  0.000024/  1.295352, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "epoch-175 lr=['0.0050000'], tr/val_loss:  0.000023/  1.297076, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "epoch-176 lr=['0.0050000'], tr/val_loss:  0.000024/  1.304038, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "epoch-177 lr=['0.0050000'], tr/val_loss:  0.000023/  1.306028, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "epoch-178 lr=['0.0050000'], tr/val_loss:  0.000023/  1.303826, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "epoch-179 lr=['0.0050000'], tr/val_loss:  0.000023/  1.303760, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "epoch-180 lr=['0.0050000'], tr/val_loss:  0.000022/  1.310006, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "epoch-181 lr=['0.0050000'], tr/val_loss:  0.000022/  1.304456, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "epoch-182 lr=['0.0050000'], tr/val_loss:  0.000022/  1.300717, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "epoch-183 lr=['0.0050000'], tr/val_loss:  0.000021/  1.303058, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "epoch-184 lr=['0.0050000'], tr/val_loss:  0.000022/  1.302502, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "epoch-185 lr=['0.0050000'], tr/val_loss:  0.000021/  1.300012, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "epoch-186 lr=['0.0050000'], tr/val_loss:  0.000021/  1.299498, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "epoch-187 lr=['0.0050000'], tr/val_loss:  0.000021/  1.293251, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "epoch-188 lr=['0.0050000'], tr/val_loss:  0.000021/  1.290953, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "epoch-189 lr=['0.0050000'], tr/val_loss:  0.000021/  1.290762, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "epoch-190 lr=['0.0050000'], tr/val_loss:  0.000021/  1.297691, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "epoch-191 lr=['0.0050000'], tr/val_loss:  0.000021/  1.295289, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "epoch-192 lr=['0.0050000'], tr/val_loss:  0.000021/  1.300960, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "epoch-193 lr=['0.0050000'], tr/val_loss:  0.000020/  1.305541, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "epoch-194 lr=['0.0050000'], tr/val_loss:  0.000020/  1.300396, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "epoch-195 lr=['0.0050000'], tr/val_loss:  0.000021/  1.301138, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "epoch-196 lr=['0.0050000'], tr/val_loss:  0.000020/  1.299928, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "epoch-197 lr=['0.0050000'], tr/val_loss:  0.000020/  1.302418, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "epoch-198 lr=['0.0050000'], tr/val_loss:  0.000020/  1.300096, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "epoch-199 lr=['0.0050000'], tr/val_loss:  0.000020/  1.300871, val:  85.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4434b21c7f422ab3b71bb3f18be6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>val_loss</td><td>‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>2e-05</td></tr><tr><td>val_acc_best</td><td>0.8625</td></tr><tr><td>val_acc_now</td><td>0.85</td></tr><tr><td>val_loss</td><td>1.30087</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glowing-sweep-79</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6qykvr7x' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/6qykvr7x</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251105_055657-6qykvr7x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a54q9jkn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251105_101902-a54q9jkn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a54q9jkn' target=\"_blank\">electric-sweep-87</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/agbicsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a54q9jkn' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a54q9jkn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '3', 'single_step': False, 'unique_name': '20251105_101911_613', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 1e-05, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'sigmoid', 'BPTT_on': True, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 6, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = deea846692c028030dda4288c5a62e02\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d18ea47e42908fd1fd969b7f53daa77d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d927659af8cbcce93fcdf7890586c422\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a7b1cb019bae1c7ff9ca5108a2227c78\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d32e74b7370101199dbd6e963d91b37\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 34d12bca5b3860bcf37e9e43ba148aff\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2ffe8c11d12fcf65fcbccccceb17dc92\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 6b6e0cb38311b5e12527b1f1f47e8bef\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 38e4136667b4877be986f5db6c6531e4\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 596467cdf6688b976b4a9bef67ce1458\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=0, sg_width=10, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=0, sg_width=10, surrogate=sigmoid, BPTT_on=True, trace_const1=1, trace_const2=0.5, TIME=10, sstep=False, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=False, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 1e-05\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch-0   lr=['0.0000100'], tr/val_loss:  1.528117/  1.342490, val:  47.92%, val_best:  47.92%, tr:  45.54%, tr_best:  45.54%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "epoch-1   lr=['0.0000100'], tr/val_loss:  0.939673/  1.187508, val:  54.58%, val_best:  54.58%, tr:  61.83%, tr_best:  61.83%, epoch time: 71.63 seconds, 1.19 minutes\n",
      "epoch-2   lr=['0.0000100'], tr/val_loss:  0.799177/  1.083434, val:  60.42%, val_best:  60.42%, tr:  67.16%, tr_best:  67.16%, epoch time: 71.01 seconds, 1.18 minutes\n",
      "epoch-3   lr=['0.0000100'], tr/val_loss:  0.721648/  0.990580, val:  60.42%, val_best:  60.42%, tr:  70.45%, tr_best:  70.45%, epoch time: 71.18 seconds, 1.19 minutes\n",
      "epoch-4   lr=['0.0000100'], tr/val_loss:  0.665930/  0.966628, val:  62.08%, val_best:  62.08%, tr:  72.57%, tr_best:  72.57%, epoch time: 71.39 seconds, 1.19 minutes\n",
      "epoch-5   lr=['0.0000100'], tr/val_loss:  0.622210/  0.953988, val:  61.67%, val_best:  62.08%, tr:  75.18%, tr_best:  75.18%, epoch time: 71.09 seconds, 1.18 minutes\n",
      "epoch-6   lr=['0.0000100'], tr/val_loss:  0.581990/  0.938191, val:  64.58%, val_best:  64.58%, tr:  77.01%, tr_best:  77.01%, epoch time: 71.08 seconds, 1.18 minutes\n",
      "epoch-7   lr=['0.0000100'], tr/val_loss:  0.554421/  0.915117, val:  65.83%, val_best:  65.83%, tr:  78.34%, tr_best:  78.34%, epoch time: 71.43 seconds, 1.19 minutes\n",
      "epoch-8   lr=['0.0000100'], tr/val_loss:  0.526108/  0.929960, val:  64.58%, val_best:  65.83%, tr:  78.92%, tr_best:  78.92%, epoch time: 71.67 seconds, 1.19 minutes\n",
      "epoch-9   lr=['0.0000100'], tr/val_loss:  0.502664/  0.909164, val:  66.25%, val_best:  66.25%, tr:  80.55%, tr_best:  80.55%, epoch time: 71.84 seconds, 1.20 minutes\n",
      "epoch-10  lr=['0.0000100'], tr/val_loss:  0.483726/  0.927571, val:  65.83%, val_best:  66.25%, tr:  81.00%, tr_best:  81.00%, epoch time: 71.36 seconds, 1.19 minutes\n",
      "epoch-11  lr=['0.0000100'], tr/val_loss:  0.462892/  0.898076, val:  62.08%, val_best:  66.25%, tr:  82.62%, tr_best:  82.62%, epoch time: 71.31 seconds, 1.19 minutes\n",
      "epoch-12  lr=['0.0000100'], tr/val_loss:  0.450950/  0.901091, val:  63.75%, val_best:  66.25%, tr:  82.55%, tr_best:  82.62%, epoch time: 70.85 seconds, 1.18 minutes\n",
      "epoch-13  lr=['0.0000100'], tr/val_loss:  0.428212/  0.902685, val:  65.42%, val_best:  66.25%, tr:  83.77%, tr_best:  83.77%, epoch time: 71.24 seconds, 1.19 minutes\n",
      "epoch-14  lr=['0.0000100'], tr/val_loss:  0.415633/  0.893073, val:  64.17%, val_best:  66.25%, tr:  84.45%, tr_best:  84.45%, epoch time: 70.66 seconds, 1.18 minutes\n",
      "epoch-15  lr=['0.0000100'], tr/val_loss:  0.396842/  0.879332, val:  66.67%, val_best:  66.67%, tr:  85.71%, tr_best:  85.71%, epoch time: 71.05 seconds, 1.18 minutes\n",
      "epoch-16  lr=['0.0000100'], tr/val_loss:  0.383882/  0.884455, val:  62.50%, val_best:  66.67%, tr:  86.16%, tr_best:  86.16%, epoch time: 71.36 seconds, 1.19 minutes\n",
      "epoch-17  lr=['0.0000100'], tr/val_loss:  0.372602/  0.881228, val:  65.83%, val_best:  66.67%, tr:  86.52%, tr_best:  86.52%, epoch time: 71.59 seconds, 1.19 minutes\n",
      "epoch-18  lr=['0.0000100'], tr/val_loss:  0.359315/  0.902567, val:  65.42%, val_best:  66.67%, tr:  87.71%, tr_best:  87.71%, epoch time: 71.57 seconds, 1.19 minutes\n",
      "epoch-19  lr=['0.0000100'], tr/val_loss:  0.345627/  0.877196, val:  63.75%, val_best:  66.67%, tr:  88.39%, tr_best:  88.39%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "epoch-20  lr=['0.0000100'], tr/val_loss:  0.333916/  0.866725, val:  65.83%, val_best:  66.67%, tr:  88.86%, tr_best:  88.86%, epoch time: 70.99 seconds, 1.18 minutes\n",
      "epoch-21  lr=['0.0000100'], tr/val_loss:  0.323453/  0.894594, val:  64.17%, val_best:  66.67%, tr:  89.11%, tr_best:  89.11%, epoch time: 71.47 seconds, 1.19 minutes\n",
      "epoch-22  lr=['0.0000100'], tr/val_loss:  0.316723/  0.873708, val:  64.58%, val_best:  66.67%, tr:  89.90%, tr_best:  89.90%, epoch time: 71.29 seconds, 1.19 minutes\n",
      "epoch-23  lr=['0.0000100'], tr/val_loss:  0.308500/  0.887798, val:  63.75%, val_best:  66.67%, tr:  89.79%, tr_best:  89.90%, epoch time: 71.23 seconds, 1.19 minutes\n",
      "epoch-24  lr=['0.0000100'], tr/val_loss:  0.296918/  0.857980, val:  65.00%, val_best:  66.67%, tr:  90.60%, tr_best:  90.60%, epoch time: 71.36 seconds, 1.19 minutes\n",
      "epoch-25  lr=['0.0000100'], tr/val_loss:  0.288492/  0.869649, val:  64.58%, val_best:  66.67%, tr:  91.25%, tr_best:  91.25%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "epoch-26  lr=['0.0000100'], tr/val_loss:  0.280538/  0.880987, val:  63.33%, val_best:  66.67%, tr:  91.25%, tr_best:  91.25%, epoch time: 70.83 seconds, 1.18 minutes\n",
      "epoch-27  lr=['0.0000100'], tr/val_loss:  0.269358/  0.889231, val:  66.67%, val_best:  66.67%, tr:  91.97%, tr_best:  91.97%, epoch time: 70.96 seconds, 1.18 minutes\n",
      "epoch-28  lr=['0.0000100'], tr/val_loss:  0.263866/  0.847643, val:  66.67%, val_best:  66.67%, tr:  92.54%, tr_best:  92.54%, epoch time: 70.67 seconds, 1.18 minutes\n",
      "epoch-29  lr=['0.0000100'], tr/val_loss:  0.258318/  0.839673, val:  66.25%, val_best:  66.67%, tr:  92.81%, tr_best:  92.81%, epoch time: 70.67 seconds, 1.18 minutes\n",
      "epoch-30  lr=['0.0000100'], tr/val_loss:  0.251255/  0.899826, val:  64.58%, val_best:  66.67%, tr:  93.24%, tr_best:  93.24%, epoch time: 71.28 seconds, 1.19 minutes\n",
      "epoch-31  lr=['0.0000100'], tr/val_loss:  0.243962/  0.864071, val:  67.50%, val_best:  67.50%, tr:  93.19%, tr_best:  93.24%, epoch time: 71.55 seconds, 1.19 minutes\n",
      "epoch-32  lr=['0.0000100'], tr/val_loss:  0.236702/  0.845003, val:  69.17%, val_best:  69.17%, tr:  93.67%, tr_best:  93.67%, epoch time: 70.91 seconds, 1.18 minutes\n",
      "epoch-33  lr=['0.0000100'], tr/val_loss:  0.230159/  0.872983, val:  68.33%, val_best:  69.17%, tr:  94.03%, tr_best:  94.03%, epoch time: 70.97 seconds, 1.18 minutes\n",
      "epoch-34  lr=['0.0000100'], tr/val_loss:  0.224134/  0.861963, val:  70.00%, val_best:  70.00%, tr:  93.85%, tr_best:  94.03%, epoch time: 71.13 seconds, 1.19 minutes\n",
      "epoch-35  lr=['0.0000100'], tr/val_loss:  0.216845/  0.837211, val:  67.92%, val_best:  70.00%, tr:  94.66%, tr_best:  94.66%, epoch time: 71.39 seconds, 1.19 minutes\n",
      "epoch-36  lr=['0.0000100'], tr/val_loss:  0.210470/  0.844431, val:  67.50%, val_best:  70.00%, tr:  94.95%, tr_best:  94.95%, epoch time: 71.00 seconds, 1.18 minutes\n",
      "epoch-37  lr=['0.0000100'], tr/val_loss:  0.203077/  0.831229, val:  69.17%, val_best:  70.00%, tr:  95.09%, tr_best:  95.09%, epoch time: 71.44 seconds, 1.19 minutes\n",
      "epoch-38  lr=['0.0000100'], tr/val_loss:  0.201545/  0.849168, val:  68.75%, val_best:  70.00%, tr:  95.13%, tr_best:  95.13%, epoch time: 70.71 seconds, 1.18 minutes\n",
      "epoch-39  lr=['0.0000100'], tr/val_loss:  0.194216/  0.847583, val:  67.92%, val_best:  70.00%, tr:  95.20%, tr_best:  95.20%, epoch time: 70.79 seconds, 1.18 minutes\n",
      "epoch-40  lr=['0.0000100'], tr/val_loss:  0.189550/  0.845501, val:  68.75%, val_best:  70.00%, tr:  95.81%, tr_best:  95.81%, epoch time: 71.33 seconds, 1.19 minutes\n",
      "epoch-41  lr=['0.0000100'], tr/val_loss:  0.184349/  0.872605, val:  65.83%, val_best:  70.00%, tr:  95.60%, tr_best:  95.81%, epoch time: 70.77 seconds, 1.18 minutes\n",
      "epoch-42  lr=['0.0000100'], tr/val_loss:  0.178660/  0.843499, val:  68.75%, val_best:  70.00%, tr:  96.26%, tr_best:  96.26%, epoch time: 71.53 seconds, 1.19 minutes\n",
      "epoch-43  lr=['0.0000100'], tr/val_loss:  0.174657/  0.855070, val:  68.75%, val_best:  70.00%, tr:  96.10%, tr_best:  96.26%, epoch time: 70.93 seconds, 1.18 minutes\n",
      "epoch-44  lr=['0.0000100'], tr/val_loss:  0.167609/  0.849920, val:  69.17%, val_best:  70.00%, tr:  96.64%, tr_best:  96.64%, epoch time: 70.85 seconds, 1.18 minutes\n",
      "epoch-45  lr=['0.0000100'], tr/val_loss:  0.165656/  0.852468, val:  69.17%, val_best:  70.00%, tr:  96.98%, tr_best:  96.98%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "epoch-46  lr=['0.0000100'], tr/val_loss:  0.162045/  0.841165, val:  67.92%, val_best:  70.00%, tr:  96.53%, tr_best:  96.98%, epoch time: 71.24 seconds, 1.19 minutes\n",
      "epoch-47  lr=['0.0000100'], tr/val_loss:  0.156740/  0.839458, val:  69.58%, val_best:  70.00%, tr:  97.05%, tr_best:  97.05%, epoch time: 71.11 seconds, 1.19 minutes\n",
      "epoch-48  lr=['0.0000100'], tr/val_loss:  0.153967/  0.879505, val:  67.08%, val_best:  70.00%, tr:  96.89%, tr_best:  97.05%, epoch time: 71.13 seconds, 1.19 minutes\n",
      "epoch-49  lr=['0.0000100'], tr/val_loss:  0.148320/  0.864308, val:  67.92%, val_best:  70.00%, tr:  97.41%, tr_best:  97.41%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "epoch-50  lr=['0.0000100'], tr/val_loss:  0.144159/  0.859580, val:  67.92%, val_best:  70.00%, tr:  97.61%, tr_best:  97.61%, epoch time: 71.11 seconds, 1.19 minutes\n",
      "epoch-51  lr=['0.0000100'], tr/val_loss:  0.141434/  0.861567, val:  66.67%, val_best:  70.00%, tr:  97.72%, tr_best:  97.72%, epoch time: 71.35 seconds, 1.19 minutes\n",
      "epoch-52  lr=['0.0000100'], tr/val_loss:  0.135676/  0.886945, val:  68.33%, val_best:  70.00%, tr:  97.90%, tr_best:  97.90%, epoch time: 71.22 seconds, 1.19 minutes\n",
      "epoch-53  lr=['0.0000100'], tr/val_loss:  0.133745/  0.877080, val:  69.17%, val_best:  70.00%, tr:  97.99%, tr_best:  97.99%, epoch time: 71.30 seconds, 1.19 minutes\n",
      "epoch-54  lr=['0.0000100'], tr/val_loss:  0.131757/  0.916977, val:  66.67%, val_best:  70.00%, tr:  98.17%, tr_best:  98.17%, epoch time: 70.90 seconds, 1.18 minutes\n",
      "epoch-55  lr=['0.0000100'], tr/val_loss:  0.127373/  0.929545, val:  66.25%, val_best:  70.00%, tr:  98.33%, tr_best:  98.33%, epoch time: 71.24 seconds, 1.19 minutes\n",
      "epoch-56  lr=['0.0000100'], tr/val_loss:  0.122125/  0.908888, val:  67.08%, val_best:  70.00%, tr:  98.49%, tr_best:  98.49%, epoch time: 71.19 seconds, 1.19 minutes\n",
      "epoch-57  lr=['0.0000100'], tr/val_loss:  0.121185/  0.878018, val:  68.75%, val_best:  70.00%, tr:  98.49%, tr_best:  98.49%, epoch time: 70.87 seconds, 1.18 minutes\n",
      "epoch-58  lr=['0.0000100'], tr/val_loss:  0.115411/  0.898856, val:  68.75%, val_best:  70.00%, tr:  98.85%, tr_best:  98.85%, epoch time: 71.07 seconds, 1.18 minutes\n",
      "epoch-59  lr=['0.0000100'], tr/val_loss:  0.113321/  0.910621, val:  67.50%, val_best:  70.00%, tr:  98.74%, tr_best:  98.85%, epoch time: 71.17 seconds, 1.19 minutes\n",
      "epoch-60  lr=['0.0000100'], tr/val_loss:  0.110386/  0.886442, val:  69.17%, val_best:  70.00%, tr:  98.94%, tr_best:  98.94%, epoch time: 71.29 seconds, 1.19 minutes\n",
      "epoch-61  lr=['0.0000100'], tr/val_loss:  0.106171/  0.873473, val:  67.08%, val_best:  70.00%, tr:  99.08%, tr_best:  99.08%, epoch time: 71.22 seconds, 1.19 minutes\n",
      "epoch-62  lr=['0.0000100'], tr/val_loss:  0.105316/  0.858990, val:  70.00%, val_best:  70.00%, tr:  98.96%, tr_best:  99.08%, epoch time: 70.96 seconds, 1.18 minutes\n",
      "epoch-63  lr=['0.0000100'], tr/val_loss:  0.102911/  0.896888, val:  70.00%, val_best:  70.00%, tr:  99.01%, tr_best:  99.08%, epoch time: 70.90 seconds, 1.18 minutes\n",
      "epoch-64  lr=['0.0000100'], tr/val_loss:  0.101765/  0.878418, val:  68.75%, val_best:  70.00%, tr:  99.03%, tr_best:  99.08%, epoch time: 71.25 seconds, 1.19 minutes\n",
      "epoch-65  lr=['0.0000100'], tr/val_loss:  0.098192/  0.885445, val:  66.67%, val_best:  70.00%, tr:  99.12%, tr_best:  99.12%, epoch time: 71.04 seconds, 1.18 minutes\n",
      "epoch-66  lr=['0.0000100'], tr/val_loss:  0.096840/  0.888938, val:  69.17%, val_best:  70.00%, tr:  99.21%, tr_best:  99.21%, epoch time: 71.01 seconds, 1.18 minutes\n",
      "epoch-67  lr=['0.0000100'], tr/val_loss:  0.094439/  0.871267, val:  68.75%, val_best:  70.00%, tr:  99.44%, tr_best:  99.44%, epoch time: 70.81 seconds, 1.18 minutes\n",
      "epoch-68  lr=['0.0000100'], tr/val_loss:  0.092516/  0.888624, val:  69.58%, val_best:  70.00%, tr:  99.32%, tr_best:  99.44%, epoch time: 71.12 seconds, 1.19 minutes\n",
      "epoch-69  lr=['0.0000100'], tr/val_loss:  0.089681/  0.884366, val:  67.92%, val_best:  70.00%, tr:  99.37%, tr_best:  99.44%, epoch time: 71.37 seconds, 1.19 minutes\n",
      "epoch-70  lr=['0.0000100'], tr/val_loss:  0.088123/  0.901736, val:  68.33%, val_best:  70.00%, tr:  99.46%, tr_best:  99.46%, epoch time: 71.02 seconds, 1.18 minutes\n",
      "epoch-71  lr=['0.0000100'], tr/val_loss:  0.085676/  0.845733, val:  72.08%, val_best:  72.08%, tr:  99.46%, tr_best:  99.46%, epoch time: 71.35 seconds, 1.19 minutes\n",
      "epoch-72  lr=['0.0000100'], tr/val_loss:  0.083684/  0.893081, val:  70.83%, val_best:  72.08%, tr:  99.41%, tr_best:  99.46%, epoch time: 71.15 seconds, 1.19 minutes\n",
      "epoch-73  lr=['0.0000100'], tr/val_loss:  0.080852/  0.893546, val:  70.83%, val_best:  72.08%, tr:  99.68%, tr_best:  99.68%, epoch time: 71.46 seconds, 1.19 minutes\n",
      "epoch-74  lr=['0.0000100'], tr/val_loss:  0.079327/  0.856159, val:  70.42%, val_best:  72.08%, tr:  99.48%, tr_best:  99.68%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "epoch-75  lr=['0.0000100'], tr/val_loss:  0.077996/  0.906744, val:  67.92%, val_best:  72.08%, tr:  99.50%, tr_best:  99.68%, epoch time: 70.73 seconds, 1.18 minutes\n",
      "epoch-76  lr=['0.0000100'], tr/val_loss:  0.075608/  0.915103, val:  67.08%, val_best:  72.08%, tr:  99.68%, tr_best:  99.68%, epoch time: 70.91 seconds, 1.18 minutes\n",
      "epoch-77  lr=['0.0000100'], tr/val_loss:  0.074877/  0.914530, val:  68.33%, val_best:  72.08%, tr:  99.64%, tr_best:  99.68%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "epoch-78  lr=['0.0000100'], tr/val_loss:  0.071022/  0.919490, val:  68.75%, val_best:  72.08%, tr:  99.75%, tr_best:  99.75%, epoch time: 71.46 seconds, 1.19 minutes\n",
      "epoch-79  lr=['0.0000100'], tr/val_loss:  0.071978/  0.914654, val:  67.50%, val_best:  72.08%, tr:  99.64%, tr_best:  99.75%, epoch time: 71.25 seconds, 1.19 minutes\n",
      "epoch-80  lr=['0.0000100'], tr/val_loss:  0.069135/  0.905341, val:  68.75%, val_best:  72.08%, tr:  99.71%, tr_best:  99.75%, epoch time: 71.22 seconds, 1.19 minutes\n",
      "epoch-81  lr=['0.0000100'], tr/val_loss:  0.068371/  0.914425, val:  69.17%, val_best:  72.08%, tr:  99.82%, tr_best:  99.82%, epoch time: 70.79 seconds, 1.18 minutes\n",
      "epoch-82  lr=['0.0000100'], tr/val_loss:  0.066659/  0.922160, val:  67.92%, val_best:  72.08%, tr:  99.71%, tr_best:  99.82%, epoch time: 70.91 seconds, 1.18 minutes\n",
      "epoch-83  lr=['0.0000100'], tr/val_loss:  0.065008/  0.909508, val:  68.75%, val_best:  72.08%, tr:  99.84%, tr_best:  99.84%, epoch time: 71.28 seconds, 1.19 minutes\n",
      "epoch-84  lr=['0.0000100'], tr/val_loss:  0.063602/  0.910717, val:  68.33%, val_best:  72.08%, tr:  99.86%, tr_best:  99.86%, epoch time: 71.51 seconds, 1.19 minutes\n",
      "epoch-85  lr=['0.0000100'], tr/val_loss:  0.062548/  0.923540, val:  70.00%, val_best:  72.08%, tr:  99.82%, tr_best:  99.86%, epoch time: 71.02 seconds, 1.18 minutes\n",
      "epoch-86  lr=['0.0000100'], tr/val_loss:  0.061435/  0.931623, val:  68.33%, val_best:  72.08%, tr:  99.89%, tr_best:  99.89%, epoch time: 71.79 seconds, 1.20 minutes\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [False]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.0625,0.5,1.0,0.25,0.125]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0, 0.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [2,4,6,8,10]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [True, False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [2,6,10,14,18]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [False]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [9]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        # \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        # \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        # \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"3\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'agbicsqd'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
