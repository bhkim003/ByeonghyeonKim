{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22894/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 20664,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 6.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 300,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = 9, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-10,-10],[-10,-10],[-9,-9]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pc8mdq32 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 9292\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251020_211543-pc8mdq32</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pc8mdq32' target=\"_blank\">still-sweep-59</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4wosfk6x' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4wosfk6x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4wosfk6x' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4wosfk6x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pc8mdq32' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pc8mdq32</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251020_211551_058', 'my_seed': 9292, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2.5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=2.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=2.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 729.0\n",
      "lif layer 1 self.abs_max_v: 729.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 139.0\n",
      "lif layer 2 self.abs_max_v: 139.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 816.0\n",
      "lif layer 1 self.abs_max_v: 920.5\n",
      "fc layer 2 self.abs_max_out: 501.0\n",
      "lif layer 2 self.abs_max_v: 553.5\n",
      "fc layer 3 self.abs_max_out: 18.0\n",
      "lif layer 1 self.abs_max_v: 1060.5\n",
      "lif layer 2 self.abs_max_v: 554.5\n",
      "fc layer 3 self.abs_max_out: 32.0\n",
      "lif layer 1 self.abs_max_v: 1276.5\n",
      "fc layer 2 self.abs_max_out: 573.0\n",
      "lif layer 2 self.abs_max_v: 740.5\n",
      "fc layer 3 self.abs_max_out: 33.0\n",
      "fc layer 3 self.abs_max_out: 79.0\n",
      "fc layer 1 self.abs_max_out: 1081.0\n",
      "lif layer 2 self.abs_max_v: 799.5\n",
      "fc layer 3 self.abs_max_out: 127.0\n",
      "fc layer 1 self.abs_max_out: 1245.0\n",
      "fc layer 1 self.abs_max_out: 1460.0\n",
      "lif layer 1 self.abs_max_v: 1460.0\n",
      "fc layer 2 self.abs_max_out: 606.0\n",
      "lif layer 2 self.abs_max_v: 859.5\n",
      "fc layer 2 self.abs_max_out: 637.0\n",
      "lif layer 2 self.abs_max_v: 866.0\n",
      "fc layer 1 self.abs_max_out: 1571.0\n",
      "lif layer 1 self.abs_max_v: 1571.0\n",
      "fc layer 2 self.abs_max_out: 683.0\n",
      "lif layer 2 self.abs_max_v: 999.5\n",
      "fc layer 3 self.abs_max_out: 133.0\n",
      "lif layer 2 self.abs_max_v: 1012.0\n",
      "fc layer 3 self.abs_max_out: 166.0\n",
      "fc layer 2 self.abs_max_out: 734.0\n",
      "lif layer 2 self.abs_max_v: 1078.5\n",
      "fc layer 2 self.abs_max_out: 811.0\n",
      "fc layer 3 self.abs_max_out: 169.0\n",
      "fc layer 2 self.abs_max_out: 910.0\n",
      "fc layer 2 self.abs_max_out: 933.0\n",
      "lif layer 2 self.abs_max_v: 1143.0\n",
      "fc layer 2 self.abs_max_out: 1063.0\n",
      "lif layer 2 self.abs_max_v: 1320.0\n",
      "fc layer 1 self.abs_max_out: 1680.0\n",
      "lif layer 1 self.abs_max_v: 1680.0\n",
      "fc layer 1 self.abs_max_out: 1717.0\n",
      "lif layer 1 self.abs_max_v: 1717.0\n",
      "fc layer 2 self.abs_max_out: 1102.0\n",
      "fc layer 3 self.abs_max_out: 178.0\n",
      "fc layer 3 self.abs_max_out: 182.0\n",
      "lif layer 2 self.abs_max_v: 1323.5\n",
      "fc layer 2 self.abs_max_out: 1210.0\n",
      "lif layer 2 self.abs_max_v: 1520.0\n",
      "fc layer 3 self.abs_max_out: 254.0\n",
      "lif layer 2 self.abs_max_v: 1526.5\n",
      "lif layer 1 self.abs_max_v: 1788.5\n",
      "fc layer 1 self.abs_max_out: 1935.0\n",
      "lif layer 1 self.abs_max_v: 1935.0\n",
      "lif layer 1 self.abs_max_v: 2199.5\n",
      "fc layer 1 self.abs_max_out: 1964.0\n",
      "fc layer 1 self.abs_max_out: 2259.0\n",
      "lif layer 1 self.abs_max_v: 2259.0\n",
      "lif layer 2 self.abs_max_v: 1546.5\n",
      "lif layer 2 self.abs_max_v: 1549.5\n",
      "lif layer 2 self.abs_max_v: 1552.0\n",
      "fc layer 3 self.abs_max_out: 279.0\n",
      "lif layer 1 self.abs_max_v: 2497.0\n",
      "lif layer 1 self.abs_max_v: 2908.5\n",
      "lif layer 2 self.abs_max_v: 1641.0\n",
      "fc layer 1 self.abs_max_out: 2603.0\n",
      "fc layer 2 self.abs_max_out: 1212.0\n",
      "fc layer 2 self.abs_max_out: 1259.0\n",
      "fc layer 3 self.abs_max_out: 282.0\n",
      "fc layer 2 self.abs_max_out: 1264.0\n",
      "fc layer 3 self.abs_max_out: 334.0\n",
      "fc layer 2 self.abs_max_out: 1273.0\n",
      "fc layer 2 self.abs_max_out: 1277.0\n",
      "fc layer 2 self.abs_max_out: 1378.0\n",
      "fc layer 3 self.abs_max_out: 345.0\n",
      "fc layer 2 self.abs_max_out: 1403.0\n",
      "fc layer 2 self.abs_max_out: 1455.0\n",
      "lif layer 1 self.abs_max_v: 2981.5\n",
      "lif layer 2 self.abs_max_v: 1671.0\n",
      "lif layer 2 self.abs_max_v: 1708.0\n",
      "fc layer 3 self.abs_max_out: 383.0\n",
      "fc layer 2 self.abs_max_out: 1553.0\n",
      "lif layer 1 self.abs_max_v: 3157.0\n",
      "lif layer 2 self.abs_max_v: 1736.5\n",
      "lif layer 2 self.abs_max_v: 1792.5\n",
      "lif layer 2 self.abs_max_v: 1926.5\n",
      "fc layer 3 self.abs_max_out: 394.0\n",
      "fc layer 2 self.abs_max_out: 1682.0\n",
      "fc layer 2 self.abs_max_out: 1715.0\n",
      "lif layer 2 self.abs_max_v: 2271.5\n",
      "fc layer 2 self.abs_max_out: 1755.0\n",
      "lif layer 2 self.abs_max_v: 2359.5\n",
      "lif layer 2 self.abs_max_v: 2437.0\n",
      "lif layer 2 self.abs_max_v: 2611.5\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "fc layer 1 self.abs_max_out: 2642.0\n",
      "fc layer 2 self.abs_max_out: 1939.0\n",
      "lif layer 1 self.abs_max_v: 3186.5\n",
      "lif layer 1 self.abs_max_v: 3285.5\n",
      "fc layer 1 self.abs_max_out: 2668.0\n",
      "fc layer 3 self.abs_max_out: 457.0\n",
      "fc layer 2 self.abs_max_out: 2004.0\n",
      "lif layer 2 self.abs_max_v: 2680.0\n",
      "lif layer 2 self.abs_max_v: 2816.5\n",
      "fc layer 1 self.abs_max_out: 2842.0\n",
      "fc layer 1 self.abs_max_out: 3000.0\n",
      "fc layer 1 self.abs_max_out: 3087.0\n",
      "fc layer 2 self.abs_max_out: 2076.0\n",
      "fc layer 2 self.abs_max_out: 2197.0\n",
      "fc layer 2 self.abs_max_out: 2212.0\n",
      "fc layer 2 self.abs_max_out: 2238.0\n",
      "lif layer 1 self.abs_max_v: 3384.5\n",
      "lif layer 1 self.abs_max_v: 3634.5\n",
      "fc layer 2 self.abs_max_out: 2326.0\n",
      "fc layer 1 self.abs_max_out: 3676.0\n",
      "lif layer 1 self.abs_max_v: 3676.0\n",
      "fc layer 3 self.abs_max_out: 460.0\n",
      "fc layer 3 self.abs_max_out: 491.0\n",
      "lif layer 2 self.abs_max_v: 2873.0\n",
      "lif layer 2 self.abs_max_v: 2886.5\n",
      "lif layer 1 self.abs_max_v: 4425.0\n",
      "lif layer 2 self.abs_max_v: 3016.5\n",
      "lif layer 1 self.abs_max_v: 4439.5\n",
      "lif layer 1 self.abs_max_v: 4710.0\n",
      "lif layer 2 self.abs_max_v: 3094.5\n",
      "lif layer 2 self.abs_max_v: 3265.0\n",
      "lif layer 2 self.abs_max_v: 3277.0\n",
      "fc layer 2 self.abs_max_out: 2350.0\n",
      "fc layer 2 self.abs_max_out: 2494.0\n",
      "fc layer 2 self.abs_max_out: 2553.0\n",
      "lif layer 2 self.abs_max_v: 3446.5\n",
      "fc layer 1 self.abs_max_out: 3731.0\n",
      "lif layer 1 self.abs_max_v: 4720.5\n",
      "lif layer 1 self.abs_max_v: 4951.5\n",
      "lif layer 1 self.abs_max_v: 5043.0\n",
      "fc layer 1 self.abs_max_out: 3773.0\n",
      "fc layer 2 self.abs_max_out: 2588.0\n",
      "lif layer 1 self.abs_max_v: 5074.0\n",
      "lif layer 1 self.abs_max_v: 5158.0\n",
      "lif layer 1 self.abs_max_v: 5189.0\n",
      "lif layer 1 self.abs_max_v: 5410.5\n",
      "lif layer 1 self.abs_max_v: 5554.5\n",
      "lif layer 1 self.abs_max_v: 5944.5\n",
      "fc layer 2 self.abs_max_out: 2645.0\n",
      "fc layer 2 self.abs_max_out: 2701.0\n",
      "fc layer 1 self.abs_max_out: 4010.0\n",
      "fc layer 1 self.abs_max_out: 4080.0\n",
      "fc layer 1 self.abs_max_out: 4855.0\n",
      "lif layer 1 self.abs_max_v: 6331.5\n",
      "lif layer 1 self.abs_max_v: 7045.0\n",
      "lif layer 1 self.abs_max_v: 7407.5\n",
      "fc layer 2 self.abs_max_out: 2800.0\n",
      "fc layer 2 self.abs_max_out: 3003.0\n",
      "fc layer 2 self.abs_max_out: 3012.0\n",
      "fc layer 2 self.abs_max_out: 3079.0\n",
      "fc layer 3 self.abs_max_out: 523.0\n",
      "lif layer 2 self.abs_max_v: 3781.5\n",
      "fc layer 1 self.abs_max_out: 5216.0\n",
      "fc layer 1 self.abs_max_out: 5377.0\n",
      "fc layer 1 self.abs_max_out: 5423.0\n",
      "lif layer 1 self.abs_max_v: 7995.0\n",
      "lif layer 1 self.abs_max_v: 8260.5\n",
      "fc layer 2 self.abs_max_out: 3209.0\n",
      "fc layer 3 self.abs_max_out: 535.0\n",
      "lif layer 2 self.abs_max_v: 3855.0\n",
      "fc layer 1 self.abs_max_out: 5427.0\n",
      "fc layer 1 self.abs_max_out: 5498.0\n",
      "fc layer 1 self.abs_max_out: 5772.0\n",
      "fc layer 3 self.abs_max_out: 544.0\n",
      "fc layer 1 self.abs_max_out: 5847.0\n",
      "fc layer 1 self.abs_max_out: 5928.0\n",
      "fc layer 2 self.abs_max_out: 3289.0\n",
      "fc layer 1 self.abs_max_out: 6002.0\n",
      "lif layer 1 self.abs_max_v: 8491.0\n",
      "lif layer 1 self.abs_max_v: 8735.5\n",
      "lif layer 1 self.abs_max_v: 9312.0\n",
      "lif layer 1 self.abs_max_v: 9660.0\n",
      "fc layer 2 self.abs_max_out: 3334.0\n",
      "fc layer 2 self.abs_max_out: 3338.0\n",
      "fc layer 1 self.abs_max_out: 7098.0\n",
      "fc layer 2 self.abs_max_out: 3678.0\n",
      "fc layer 3 self.abs_max_out: 556.0\n",
      "lif layer 2 self.abs_max_v: 4014.0\n",
      "fc layer 2 self.abs_max_out: 3694.0\n",
      "lif layer 2 self.abs_max_v: 4144.5\n",
      "lif layer 2 self.abs_max_v: 4226.5\n",
      "lif layer 2 self.abs_max_v: 4241.5\n",
      "fc layer 2 self.abs_max_out: 3777.0\n",
      "fc layer 3 self.abs_max_out: 564.0\n",
      "lif layer 1 self.abs_max_v: 10144.5\n",
      "fc layer 2 self.abs_max_out: 3825.0\n",
      "lif layer 1 self.abs_max_v: 10185.5\n",
      "lif layer 2 self.abs_max_v: 4244.0\n",
      "fc layer 2 self.abs_max_out: 3893.0\n",
      "fc layer 1 self.abs_max_out: 7414.0\n",
      "lif layer 2 self.abs_max_v: 4285.5\n",
      "lif layer 2 self.abs_max_v: 4538.0\n",
      "lif layer 2 self.abs_max_v: 4653.0\n",
      "fc layer 1 self.abs_max_out: 7688.0\n",
      "fc layer 2 self.abs_max_out: 3915.0\n",
      "fc layer 2 self.abs_max_out: 3920.0\n",
      "fc layer 2 self.abs_max_out: 4030.0\n",
      "lif layer 1 self.abs_max_v: 11391.5\n",
      "lif layer 2 self.abs_max_v: 4942.5\n",
      "lif layer 2 self.abs_max_v: 4951.0\n",
      "lif layer 2 self.abs_max_v: 5050.0\n",
      "fc layer 2 self.abs_max_out: 4168.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.915642/  2.045702, val:  46.25%, val_best:  46.25%, tr:  96.53%, tr_best:  96.53%, epoch time: 299.35 seconds, 4.99 minutes\n",
      "total_backward_count 44360 real_backward_count 9748  21.975%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 4171.0\n",
      "lif layer 2 self.abs_max_v: 5391.5\n",
      "fc layer 3 self.abs_max_out: 574.0\n",
      "fc layer 3 self.abs_max_out: 579.0\n",
      "fc layer 1 self.abs_max_out: 7757.0\n",
      "lif layer 2 self.abs_max_v: 5693.5\n",
      "fc layer 1 self.abs_max_out: 7777.0\n",
      "lif layer 2 self.abs_max_v: 5924.5\n",
      "fc layer 1 self.abs_max_out: 8153.0\n",
      "fc layer 1 self.abs_max_out: 8379.0\n",
      "lif layer 2 self.abs_max_v: 6000.0\n",
      "lif layer 2 self.abs_max_v: 6234.5\n",
      "lif layer 1 self.abs_max_v: 13194.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.900678/  2.019847, val:  42.92%, val_best:  46.25%, tr:  99.39%, tr_best:  99.39%, epoch time: 298.05 seconds, 4.97 minutes\n",
      "total_backward_count 88720 real_backward_count 16855  18.998%\n",
      "lif layer 2 self.abs_max_v: 6371.0\n",
      "lif layer 2 self.abs_max_v: 6372.5\n",
      "lif layer 2 self.abs_max_v: 6545.5\n",
      "fc layer 3 self.abs_max_out: 580.0\n",
      "fc layer 1 self.abs_max_out: 8533.0\n",
      "fc layer 1 self.abs_max_out: 8595.0\n",
      "lif layer 2 self.abs_max_v: 6670.0\n",
      "lif layer 2 self.abs_max_v: 6865.5\n",
      "lif layer 2 self.abs_max_v: 6877.0\n",
      "lif layer 2 self.abs_max_v: 6900.5\n",
      "lif layer 2 self.abs_max_v: 7137.5\n",
      "fc layer 2 self.abs_max_out: 4238.0\n",
      "lif layer 2 self.abs_max_v: 7613.0\n",
      "lif layer 1 self.abs_max_v: 14283.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.926856/  2.046974, val:  51.67%, val_best:  51.67%, tr:  99.44%, tr_best:  99.44%, epoch time: 297.04 seconds, 4.95 minutes\n",
      "total_backward_count 133080 real_backward_count 23572  17.713%\n",
      "fc layer 1 self.abs_max_out: 8807.0\n",
      "fc layer 2 self.abs_max_out: 4288.0\n",
      "lif layer 2 self.abs_max_v: 7955.5\n",
      "fc layer 1 self.abs_max_out: 9040.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.922066/  2.045004, val:  55.00%, val_best:  55.00%, tr:  99.75%, tr_best:  99.75%, epoch time: 298.14 seconds, 4.97 minutes\n",
      "total_backward_count 177440 real_backward_count 29823  16.807%\n",
      "fc layer 1 self.abs_max_out: 9068.0\n",
      "fc layer 1 self.abs_max_out: 9164.0\n",
      "fc layer 1 self.abs_max_out: 9534.0\n",
      "fc layer 1 self.abs_max_out: 9579.0\n",
      "fc layer 2 self.abs_max_out: 4300.0\n",
      "lif layer 1 self.abs_max_v: 14613.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.915856/  2.001978, val:  57.08%, val_best:  57.08%, tr:  99.82%, tr_best:  99.82%, epoch time: 298.40 seconds, 4.97 minutes\n",
      "total_backward_count 221800 real_backward_count 36031  16.245%\n",
      "fc layer 1 self.abs_max_out: 9627.0\n",
      "fc layer 2 self.abs_max_out: 4412.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.898629/  2.009076, val:  58.75%, val_best:  58.75%, tr:  99.82%, tr_best:  99.82%, epoch time: 297.59 seconds, 4.96 minutes\n",
      "total_backward_count 266160 real_backward_count 41876  15.733%\n",
      "fc layer 1 self.abs_max_out: 9764.0\n",
      "fc layer 2 self.abs_max_out: 4512.0\n",
      "fc layer 2 self.abs_max_out: 4876.0\n",
      "fc layer 1 self.abs_max_out: 9941.0\n",
      "fc layer 3 self.abs_max_out: 592.0\n",
      "lif layer 1 self.abs_max_v: 15410.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.890342/  2.028484, val:  55.00%, val_best:  58.75%, tr:  99.91%, tr_best:  99.91%, epoch time: 298.12 seconds, 4.97 minutes\n",
      "total_backward_count 310520 real_backward_count 47605  15.331%\n",
      "fc layer 2 self.abs_max_out: 4917.0\n",
      "fc layer 1 self.abs_max_out: 10068.0\n",
      "lif layer 1 self.abs_max_v: 15712.5\n",
      "lif layer 1 self.abs_max_v: 16389.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.893847/  1.996999, val:  62.08%, val_best:  62.08%, tr:  99.84%, tr_best:  99.91%, epoch time: 298.15 seconds, 4.97 minutes\n",
      "total_backward_count 354880 real_backward_count 53044  14.947%\n",
      "fc layer 2 self.abs_max_out: 4936.0\n",
      "fc layer 2 self.abs_max_out: 5087.0\n",
      "fc layer 3 self.abs_max_out: 603.0\n",
      "fc layer 1 self.abs_max_out: 10114.0\n",
      "fc layer 1 self.abs_max_out: 10168.0\n",
      "fc layer 3 self.abs_max_out: 626.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.879933/  1.984206, val:  57.08%, val_best:  62.08%, tr:  99.91%, tr_best:  99.91%, epoch time: 297.58 seconds, 4.96 minutes\n",
      "total_backward_count 399240 real_backward_count 58189  14.575%\n",
      "fc layer 1 self.abs_max_out: 10315.0\n",
      "fc layer 1 self.abs_max_out: 10335.0\n",
      "lif layer 1 self.abs_max_v: 16420.5\n",
      "lif layer 1 self.abs_max_v: 16669.0\n",
      "lif layer 1 self.abs_max_v: 16774.0\n",
      "lif layer 1 self.abs_max_v: 17758.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.867254/  1.966569, val:  72.08%, val_best:  72.08%, tr:  99.89%, tr_best:  99.91%, epoch time: 299.05 seconds, 4.98 minutes\n",
      "total_backward_count 443600 real_backward_count 63210  14.249%\n",
      "fc layer 1 self.abs_max_out: 10505.0\n",
      "fc layer 2 self.abs_max_out: 5184.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.861259/  1.982100, val:  67.08%, val_best:  72.08%, tr:  99.77%, tr_best:  99.91%, epoch time: 297.34 seconds, 4.96 minutes\n",
      "total_backward_count 487960 real_backward_count 68177  13.972%\n",
      "fc layer 3 self.abs_max_out: 644.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.855511/  1.947568, val:  81.67%, val_best:  81.67%, tr:  99.98%, tr_best:  99.98%, epoch time: 297.80 seconds, 4.96 minutes\n",
      "total_backward_count 532320 real_backward_count 72866  13.688%\n",
      "fc layer 1 self.abs_max_out: 10523.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.836629/  1.933871, val:  77.50%, val_best:  81.67%, tr:  99.95%, tr_best:  99.98%, epoch time: 301.61 seconds, 5.03 minutes\n",
      "total_backward_count 576680 real_backward_count 77274  13.400%\n",
      "fc layer 1 self.abs_max_out: 10598.0\n",
      "fc layer 3 self.abs_max_out: 652.0\n",
      "fc layer 1 self.abs_max_out: 10696.0\n",
      "lif layer 1 self.abs_max_v: 18005.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.826482/  1.945655, val:  68.33%, val_best:  81.67%, tr:  99.95%, tr_best:  99.98%, epoch time: 300.08 seconds, 5.00 minutes\n",
      "total_backward_count 621040 real_backward_count 81518  13.126%\n",
      "fc layer 1 self.abs_max_out: 10861.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.833194/  1.931429, val:  80.00%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.44 seconds, 4.99 minutes\n",
      "total_backward_count 665400 real_backward_count 85713  12.881%\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.835084/  1.917707, val:  84.17%, val_best:  84.17%, tr:  99.93%, tr_best: 100.00%, epoch time: 299.40 seconds, 4.99 minutes\n",
      "total_backward_count 709760 real_backward_count 89737  12.643%\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.830747/  1.933823, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.14 seconds, 5.00 minutes\n",
      "total_backward_count 754120 real_backward_count 93810  12.440%\n",
      "lif layer 1 self.abs_max_v: 18123.0\n",
      "lif layer 1 self.abs_max_v: 18176.5\n",
      "fc layer 1 self.abs_max_out: 10956.0\n",
      "lif layer 1 self.abs_max_v: 18230.5\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.823182/  1.915537, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.34 seconds, 4.99 minutes\n",
      "total_backward_count 798480 real_backward_count 97770  12.245%\n",
      "lif layer 1 self.abs_max_v: 18803.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.810210/  1.932883, val:  78.33%, val_best:  85.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 300.12 seconds, 5.00 minutes\n",
      "total_backward_count 842840 real_backward_count 101544  12.048%\n",
      "fc layer 1 self.abs_max_out: 11049.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.793676/  1.898540, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.56 seconds, 5.03 minutes\n",
      "total_backward_count 887200 real_backward_count 105178  11.855%\n",
      "fc layer 1 self.abs_max_out: 11267.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.796954/  1.887680, val:  82.50%, val_best:  85.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 299.35 seconds, 4.99 minutes\n",
      "total_backward_count 931560 real_backward_count 108708  11.669%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.782109/  1.908820, val:  67.08%, val_best:  85.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 300.96 seconds, 5.02 minutes\n",
      "total_backward_count 975920 real_backward_count 112188  11.496%\n",
      "fc layer 1 self.abs_max_out: 11313.0\n",
      "fc layer 3 self.abs_max_out: 656.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.788510/  1.905430, val:  81.25%, val_best:  85.00%, tr:  99.95%, tr_best: 100.00%, epoch time: 300.05 seconds, 5.00 minutes\n",
      "total_backward_count 1020280 real_backward_count 115538  11.324%\n",
      "fc layer 3 self.abs_max_out: 668.0\n",
      "fc layer 3 self.abs_max_out: 675.0\n",
      "fc layer 1 self.abs_max_out: 11317.0\n",
      "lif layer 1 self.abs_max_v: 19164.5\n",
      "fc layer 1 self.abs_max_out: 11357.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.778010/  1.889742, val:  80.00%, val_best:  85.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 299.39 seconds, 4.99 minutes\n",
      "total_backward_count 1064640 real_backward_count 118931  11.171%\n",
      "fc layer 1 self.abs_max_out: 11508.0\n",
      "fc layer 1 self.abs_max_out: 11515.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.766733/  1.869313, val:  80.42%, val_best:  85.00%, tr:  99.95%, tr_best: 100.00%, epoch time: 300.70 seconds, 5.01 minutes\n",
      "total_backward_count 1109000 real_backward_count 122335  11.031%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.767059/  1.885135, val:  76.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.30 seconds, 5.02 minutes\n",
      "total_backward_count 1153360 real_backward_count 125563  10.887%\n",
      "fc layer 1 self.abs_max_out: 11534.0\n",
      "fc layer 1 self.abs_max_out: 11623.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.757316/  1.872178, val:  86.25%, val_best:  86.25%, tr:  99.95%, tr_best: 100.00%, epoch time: 300.52 seconds, 5.01 minutes\n",
      "total_backward_count 1197720 real_backward_count 128687  10.744%\n",
      "fc layer 3 self.abs_max_out: 677.0\n",
      "fc layer 3 self.abs_max_out: 680.0\n",
      "fc layer 2 self.abs_max_out: 5347.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.758701/  1.887421, val:  77.92%, val_best:  86.25%, tr:  99.95%, tr_best: 100.00%, epoch time: 299.54 seconds, 4.99 minutes\n",
      "total_backward_count 1242080 real_backward_count 131791  10.611%\n",
      "fc layer 2 self.abs_max_out: 5390.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.743996/  1.850281, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.77 seconds, 5.01 minutes\n",
      "total_backward_count 1286440 real_backward_count 134822  10.480%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.746714/  1.862009, val:  82.92%, val_best:  86.25%, tr:  99.95%, tr_best: 100.00%, epoch time: 300.75 seconds, 5.01 minutes\n",
      "total_backward_count 1330800 real_backward_count 137782  10.353%\n",
      "fc layer 3 self.abs_max_out: 681.0\n",
      "lif layer 1 self.abs_max_v: 19288.0\n",
      "lif layer 1 self.abs_max_v: 19670.0\n",
      "lif layer 1 self.abs_max_v: 19736.0\n",
      "fc layer 3 self.abs_max_out: 698.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.749498/  1.864982, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.54 seconds, 4.99 minutes\n",
      "total_backward_count 1375160 real_backward_count 140726  10.233%\n",
      "fc layer 1 self.abs_max_out: 11681.0\n",
      "fc layer 3 self.abs_max_out: 707.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.736863/  1.858671, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.64 seconds, 4.99 minutes\n",
      "total_backward_count 1419520 real_backward_count 143651  10.120%\n",
      "lif layer 1 self.abs_max_v: 19804.5\n",
      "lif layer 1 self.abs_max_v: 19906.5\n",
      "fc layer 1 self.abs_max_out: 11710.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.737217/  1.859135, val:  80.42%, val_best:  86.25%, tr:  99.98%, tr_best: 100.00%, epoch time: 300.94 seconds, 5.02 minutes\n",
      "total_backward_count 1463880 real_backward_count 146394  10.000%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.729177/  1.853901, val:  85.83%, val_best:  86.25%, tr:  99.98%, tr_best: 100.00%, epoch time: 301.21 seconds, 5.02 minutes\n",
      "total_backward_count 1508240 real_backward_count 149294   9.899%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.717668/  1.842804, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.28 seconds, 4.99 minutes\n",
      "total_backward_count 1552600 real_backward_count 152087   9.796%\n",
      "fc layer 3 self.abs_max_out: 709.0\n",
      "fc layer 1 self.abs_max_out: 11731.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.710964/  1.824132, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.51 seconds, 5.01 minutes\n",
      "total_backward_count 1596960 real_backward_count 154763   9.691%\n",
      "lif layer 1 self.abs_max_v: 20272.5\n",
      "lif layer 1 self.abs_max_v: 20462.5\n",
      "fc layer 3 self.abs_max_out: 715.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.692274/  1.825075, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.67 seconds, 5.01 minutes\n",
      "total_backward_count 1641320 real_backward_count 157327   9.585%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.691565/  1.825253, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.88 seconds, 5.01 minutes\n",
      "total_backward_count 1685680 real_backward_count 159920   9.487%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.695565/  1.821680, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.68 seconds, 5.03 minutes\n",
      "total_backward_count 1730040 real_backward_count 162501   9.393%\n",
      "fc layer 1 self.abs_max_out: 11836.0\n",
      "fc layer 3 self.abs_max_out: 725.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.694083/  1.819021, val:  82.92%, val_best:  87.50%, tr:  99.98%, tr_best: 100.00%, epoch time: 301.74 seconds, 5.03 minutes\n",
      "total_backward_count 1774400 real_backward_count 165007   9.299%\n",
      "lif layer 1 self.abs_max_v: 20527.0\n",
      "lif layer 1 self.abs_max_v: 20673.5\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.698559/  1.823921, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.87 seconds, 5.03 minutes\n",
      "total_backward_count 1818760 real_backward_count 167503   9.210%\n",
      "fc layer 1 self.abs_max_out: 11993.0\n",
      "lif layer 1 self.abs_max_v: 21258.5\n",
      "lif layer 1 self.abs_max_v: 21309.5\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.700259/  1.821563, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 302.53 seconds, 5.04 minutes\n",
      "total_backward_count 1863120 real_backward_count 169922   9.120%\n",
      "fc layer 3 self.abs_max_out: 728.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.689848/  1.813718, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.30 seconds, 5.02 minutes\n",
      "total_backward_count 1907480 real_backward_count 172247   9.030%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.689633/  1.822237, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.64 seconds, 5.01 minutes\n",
      "total_backward_count 1951840 real_backward_count 174672   8.949%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.690799/  1.813234, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.16 seconds, 5.02 minutes\n",
      "total_backward_count 1996200 real_backward_count 176999   8.867%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.681882/  1.809481, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.56 seconds, 4.99 minutes\n",
      "total_backward_count 2040560 real_backward_count 179352   8.789%\n",
      "fc layer 3 self.abs_max_out: 738.0\n",
      "fc layer 3 self.abs_max_out: 739.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.668458/  1.815960, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 296.69 seconds, 4.94 minutes\n",
      "total_backward_count 2084920 real_backward_count 181556   8.708%\n",
      "fc layer 3 self.abs_max_out: 743.0\n",
      "fc layer 3 self.abs_max_out: 757.0\n",
      "fc layer 3 self.abs_max_out: 811.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.663004/  1.810399, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.73 seconds, 5.00 minutes\n",
      "total_backward_count 2129280 real_backward_count 183806   8.632%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.655251/  1.801333, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.80 seconds, 5.00 minutes\n",
      "total_backward_count 2173640 real_backward_count 186002   8.557%\n",
      "fc layer 1 self.abs_max_out: 12096.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.658350/  1.782910, val:  88.33%, val_best:  88.33%, tr:  99.95%, tr_best: 100.00%, epoch time: 300.71 seconds, 5.01 minutes\n",
      "total_backward_count 2218000 real_backward_count 188272   8.488%\n",
      "lif layer 2 self.abs_max_v: 8110.5\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.650805/  1.808642, val:  82.50%, val_best:  88.33%, tr:  99.98%, tr_best: 100.00%, epoch time: 300.05 seconds, 5.00 minutes\n",
      "total_backward_count 2262360 real_backward_count 190372   8.415%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.668776/  1.796133, val:  86.67%, val_best:  88.33%, tr:  99.98%, tr_best: 100.00%, epoch time: 298.77 seconds, 4.98 minutes\n",
      "total_backward_count 2306720 real_backward_count 192513   8.346%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.667558/  1.794018, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.34 seconds, 4.99 minutes\n",
      "total_backward_count 2351080 real_backward_count 194576   8.276%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.648738/  1.786967, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.54 seconds, 5.01 minutes\n",
      "total_backward_count 2395440 real_backward_count 196662   8.210%\n",
      "fc layer 1 self.abs_max_out: 12143.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.641777/  1.789681, val:  82.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.90 seconds, 5.01 minutes\n",
      "total_backward_count 2439800 real_backward_count 198657   8.142%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.632207/  1.777180, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.85 seconds, 5.01 minutes\n",
      "total_backward_count 2484160 real_backward_count 200685   8.079%\n",
      "lif layer 1 self.abs_max_v: 21326.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.636426/  1.787548, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.73 seconds, 5.01 minutes\n",
      "total_backward_count 2528520 real_backward_count 202739   8.018%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.641795/  1.778672, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.53 seconds, 5.01 minutes\n",
      "total_backward_count 2572880 real_backward_count 204653   7.954%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.631423/  1.773303, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.78 seconds, 5.01 minutes\n",
      "total_backward_count 2617240 real_backward_count 206616   7.894%\n",
      "fc layer 1 self.abs_max_out: 12341.0\n",
      "lif layer 1 self.abs_max_v: 21684.0\n",
      "lif layer 1 self.abs_max_v: 21751.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.622283/  1.756465, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 302.63 seconds, 5.04 minutes\n",
      "total_backward_count 2661600 real_backward_count 208600   7.837%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.618426/  1.773418, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.41 seconds, 4.99 minutes\n",
      "total_backward_count 2705960 real_backward_count 210542   7.781%\n",
      "fc layer 1 self.abs_max_out: 12391.0\n",
      "lif layer 1 self.abs_max_v: 21821.5\n",
      "lif layer 1 self.abs_max_v: 21893.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.632900/  1.776973, val:  81.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.14 seconds, 5.00 minutes\n",
      "total_backward_count 2750320 real_backward_count 212524   7.727%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.611617/  1.758711, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 302.08 seconds, 5.03 minutes\n",
      "total_backward_count 2794680 real_backward_count 214366   7.671%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.619076/  1.748592, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.10 seconds, 5.02 minutes\n",
      "total_backward_count 2839040 real_backward_count 216162   7.614%\n",
      "fc layer 2 self.abs_max_out: 5416.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.614540/  1.770304, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.92 seconds, 5.02 minutes\n",
      "total_backward_count 2883400 real_backward_count 217956   7.559%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.622787/  1.767795, val:  84.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.58 seconds, 5.01 minutes\n",
      "total_backward_count 2927760 real_backward_count 219808   7.508%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.598482/  1.733123, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.65 seconds, 5.03 minutes\n",
      "total_backward_count 2972120 real_backward_count 221574   7.455%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.601745/  1.754793, val:  83.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.72 seconds, 5.01 minutes\n",
      "total_backward_count 3016480 real_backward_count 223373   7.405%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.588749/  1.734128, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.43 seconds, 5.01 minutes\n",
      "total_backward_count 3060840 real_backward_count 225119   7.355%\n",
      "fc layer 1 self.abs_max_out: 12408.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.589658/  1.742753, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.63 seconds, 5.01 minutes\n",
      "total_backward_count 3105200 real_backward_count 226903   7.307%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.591133/  1.748654, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.24 seconds, 5.00 minutes\n",
      "total_backward_count 3149560 real_backward_count 228595   7.258%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.589738/  1.735526, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.94 seconds, 5.02 minutes\n",
      "total_backward_count 3193920 real_backward_count 230324   7.211%\n",
      "fc layer 3 self.abs_max_out: 814.0\n",
      "fc layer 3 self.abs_max_out: 815.0\n",
      "fc layer 3 self.abs_max_out: 870.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.572466/  1.757569, val:  81.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 302.49 seconds, 5.04 minutes\n",
      "total_backward_count 3238280 real_backward_count 232076   7.167%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.593484/  1.751693, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 298.76 seconds, 4.98 minutes\n",
      "total_backward_count 3282640 real_backward_count 233684   7.119%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.582525/  1.739679, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.98 seconds, 5.02 minutes\n",
      "total_backward_count 3327000 real_backward_count 235378   7.075%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.581978/  1.715808, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.51 seconds, 5.01 minutes\n",
      "total_backward_count 3371360 real_backward_count 236990   7.030%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.571262/  1.740744, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.84 seconds, 5.00 minutes\n",
      "total_backward_count 3415720 real_backward_count 238633   6.986%\n",
      "fc layer 1 self.abs_max_out: 12559.0\n",
      "lif layer 1 self.abs_max_v: 22084.0\n",
      "lif layer 1 self.abs_max_v: 22196.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.578717/  1.715093, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.55 seconds, 4.99 minutes\n",
      "total_backward_count 3460080 real_backward_count 240314   6.945%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.562393/  1.687742, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 303.01 seconds, 5.05 minutes\n",
      "total_backward_count 3504440 real_backward_count 241921   6.903%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.542863/  1.712677, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 302.28 seconds, 5.04 minutes\n",
      "total_backward_count 3548800 real_backward_count 243501   6.862%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.551843/  1.709701, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.59 seconds, 5.01 minutes\n",
      "total_backward_count 3593160 real_backward_count 245105   6.821%\n",
      "fc layer 1 self.abs_max_out: 12560.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.546487/  1.690283, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.47 seconds, 5.01 minutes\n",
      "total_backward_count 3637520 real_backward_count 246649   6.781%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.541380/  1.692329, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.77 seconds, 5.00 minutes\n",
      "total_backward_count 3681880 real_backward_count 248225   6.742%\n",
      "fc layer 1 self.abs_max_out: 12638.0\n",
      "lif layer 1 self.abs_max_v: 22315.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.551325/  1.724051, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.18 seconds, 5.00 minutes\n",
      "total_backward_count 3726240 real_backward_count 249748   6.702%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.557390/  1.708154, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.53 seconds, 5.01 minutes\n",
      "total_backward_count 3770600 real_backward_count 251233   6.663%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.559628/  1.710084, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.23 seconds, 5.02 minutes\n",
      "total_backward_count 3814960 real_backward_count 252735   6.625%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.549896/  1.716190, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.44 seconds, 5.01 minutes\n",
      "total_backward_count 3859320 real_backward_count 254203   6.587%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.542606/  1.693382, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.18 seconds, 5.02 minutes\n",
      "total_backward_count 3903680 real_backward_count 255645   6.549%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.539270/  1.694780, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.55 seconds, 5.03 minutes\n",
      "total_backward_count 3948040 real_backward_count 257108   6.512%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.529457/  1.691560, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.32 seconds, 5.02 minutes\n",
      "total_backward_count 3992400 real_backward_count 258484   6.474%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.528855/  1.687855, val:  85.83%, val_best:  90.83%, tr:  99.98%, tr_best: 100.00%, epoch time: 300.99 seconds, 5.02 minutes\n",
      "total_backward_count 4036760 real_backward_count 259862   6.437%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.532156/  1.686465, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.74 seconds, 5.00 minutes\n",
      "total_backward_count 4081120 real_backward_count 261233   6.401%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.524265/  1.697376, val:  82.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.71 seconds, 5.03 minutes\n",
      "total_backward_count 4125480 real_backward_count 262628   6.366%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.523430/  1.690268, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.28 seconds, 5.02 minutes\n",
      "total_backward_count 4169840 real_backward_count 263996   6.331%\n",
      "fc layer 3 self.abs_max_out: 875.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.520462/  1.682967, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 298.14 seconds, 4.97 minutes\n",
      "total_backward_count 4214200 real_backward_count 265395   6.298%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.509208/  1.707469, val:  82.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.37 seconds, 5.01 minutes\n",
      "total_backward_count 4258560 real_backward_count 266715   6.263%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.515906/  1.676027, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.07 seconds, 5.02 minutes\n",
      "total_backward_count 4302920 real_backward_count 268033   6.229%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.509628/  1.683134, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.51 seconds, 5.01 minutes\n",
      "total_backward_count 4347280 real_backward_count 269304   6.195%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.519656/  1.680969, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.97 seconds, 5.00 minutes\n",
      "total_backward_count 4391640 real_backward_count 270660   6.163%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.521855/  1.680093, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.73 seconds, 5.03 minutes\n",
      "total_backward_count 4436000 real_backward_count 272007   6.132%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.500301/  1.659807, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 303.03 seconds, 5.05 minutes\n",
      "total_backward_count 4480360 real_backward_count 273339   6.101%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.502169/  1.664552, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.54 seconds, 5.03 minutes\n",
      "total_backward_count 4524720 real_backward_count 274576   6.068%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.495737/  1.671903, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.59 seconds, 5.01 minutes\n",
      "total_backward_count 4569080 real_backward_count 275844   6.037%\n",
      "fc layer 3 self.abs_max_out: 900.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.513685/  1.678043, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.09 seconds, 5.02 minutes\n",
      "total_backward_count 4613440 real_backward_count 277100   6.006%\n",
      "fc layer 3 self.abs_max_out: 901.0\n",
      "fc layer 3 self.abs_max_out: 903.0\n",
      "fc layer 1 self.abs_max_out: 12647.0\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.512895/  1.657915, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.86 seconds, 5.01 minutes\n",
      "total_backward_count 4657800 real_backward_count 278385   5.977%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.513249/  1.684766, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 303.88 seconds, 5.06 minutes\n",
      "total_backward_count 4702160 real_backward_count 279689   5.948%\n",
      "fc layer 3 self.abs_max_out: 922.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.515228/  1.692934, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.80 seconds, 5.01 minutes\n",
      "total_backward_count 4746520 real_backward_count 280923   5.919%\n",
      "fc layer 3 self.abs_max_out: 925.0\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.507110/  1.660625, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.68 seconds, 5.03 minutes\n",
      "total_backward_count 4790880 real_backward_count 282241   5.891%\n",
      "fc layer 3 self.abs_max_out: 928.0\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.509222/  1.684253, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.68 seconds, 5.01 minutes\n",
      "total_backward_count 4835240 real_backward_count 283562   5.864%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.505291/  1.678732, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.21 seconds, 5.00 minutes\n",
      "total_backward_count 4879600 real_backward_count 284756   5.836%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.510476/  1.669909, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.32 seconds, 5.01 minutes\n",
      "total_backward_count 4923960 real_backward_count 285978   5.808%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.495231/  1.670688, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.53 seconds, 4.99 minutes\n",
      "total_backward_count 4968320 real_backward_count 287193   5.780%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.493119/  1.650578, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.43 seconds, 5.02 minutes\n",
      "total_backward_count 5012680 real_backward_count 288392   5.753%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.480449/  1.645270, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.48 seconds, 5.01 minutes\n",
      "total_backward_count 5057040 real_backward_count 289620   5.727%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.475740/  1.641115, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.43 seconds, 5.01 minutes\n",
      "total_backward_count 5101400 real_backward_count 290849   5.701%\n",
      "fc layer 3 self.abs_max_out: 930.0\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.465626/  1.630683, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.47 seconds, 4.99 minutes\n",
      "total_backward_count 5145760 real_backward_count 292070   5.676%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.455549/  1.623887, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.90 seconds, 5.01 minutes\n",
      "total_backward_count 5190120 real_backward_count 293231   5.650%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.465292/  1.625430, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.83 seconds, 5.01 minutes\n",
      "total_backward_count 5234480 real_backward_count 294434   5.625%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.465886/  1.634209, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 302.49 seconds, 5.04 minutes\n",
      "total_backward_count 5278840 real_backward_count 295552   5.599%\n",
      "fc layer 3 self.abs_max_out: 933.0\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.458728/  1.627564, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 303.28 seconds, 5.05 minutes\n",
      "total_backward_count 5323200 real_backward_count 296705   5.574%\n",
      "fc layer 3 self.abs_max_out: 934.0\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.458642/  1.615393, val:  87.92%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.52 seconds, 5.03 minutes\n",
      "total_backward_count 5367560 real_backward_count 297808   5.548%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.458476/  1.628962, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.38 seconds, 5.02 minutes\n",
      "total_backward_count 5411920 real_backward_count 298968   5.524%\n",
      "fc layer 3 self.abs_max_out: 937.0\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.436192/  1.641402, val:  86.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.17 seconds, 5.00 minutes\n",
      "total_backward_count 5456280 real_backward_count 300019   5.499%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.443342/  1.622312, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.59 seconds, 5.03 minutes\n",
      "total_backward_count 5500640 real_backward_count 301108   5.474%\n",
      "fc layer 3 self.abs_max_out: 947.0\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.449169/  1.625337, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.43 seconds, 5.02 minutes\n",
      "total_backward_count 5545000 real_backward_count 302255   5.451%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.438366/  1.612761, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.35 seconds, 5.01 minutes\n",
      "total_backward_count 5589360 real_backward_count 303403   5.428%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.448425/  1.638411, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.26 seconds, 5.02 minutes\n",
      "total_backward_count 5633720 real_backward_count 304526   5.405%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.438774/  1.615796, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 302.22 seconds, 5.04 minutes\n",
      "total_backward_count 5678080 real_backward_count 305607   5.382%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.429305/  1.607929, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.22 seconds, 5.02 minutes\n",
      "total_backward_count 5722440 real_backward_count 306656   5.359%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.435213/  1.625877, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.66 seconds, 5.01 minutes\n",
      "total_backward_count 5766800 real_backward_count 307710   5.336%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.438669/  1.614574, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.34 seconds, 5.01 minutes\n",
      "total_backward_count 5811160 real_backward_count 308826   5.314%\n",
      "lif layer 2 self.abs_max_v: 8144.0\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.434537/  1.629501, val:  85.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 298.06 seconds, 4.97 minutes\n",
      "total_backward_count 5855520 real_backward_count 309861   5.292%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.437812/  1.605800, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.39 seconds, 5.01 minutes\n",
      "total_backward_count 5899880 real_backward_count 310953   5.270%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.425271/  1.608701, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.93 seconds, 5.03 minutes\n",
      "total_backward_count 5944240 real_backward_count 312048   5.250%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.420474/  1.606533, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.55 seconds, 5.01 minutes\n",
      "total_backward_count 5988600 real_backward_count 313117   5.229%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.419610/  1.594858, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.19 seconds, 5.00 minutes\n",
      "total_backward_count 6032960 real_backward_count 314189   5.208%\n",
      "fc layer 3 self.abs_max_out: 949.0\n",
      "fc layer 3 self.abs_max_out: 956.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.414126/  1.604887, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.87 seconds, 5.01 minutes\n",
      "total_backward_count 6077320 real_backward_count 315236   5.187%\n",
      "fc layer 3 self.abs_max_out: 987.0\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.417437/  1.618708, val:  85.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.80 seconds, 5.03 minutes\n",
      "total_backward_count 6121680 real_backward_count 316240   5.166%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.428204/  1.612351, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.49 seconds, 5.01 minutes\n",
      "total_backward_count 6166040 real_backward_count 317253   5.145%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.430098/  1.626831, val:  86.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.34 seconds, 5.01 minutes\n",
      "total_backward_count 6210400 real_backward_count 318328   5.126%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.434717/  1.612733, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.55 seconds, 5.01 minutes\n",
      "total_backward_count 6254760 real_backward_count 319363   5.106%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.423764/  1.604749, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 298.13 seconds, 4.97 minutes\n",
      "total_backward_count 6299120 real_backward_count 320407   5.087%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.424532/  1.619441, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.54 seconds, 5.01 minutes\n",
      "total_backward_count 6343480 real_backward_count 321449   5.067%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.424277/  1.612511, val:  88.33%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.70 seconds, 5.01 minutes\n",
      "total_backward_count 6387840 real_backward_count 322505   5.049%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.426847/  1.601915, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.78 seconds, 5.00 minutes\n",
      "total_backward_count 6432200 real_backward_count 323498   5.029%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.423522/  1.602108, val:  86.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.02 seconds, 5.00 minutes\n",
      "total_backward_count 6476560 real_backward_count 324445   5.010%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.420714/  1.603043, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.52 seconds, 5.03 minutes\n",
      "total_backward_count 6520920 real_backward_count 325434   4.991%\n",
      "lif layer 2 self.abs_max_v: 8244.5\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.417887/  1.610756, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.07 seconds, 5.00 minutes\n",
      "total_backward_count 6565280 real_backward_count 326388   4.971%\n",
      "lif layer 2 self.abs_max_v: 8262.5\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.411642/  1.609834, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.01 seconds, 4.98 minutes\n",
      "total_backward_count 6609640 real_backward_count 327421   4.954%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.402411/  1.604916, val:  82.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 298.74 seconds, 4.98 minutes\n",
      "total_backward_count 6654000 real_backward_count 328340   4.934%\n",
      "fc layer 3 self.abs_max_out: 991.0\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.405892/  1.599381, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.45 seconds, 4.99 minutes\n",
      "total_backward_count 6698360 real_backward_count 329320   4.916%\n",
      "lif layer 2 self.abs_max_v: 8334.5\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.408151/  1.609126, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 298.51 seconds, 4.98 minutes\n",
      "total_backward_count 6742720 real_backward_count 330221   4.897%\n",
      "lif layer 2 self.abs_max_v: 8357.0\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.413015/  1.605236, val:  85.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.56 seconds, 4.99 minutes\n",
      "total_backward_count 6787080 real_backward_count 331232   4.880%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.411785/  1.606646, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.17 seconds, 5.00 minutes\n",
      "total_backward_count 6831440 real_backward_count 332202   4.863%\n",
      "lif layer 2 self.abs_max_v: 8517.5\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.406205/  1.602357, val:  88.33%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.96 seconds, 5.03 minutes\n",
      "total_backward_count 6875800 real_backward_count 333224   4.846%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.405573/  1.596046, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.18 seconds, 5.00 minutes\n",
      "total_backward_count 6920160 real_backward_count 334165   4.829%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.405342/  1.608234, val:  86.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.64 seconds, 4.99 minutes\n",
      "total_backward_count 6964520 real_backward_count 335163   4.812%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.406845/  1.603172, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 298.59 seconds, 4.98 minutes\n",
      "total_backward_count 7008880 real_backward_count 336163   4.796%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.401307/  1.594258, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.58 seconds, 5.03 minutes\n",
      "total_backward_count 7053240 real_backward_count 337155   4.780%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.403581/  1.597103, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.45 seconds, 5.01 minutes\n",
      "total_backward_count 7097600 real_backward_count 338089   4.763%\n",
      "fc layer 3 self.abs_max_out: 999.0\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.403398/  1.604125, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.81 seconds, 5.00 minutes\n",
      "total_backward_count 7141960 real_backward_count 338949   4.746%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.403943/  1.577757, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.63 seconds, 5.01 minutes\n",
      "total_backward_count 7186320 real_backward_count 339892   4.730%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.384663/  1.592672, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.04 seconds, 5.02 minutes\n",
      "total_backward_count 7230680 real_backward_count 340810   4.713%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.389587/  1.575135, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.70 seconds, 5.01 minutes\n",
      "total_backward_count 7275040 real_backward_count 341718   4.697%\n",
      "lif layer 2 self.abs_max_v: 8531.0\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.389854/  1.584146, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.04 seconds, 5.02 minutes\n",
      "total_backward_count 7319400 real_backward_count 342595   4.681%\n",
      "fc layer 3 self.abs_max_out: 1000.0\n",
      "fc layer 3 self.abs_max_out: 1007.0\n",
      "fc layer 3 self.abs_max_out: 1026.0\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.389085/  1.594271, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.20 seconds, 5.00 minutes\n",
      "total_backward_count 7363760 real_backward_count 343469   4.664%\n",
      "lif layer 2 self.abs_max_v: 8584.0\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.396008/  1.588295, val:  87.92%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.61 seconds, 5.01 minutes\n",
      "total_backward_count 7408120 real_backward_count 344423   4.649%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.393271/  1.583161, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.42 seconds, 5.02 minutes\n",
      "total_backward_count 7452480 real_backward_count 345333   4.634%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.390202/  1.575415, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.78 seconds, 5.00 minutes\n",
      "total_backward_count 7496840 real_backward_count 346266   4.619%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.394763/  1.585616, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.09 seconds, 4.98 minutes\n",
      "total_backward_count 7541200 real_backward_count 347126   4.603%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.386344/  1.575243, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 298.80 seconds, 4.98 minutes\n",
      "total_backward_count 7585560 real_backward_count 348019   4.588%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.382331/  1.593651, val:  88.33%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 298.77 seconds, 4.98 minutes\n",
      "total_backward_count 7629920 real_backward_count 348852   4.572%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.381456/  1.581640, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.33 seconds, 4.99 minutes\n",
      "total_backward_count 7674280 real_backward_count 349686   4.557%\n",
      "fc layer 3 self.abs_max_out: 1069.0\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.388235/  1.574606, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.02 seconds, 4.98 minutes\n",
      "total_backward_count 7718640 real_backward_count 350586   4.542%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.387832/  1.573914, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 301.21 seconds, 5.02 minutes\n",
      "total_backward_count 7763000 real_backward_count 351425   4.527%\n",
      "lif layer 2 self.abs_max_v: 8823.0\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.375449/  1.570393, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.17 seconds, 5.00 minutes\n",
      "total_backward_count 7807360 real_backward_count 352269   4.512%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.375831/  1.587799, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.03 seconds, 5.00 minutes\n",
      "total_backward_count 7851720 real_backward_count 353148   4.498%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.380912/  1.573679, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.27 seconds, 4.99 minutes\n",
      "total_backward_count 7896080 real_backward_count 353965   4.483%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.384361/  1.592878, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 297.64 seconds, 4.96 minutes\n",
      "total_backward_count 7940440 real_backward_count 354812   4.468%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.390522/  1.574294, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.98 seconds, 5.02 minutes\n",
      "total_backward_count 7984800 real_backward_count 355700   4.455%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.376588/  1.562494, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.53 seconds, 5.01 minutes\n",
      "total_backward_count 8029160 real_backward_count 356566   4.441%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.379793/  1.583072, val:  85.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.57 seconds, 5.01 minutes\n",
      "total_backward_count 8073520 real_backward_count 357434   4.427%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.376568/  1.569369, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.47 seconds, 5.01 minutes\n",
      "total_backward_count 8117880 real_backward_count 358333   4.414%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.370029/  1.582946, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 299.80 seconds, 5.00 minutes\n",
      "total_backward_count 8162240 real_backward_count 359189   4.401%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.365214/  1.546481, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.45 seconds, 5.01 minutes\n",
      "total_backward_count 8206600 real_backward_count 360042   4.387%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.359243/  1.565157, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.42 seconds, 5.01 minutes\n",
      "total_backward_count 8250960 real_backward_count 360899   4.374%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.361006/  1.552534, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 294.14 seconds, 4.90 minutes\n",
      "total_backward_count 8295320 real_backward_count 361688   4.360%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.356371/  1.554498, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.23 seconds, 5.00 minutes\n",
      "total_backward_count 8339680 real_backward_count 362543   4.347%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.349580/  1.541939, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 298.05 seconds, 4.97 minutes\n",
      "total_backward_count 8384040 real_backward_count 363401   4.334%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.345074/  1.556814, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 298.63 seconds, 4.98 minutes\n",
      "total_backward_count 8428400 real_backward_count 364197   4.321%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.348558/  1.559547, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.86 seconds, 5.01 minutes\n",
      "total_backward_count 8472760 real_backward_count 365064   4.309%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.347946/  1.550606, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 300.67 seconds, 5.01 minutes\n",
      "total_backward_count 8517120 real_backward_count 365891   4.296%\n",
      "fc layer 3 self.abs_max_out: 1099.0\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.353315/  1.561442, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 298.78 seconds, 4.98 minutes\n",
      "total_backward_count 8561480 real_backward_count 366678   4.283%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.354410/  1.554350, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 304.49 seconds, 5.07 minutes\n",
      "total_backward_count 8605840 real_backward_count 367477   4.270%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.337017/  1.534512, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 305.58 seconds, 5.09 minutes\n",
      "total_backward_count 8650200 real_backward_count 368343   4.258%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.338562/  1.545018, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 312.47 seconds, 5.21 minutes\n",
      "total_backward_count 8694560 real_backward_count 369120   4.245%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.334841/  1.534854, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 314.38 seconds, 5.24 minutes\n",
      "total_backward_count 8738920 real_backward_count 369968   4.234%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.330160/  1.537671, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 312.58 seconds, 5.21 minutes\n",
      "total_backward_count 8783280 real_backward_count 370744   4.221%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.326647/  1.534549, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 309.52 seconds, 5.16 minutes\n",
      "total_backward_count 8827640 real_backward_count 371514   4.209%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.331380/  1.528756, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 311.02 seconds, 5.18 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e86eff106624f0dad69310a6065bc83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñá‚ñá‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñá‚ñá‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.33138</td></tr><tr><td>val_acc_best</td><td>0.925</td></tr><tr><td>val_acc_now</td><td>0.90417</td></tr><tr><td>val_loss</td><td>1.52876</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">still-sweep-59</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pc8mdq32' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/pc8mdq32</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251020_211543-pc8mdq32/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cww2cvjm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 19508\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251021_135934-cww2cvjm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cww2cvjm' target=\"_blank\">prime-sweep-63</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4wosfk6x' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4wosfk6x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4wosfk6x' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/4wosfk6x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cww2cvjm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/cww2cvjm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251021_135943_162', 'my_seed': 19508, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 726.0\n",
      "lif layer 1 self.abs_max_v: 726.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 824.0\n",
      "fc layer 2 self.abs_max_out: 388.0\n",
      "lif layer 2 self.abs_max_v: 392.5\n",
      "fc layer 1 self.abs_max_out: 765.0\n",
      "lif layer 1 self.abs_max_v: 932.5\n",
      "fc layer 2 self.abs_max_out: 494.0\n",
      "lif layer 2 self.abs_max_v: 550.5\n",
      "fc layer 3 self.abs_max_out: 32.0\n",
      "fc layer 1 self.abs_max_out: 817.0\n",
      "lif layer 1 self.abs_max_v: 984.0\n",
      "lif layer 2 self.abs_max_v: 562.5\n",
      "fc layer 3 self.abs_max_out: 80.0\n",
      "fc layer 1 self.abs_max_out: 824.0\n",
      "lif layer 1 self.abs_max_v: 1290.0\n",
      "fc layer 2 self.abs_max_out: 501.0\n",
      "lif layer 2 self.abs_max_v: 711.5\n",
      "fc layer 3 self.abs_max_out: 93.0\n",
      "fc layer 1 self.abs_max_out: 853.0\n",
      "fc layer 2 self.abs_max_out: 555.0\n",
      "fc layer 3 self.abs_max_out: 97.0\n",
      "fc layer 1 self.abs_max_out: 1001.0\n",
      "fc layer 2 self.abs_max_out: 602.0\n",
      "lif layer 2 self.abs_max_v: 801.0\n",
      "fc layer 3 self.abs_max_out: 103.0\n",
      "fc layer 1 self.abs_max_out: 1309.0\n",
      "lif layer 1 self.abs_max_v: 1309.0\n",
      "fc layer 1 self.abs_max_out: 1847.0\n",
      "lif layer 1 self.abs_max_v: 1847.0\n",
      "fc layer 2 self.abs_max_out: 657.0\n",
      "lif layer 2 self.abs_max_v: 902.0\n",
      "fc layer 2 self.abs_max_out: 671.0\n",
      "lif layer 2 self.abs_max_v: 962.0\n",
      "fc layer 3 self.abs_max_out: 190.0\n",
      "lif layer 2 self.abs_max_v: 974.5\n",
      "lif layer 2 self.abs_max_v: 1038.5\n",
      "fc layer 2 self.abs_max_out: 764.0\n",
      "fc layer 2 self.abs_max_out: 777.0\n",
      "fc layer 2 self.abs_max_out: 1128.0\n",
      "lif layer 2 self.abs_max_v: 1128.0\n",
      "fc layer 1 self.abs_max_out: 2006.0\n",
      "lif layer 1 self.abs_max_v: 2006.0\n",
      "lif layer 2 self.abs_max_v: 1242.5\n",
      "lif layer 2 self.abs_max_v: 1301.5\n",
      "fc layer 3 self.abs_max_out: 226.0\n",
      "lif layer 2 self.abs_max_v: 1335.0\n",
      "lif layer 2 self.abs_max_v: 1360.0\n",
      "fc layer 3 self.abs_max_out: 273.0\n",
      "fc layer 3 self.abs_max_out: 358.0\n",
      "lif layer 2 self.abs_max_v: 1450.5\n",
      "fc layer 3 self.abs_max_out: 402.0\n",
      "lif layer 2 self.abs_max_v: 1465.0\n",
      "fc layer 1 self.abs_max_out: 2374.0\n",
      "lif layer 1 self.abs_max_v: 2374.0\n",
      "fc layer 2 self.abs_max_out: 1249.0\n",
      "fc layer 2 self.abs_max_out: 1250.0\n",
      "fc layer 1 self.abs_max_out: 2497.0\n",
      "lif layer 1 self.abs_max_v: 2497.0\n",
      "fc layer 2 self.abs_max_out: 1348.0\n",
      "lif layer 2 self.abs_max_v: 1507.0\n",
      "fc layer 2 self.abs_max_out: 1357.0\n",
      "fc layer 2 self.abs_max_out: 1410.0\n",
      "fc layer 3 self.abs_max_out: 409.0\n",
      "lif layer 2 self.abs_max_v: 1561.5\n",
      "fc layer 2 self.abs_max_out: 1569.0\n",
      "lif layer 2 self.abs_max_v: 1710.5\n",
      "fc layer 2 self.abs_max_out: 1589.0\n",
      "lif layer 2 self.abs_max_v: 1782.0\n",
      "fc layer 2 self.abs_max_out: 1606.0\n",
      "lif layer 2 self.abs_max_v: 1965.5\n",
      "lif layer 1 self.abs_max_v: 2538.0\n",
      "lif layer 2 self.abs_max_v: 2195.0\n",
      "fc layer 2 self.abs_max_out: 1636.0\n",
      "fc layer 1 self.abs_max_out: 3506.0\n",
      "lif layer 1 self.abs_max_v: 3506.0\n",
      "lif layer 2 self.abs_max_v: 2253.0\n",
      "lif layer 2 self.abs_max_v: 2454.5\n",
      "lif layer 2 self.abs_max_v: 2630.5\n",
      "lif layer 2 self.abs_max_v: 2666.0\n",
      "fc layer 2 self.abs_max_out: 1640.0\n",
      "fc layer 2 self.abs_max_out: 1683.0\n",
      "fc layer 2 self.abs_max_out: 1690.0\n",
      "fc layer 2 self.abs_max_out: 1856.0\n",
      "fc layer 2 self.abs_max_out: 1900.0\n",
      "fc layer 2 self.abs_max_out: 1932.0\n",
      "lif layer 2 self.abs_max_v: 2721.5\n",
      "fc layer 2 self.abs_max_out: 2062.0\n",
      "fc layer 2 self.abs_max_out: 2167.0\n",
      "fc layer 2 self.abs_max_out: 2342.0\n",
      "fc layer 2 self.abs_max_out: 2349.0\n",
      "fc layer 3 self.abs_max_out: 413.0\n",
      "lif layer 1 self.abs_max_v: 3556.0\n",
      "fc layer 3 self.abs_max_out: 432.0\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "fc layer 1 self.abs_max_out: 3580.0\n",
      "lif layer 1 self.abs_max_v: 3921.5\n",
      "lif layer 2 self.abs_max_v: 2813.0\n",
      "lif layer 2 self.abs_max_v: 2948.0\n",
      "lif layer 2 self.abs_max_v: 2982.0\n",
      "fc layer 2 self.abs_max_out: 2699.0\n",
      "lif layer 2 self.abs_max_v: 3076.0\n",
      "lif layer 2 self.abs_max_v: 3081.5\n",
      "lif layer 2 self.abs_max_v: 3127.0\n",
      "lif layer 2 self.abs_max_v: 3223.5\n",
      "lif layer 2 self.abs_max_v: 3284.0\n",
      "lif layer 2 self.abs_max_v: 3467.0\n",
      "lif layer 2 self.abs_max_v: 3489.5\n",
      "fc layer 1 self.abs_max_out: 3594.0\n",
      "fc layer 1 self.abs_max_out: 3673.0\n",
      "fc layer 1 self.abs_max_out: 3745.0\n",
      "fc layer 1 self.abs_max_out: 3839.0\n",
      "lif layer 2 self.abs_max_v: 3497.0\n",
      "fc layer 2 self.abs_max_out: 2715.0\n",
      "fc layer 3 self.abs_max_out: 500.0\n",
      "fc layer 1 self.abs_max_out: 4249.0\n",
      "lif layer 1 self.abs_max_v: 4249.0\n",
      "lif layer 2 self.abs_max_v: 3765.5\n",
      "fc layer 1 self.abs_max_out: 4353.0\n",
      "lif layer 1 self.abs_max_v: 4353.0\n",
      "fc layer 2 self.abs_max_out: 2740.0\n",
      "lif layer 1 self.abs_max_v: 4446.5\n",
      "lif layer 1 self.abs_max_v: 4468.0\n",
      "fc layer 1 self.abs_max_out: 4356.0\n",
      "fc layer 1 self.abs_max_out: 4480.0\n",
      "lif layer 1 self.abs_max_v: 4480.0\n",
      "fc layer 1 self.abs_max_out: 4579.0\n",
      "lif layer 1 self.abs_max_v: 4579.0\n",
      "fc layer 2 self.abs_max_out: 2828.0\n",
      "lif layer 1 self.abs_max_v: 5006.0\n",
      "fc layer 1 self.abs_max_out: 4632.0\n",
      "lif layer 1 self.abs_max_v: 5051.5\n",
      "lif layer 1 self.abs_max_v: 6060.0\n",
      "fc layer 1 self.abs_max_out: 4881.0\n",
      "fc layer 2 self.abs_max_out: 2843.0\n",
      "fc layer 2 self.abs_max_out: 2866.0\n",
      "fc layer 2 self.abs_max_out: 2914.0\n",
      "fc layer 1 self.abs_max_out: 5094.0\n",
      "fc layer 2 self.abs_max_out: 2933.0\n",
      "fc layer 1 self.abs_max_out: 5309.0\n",
      "fc layer 2 self.abs_max_out: 2963.0\n",
      "fc layer 2 self.abs_max_out: 3007.0\n",
      "lif layer 1 self.abs_max_v: 6192.0\n",
      "fc layer 2 self.abs_max_out: 3049.0\n",
      "fc layer 2 self.abs_max_out: 3115.0\n",
      "fc layer 1 self.abs_max_out: 5480.0\n",
      "fc layer 1 self.abs_max_out: 5750.0\n",
      "lif layer 2 self.abs_max_v: 3880.5\n",
      "lif layer 2 self.abs_max_v: 4061.5\n",
      "fc layer 2 self.abs_max_out: 3250.0\n",
      "fc layer 1 self.abs_max_out: 5792.0\n",
      "fc layer 1 self.abs_max_out: 5844.0\n",
      "fc layer 2 self.abs_max_out: 3363.0\n",
      "fc layer 1 self.abs_max_out: 6359.0\n",
      "lif layer 1 self.abs_max_v: 6359.0\n",
      "lif layer 2 self.abs_max_v: 4118.0\n",
      "lif layer 2 self.abs_max_v: 4358.0\n",
      "fc layer 2 self.abs_max_out: 3509.0\n",
      "lif layer 2 self.abs_max_v: 4679.0\n",
      "lif layer 1 self.abs_max_v: 6375.0\n",
      "lif layer 1 self.abs_max_v: 6562.5\n",
      "lif layer 1 self.abs_max_v: 6584.5\n",
      "fc layer 1 self.abs_max_out: 6761.0\n",
      "lif layer 1 self.abs_max_v: 6761.0\n",
      "fc layer 2 self.abs_max_out: 3706.0\n",
      "fc layer 2 self.abs_max_out: 3772.0\n",
      "lif layer 1 self.abs_max_v: 6995.0\n",
      "fc layer 1 self.abs_max_out: 7226.0\n",
      "lif layer 1 self.abs_max_v: 7226.0\n",
      "fc layer 2 self.abs_max_out: 3844.0\n",
      "fc layer 1 self.abs_max_out: 7297.0\n",
      "lif layer 1 self.abs_max_v: 7297.0\n",
      "lif layer 2 self.abs_max_v: 4699.5\n",
      "lif layer 2 self.abs_max_v: 5095.0\n",
      "lif layer 2 self.abs_max_v: 5156.5\n",
      "fc layer 2 self.abs_max_out: 3959.0\n",
      "fc layer 2 self.abs_max_out: 3976.0\n",
      "lif layer 1 self.abs_max_v: 7600.0\n",
      "lif layer 1 self.abs_max_v: 7619.0\n",
      "lif layer 1 self.abs_max_v: 7636.0\n",
      "fc layer 2 self.abs_max_out: 4139.0\n",
      "fc layer 2 self.abs_max_out: 4175.0\n",
      "lif layer 1 self.abs_max_v: 9170.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.975784/  2.073878, val:  37.08%, val_best:  37.08%, tr:  95.99%, tr_best:  95.99%, epoch time: 311.31 seconds, 5.19 minutes\n",
      "total_backward_count 44360 real_backward_count 10083  22.730%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 7359.0\n",
      "fc layer 1 self.abs_max_out: 7568.0\n",
      "fc layer 2 self.abs_max_out: 4184.0\n",
      "lif layer 2 self.abs_max_v: 5165.0\n",
      "fc layer 2 self.abs_max_out: 4286.0\n",
      "fc layer 2 self.abs_max_out: 4351.0\n",
      "lif layer 2 self.abs_max_v: 5275.0\n",
      "fc layer 1 self.abs_max_out: 7884.0\n",
      "lif layer 2 self.abs_max_v: 5560.0\n",
      "lif layer 1 self.abs_max_v: 9221.5\n",
      "lif layer 1 self.abs_max_v: 9895.0\n",
      "fc layer 1 self.abs_max_out: 8401.0\n",
      "lif layer 1 self.abs_max_v: 9913.0\n",
      "fc layer 2 self.abs_max_out: 4356.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.964225/  2.070518, val:  67.08%, val_best:  67.08%, tr:  99.44%, tr_best:  99.44%, epoch time: 311.15 seconds, 5.19 minutes\n",
      "total_backward_count 88720 real_backward_count 17112  19.288%\n",
      "fc layer 1 self.abs_max_out: 8618.0\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'random', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        # \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [4.5, 4.0, 3.5, 3.0, 2.5]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/512]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [9]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"4\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = '4wosfk6x'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
