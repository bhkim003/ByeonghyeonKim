{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ssp.train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch   \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F   \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from scipy import io\n",
    "import itertools\n",
    "import math\n",
    "import datetime\n",
    "import wandb\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "\n",
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_train_system( \n",
    "    gpu = 3,\n",
    "    Conv_net = True,\n",
    "    SAE_net = True,\n",
    "\n",
    "    # hyperparameter\n",
    "    dataset_num = 16,\n",
    "    spike_length = 50,\n",
    "    num_cluster = 4,  # 클러스터 수 설정 # 논문엔 4개라는데 여기서는 3개로 했네\n",
    "    training_cycle = 2400, # 그 초기 몇개까지만 cluster update할지\n",
    "\n",
    "\n",
    "    batch_size = 32,\n",
    "    max_epoch = 7000,\n",
    "    learning_rate = 0.001,\n",
    "    normalize_on = False, # True or False #이거 안 씀 # 이거 별로 안 좋은 normalize같음 # 쓸 거면 다른 거 써라.\n",
    "    need_bias = False,\n",
    "    # first_layer_no_train = False\n",
    "    lif_add_at_first = False,\n",
    "    my_seed = 42,\n",
    "\n",
    "    TIME = 10, # SAE일 때만 유효\n",
    "    v_decay = 0.5,\n",
    "    v_threshold = 0.5,\n",
    "    v_reset = 10000.0, # 10000이상 일 시 hard reset\n",
    "    BPTT_on = True,\n",
    "\n",
    "    SAE_hidden_nomean = True,\n",
    "    current_time = '20250101_210938_786',\n",
    "\n",
    "    optimizer = 'Adam',\n",
    "    coarse_com_mode = True,\n",
    "    coarse_com_config = (2.0, -2.0), # (max, min)\n",
    "\n",
    "    sae_l2_norm_bridge = True,\n",
    "    sae_lif_bridge = False,\n",
    "\n",
    "    accuracy_check_epoch_term = 5,\n",
    "    ):\n",
    "    seed_assign(my_seed)\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print(hyperparameters)\n",
    "    # JSON으로 저장\n",
    "    with open(f\"result_save/cluster_accuracy_history_{current_time}.json\", 'w') as f:\n",
    "        json.dump(hyperparameters, f, indent=4)\n",
    "    ######################################################################################\n",
    "\n",
    "    \n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{current_time}_SAE_net_{SAE_net}_v_threshold_{v_threshold}'\n",
    "    wandb.define_metric(\"best_mean_cluster_accuracy_post_training_cycle_all_dataset2\", summary=\"max\")\n",
    "\n",
    "\n",
    "    my_path_ground_BH = '/data2/spike_sorting/quiroga/BH/'\n",
    "\n",
    "\n",
    "    filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "                \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "                \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "                \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "\n",
    "    spike_tot = [\"BH_Spike_e1n005.npy\", \"BH_Spike_e1n010.npy\", \"BH_Spike_e1n015.npy\", \"BH_Spike_e1n020.npy\",\n",
    "                \"BH_Spike_e2n005.npy\", \"BH_Spike_e2n010.npy\", \"BH_Spike_e2n015.npy\", \"BH_Spike_e2n020.npy\",\n",
    "                \"BH_Spike_d1n005.npy\", \"BH_Spike_d1n010.npy\", \"BH_Spike_d1n015.npy\", \"BH_Spike_d1n020.npy\",\n",
    "                \"BH_Spike_d2n005.npy\", \"BH_Spike_d2n010.npy\", \"BH_Spike_d2n015.npy\", \"BH_Spike_d2n020.npy\"]\n",
    "\n",
    "    label_tot = [\"BH_Label_e1n005.npy\", \"BH_Label_e1n010.npy\", \"BH_Label_e1n015.npy\", \"BH_Label_e1n020.npy\",\n",
    "                \"BH_Label_e2n005.npy\", \"BH_Label_e2n010.npy\", \"BH_Label_e2n015.npy\", \"BH_Label_e2n020.npy\",\n",
    "                \"BH_Label_d1n005.npy\", \"BH_Label_d1n010.npy\", \"BH_Label_d1n015.npy\", \"BH_Label_d1n020.npy\",\n",
    "                \"BH_Label_d2n005.npy\", \"BH_Label_d2n010.npy\", \"BH_Label_d2n015.npy\", \"BH_Label_d2n020.npy\"]\n",
    "\n",
    "    template =  [\"BH_Spike_TEMPLATE_e1n005.npy\", \"BH_Spike_TEMPLATE_e1n010.npy\", \"BH_Spike_TEMPLATE_e1n015.npy\", \"BH_Spike_TEMPLATE_e1n020.npy\",\n",
    "                \"BH_Spike_TEMPLATE_e2n005.npy\", \"BH_Spike_TEMPLATE_e2n010.npy\", \"BH_Spike_TEMPLATE_e2n015.npy\", \"BH_Spike_TEMPLATE_e2n020.npy\",\n",
    "                \"BH_Spike_TEMPLATE_d1n005.npy\", \"BH_Spike_TEMPLATE_d1n010.npy\", \"BH_Spike_TEMPLATE_d1n015.npy\", \"BH_Spike_TEMPLATE_d1n020.npy\",\n",
    "                \"BH_Spike_TEMPLATE_d2n005.npy\", \"BH_Spike_TEMPLATE_d2n010.npy\", \"BH_Spike_TEMPLATE_d2n015.npy\", \"BH_Spike_TEMPLATE_d2n020.npy\"]\n",
    "\n",
    "    AE_train_path_gt_detect = 'BH_quiroga_training_dataset_gt_detect.pt' \n",
    "    AE_test_path_gt_detect = 'BH_quiroga_test_dataset_gt_detect.pt'\n",
    "\n",
    "    AE_train_path_real_detect = 'BH_quiroga_training_dataset_real_detect.pt'\n",
    "    AE_test_path_real_detect = 'BH_quiroga_test_dataset_real_detect.pt'\n",
    "\n",
    "    AE_train_data = AE_train_path_real_detect #AE_train_path_gt_detect #AE_train_path_real_detect\n",
    "    AE_test_data = AE_test_path_real_detect #AE_test_path_gt_detect  #AE_test_path_real_detect\n",
    "\n",
    "    # thr_tot = np.array([0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7])\n",
    "    cos_thr = np.array([0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.85, 0.95, 0.9, 0.8, 0.95, 0.95, 0.95, 0.95, 0.8])\n",
    "\n",
    "\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= f'{gpu}'\n",
    "\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "    if coarse_com_mode == True:\n",
    "        level_num = TIME\n",
    "        TIME = spike_length\n",
    "        spike_length = level_num\n",
    "        level_interval = (coarse_com_config[0] - coarse_com_config[1]) / (level_num-1)  # max - min\n",
    "        levels = [coarse_com_config[1] + level_interval * i for i in range(level_num)]\n",
    "        levels = torch.tensor(levels).to(torch.float).to(device)\n",
    "        levels = levels.repeat(TIME,1) \n",
    "        # print('levels', levels, levels.shape) # TIME, level_num\n",
    "\n",
    "    n_sample = spike_length\n",
    "\n",
    "    class spikedataset(Dataset):\n",
    "        def __init__(self, path, transform = None):    \n",
    "            self.transform = transform\n",
    "            self.spike = torch.load(path)\n",
    "            \n",
    "        def __getitem__(self, index):\n",
    "            spike = self.spike[index]            \n",
    "            if self.transform is not None:\n",
    "                spike = self.transform(spike)\n",
    "            return spike\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.spike)\n",
    "\n",
    "    train_dataset = spikedataset(my_path_ground_BH + AE_train_data)\n",
    "    train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    test_dataset = spikedataset(my_path_ground_BH + AE_test_data)\n",
    "    test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "\n",
    "\n",
    "    # 모델 초기화\n",
    "    if SAE_net == False:\n",
    "        if Conv_net == True:\n",
    "            net = Autoencoder_conv1(input_channels=1, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = 4, padding = 0, stride = 2, kernel_size = 3, need_bias=need_bias)\n",
    "            net = torch.nn.DataParallel(net)\n",
    "        else:\n",
    "            net = Autoencoder_only_FC(encoder_ch=[96, 64, 32, 4], decoder_ch=[32,64,96,n_sample], n_sample=n_sample, need_bias=need_bias)\n",
    "            net = torch.nn.DataParallel(net)\n",
    "    else:\n",
    "        if Conv_net == True: \n",
    "            net = SAE_conv1(input_channels=1, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = 4, padding = 0, stride = 2, kernel_size = 3, \n",
    "                                synapse_fc_trace_const1=1, \n",
    "                                synapse_fc_trace_const2=v_decay, #안씀 \n",
    "                                TIME=TIME, v_init=0.0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, \n",
    "                                sg_width=4.0, surrogate='sigmoid', BPTT_on=BPTT_on, need_bias=need_bias, lif_add_at_first=lif_add_at_first,\n",
    "                                sae_l2_norm_bridge = sae_l2_norm_bridge, sae_lif_bridge = sae_lif_bridge)\n",
    "            net = torch.nn.DataParallel(net)\n",
    "        else:\n",
    "            net = SAE_fc_only(encoder_ch=[96, 64, 32, 4], \n",
    "                                decoder_ch=[32,64,96,n_sample], \n",
    "                                in_channels=n_sample, # in_channel 이 여기선 걍 lenght.\n",
    "                                synapse_fc_trace_const1=1,\n",
    "                                synapse_fc_trace_const2=v_decay,  #안씀 \n",
    "                                TIME=TIME, v_init=0.0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, \n",
    "                                sg_width=4.0, surrogate='sigmoid', BPTT_on=BPTT_on, need_bias=need_bias, lif_add_at_first=lif_add_at_first,\n",
    "                                sae_l2_norm_bridge = sae_l2_norm_bridge, sae_lif_bridge = sae_lif_bridge)\n",
    "            net = torch.nn.DataParallel(net)\n",
    "\n",
    "    # net = torch.load('/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/net_save/save_now_net_AE_re_e7000.pth')\n",
    "    # net = torch.load('/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/net_save/save_now_net_20250101_210938_786.pth')\n",
    "    # load했으면 torch.nn.DataParallel 하지마\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    wandb.watch(net, log=\"all\", log_freq = 10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if SAE_net == True:\n",
    "        assert 'SAE' in net.module.__class__.__name__\n",
    "\n",
    "\n",
    "\n",
    "    net = net.to(device)\n",
    "    print(net)\n",
    "    print('Device:',device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    elif optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(net.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        assert False, 'optimizer를 잘못 입력했습니다.'\n",
    "        \n",
    "    loss_history = []\n",
    "    mean_cluster_accuracy_during_training_cycle_all_dataset_history = []\n",
    "    mean_cluster_accuracy_post_training_cycle_all_dataset_history = []\n",
    "    mean_cluster_accuracy_total_all_dataset_history = []\n",
    "\n",
    "    tau = np.zeros(num_cluster)\n",
    "\n",
    "    print(f\"\\nStart Training, current_time = {current_time}\")\n",
    "    mean_cluster_accuracy_post_training_cycle_all_dataset = 0\n",
    "    best_mean_cluster_accuracy_post_training_cycle_all_dataset = 0\n",
    "\n",
    "    if SAE_net == True:\n",
    "        assert 'SAE' in net.module.__class__.__name__\n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "\n",
    "        ae_train_start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        iter = 0\n",
    "        net.train()\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            spike = data\n",
    "            spike = spike.to(device) # batch, time\n",
    "            if coarse_com_mode == True and 'SAE' in net.module.__class__.__name__:\n",
    "                spike = spike.unsqueeze(2).repeat(1, 1, level_num) # spike_length == level_num # (batch, time, feature)로 변환 \n",
    "                spike = (spike > levels).to(torch.float) \n",
    "                # spike: batch, time, level_num\n",
    "                # levels: time, level_num\n",
    "                assert spike.shape[0] == batch_size and spike.shape[1] == TIME and spike.shape[2] == spike_length\n",
    "            elif 'SAE' in net.module.__class__.__name__:\n",
    "                spike = spike.unsqueeze(-1).repeat(1, 1, TIME).permute(0,2,1) # (batch, time, feature)로 변환\n",
    "            \n",
    "            spike_class = net(spike)\n",
    "\n",
    "            # if 'SAE' in net.module.__class__.__name__:\n",
    "            #     spike = spike.mean(dim=1)# Time 방향으로 평균\n",
    "            #     spike_class = spike_class.mean(dim=1)# Time 방향으로 평균\n",
    "\n",
    "            if 'SAE' in net.module.__class__.__name__:\n",
    "                loss1 = criterion(spike_class[:, :, 5:25], spike[:, :, 5:25])\n",
    "                loss2 = criterion(spike_class[:, :, 0:5], spike[:, :, 0:5])\n",
    "                loss3 = criterion(spike_class[:, :, 25:spike_length], spike[:, :, 25:spike_length])\n",
    "            else:\n",
    "                loss1 = criterion(spike_class[:, 5:25], spike[:, 5:25])\n",
    "                loss2 = criterion(spike_class[:, 0:5], spike[:, 0:5])\n",
    "                loss3 = criterion(spike_class[:, 25:spike_length], spike[:, 25:spike_length])\n",
    "\n",
    "            loss = loss1 * 2.125 + (loss2 + loss3)/4\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            # print(f'\\nepoch-{epoch}, running_loss : {running_loss:.5f}, iter percent {iter/len(train_loader)*100:.2f}%')\n",
    "            iter += 1\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        loss_history.append((epoch, avg_loss))\n",
    "        print(f'\\nepoch-{epoch} loss : {avg_loss:.5f}')\n",
    "        print(f\"ae train 실행 시간: {time.time()-ae_train_start_time:.3f}초\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        cluster_accuracy_during_training_cycle_all_dataset = np.zeros(dataset_num)\n",
    "        cluster_accuracy_post_training_cycle_all_dataset = np.zeros(dataset_num)\n",
    "        cluster_accuracy_total_all_dataset = np.zeros(dataset_num)    \n",
    "\n",
    "        if(epoch % accuracy_check_epoch_term == 0 or epoch == 1 or epoch == max_epoch-1): \n",
    "            accuracy_check_start_time = time.time()\n",
    "            print(f'\\nepoch-{epoch} accuracy check')\n",
    "            for ds in range(dataset_num):\n",
    "                # print('\\n', spike_tot[ds])\n",
    "\n",
    "                spike_template = np.load(my_path_ground_BH + template[ds])\n",
    "                spike = np.load(my_path_ground_BH + spike_tot[ds])\n",
    "                label = np.load(my_path_ground_BH + label_tot[ds])\n",
    "                \n",
    "                hidden_size = 4*TIME if 'SAE' in net.module.__class__.__name__ and SAE_hidden_nomean == True else 4\n",
    "\n",
    "                Cluster = np.zeros((num_cluster, hidden_size))\n",
    "                assert Cluster.shape[-1] == hidden_size, '이거 hidden dim 4 아니게 할 거면 잘 바꿔라'\n",
    "                \n",
    "                net.eval()\n",
    "                with torch.no_grad():\n",
    "                    spike_torch = torch.from_numpy(spike_template)\n",
    "                    spike_torch = spike_torch.float().to(device)\n",
    "                    if coarse_com_mode == True and 'SAE' in net.module.__class__.__name__:\n",
    "                        spike_torch = spike_torch.unsqueeze(2).repeat(1, 1, level_num) # spike_length == level_num # (batch, time, feature)로 변환 \n",
    "                    elif 'SAE' in net.module.__class__.__name__:\n",
    "                        spike_torch = spike_torch.unsqueeze(1).repeat(1, TIME, 1) # (batch, time, feature)로 변환\n",
    "                    inner_inf = net.module.encoder(spike_torch)\n",
    "                    # if 'SAE' in net.module.__class__.__name__:\n",
    "                    #     tensors = [inner_inf[0][i] for i in range(TIME)] \n",
    "                    #     all_equal = all(torch.equal(tensors[0], t) for t in tensors)\n",
    "                    #     print(all_equal, inner_inf)\n",
    "\n",
    "                    if coarse_com_mode == True and 'SAE' in net.module.__class__.__name__:\n",
    "                        if SAE_hidden_nomean == True:\n",
    "                            inner_inf = inner_inf.reshape(spike_template.shape[0],-1)# time*feature 펼치기\n",
    "                        else:\n",
    "                            inner_inf = inner_inf.mean(dim=1)# Time 방향으로 평균\n",
    "                    Cluster = inner_inf.cpu().detach().numpy()\n",
    "\n",
    "                encoder_batch = 128\n",
    "                spike_hidden = np.zeros((len(spike), hidden_size))\n",
    "                net.eval()\n",
    "                with torch.no_grad():\n",
    "                    now_index = 0\n",
    "                    while (1):\n",
    "                        now_end_index = now_index+encoder_batch if now_index+encoder_batch < len(spike) else len(spike)\n",
    "                        spike_batch = spike[now_index:now_end_index] \n",
    "                        spike_torch = torch.from_numpy(spike_batch)\n",
    "                        spike_torch = spike_torch.float().to(device)\n",
    "                        if coarse_com_mode == True:\n",
    "                            spike_torch = spike_torch.unsqueeze(2).repeat(1, 1, level_num) # spike_length == level_num # (batch, time, feature)로 변환 \n",
    "                        elif 'SAE' in net.module.__class__.__name__:\n",
    "                            spike_torch = spike_torch.unsqueeze(1).repeat(1, TIME, 1) # (batch, time, feature)로 변환\n",
    "                        inner_inf = net.module.encoder(spike_torch)\n",
    "                        if 'SAE' in net.module.__class__.__name__:\n",
    "                            if SAE_hidden_nomean == True:\n",
    "                                inner_inf = inner_inf.reshape(spike_batch.shape[0],-1)# 펼치기\n",
    "                            else:\n",
    "                                inner_inf = inner_inf.mean(dim=1)# Time 방향으로 평균\n",
    "                        spike_hidden[now_index:now_end_index] = inner_inf.cpu().detach().numpy()\n",
    "                        now_index += encoder_batch\n",
    "                        if (now_index >= len(spike)):\n",
    "                            break\n",
    "                    \n",
    "                spike_id = np.zeros(len(spike))\n",
    "                distance_sm = np.zeros(num_cluster)\n",
    "                tau = np.zeros(num_cluster)\n",
    "                \n",
    "                for spike_index in range(len(spike)): \n",
    "                    for q in range(num_cluster):\n",
    "                        tau[q] = np.dot(spike_hidden[spike_index, :], Cluster[q, :]) # 이거 l2norm 거쳐서 나온 거니까 분모 1임.\n",
    "                        if 'SAE' in net.module.__class__.__name__: # AE 때는 l2norm거쳐서 나와서 괜찮음\n",
    "                            denominator =  np.linalg.norm(spike_hidden[spike_index, :])*np.linalg.norm(Cluster[q, :]) + 1e-12\n",
    "                            tau[q] = tau[q] / denominator\n",
    "\n",
    "                    # for i in range(num_cluster): # l2 distance\n",
    "                    #     distance_sm[i] = np.sum(np.power(np.abs(Cluster[i] - spike_hidden[spike_index, :]), 2))\n",
    "                    distance_sm = np.sum(np.power(np.abs(Cluster - spike_hidden[spike_index, :]), 2), axis=1)\n",
    "\n",
    "                    m = np.argmin(distance_sm)\n",
    "                    spike_id[spike_index] = m + 1\n",
    "                    # print(spike_tot[ds], spike_index,np.max(tau))\n",
    "                    if(np.max(tau) >= cos_thr[ds] and spike_index < training_cycle): # 원래 1400 아니냐?\n",
    "                        Cluster[m] = (Cluster[m] * 15 + spike_hidden[spike_index, :])/16\n",
    "                \n",
    "                # print('Cluster',Cluster)\n",
    "                # print('spike_id', spike_id)\n",
    "\n",
    "                # spike id 분포 확인하기\n",
    "                # unique_elements, counts = np.unique(spike_id, return_counts=True)\n",
    "                # print(\"Unique elements:\", unique_elements)\n",
    "                # print(\"Counts:\", counts)\n",
    "\n",
    "                cluster_accuracy_during_training_cycle = np.zeros(math.factorial(num_cluster))\n",
    "                cluster_accuracy_post_training_cycle = np.zeros(math.factorial(num_cluster))\n",
    "                cluster_accuracy_total = np.zeros(math.factorial(num_cluster))\n",
    "                \n",
    "                label_converter_ground = list(range(1, num_cluster + 1)) # [1, 2, 3, 4] 생성\n",
    "                label_converter_permutations = list(itertools.permutations(label_converter_ground)) # 모든 순열 구하기\n",
    "                perm_i = 0\n",
    "                perm_start_time = time.time()\n",
    "                for perm in label_converter_permutations:\n",
    "                    label_converter = list(perm)\n",
    "                    # print(label_converter)\n",
    "                    correct_during_training_cycle = 0\n",
    "                    correct_post_training_cycle = 0\n",
    "\n",
    "                    assert len(spike_id) == len(label), 'spike_id랑 label 길이 같아야 됨.'\n",
    "                    for i in range(len(spike_id)):\n",
    "                        if(label_converter[int(spike_id[i]-1)] == label[i]):\n",
    "                            if i < training_cycle:\n",
    "                                correct_during_training_cycle += 1\n",
    "                            else:\n",
    "                                correct_post_training_cycle += 1\n",
    "\n",
    "                    cluster_accuracy_during_training_cycle[perm_i] = correct_during_training_cycle/training_cycle\n",
    "                    cluster_accuracy_post_training_cycle[perm_i] = correct_post_training_cycle/(len(spike_id)-training_cycle)\n",
    "                    cluster_accuracy_total[perm_i] = (correct_during_training_cycle+correct_post_training_cycle)/(len(spike_id))\n",
    "                    perm_i += 1\n",
    "                # print(f\"perm 실행 시간: {time.time()-perm_start_time:.3f}초\")\n",
    "                \n",
    "                cluster_accuracy_during_training_cycle_all_dataset[ds] = np.max(cluster_accuracy_during_training_cycle)\n",
    "                cluster_accuracy_post_training_cycle_all_dataset[ds] = cluster_accuracy_post_training_cycle[np.argmax(cluster_accuracy_during_training_cycle)]\n",
    "                cluster_accuracy_total_all_dataset[ds] = cluster_accuracy_total[np.argmax(cluster_accuracy_during_training_cycle)]\n",
    "\n",
    "            print('cluster_accuracy_post_training_cycle_all_dataset', cluster_accuracy_post_training_cycle_all_dataset)\n",
    "\n",
    "            mean_cluster_accuracy_during_training_cycle_all_dataset = np.mean(cluster_accuracy_during_training_cycle_all_dataset)\n",
    "            mean_cluster_accuracy_post_training_cycle_all_dataset = np.mean(cluster_accuracy_post_training_cycle_all_dataset)\n",
    "            mean_cluster_accuracy_total_all_dataset = np.mean(cluster_accuracy_total_all_dataset)\n",
    "            \n",
    "            mean_cluster_accuracy_during_training_cycle_all_dataset_history.append((epoch, mean_cluster_accuracy_during_training_cycle_all_dataset*100))\n",
    "            mean_cluster_accuracy_post_training_cycle_all_dataset_history.append((epoch, mean_cluster_accuracy_post_training_cycle_all_dataset*100))\n",
    "            mean_cluster_accuracy_total_all_dataset_history.append((epoch, mean_cluster_accuracy_total_all_dataset*100))\n",
    "            print(f\"mean_cluster_accuracy_during_training_cycle : {mean_cluster_accuracy_during_training_cycle_all_dataset*100:.2f}%, post_traincycle_acc : {mean_cluster_accuracy_post_training_cycle_all_dataset*100:.2f}%, total_acc : {mean_cluster_accuracy_total_all_dataset*100:.2f}%\")\n",
    "\n",
    "            if mean_cluster_accuracy_post_training_cycle_all_dataset > best_mean_cluster_accuracy_post_training_cycle_all_dataset:\n",
    "                # torch.save(net, f\"net_save/save_now_net_{current_time}.pth\")\n",
    "                # print('save model')\n",
    "                best_mean_cluster_accuracy_post_training_cycle_all_dataset = mean_cluster_accuracy_post_training_cycle_all_dataset\n",
    "            print(f\"best_mean_cluster_accuracy_post_training_cycle_all_dataset : {best_mean_cluster_accuracy_post_training_cycle_all_dataset*100:.2f}%\")\n",
    "            print(f\"accuracy_check 실행 시간: {time.time()-accuracy_check_start_time:.3f}초\")\n",
    "\n",
    "        wandb.log({\"avg_loss\": avg_loss})\n",
    "        wandb.log({\"mean_cluster_accuracy_post_training_cycle_all_dataset\": mean_cluster_accuracy_post_training_cycle_all_dataset})\n",
    "        wandb.log({\"best_mean_cluster_accuracy_post_training_cycle_all_dataset\": best_mean_cluster_accuracy_post_training_cycle_all_dataset})\n",
    "        wandb.log({\"best_mean_cluster_accuracy_post_training_cycle_all_dataset2\": best_mean_cluster_accuracy_post_training_cycle_all_dataset})\n",
    "\n",
    "\n",
    "        # 저장\n",
    "        with open(f\"result_save/cluster_accuracy_history_{current_time}.pkl\", \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"loss_history\": loss_history,\n",
    "                \"mean_cluster_accuracy_during_training_cycle_all_dataset_history\": mean_cluster_accuracy_during_training_cycle_all_dataset_history,\n",
    "                \"mean_cluster_accuracy_post_training_cycle_all_dataset_history\": mean_cluster_accuracy_post_training_cycle_all_dataset_history,\n",
    "                \"mean_cluster_accuracy_total_all_dataset_history\": mean_cluster_accuracy_total_all_dataset_history,\n",
    "            }, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20250103_193812-itf1svel</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/spike_sorting%20just%20run/runs/itf1svel' target=\"_blank\">feasible-blaze-164</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/spike_sorting%20just%20run' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/spike_sorting%20just%20run' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/spike_sorting%20just%20run</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/spike_sorting%20just%20run/runs/itf1svel' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/spike_sorting%20just%20run/runs/itf1svel</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpu': 2, 'Conv_net': True, 'SAE_net': True, 'dataset_num': 16, 'spike_length': 50, 'num_cluster': 4, 'training_cycle': 1400, 'batch_size': 32, 'max_epoch': 7000, 'learning_rate': 0.001, 'normalize_on': False, 'need_bias': False, 'lif_add_at_first': False, 'my_seed': 42, 'TIME': 50, 'v_decay': 0.5, 'v_threshold': 0.5, 'v_reset': 0.0, 'BPTT_on': True, 'SAE_hidden_nomean': False, 'current_time': '20250103_193810_311', 'optimizer': 'Adam', 'coarse_com_mode': True, 'sae_l2_norm_bridge': False, 'sae_lif_bridge': True, 'accuracy_check_epoch_term': 50, 'coarse_com_config': (3.0, -3.0)}\n",
      "conv length [5, 11, 24, 50]\n",
      "DataParallel(\n",
      "  (module): SAE_conv1(\n",
      "    (encoder): Sequential(\n",
      "      (0): SSBH_DimChanger_one_two()\n",
      "      (1): SSBH_DimChanger_for_unsuqeeze()\n",
      "      (2): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (3): Conv1d(1, 32, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      (4): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (5): LIF_layer()\n",
      "      (6): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (7): Conv1d(32, 64, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      (8): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (9): LIF_layer()\n",
      "      (10): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (11): Conv1d(64, 96, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      (12): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (13): LIF_layer()\n",
      "      (14): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (15): SSBH_DimChanger_for_fc()\n",
      "      (16): Linear(in_features=480, out_features=4, bias=False)\n",
      "      (17): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (18): LIF_layer()\n",
      "      (19): SSBH_DimChanger_one_two()\n",
      "    )\n",
      "    (decoder): Sequential(\n",
      "      (0): SSBH_DimChanger_one_two()\n",
      "      (1): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (2): Linear(in_features=4, out_features=480, bias=False)\n",
      "      (3): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (4): LIF_layer()\n",
      "      (5): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (6): SSBH_DimChanger_for_conv1()\n",
      "      (7): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (8): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (9): ConvTranspose1d(96, 64, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      (10): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (11): LIF_layer()\n",
      "      (12): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (13): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(2,), output_padding=(1,), bias=False)\n",
      "      (14): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (15): LIF_layer()\n",
      "      (16): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (17): ConvTranspose1d(32, 1, kernel_size=(3,), stride=(2,), output_padding=(1,), bias=False)\n",
      "      (18): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (19): SSBH_DimChanger_for_suqeeze()\n",
      "      (20): SSBH_DimChanger_one_two()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Device: cuda\n",
      "\n",
      "Start Training, current_time = 20250103_193810_311\n",
      "\n",
      "epoch-0 loss : 0.19649\n",
      "ae train 실행 시간: 191.305초\n",
      "\n",
      "epoch-0 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.65704825 0.67247879 0.63071738 0.54532305 0.60845771 0.52264151\n",
      " 0.49278966 0.41815616 0.397882   0.47265625 0.49324324 0.47368421\n",
      " 0.42057026 0.45635306 0.41519608 0.3248925 ]\n",
      "mean_cluster_accuracy_during_training_cycle : 51.27%, post_traincycle_acc : 50.01%, total_acc : 50.52%\n",
      "best_mean_cluster_accuracy_post_training_cycle_all_dataset : 50.01%\n",
      "accuracy_check 실행 시간: 17.384초\n",
      "\n",
      "epoch-1 loss : 0.02357\n",
      "ae train 실행 시간: 194.239초\n",
      "\n",
      "epoch-1 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.79990539 0.7728558  0.61386615 0.52025072 0.55870647 0.42264151\n",
      " 0.45599204 0.39651929 0.5910237  0.46630859 0.40444015 0.35700099\n",
      " 0.51374745 0.24636275 0.29019608 0.35929288]\n",
      "mean_cluster_accuracy_during_training_cycle : 47.41%, post_traincycle_acc : 48.56%, total_acc : 48.09%\n",
      "best_mean_cluster_accuracy_post_training_cycle_all_dataset : 50.01%\n",
      "accuracy_check 실행 시간: 17.397초\n",
      "\n",
      "epoch-2 loss : 0.01154\n",
      "ae train 실행 시간: 196.287초\n",
      "\n",
      "epoch-3 loss : 0.00861\n",
      "ae train 실행 시간: 195.013초\n",
      "\n",
      "epoch-4 loss : 0.00796\n",
      "ae train 실행 시간: 192.579초\n",
      "\n",
      "epoch-5 loss : 0.00728\n",
      "ae train 실행 시간: 188.579초\n",
      "\n",
      "epoch-6 loss : 0.00789\n",
      "ae train 실행 시간: 195.282초\n",
      "\n",
      "epoch-7 loss : 0.00751\n",
      "ae train 실행 시간: 194.787초\n",
      "\n",
      "epoch-8 loss : 0.00758\n",
      "ae train 실행 시간: 194.488초\n",
      "\n",
      "epoch-9 loss : 0.00990\n",
      "ae train 실행 시간: 191.080초\n",
      "\n",
      "epoch-10 loss : 0.01487\n",
      "ae train 실행 시간: 190.030초\n",
      "\n",
      "epoch-11 loss : 0.00957\n",
      "ae train 실행 시간: 191.710초\n",
      "\n",
      "epoch-12 loss : 0.00962\n",
      "ae train 실행 시간: 192.033초\n",
      "\n",
      "epoch-13 loss : 0.01116\n",
      "ae train 실행 시간: 192.037초\n",
      "\n",
      "epoch-14 loss : 0.01529\n",
      "ae train 실행 시간: 195.602초\n",
      "\n",
      "epoch-15 loss : 0.01367\n",
      "ae train 실행 시간: 193.393초\n",
      "\n",
      "epoch-16 loss : 0.00794\n",
      "ae train 실행 시간: 195.941초\n",
      "\n",
      "epoch-17 loss : 0.00725\n",
      "ae train 실행 시간: 197.594초\n",
      "\n",
      "epoch-18 loss : 0.00812\n",
      "ae train 실행 시간: 194.238초\n",
      "\n",
      "epoch-19 loss : 0.00703\n",
      "ae train 실행 시간: 192.121초\n",
      "\n",
      "epoch-20 loss : 0.00786\n",
      "ae train 실행 시간: 194.007초\n",
      "\n",
      "epoch-21 loss : 0.00668\n",
      "ae train 실행 시간: 195.222초\n",
      "\n",
      "epoch-22 loss : 0.00837\n",
      "ae train 실행 시간: 196.096초\n",
      "\n",
      "epoch-23 loss : 0.00747\n",
      "ae train 실행 시간: 196.810초\n",
      "\n",
      "epoch-24 loss : 0.00645\n",
      "ae train 실행 시간: 194.671초\n",
      "\n",
      "epoch-25 loss : 0.00669\n",
      "ae train 실행 시간: 191.018초\n",
      "\n",
      "epoch-26 loss : 0.00713\n",
      "ae train 실행 시간: 192.360초\n",
      "\n",
      "epoch-27 loss : 0.00628\n",
      "ae train 실행 시간: 191.219초\n",
      "\n",
      "epoch-28 loss : 0.00746\n",
      "ae train 실행 시간: 196.719초\n",
      "\n",
      "epoch-29 loss : 0.00538\n",
      "ae train 실행 시간: 187.716초\n",
      "\n",
      "epoch-30 loss : 0.00492\n",
      "ae train 실행 시간: 193.099초\n",
      "\n",
      "epoch-31 loss : 0.00464\n",
      "ae train 실행 시간: 189.657초\n",
      "\n",
      "epoch-32 loss : 0.00495\n",
      "ae train 실행 시간: 193.448초\n",
      "\n",
      "epoch-33 loss : 0.00504\n",
      "ae train 실행 시간: 193.397초\n",
      "\n",
      "epoch-34 loss : 0.00500\n",
      "ae train 실행 시간: 191.242초\n",
      "\n",
      "epoch-35 loss : 0.00511\n",
      "ae train 실행 시간: 195.331초\n",
      "\n",
      "epoch-36 loss : 0.00594\n",
      "ae train 실행 시간: 194.047초\n",
      "\n",
      "epoch-37 loss : 0.00644\n",
      "ae train 실행 시간: 193.688초\n",
      "\n",
      "epoch-38 loss : 0.00568\n",
      "ae train 실행 시간: 190.404초\n",
      "\n",
      "epoch-39 loss : 0.00549\n",
      "ae train 실행 시간: 191.681초\n",
      "\n",
      "epoch-40 loss : 0.00539\n",
      "ae train 실행 시간: 190.114초\n",
      "\n",
      "epoch-41 loss : 0.00553\n",
      "ae train 실행 시간: 192.633초\n",
      "\n",
      "epoch-42 loss : 0.00613\n",
      "ae train 실행 시간: 193.524초\n",
      "\n",
      "epoch-43 loss : 0.00622\n",
      "ae train 실행 시간: 193.195초\n",
      "\n",
      "epoch-44 loss : 0.00750\n",
      "ae train 실행 시간: 191.974초\n",
      "\n",
      "epoch-45 loss : 0.00646\n",
      "ae train 실행 시간: 191.494초\n",
      "\n",
      "epoch-46 loss : 0.00658\n",
      "ae train 실행 시간: 189.180초\n",
      "\n",
      "epoch-47 loss : 0.00671\n",
      "ae train 실행 시간: 189.275초\n",
      "\n",
      "epoch-48 loss : 0.00673\n",
      "ae train 실행 시간: 192.969초\n",
      "\n",
      "epoch-49 loss : 0.00682\n",
      "ae train 실행 시간: 191.541초\n",
      "\n",
      "epoch-50 loss : 0.00799\n",
      "ae train 실행 시간: 192.280초\n",
      "\n",
      "epoch-50 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.77578051 0.67483506 0.48387097 0.56027001 0.57512438 0.4490566\n",
      " 0.45151666 0.38240828 0.64346949 0.47705078 0.45415058 0.43694141\n",
      " 0.42260692 0.43307468 0.37254902 0.40086001]\n",
      "mean_cluster_accuracy_during_training_cycle : 50.82%, post_traincycle_acc : 49.96%, total_acc : 50.31%\n",
      "best_mean_cluster_accuracy_post_training_cycle_all_dataset : 50.01%\n",
      "accuracy_check 실행 시간: 19.711초\n",
      "\n",
      "epoch-51 loss : 0.00709\n",
      "ae train 실행 시간: 189.919초\n",
      "\n",
      "epoch-52 loss : 0.00650\n",
      "ae train 실행 시간: 188.601초\n",
      "\n",
      "epoch-53 loss : 0.00631\n",
      "ae train 실행 시간: 193.075초\n",
      "\n",
      "epoch-54 loss : 0.00677\n",
      "ae train 실행 시간: 190.765초\n",
      "\n",
      "epoch-55 loss : 0.00638\n",
      "ae train 실행 시간: 193.119초\n",
      "\n",
      "epoch-56 loss : 0.00635\n",
      "ae train 실행 시간: 193.996초\n",
      "\n",
      "epoch-57 loss : 0.00715\n",
      "ae train 실행 시간: 192.365초\n",
      "\n",
      "epoch-58 loss : 0.00734\n",
      "ae train 실행 시간: 194.051초\n",
      "\n",
      "epoch-59 loss : 0.00656\n",
      "ae train 실행 시간: 192.925초\n",
      "\n",
      "epoch-60 loss : 0.00623\n",
      "ae train 실행 시간: 196.308초\n",
      "\n",
      "epoch-61 loss : 0.00722\n",
      "ae train 실행 시간: 193.831초\n",
      "\n",
      "epoch-62 loss : 0.00671\n",
      "ae train 실행 시간: 193.226초\n",
      "\n",
      "epoch-63 loss : 0.00580\n",
      "ae train 실행 시간: 191.723초\n",
      "\n",
      "epoch-64 loss : 0.00565\n",
      "ae train 실행 시간: 190.269초\n",
      "\n",
      "epoch-65 loss : 0.00627\n",
      "ae train 실행 시간: 191.372초\n",
      "\n",
      "epoch-66 loss : 0.00683\n",
      "ae train 실행 시간: 191.443초\n",
      "\n",
      "epoch-67 loss : 0.00524\n",
      "ae train 실행 시간: 190.016초\n",
      "\n",
      "epoch-68 loss : 0.00501\n",
      "ae train 실행 시간: 192.840초\n",
      "\n",
      "epoch-69 loss : 0.00429\n",
      "ae train 실행 시간: 193.127초\n",
      "\n",
      "epoch-70 loss : 0.00463\n",
      "ae train 실행 시간: 194.248초\n",
      "\n",
      "epoch-71 loss : 0.00460\n",
      "ae train 실행 시간: 190.862초\n",
      "\n",
      "epoch-72 loss : 0.00428\n",
      "ae train 실행 시간: 192.530초\n",
      "\n",
      "epoch-73 loss : 0.00376\n",
      "ae train 실행 시간: 191.105초\n",
      "\n",
      "epoch-74 loss : 0.00415\n",
      "ae train 실행 시간: 191.968초\n",
      "\n",
      "epoch-75 loss : 0.00461\n",
      "ae train 실행 시간: 191.299초\n",
      "\n",
      "epoch-76 loss : 0.00429\n",
      "ae train 실행 시간: 192.848초\n",
      "\n",
      "epoch-77 loss : 0.00420\n",
      "ae train 실행 시간: 191.217초\n",
      "\n",
      "epoch-78 loss : 0.00441\n",
      "ae train 실행 시간: 192.293초\n",
      "\n",
      "epoch-79 loss : 0.00502\n",
      "ae train 실행 시간: 190.270초\n",
      "\n",
      "epoch-80 loss : 0.00477\n",
      "ae train 실행 시간: 194.382초\n",
      "\n",
      "epoch-81 loss : 0.00539\n",
      "ae train 실행 시간: 192.697초\n",
      "\n",
      "epoch-82 loss : 0.00578\n",
      "ae train 실행 시간: 191.216초\n",
      "\n",
      "epoch-83 loss : 0.00603\n",
      "ae train 실행 시간: 190.173초\n",
      "\n",
      "epoch-84 loss : 0.00502\n",
      "ae train 실행 시간: 193.075초\n",
      "\n",
      "epoch-85 loss : 0.00456\n",
      "ae train 실행 시간: 194.159초\n",
      "\n",
      "epoch-86 loss : 0.00382\n",
      "ae train 실행 시간: 193.800초\n",
      "\n",
      "epoch-87 loss : 0.00375\n",
      "ae train 실행 시간: 192.138초\n",
      "\n",
      "epoch-88 loss : 0.00357\n",
      "ae train 실행 시간: 191.335초\n",
      "\n",
      "epoch-89 loss : 0.00367\n",
      "ae train 실행 시간: 190.597초\n",
      "\n",
      "epoch-90 loss : 0.00507\n",
      "ae train 실행 시간: 192.360초\n",
      "\n",
      "epoch-91 loss : 0.00463\n",
      "ae train 실행 시간: 190.959초\n",
      "\n",
      "epoch-92 loss : 0.00435\n",
      "ae train 실행 시간: 192.430초\n",
      "\n",
      "epoch-93 loss : 0.00495\n",
      "ae train 실행 시간: 191.750초\n",
      "\n",
      "epoch-94 loss : 0.00561\n",
      "ae train 실행 시간: 191.942초\n",
      "\n",
      "epoch-95 loss : 0.00577\n",
      "ae train 실행 시간: 190.485초\n",
      "\n",
      "epoch-96 loss : 0.00449\n",
      "ae train 실행 시간: 191.438초\n",
      "\n",
      "epoch-97 loss : 0.00380\n",
      "ae train 실행 시간: 191.091초\n",
      "\n",
      "epoch-98 loss : 0.00357\n",
      "ae train 실행 시간: 192.333초\n",
      "\n",
      "epoch-99 loss : 0.00344\n",
      "ae train 실행 시간: 193.959초\n",
      "\n",
      "epoch-100 loss : 0.00349\n",
      "ae train 실행 시간: 197.307초\n",
      "\n",
      "epoch-100 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.72894986 0.633836   0.50842561 0.44214079 0.59154229 0.52028302\n",
      " 0.48334162 0.43273754 0.57539082 0.43896484 0.46187259 0.4448858\n",
      " 0.64002037 0.49806014 0.40196078 0.41614907]\n",
      "mean_cluster_accuracy_during_training_cycle : 50.49%, post_traincycle_acc : 51.37%, total_acc : 51.01%\n",
      "best_mean_cluster_accuracy_post_training_cycle_all_dataset : 51.37%\n",
      "accuracy_check 실행 시간: 18.402초\n",
      "\n",
      "epoch-101 loss : 0.00339\n",
      "ae train 실행 시간: 192.136초\n",
      "\n",
      "epoch-102 loss : 0.00387\n",
      "ae train 실행 시간: 189.813초\n",
      "\n",
      "epoch-103 loss : 0.00374\n",
      "ae train 실행 시간: 192.862초\n",
      "\n",
      "epoch-104 loss : 0.00342\n",
      "ae train 실행 시간: 193.118초\n",
      "\n",
      "epoch-105 loss : 0.00347\n",
      "ae train 실행 시간: 193.498초\n",
      "\n",
      "epoch-106 loss : 0.00438\n",
      "ae train 실행 시간: 193.953초\n",
      "\n",
      "epoch-107 loss : 0.00397\n",
      "ae train 실행 시간: 193.176초\n",
      "\n",
      "epoch-108 loss : 0.00463\n",
      "ae train 실행 시간: 195.046초\n",
      "\n",
      "epoch-109 loss : 0.00452\n",
      "ae train 실행 시간: 192.323초\n",
      "\n",
      "epoch-110 loss : 0.00370\n",
      "ae train 실행 시간: 191.012초\n",
      "\n",
      "epoch-111 loss : 0.00392\n",
      "ae train 실행 시간: 192.542초\n",
      "\n",
      "epoch-112 loss : 0.00440\n",
      "ae train 실행 시간: 194.546초\n",
      "\n",
      "epoch-113 loss : 0.00387\n",
      "ae train 실행 시간: 194.237초\n",
      "\n",
      "epoch-114 loss : 0.00478\n",
      "ae train 실행 시간: 193.549초\n",
      "\n",
      "epoch-115 loss : 0.00524\n",
      "ae train 실행 시간: 189.779초\n",
      "\n",
      "epoch-116 loss : 0.00629\n",
      "ae train 실행 시간: 193.161초\n",
      "\n",
      "epoch-117 loss : 0.00662\n",
      "ae train 실행 시간: 195.078초\n",
      "\n",
      "epoch-118 loss : 0.00635\n",
      "ae train 실행 시간: 195.322초\n",
      "\n",
      "epoch-119 loss : 0.00586\n",
      "ae train 실행 시간: 192.232초\n",
      "\n",
      "epoch-120 loss : 0.00474\n",
      "ae train 실행 시간: 191.270초\n",
      "\n",
      "epoch-121 loss : 0.00444\n",
      "ae train 실행 시간: 193.385초\n",
      "\n",
      "epoch-122 loss : 0.00450\n",
      "ae train 실행 시간: 192.675초\n",
      "\n",
      "epoch-123 loss : 0.00495\n",
      "ae train 실행 시간: 193.908초\n",
      "\n",
      "epoch-124 loss : 0.00748\n",
      "ae train 실행 시간: 190.662초\n",
      "\n",
      "epoch-125 loss : 0.00469\n",
      "ae train 실행 시간: 193.635초\n",
      "\n",
      "epoch-126 loss : 0.00438\n",
      "ae train 실행 시간: 194.312초\n",
      "\n",
      "epoch-127 loss : 0.00363\n",
      "ae train 실행 시간: 197.253초\n",
      "\n",
      "epoch-128 loss : 0.00417\n",
      "ae train 실행 시간: 195.291초\n",
      "\n",
      "epoch-129 loss : 0.00433\n",
      "ae train 실행 시간: 193.143초\n",
      "\n",
      "epoch-130 loss : 0.00551\n",
      "ae train 실행 시간: 194.910초\n",
      "\n",
      "epoch-131 loss : 0.00571\n",
      "ae train 실행 시간: 191.542초\n",
      "\n",
      "epoch-132 loss : 0.00479\n",
      "ae train 실행 시간: 190.582초\n",
      "\n",
      "epoch-133 loss : 0.00522\n",
      "ae train 실행 시간: 193.575초\n",
      "\n",
      "epoch-134 loss : 0.00415\n",
      "ae train 실행 시간: 194.116초\n",
      "\n",
      "epoch-135 loss : 0.00403\n",
      "ae train 실행 시간: 195.159초\n",
      "\n",
      "epoch-136 loss : 0.00352\n",
      "ae train 실행 시간: 193.611초\n",
      "\n",
      "epoch-137 loss : 0.00358\n",
      "ae train 실행 시간: 193.532초\n",
      "\n",
      "epoch-138 loss : 0.00365\n",
      "ae train 실행 시간: 192.980초\n",
      "\n",
      "epoch-139 loss : 0.00379\n",
      "ae train 실행 시간: 194.949초\n",
      "\n",
      "epoch-140 loss : 0.00340\n",
      "ae train 실행 시간: 191.920초\n",
      "\n",
      "epoch-141 loss : 0.00399\n",
      "ae train 실행 시간: 196.767초\n",
      "\n",
      "epoch-142 loss : 0.00503\n",
      "ae train 실행 시간: 193.864초\n",
      "\n",
      "epoch-143 loss : 0.00409\n",
      "ae train 실행 시간: 190.237초\n",
      "\n",
      "epoch-144 loss : 0.00466\n",
      "ae train 실행 시간: 191.950초\n",
      "\n",
      "epoch-145 loss : 0.00433\n",
      "ae train 실행 시간: 192.243초\n",
      "\n",
      "epoch-146 loss : 0.00434\n",
      "ae train 실행 시간: 188.637초\n",
      "\n",
      "epoch-147 loss : 0.00468\n",
      "ae train 실행 시간: 193.228초\n",
      "\n",
      "epoch-148 loss : 0.00395\n",
      "ae train 실행 시간: 192.316초\n",
      "\n",
      "epoch-149 loss : 0.00448\n",
      "ae train 실행 시간: 192.250초\n",
      "\n",
      "epoch-150 loss : 0.00454\n",
      "ae train 실행 시간: 192.838초\n",
      "\n",
      "epoch-150 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.63576159 0.50518379 0.33124699 0.31581485 0.50049751 0.39669811\n",
      " 0.3983093  0.37535278 0.55320222 0.45947266 0.45415058 0.45580933\n",
      " 0.54684318 0.4442289  0.46176471 0.44720497]\n",
      "mean_cluster_accuracy_during_training_cycle : 46.73%, post_traincycle_acc : 45.51%, total_acc : 46.00%\n",
      "best_mean_cluster_accuracy_post_training_cycle_all_dataset : 51.37%\n",
      "accuracy_check 실행 시간: 19.624초\n",
      "\n",
      "epoch-151 loss : 0.00492\n",
      "ae train 실행 시간: 192.733초\n",
      "\n",
      "epoch-152 loss : 0.00487\n",
      "ae train 실행 시간: 193.431초\n",
      "\n",
      "epoch-153 loss : 0.00471\n",
      "ae train 실행 시간: 191.365초\n",
      "\n",
      "epoch-154 loss : 0.00471\n",
      "ae train 실행 시간: 192.000초\n",
      "\n",
      "epoch-155 loss : 0.00533\n",
      "ae train 실행 시간: 194.695초\n",
      "\n",
      "epoch-156 loss : 0.00546\n",
      "ae train 실행 시간: 192.891초\n",
      "\n",
      "epoch-157 loss : 0.00541\n",
      "ae train 실행 시간: 191.993초\n",
      "\n",
      "epoch-158 loss : 0.00583\n",
      "ae train 실행 시간: 194.550초\n",
      "\n",
      "epoch-159 loss : 0.00561\n",
      "ae train 실행 시간: 198.047초\n",
      "\n",
      "epoch-160 loss : 0.00573\n",
      "ae train 실행 시간: 193.780초\n",
      "\n",
      "epoch-161 loss : 0.00506\n",
      "ae train 실행 시간: 188.915초\n",
      "\n",
      "epoch-162 loss : 0.00483\n",
      "ae train 실행 시간: 193.369초\n",
      "\n",
      "epoch-163 loss : 0.00471\n",
      "ae train 실행 시간: 193.869초\n",
      "\n",
      "epoch-164 loss : 0.00548\n",
      "ae train 실행 시간: 195.282초\n",
      "\n",
      "epoch-165 loss : 0.00554\n",
      "ae train 실행 시간: 192.911초\n",
      "\n",
      "epoch-166 loss : 0.00482\n",
      "ae train 실행 시간: 193.422초\n",
      "\n",
      "epoch-167 loss : 0.00441\n",
      "ae train 실행 시간: 189.543초\n",
      "\n",
      "epoch-168 loss : 0.00431\n",
      "ae train 실행 시간: 196.175초\n",
      "\n",
      "epoch-169 loss : 0.00452\n",
      "ae train 실행 시간: 195.845초\n",
      "\n",
      "epoch-170 loss : 0.00473\n",
      "ae train 실행 시간: 190.785초\n",
      "\n",
      "epoch-171 loss : 0.00563\n",
      "ae train 실행 시간: 194.363초\n",
      "\n",
      "epoch-172 loss : 0.00420\n",
      "ae train 실행 시간: 191.271초\n",
      "\n",
      "epoch-173 loss : 0.00379\n",
      "ae train 실행 시간: 190.842초\n",
      "\n",
      "epoch-174 loss : 0.00412\n",
      "ae train 실행 시간: 192.396초\n",
      "\n",
      "epoch-175 loss : 0.00395\n",
      "ae train 실행 시간: 191.808초\n",
      "\n",
      "epoch-176 loss : 0.00386\n",
      "ae train 실행 시간: 192.305초\n",
      "\n",
      "epoch-177 loss : 0.00413\n",
      "ae train 실행 시간: 193.100초\n",
      "\n",
      "epoch-178 loss : 0.00379\n",
      "ae train 실행 시간: 196.322초\n",
      "\n",
      "epoch-179 loss : 0.00377\n",
      "ae train 실행 시간: 194.708초\n",
      "\n",
      "epoch-180 loss : 0.00408\n",
      "ae train 실행 시간: 192.335초\n",
      "\n",
      "epoch-181 loss : 0.00465\n",
      "ae train 실행 시간: 191.122초\n",
      "\n",
      "epoch-182 loss : 0.00437\n",
      "ae train 실행 시간: 189.813초\n",
      "\n",
      "epoch-183 loss : 0.00387\n",
      "ae train 실행 시간: 194.913초\n",
      "\n",
      "epoch-184 loss : 0.00394\n",
      "ae train 실행 시간: 191.394초\n",
      "\n",
      "epoch-185 loss : 0.00444\n",
      "ae train 실행 시간: 192.481초\n",
      "\n",
      "epoch-186 loss : 0.00449\n",
      "ae train 실행 시간: 195.412초\n",
      "\n",
      "epoch-187 loss : 0.00471\n",
      "ae train 실행 시간: 195.070초\n",
      "\n",
      "epoch-188 loss : 0.00512\n",
      "ae train 실행 시간: 190.642초\n",
      "\n",
      "epoch-189 loss : 0.00454\n",
      "ae train 실행 시간: 195.206초\n",
      "\n",
      "epoch-190 loss : 0.00438\n",
      "ae train 실행 시간: 192.016초\n",
      "\n",
      "epoch-191 loss : 0.00457\n",
      "ae train 실행 시간: 193.776초\n",
      "\n",
      "epoch-192 loss : 0.00376\n",
      "ae train 실행 시간: 191.549초\n",
      "\n",
      "epoch-193 loss : 0.00593\n",
      "ae train 실행 시간: 194.888초\n",
      "\n",
      "epoch-194 loss : 0.00477\n",
      "ae train 실행 시간: 192.909초\n",
      "\n",
      "epoch-195 loss : 0.00471\n",
      "ae train 실행 시간: 196.058초\n",
      "\n",
      "epoch-196 loss : 0.00438\n",
      "ae train 실행 시간: 192.674초\n",
      "\n",
      "epoch-197 loss : 0.00561\n",
      "ae train 실행 시간: 191.531초\n",
      "\n",
      "epoch-198 loss : 0.00488\n",
      "ae train 실행 시간: 192.199초\n",
      "\n",
      "epoch-199 loss : 0.00437\n",
      "ae train 실행 시간: 191.812초\n",
      "\n",
      "epoch-200 loss : 0.00392\n",
      "ae train 실행 시간: 195.906초\n",
      "\n",
      "epoch-200 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.72753075 0.73562677 0.67067886 0.61427194 0.55422886 0.47311321\n",
      " 0.37891596 0.38099718 0.69238527 0.61279297 0.55212355 0.5183714\n",
      " 0.53207739 0.371484   0.43235294 0.38652652]\n",
      "mean_cluster_accuracy_during_training_cycle : 54.05%, post_traincycle_acc : 53.96%, total_acc : 54.00%\n",
      "best_mean_cluster_accuracy_post_training_cycle_all_dataset : 53.96%\n",
      "accuracy_check 실행 시간: 18.927초\n",
      "\n",
      "epoch-201 loss : 0.00604\n",
      "ae train 실행 시간: 196.196초\n",
      "\n",
      "epoch-202 loss : 0.00585\n",
      "ae train 실행 시간: 195.117초\n",
      "\n",
      "epoch-203 loss : 0.00503\n",
      "ae train 실행 시간: 189.729초\n",
      "\n",
      "epoch-204 loss : 0.00451\n",
      "ae train 실행 시간: 192.325초\n",
      "\n",
      "epoch-205 loss : 0.00520\n",
      "ae train 실행 시간: 190.119초\n",
      "\n",
      "epoch-206 loss : 0.00627\n",
      "ae train 실행 시간: 193.516초\n",
      "\n",
      "epoch-207 loss : 0.00564\n",
      "ae train 실행 시간: 192.911초\n",
      "\n",
      "epoch-208 loss : 0.00463\n",
      "ae train 실행 시간: 191.118초\n",
      "\n",
      "epoch-209 loss : 0.00583\n",
      "ae train 실행 시간: 191.136초\n",
      "\n",
      "epoch-210 loss : 0.00723\n",
      "ae train 실행 시간: 190.680초\n",
      "\n",
      "epoch-211 loss : 0.00635\n",
      "ae train 실행 시간: 187.528초\n",
      "\n",
      "epoch-212 loss : 0.00684\n",
      "ae train 실행 시간: 190.884초\n",
      "\n",
      "epoch-213 loss : 0.00686\n",
      "ae train 실행 시간: 192.484초\n",
      "\n",
      "epoch-214 loss : 0.00699\n",
      "ae train 실행 시간: 191.829초\n",
      "\n",
      "epoch-215 loss : 0.00728\n",
      "ae train 실행 시간: 191.759초\n",
      "\n",
      "epoch-216 loss : 0.00713\n",
      "ae train 실행 시간: 193.103초\n",
      "\n",
      "epoch-217 loss : 0.00736\n",
      "ae train 실행 시간: 196.393초\n",
      "\n",
      "epoch-218 loss : 0.00890\n",
      "ae train 실행 시간: 193.694초\n",
      "\n",
      "epoch-219 loss : 0.00886\n",
      "ae train 실행 시간: 193.449초\n",
      "\n",
      "epoch-220 loss : 0.00855\n",
      "ae train 실행 시간: 193.339초\n",
      "\n",
      "epoch-221 loss : 0.00904\n",
      "ae train 실행 시간: 191.710초\n",
      "\n",
      "epoch-222 loss : 0.00967\n",
      "ae train 실행 시간: 193.529초\n",
      "\n",
      "epoch-223 loss : 0.00705\n",
      "ae train 실행 시간: 192.525초\n",
      "\n",
      "epoch-224 loss : 0.00636\n",
      "ae train 실행 시간: 192.406초\n",
      "\n",
      "epoch-225 loss : 0.00602\n",
      "ae train 실행 시간: 196.723초\n",
      "\n",
      "epoch-226 loss : 0.00617\n",
      "ae train 실행 시간: 190.807초\n",
      "\n",
      "epoch-227 loss : 0.00597\n",
      "ae train 실행 시간: 190.974초\n",
      "\n",
      "epoch-228 loss : 0.00577\n",
      "ae train 실행 시간: 194.483초\n",
      "\n",
      "epoch-229 loss : 0.00638\n",
      "ae train 실행 시간: 190.976초\n",
      "\n",
      "epoch-230 loss : 0.00559\n",
      "ae train 실행 시간: 188.831초\n",
      "\n",
      "epoch-231 loss : 0.00675\n",
      "ae train 실행 시간: 190.305초\n",
      "\n",
      "epoch-232 loss : 0.00635\n",
      "ae train 실행 시간: 189.741초\n",
      "\n",
      "epoch-233 loss : 0.00629\n",
      "ae train 실행 시간: 193.114초\n",
      "\n",
      "epoch-234 loss : 0.00657\n",
      "ae train 실행 시간: 193.149초\n",
      "\n",
      "epoch-235 loss : 0.00660\n",
      "ae train 실행 시간: 191.986초\n",
      "\n",
      "epoch-236 loss : 0.00646\n",
      "ae train 실행 시간: 191.126초\n",
      "\n",
      "epoch-237 loss : 0.00585\n",
      "ae train 실행 시간: 192.726초\n",
      "\n",
      "epoch-238 loss : 0.00609\n",
      "ae train 실행 시간: 194.077초\n",
      "\n",
      "epoch-239 loss : 0.00602\n",
      "ae train 실행 시간: 193.656초\n",
      "\n",
      "epoch-240 loss : 0.00528\n",
      "ae train 실행 시간: 193.857초\n",
      "\n",
      "epoch-241 loss : 0.00685\n",
      "ae train 실행 시간: 194.531초\n",
      "\n",
      "epoch-242 loss : 0.00668\n",
      "ae train 실행 시간: 197.254초\n",
      "\n",
      "epoch-243 loss : 0.00734\n",
      "ae train 실행 시간: 193.421초\n",
      "\n",
      "epoch-244 loss : 0.00621\n",
      "ae train 실행 시간: 193.395초\n",
      "\n",
      "epoch-245 loss : 0.00568\n",
      "ae train 실행 시간: 192.100초\n",
      "\n",
      "epoch-246 loss : 0.00532\n",
      "ae train 실행 시간: 190.212초\n",
      "\n",
      "epoch-247 loss : 0.00540\n",
      "ae train 실행 시간: 194.514초\n",
      "\n",
      "epoch-248 loss : 0.00484\n",
      "ae train 실행 시간: 193.527초\n",
      "\n",
      "epoch-249 loss : 0.00550\n",
      "ae train 실행 시간: 192.617초\n",
      "\n",
      "epoch-250 loss : 0.00754\n",
      "ae train 실행 시간: 191.389초\n",
      "\n",
      "epoch-250 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.72469253 0.65786993 0.57583052 0.53423337 0.38955224 0.41698113\n",
      " 0.30233715 0.32831609 0.65708522 0.54296875 0.46862934 0.41857001\n",
      " 0.40478615 0.52230844 0.42647059 0.36024845]\n",
      "mean_cluster_accuracy_during_training_cycle : 50.80%, post_traincycle_acc : 48.32%, total_acc : 49.33%\n",
      "best_mean_cluster_accuracy_post_training_cycle_all_dataset : 53.96%\n",
      "accuracy_check 실행 시간: 19.062초\n",
      "\n",
      "epoch-251 loss : 0.00621\n",
      "ae train 실행 시간: 191.580초\n",
      "\n",
      "epoch-252 loss : 0.00598\n",
      "ae train 실행 시간: 195.467초\n",
      "\n",
      "epoch-253 loss : 0.00578\n",
      "ae train 실행 시간: 194.852초\n",
      "\n",
      "epoch-254 loss : 0.00542\n",
      "ae train 실행 시간: 191.976초\n",
      "\n",
      "epoch-255 loss : 0.00484\n",
      "ae train 실행 시간: 193.473초\n",
      "\n",
      "epoch-256 loss : 0.00510\n",
      "ae train 실행 시간: 193.239초\n",
      "\n",
      "epoch-257 loss : 0.00543\n",
      "ae train 실행 시간: 193.674초\n",
      "\n",
      "epoch-258 loss : 0.00484\n",
      "ae train 실행 시간: 192.702초\n",
      "\n",
      "epoch-259 loss : 0.00604\n",
      "ae train 실행 시간: 192.669초\n",
      "\n",
      "epoch-260 loss : 0.00613\n",
      "ae train 실행 시간: 192.581초\n",
      "\n",
      "epoch-261 loss : 0.00675\n",
      "ae train 실행 시간: 191.949초\n",
      "\n",
      "epoch-262 loss : 0.00596\n",
      "ae train 실행 시간: 192.094초\n",
      "\n",
      "epoch-263 loss : 0.00558\n",
      "ae train 실행 시간: 194.369초\n",
      "\n",
      "epoch-264 loss : 0.00679\n",
      "ae train 실행 시간: 193.196초\n",
      "\n",
      "epoch-265 loss : 0.00736\n",
      "ae train 실행 시간: 193.483초\n",
      "\n",
      "epoch-266 loss : 0.00620\n",
      "ae train 실행 시간: 193.654초\n",
      "\n",
      "epoch-267 loss : 0.00706\n",
      "ae train 실행 시간: 191.111초\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch-268 loss : 0.00602\n",
      "ae train 실행 시간: 192.811초\n",
      "\n",
      "epoch-269 loss : 0.00633\n",
      "ae train 실행 시간: 192.776초\n",
      "\n",
      "epoch-270 loss : 0.00631\n",
      "ae train 실행 시간: 190.294초\n",
      "\n",
      "epoch-271 loss : 0.00655\n",
      "ae train 실행 시간: 192.529초\n",
      "\n",
      "epoch-272 loss : 0.00661\n",
      "ae train 실행 시간: 194.617초\n",
      "\n",
      "epoch-273 loss : 0.00628\n",
      "ae train 실행 시간: 192.636초\n",
      "\n",
      "epoch-274 loss : 0.00594\n",
      "ae train 실행 시간: 192.997초\n",
      "\n",
      "epoch-275 loss : 0.00521\n",
      "ae train 실행 시간: 194.456초\n",
      "\n",
      "epoch-276 loss : 0.00472\n",
      "ae train 실행 시간: 195.411초\n",
      "\n",
      "epoch-277 loss : 0.00526\n",
      "ae train 실행 시간: 193.469초\n",
      "\n",
      "epoch-278 loss : 0.00569\n",
      "ae train 실행 시간: 195.622초\n",
      "\n",
      "epoch-279 loss : 0.00664\n",
      "ae train 실행 시간: 192.824초\n",
      "\n",
      "epoch-280 loss : 0.00725\n",
      "ae train 실행 시간: 192.156초\n",
      "\n",
      "epoch-281 loss : 0.00654\n",
      "ae train 실행 시간: 192.332초\n",
      "\n",
      "epoch-282 loss : 0.00623\n",
      "ae train 실행 시간: 189.673초\n",
      "\n",
      "epoch-283 loss : 0.00595\n",
      "ae train 실행 시간: 193.756초\n",
      "\n",
      "epoch-284 loss : 0.00552\n",
      "ae train 실행 시간: 191.008초\n",
      "\n",
      "epoch-285 loss : 0.00618\n",
      "ae train 실행 시간: 194.284초\n",
      "\n",
      "epoch-286 loss : 0.00526\n",
      "ae train 실행 시간: 192.123초\n",
      "\n",
      "epoch-287 loss : 0.00557\n",
      "ae train 실행 시간: 193.472초\n",
      "\n",
      "epoch-288 loss : 0.00455\n",
      "ae train 실행 시간: 191.021초\n",
      "\n",
      "epoch-289 loss : 0.00456\n",
      "ae train 실행 시간: 189.704초\n",
      "\n",
      "epoch-290 loss : 0.00548\n",
      "ae train 실행 시간: 190.179초\n",
      "\n",
      "epoch-291 loss : 0.00530\n",
      "ae train 실행 시간: 191.673초\n",
      "\n",
      "epoch-292 loss : 0.00526\n",
      "ae train 실행 시간: 195.548초\n",
      "\n",
      "epoch-293 loss : 0.00494\n",
      "ae train 실행 시간: 192.983초\n",
      "\n",
      "epoch-294 loss : 0.00513\n",
      "ae train 실행 시간: 193.851초\n",
      "\n",
      "epoch-295 loss : 0.00518\n",
      "ae train 실행 시간: 192.099초\n",
      "\n",
      "epoch-296 loss : 0.00727\n",
      "ae train 실행 시간: 195.145초\n",
      "\n",
      "epoch-297 loss : 0.00664\n",
      "ae train 실행 시간: 189.554초\n",
      "\n",
      "epoch-298 loss : 0.00612\n",
      "ae train 실행 시간: 191.617초\n",
      "\n",
      "epoch-299 loss : 0.00582\n",
      "ae train 실행 시간: 191.673초\n",
      "\n",
      "epoch-300 loss : 0.00599\n",
      "ae train 실행 시간: 190.480초\n",
      "\n",
      "epoch-300 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.69347209 0.66399623 0.55127588 0.46432015 0.41442786 0.42122642\n",
      " 0.38488314 0.33066792 0.65557237 0.57617188 0.46621622 0.45382324\n",
      " 0.50101833 0.46217265 0.37303922 0.37649307]\n",
      "mean_cluster_accuracy_during_training_cycle : 50.61%, post_traincycle_acc : 48.68%, total_acc : 49.46%\n",
      "best_mean_cluster_accuracy_post_training_cycle_all_dataset : 53.96%\n",
      "accuracy_check 실행 시간: 18.648초\n",
      "\n",
      "epoch-301 loss : 0.00551\n",
      "ae train 실행 시간: 190.118초\n",
      "\n",
      "epoch-302 loss : 0.00557\n",
      "ae train 실행 시간: 189.878초\n",
      "\n",
      "epoch-303 loss : 0.00535\n",
      "ae train 실행 시간: 192.846초\n",
      "\n",
      "epoch-304 loss : 0.00534\n",
      "ae train 실행 시간: 194.203초\n",
      "\n",
      "epoch-305 loss : 0.00479\n",
      "ae train 실행 시간: 193.848초\n",
      "\n",
      "epoch-306 loss : 0.00445\n",
      "ae train 실행 시간: 193.873초\n",
      "\n",
      "epoch-307 loss : 0.00518\n",
      "ae train 실행 시간: 195.182초\n",
      "\n",
      "epoch-308 loss : 0.00470\n",
      "ae train 실행 시간: 191.579초\n",
      "\n",
      "epoch-309 loss : 0.00506\n",
      "ae train 실행 시간: 195.405초\n",
      "\n",
      "epoch-310 loss : 0.00532\n",
      "ae train 실행 시간: 194.816초\n",
      "\n",
      "epoch-311 loss : 0.00661\n",
      "ae train 실행 시간: 193.339초\n",
      "\n",
      "epoch-312 loss : 0.00488\n",
      "ae train 실행 시간: 193.079초\n",
      "\n",
      "epoch-313 loss : 0.00631\n",
      "ae train 실행 시간: 194.091초\n",
      "\n",
      "epoch-314 loss : 0.00529\n",
      "ae train 실행 시간: 191.618초\n",
      "\n",
      "epoch-315 loss : 0.00540\n",
      "ae train 실행 시간: 194.598초\n",
      "\n",
      "epoch-316 loss : 0.00548\n",
      "ae train 실행 시간: 192.152초\n",
      "\n",
      "epoch-317 loss : 0.00519\n",
      "ae train 실행 시간: 195.363초\n",
      "\n",
      "epoch-318 loss : 0.00551\n",
      "ae train 실행 시간: 194.139초\n",
      "\n",
      "epoch-319 loss : 0.00534\n",
      "ae train 실행 시간: 196.493초\n",
      "\n",
      "epoch-320 loss : 0.00573\n",
      "ae train 실행 시간: 193.819초\n",
      "\n",
      "epoch-321 loss : 0.00426\n",
      "ae train 실행 시간: 193.026초\n",
      "\n",
      "epoch-322 loss : 0.00422\n",
      "ae train 실행 시간: 192.695초\n",
      "\n",
      "epoch-323 loss : 0.00442\n",
      "ae train 실행 시간: 192.476초\n",
      "\n",
      "epoch-324 loss : 0.00532\n",
      "ae train 실행 시간: 192.459초\n",
      "\n",
      "epoch-325 loss : 0.00478\n",
      "ae train 실행 시간: 192.843초\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpu = 2\n",
    "Conv_net = True\n",
    "SAE_net = True\n",
    "\n",
    "# hyperparameter\n",
    "dataset_num = 16\n",
    "spike_length = 50 # coarse_com_mode일 때는 time step이 됨.\n",
    "num_cluster = 4  # 클러스터 수 설정 # 논문엔 4개라는데 여기서는 3개로 했네\n",
    "training_cycle = 1400 #1400 2400 # 그 초기 몇개까지만 cluster update할지\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "max_epoch = 7000\n",
    "learning_rate = 0.001\n",
    "normalize_on = False # True or False #이거 안 씀 # 이거 별로 안 좋은 normalize같음 # 쓸 거면 다른 거 써라.\n",
    "need_bias = False\n",
    "# first_layer_no_train = False\n",
    "lif_add_at_first = False\n",
    "my_seed = 42\n",
    "\n",
    "TIME = 50 # SAE일 때만 유효. coarse_com_mode일 때는 level_num이 됨. 즉 feature 개수.\n",
    "v_decay = 0.5 # -cor\n",
    "v_threshold = 0.5 # -cor\n",
    "v_reset = 0.0 # -cor # 10000이상 일 시 hard reset\n",
    "BPTT_on = True # +cor\n",
    "\n",
    "SAE_hidden_nomean = False # False가 나았다 이상하게.\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + f\"_{str(int(datetime.datetime.now().microsecond / 1000)).zfill(3)}\"\n",
    "\n",
    "optimizer = 'Adam' #'Adam', 'SGD' # 둘다 준수함. loss 줄이는 거는 adam이 좋긴한데, cluster accuracy는 비슷함.\n",
    "\n",
    "coarse_com_mode = True\n",
    "coarse_com_config = (3.0, -3.0) # (max, min) (2.0, -2.0) (3.0 -3.0)\n",
    "\n",
    "sae_l2_norm_bridge = False # True False\n",
    "sae_lif_bridge = True # False True\n",
    "\n",
    "accuracy_check_epoch_term = 50\n",
    "\n",
    "wandb.init(project= f'spike_sorting just run',save_code=False)\n",
    "\n",
    "cluster_train_system( \n",
    "    gpu = gpu,\n",
    "    Conv_net = Conv_net,\n",
    "    SAE_net = SAE_net,\n",
    "\n",
    "    # hyperparameter\n",
    "    dataset_num = dataset_num,\n",
    "    spike_length = spike_length,\n",
    "    num_cluster = num_cluster,  # 클러스터 수 설정 # 논문엔 4개라는데 여기서는 3개로 했네\n",
    "    training_cycle = training_cycle, # 그 초기 몇개까지만 cluster update할지\n",
    "\n",
    "\n",
    "    batch_size = batch_size,\n",
    "    max_epoch = max_epoch,\n",
    "    learning_rate = learning_rate,\n",
    "    normalize_on = normalize_on, # True or False #이거 안 씀 # 이거 별로 안 좋은 normalize같음 # 쓸 거면 다른 거 써라.\n",
    "    need_bias = need_bias,\n",
    "    # first_layer_no_train = False\n",
    "    lif_add_at_first = lif_add_at_first,\n",
    "    my_seed = my_seed,\n",
    "\n",
    "    TIME = TIME, # SAE일 때만 유효\n",
    "    v_decay = v_decay,\n",
    "    v_threshold = v_threshold,\n",
    "    v_reset = v_reset, # 10000이상 일 시 hard reset\n",
    "    BPTT_on = BPTT_on,\n",
    "\n",
    "    SAE_hidden_nomean = SAE_hidden_nomean,\n",
    "    \n",
    "    current_time = current_time,\n",
    "\n",
    "    optimizer = optimizer, #'Adam', 'SGD'\n",
    "\n",
    "    coarse_com_mode = coarse_com_mode,\n",
    "    coarse_com_config = coarse_com_config, # (max, min)\n",
    "\n",
    "    \n",
    "    sae_l2_norm_bridge = sae_l2_norm_bridge,\n",
    "    sae_lif_bridge = sae_lif_bridge,\n",
    "\n",
    "    accuracy_check_epoch_term = accuracy_check_epoch_term,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep code\n",
    "\n",
    "\n",
    "from unittest import TextTestRunner\n",
    "\n",
    "\n",
    "unique_name_hyper = 'cluster_train_system'\n",
    "# run_name = 'spike_sorting'\n",
    "sweep_start_time =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + f\"_{str(int(datetime.datetime.now().microsecond / 1000)).zfill(3)}\"\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes'\n",
    "    'name': f'spike_sorting_{sweep_start_time}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'best_mean_cluster_accuracy_post_training_cycle_all_dataset'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"gpu\": {\"values\": [1]},  # 이건 sweep parameter아님. hyper_iter에서 직접 설정\n",
    "        \"Conv_net\": {\"values\": [True]}, \n",
    "        \"SAE_net\": {\"values\": [True]}, \n",
    "\n",
    "        \"dataset_num\": {\"values\": [16]}, \n",
    "        \"spike_length\": {\"values\": [50]},  \n",
    "        \"num_cluster\": {\"values\": [4]}, \n",
    "        \"training_cycle\": {\"values\": [1400, 2400]}, # [1400, 2400]\n",
    "\n",
    "        \"batch_size\": {\"values\": [32]}, \n",
    "        \"max_epoch\": {\"values\": [10]}, \n",
    "        \"learning_rate\": {\"values\": [0.001]},\n",
    "        \"normalize_on\": {\"values\": [False]},\n",
    "        \"need_bias\": {\"values\": [False]}, \n",
    "\n",
    "        \"lif_add_at_first\": {\"values\": [True, False]}, # [True, False]\n",
    "        \"my_seed\": {\"values\": [42]}, \n",
    "\n",
    "        \"TIME\": {\"values\": [50,40,30,20]}, #  [4,6,8,10]\n",
    "        \"v_decay\": {\"values\": [0.25,0.50,0.75,0.875]}, # [0.25,0.50,0.75]\n",
    "        \"v_threshold\": {\"values\": [0.25,0.50,0.75,1.0]}, # [0.25,0.50,0.75]\n",
    "        \"v_reset\": {\"values\": [0.0, 10000.0]},  # [0.0, 10000.0]\n",
    "        \"BPTT_on\": {\"values\": [True, False]},  # [True, False]\n",
    "\n",
    "        \"SAE_hidden_nomean\": {\"values\": [True, False]}, # [True, False]\n",
    "\n",
    "        # \"current_time\": {\"values\": [current_time]} #밑에서 직접설정됨.\n",
    "\n",
    "        \"optimizer\": {\"values\": ['Adam', 'SGD']}, # ['Adam', 'SGD']\n",
    "\n",
    "        \"coarse_com_mode\": {\"values\": [True]}, # ['Adam', 'SGD']\n",
    "        \"coarse_com_config\": {\"values\": [(2.0, -2.0), (3.0, -3.0)]}, # ['Adam', 'SGD']\n",
    "\n",
    "        \"sae_l2_norm_bridge\": {\"values\": [False]}, # [True, False]\n",
    "        \"sae_lif_bridge\": {\"values\": [True]}, # [False, True]\n",
    "        \n",
    "        \"accuracy_check_epoch_term\": {\"values\": [5]}, # [False, True]\n",
    "        \n",
    "        \n",
    "     }\n",
    "}\n",
    "\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code = False)\n",
    "    gpu  =  3\n",
    "    Conv_net  =  wandb.config.Conv_net\n",
    "    SAE_net  =  wandb.config.SAE_net\n",
    "\n",
    "    dataset_num  =  wandb.config.dataset_num\n",
    "    spike_length  =  wandb.config.spike_length\n",
    "    num_cluster  =  wandb.config.num_cluster\n",
    "    training_cycle  =  wandb.config.training_cycle\n",
    "\n",
    "    batch_size  =  wandb.config.batch_size\n",
    "    max_epoch  =  wandb.config.max_epoch\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    normalize_on  =  wandb.config.normalize_on\n",
    "    need_bias  =  wandb.config.need_bias\n",
    "\n",
    "    lif_add_at_first  =  wandb.config.lif_add_at_first\n",
    "    my_seed  =  wandb.config.my_seed\n",
    "\n",
    "\n",
    "    TIME  =  wandb.config.TIME\n",
    "    v_decay  =  wandb.config.v_decay\n",
    "    v_threshold  =  wandb.config.v_threshold\n",
    "    v_reset  =  wandb.config.v_reset\n",
    "    BPTT_on  =  wandb.config.BPTT_on\n",
    "\n",
    "    SAE_hidden_nomean  =  wandb.config.SAE_hidden_nomean\n",
    "    \n",
    "    current_time =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + f\"_{str(int(datetime.datetime.now().microsecond / 1000)).zfill(3)}\"\n",
    "\n",
    "    optimizer  =  wandb.config.optimizer\n",
    "\n",
    "    coarse_com_mode = wandb.config.coarse_com_mode\n",
    "    coarse_com_config = wandb.config.coarse_com_config # (max, min)\n",
    "\n",
    "    sae_l2_norm_bridge = wandb.config.sae_l2_norm_bridge\n",
    "    sae_lif_bridge = wandb.config.sae_lif_bridge\n",
    "\n",
    "    accuracy_check_epoch_term = wandb.config.accuracy_check_epoch_term\n",
    "\n",
    "\n",
    "    cluster_train_system( \n",
    "        gpu = gpu,\n",
    "        Conv_net = Conv_net,\n",
    "        SAE_net = SAE_net,\n",
    "\n",
    "        # hyperparameter\n",
    "        dataset_num = dataset_num,\n",
    "        spike_length = spike_length,\n",
    "        num_cluster = num_cluster,  # 클러스터 수 설정 # 논문엔 4개라는데 여기서는 3개로 했네\n",
    "        training_cycle = training_cycle, # 그 초기 몇개까지만 cluster update할지\n",
    "\n",
    "\n",
    "        batch_size = batch_size,\n",
    "        max_epoch = max_epoch,\n",
    "        learning_rate = learning_rate,\n",
    "        normalize_on = normalize_on, # True or False #이거 안 씀 # 이거 별로 안 좋은 normalize같음 # 쓸 거면 다른 거 써라.\n",
    "        need_bias = need_bias,\n",
    "        # first_layer_no_train = False\n",
    "        lif_add_at_first = lif_add_at_first,\n",
    "        my_seed = my_seed,\n",
    "\n",
    "        TIME = TIME, # SAE일 때만 유효\n",
    "        v_decay = v_decay,\n",
    "        v_threshold = v_threshold,\n",
    "        v_reset = v_reset, # 10000이상 일 시 hard reset\n",
    "        BPTT_on = BPTT_on,\n",
    "\n",
    "        SAE_hidden_nomean = SAE_hidden_nomean,\n",
    "\n",
    "        current_time = current_time,\n",
    "\n",
    "        optimizer = optimizer, #'Adam', 'SGD'\n",
    "\n",
    "        coarse_com_mode = coarse_com_mode,\n",
    "        coarse_com_config = coarse_com_config, # (max, min)\n",
    "        \n",
    "        sae_l2_norm_bridge = sae_l2_norm_bridge,\n",
    "        sae_lif_bridge = sae_lif_bridge,\n",
    "\n",
    "        accuracy_check_epoch_term = accuracy_check_epoch_term,\n",
    "        )\n",
    "    \n",
    "# sweep_id = 'ygoj9jt4'\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'spike_sorting {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=100000, project=f'spike_sorting {unique_name_hyper}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# current_time = '20250102_225243_972'\n",
    "\n",
    "with open(f\"result_save/cluster_accuracy_history_{current_time}.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "\n",
    "# JSON으로 저장\n",
    "with open(f\"result_save/cluster_accuracy_history_{current_time}.json\", 'r') as f:\n",
    "    loaded_hyperparameters = json.load(f)\n",
    "\n",
    "loss_history = data['loss_history']\n",
    "mean_cluster_accuracy_during_training_cycle_all_dataset_history = data['mean_cluster_accuracy_during_training_cycle_all_dataset_history']\n",
    "mean_cluster_accuracy_post_training_cycle_all_dataset_history = data['mean_cluster_accuracy_post_training_cycle_all_dataset_history']\n",
    "mean_cluster_accuracy_total_all_dataset_history = data['mean_cluster_accuracy_total_all_dataset_history']\n",
    "print(data)\n",
    "max_acc = 0\n",
    "for i in mean_cluster_accuracy_post_training_cycle_all_dataset_history:\n",
    "    if i[1] > max_acc:\n",
    "        max_acc = i[1]\n",
    "\n",
    "# 설정 정보 제목 작성\n",
    "title = (\n",
    "    f\"Dataset Num: {loaded_hyperparameters['dataset_num']}, Conv {loaded_hyperparameters['Conv_net']}, SAE {loaded_hyperparameters['SAE_net']}, Current time {loaded_hyperparameters['current_time']}, Spike Length: {loaded_hyperparameters['spike_length']}, Num Cluster: {loaded_hyperparameters['num_cluster']}, \"\n",
    "    f\"Training Cycle: {loaded_hyperparameters['training_cycle']}, Batch Size: {loaded_hyperparameters['batch_size']}, Max Epoch: {loaded_hyperparameters['max_epoch']}, \\n\"\n",
    "    f\"Learning Rate: {loaded_hyperparameters['learning_rate']}, Input Normalize: {loaded_hyperparameters['normalize_on']}, Need Bias: {loaded_hyperparameters['need_bias']}, \"\n",
    "    f\"LIF Add at First: {loaded_hyperparameters['lif_add_at_first']}, TIME: {loaded_hyperparameters['TIME']}, Seed: {loaded_hyperparameters['my_seed']}, Best ACC: {max_acc:.2f}%\"\n",
    ")\n",
    "\n",
    "# 데이터 리스트와 라벨 설정 (Loss 제외)\n",
    "data_list = [\n",
    "    (\"Mean Cluster Accuracy (During Training Cycle)\", mean_cluster_accuracy_during_training_cycle_all_dataset_history),\n",
    "    (\"Mean Cluster Accuracy (Post Training Cycle)\", mean_cluster_accuracy_post_training_cycle_all_dataset_history),\n",
    "    (\"Mean Cluster Accuracy (Total)\", mean_cluster_accuracy_total_all_dataset_history),\n",
    "]\n",
    "\n",
    "# 플롯 생성\n",
    "fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# 첫 번째 y축: Accuracy 관련 데이터\n",
    "for label, data in data_list:\n",
    "    epochs, values = zip(*data)  # epoch, value 분리\n",
    "    ax1.plot(epochs, values, label=label)\n",
    "\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Clurstering Accuracy [%]\", color=\"blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "ax1.legend(loc=\"center right\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# x축을 정수만 표시하도록 설정\n",
    "ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "# 두 번째 y축: Loss History\n",
    "ax2 = ax1.twinx()\n",
    "epochs, values = zip(*loss_history)\n",
    "ax2.plot(epochs, values, label=\"AE Loss History\", color=\"red\", linestyle=\"--\")\n",
    "ax2.set_ylabel(\"Loss\", color=\"red\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "ax2.legend(loc=\"center left\")\n",
    "\n",
    "# 제목 추가\n",
    "plt.title(title, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'net_save/{current_time}', dpi=300, bbox_inches=\"tight\")  # dpi=300은 고해상도로 저장, bbox_inches=\"tight\"는 여백 최소화\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
