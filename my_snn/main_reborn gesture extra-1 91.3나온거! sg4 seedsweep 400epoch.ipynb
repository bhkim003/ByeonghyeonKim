{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33550/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    epoch_100_val_acc_best = 0\n",
    "    epoch_200_val_acc_best = 0\n",
    "    epoch_300_val_acc_best = 0\n",
    "    epoch_400_val_acc_best = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "                \n",
    "                epoch_100_val_acc_best = val_acc_best if epoch == 99 else epoch_100_val_acc_best\n",
    "                epoch_200_val_acc_best = val_acc_best if epoch == 199 else epoch_200_val_acc_best\n",
    "                epoch_300_val_acc_best = val_acc_best if epoch == 299 else epoch_300_val_acc_best\n",
    "                epoch_400_val_acc_best = val_acc_best if epoch == 399 else epoch_400_val_acc_best\n",
    "                \n",
    "                wandb.log({\"epoch_100_val_acc_best\": epoch_100_val_acc_best}) \n",
    "                wandb.log({\"epoch_200_val_acc_best\": epoch_200_val_acc_best}) \n",
    "                wandb.log({\"epoch_300_val_acc_best\": epoch_300_val_acc_best}) \n",
    "                wandb.log({\"epoch_400_val_acc_best\": epoch_400_val_acc_best}) \n",
    "\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"3\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.25,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-9,-9],[-9,-9],[-8,-8]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8odmu475 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 400\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 34325\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251128_134022-8odmu475</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8odmu475' target=\"_blank\">lunar-sweep-2</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vdepwll5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vdepwll5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vdepwll5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vdepwll5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8odmu475' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8odmu475</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251128_134031_506', 'my_seed': 34325, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 400, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 189.0\n",
      "lif layer 1 self.abs_max_v: 189.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 142.0\n",
      "lif layer 2 self.abs_max_v: 142.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 213.0\n",
      "fc layer 2 self.abs_max_out: 159.0\n",
      "lif layer 2 self.abs_max_v: 194.0\n",
      "fc layer 3 self.abs_max_out: 18.0\n",
      "lif layer 1 self.abs_max_v: 216.5\n",
      "fc layer 2 self.abs_max_out: 231.0\n",
      "lif layer 2 self.abs_max_v: 284.5\n",
      "fc layer 3 self.abs_max_out: 109.0\n",
      "lif layer 1 self.abs_max_v: 270.0\n",
      "fc layer 2 self.abs_max_out: 261.0\n",
      "lif layer 2 self.abs_max_v: 292.0\n",
      "lif layer 1 self.abs_max_v: 270.5\n",
      "fc layer 1 self.abs_max_out: 249.0\n",
      "lif layer 1 self.abs_max_v: 343.0\n",
      "lif layer 2 self.abs_max_v: 365.0\n",
      "fc layer 1 self.abs_max_out: 326.0\n",
      "lif layer 1 self.abs_max_v: 418.5\n",
      "fc layer 2 self.abs_max_out: 389.0\n",
      "lif layer 2 self.abs_max_v: 421.5\n",
      "fc layer 2 self.abs_max_out: 473.0\n",
      "lif layer 2 self.abs_max_v: 473.0\n",
      "fc layer 3 self.abs_max_out: 124.0\n",
      "fc layer 1 self.abs_max_out: 408.0\n",
      "fc layer 3 self.abs_max_out: 125.0\n",
      "fc layer 1 self.abs_max_out: 531.0\n",
      "lif layer 1 self.abs_max_v: 531.0\n",
      "lif layer 1 self.abs_max_v: 532.0\n",
      "lif layer 2 self.abs_max_v: 501.0\n",
      "fc layer 1 self.abs_max_out: 557.0\n",
      "lif layer 1 self.abs_max_v: 623.0\n",
      "fc layer 1 self.abs_max_out: 568.0\n",
      "lif layer 1 self.abs_max_v: 704.5\n",
      "lif layer 2 self.abs_max_v: 514.5\n",
      "lif layer 1 self.abs_max_v: 778.5\n",
      "lif layer 2 self.abs_max_v: 610.5\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 1 self.abs_max_out: 626.0\n",
      "lif layer 2 self.abs_max_v: 736.5\n",
      "fc layer 3 self.abs_max_out: 147.0\n",
      "fc layer 1 self.abs_max_out: 722.0\n",
      "lif layer 2 self.abs_max_v: 808.5\n",
      "fc layer 3 self.abs_max_out: 154.0\n",
      "fc layer 3 self.abs_max_out: 185.0\n",
      "fc layer 1 self.abs_max_out: 938.0\n",
      "lif layer 1 self.abs_max_v: 938.0\n",
      "fc layer 3 self.abs_max_out: 213.0\n",
      "fc layer 2 self.abs_max_out: 483.0\n",
      "fc layer 2 self.abs_max_out: 582.0\n",
      "fc layer 2 self.abs_max_out: 596.0\n",
      "fc layer 3 self.abs_max_out: 243.0\n",
      "fc layer 2 self.abs_max_out: 608.0\n",
      "fc layer 3 self.abs_max_out: 246.0\n",
      "lif layer 2 self.abs_max_v: 827.5\n",
      "lif layer 2 self.abs_max_v: 889.0\n",
      "fc layer 3 self.abs_max_out: 250.0\n",
      "fc layer 2 self.abs_max_out: 616.0\n",
      "lif layer 2 self.abs_max_v: 943.0\n",
      "lif layer 2 self.abs_max_v: 1000.5\n",
      "fc layer 3 self.abs_max_out: 260.0\n",
      "fc layer 2 self.abs_max_out: 619.0\n",
      "fc layer 2 self.abs_max_out: 652.0\n",
      "lif layer 1 self.abs_max_v: 946.0\n",
      "lif layer 2 self.abs_max_v: 1029.0\n",
      "lif layer 1 self.abs_max_v: 1018.0\n",
      "lif layer 2 self.abs_max_v: 1078.5\n",
      "lif layer 2 self.abs_max_v: 1094.5\n",
      "lif layer 1 self.abs_max_v: 1025.5\n",
      "lif layer 2 self.abs_max_v: 1102.5\n",
      "lif layer 2 self.abs_max_v: 1112.5\n",
      "lif layer 2 self.abs_max_v: 1122.5\n",
      "fc layer 2 self.abs_max_out: 684.0\n",
      "fc layer 2 self.abs_max_out: 694.0\n",
      "fc layer 2 self.abs_max_out: 721.0\n",
      "lif layer 2 self.abs_max_v: 1134.0\n",
      "lif layer 1 self.abs_max_v: 1054.5\n",
      "fc layer 1 self.abs_max_out: 995.0\n",
      "lif layer 1 self.abs_max_v: 1113.5\n",
      "lif layer 2 self.abs_max_v: 1189.0\n",
      "fc layer 2 self.abs_max_out: 743.0\n",
      "fc layer 2 self.abs_max_out: 748.0\n",
      "lif layer 2 self.abs_max_v: 1295.5\n",
      "fc layer 3 self.abs_max_out: 263.0\n",
      "lif layer 1 self.abs_max_v: 1122.5\n",
      "lif layer 1 self.abs_max_v: 1140.5\n",
      "lif layer 1 self.abs_max_v: 1171.0\n",
      "lif layer 1 self.abs_max_v: 1248.5\n",
      "lif layer 1 self.abs_max_v: 1363.5\n",
      "lif layer 1 self.abs_max_v: 1374.0\n",
      "fc layer 3 self.abs_max_out: 275.0\n",
      "fc layer 2 self.abs_max_out: 762.0\n",
      "fc layer 2 self.abs_max_out: 910.0\n",
      "fc layer 1 self.abs_max_out: 1006.0\n",
      "fc layer 3 self.abs_max_out: 306.0\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "fc layer 3 self.abs_max_out: 329.0\n",
      "fc layer 1 self.abs_max_out: 1082.0\n",
      "fc layer 2 self.abs_max_out: 967.0\n",
      "lif layer 2 self.abs_max_v: 1298.5\n",
      "lif layer 2 self.abs_max_v: 1305.5\n",
      "lif layer 2 self.abs_max_v: 1312.0\n",
      "lif layer 2 self.abs_max_v: 1402.0\n",
      "fc layer 2 self.abs_max_out: 1005.0\n",
      "lif layer 1 self.abs_max_v: 1433.0\n",
      "fc layer 3 self.abs_max_out: 341.0\n",
      "fc layer 1 self.abs_max_out: 1173.0\n",
      "fc layer 1 self.abs_max_out: 1219.0\n",
      "lif layer 2 self.abs_max_v: 1492.5\n",
      "lif layer 2 self.abs_max_v: 1555.5\n",
      "lif layer 2 self.abs_max_v: 1769.0\n",
      "fc layer 1 self.abs_max_out: 1223.0\n",
      "fc layer 3 self.abs_max_out: 344.0\n",
      "fc layer 1 self.abs_max_out: 1392.0\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "fc layer 3 self.abs_max_out: 357.0\n",
      "fc layer 3 self.abs_max_out: 377.0\n",
      "fc layer 2 self.abs_max_out: 1007.0\n",
      "fc layer 3 self.abs_max_out: 389.0\n",
      "lif layer 1 self.abs_max_v: 1497.5\n",
      "fc layer 2 self.abs_max_out: 1099.0\n",
      "fc layer 1 self.abs_max_out: 1587.0\n",
      "lif layer 1 self.abs_max_v: 1587.0\n",
      "fc layer 2 self.abs_max_out: 1127.0\n",
      "fc layer 2 self.abs_max_out: 1208.0\n",
      "lif layer 1 self.abs_max_v: 1609.5\n",
      "lif layer 1 self.abs_max_v: 1651.0\n",
      "lif layer 1 self.abs_max_v: 1917.0\n",
      "lif layer 1 self.abs_max_v: 1965.5\n",
      "lif layer 1 self.abs_max_v: 2054.0\n",
      "fc layer 3 self.abs_max_out: 397.0\n",
      "fc layer 3 self.abs_max_out: 434.0\n",
      "lif layer 1 self.abs_max_v: 2101.0\n",
      "lif layer 1 self.abs_max_v: 2129.5\n",
      "fc layer 1 self.abs_max_out: 1673.0\n",
      "fc layer 2 self.abs_max_out: 1254.0\n",
      "lif layer 1 self.abs_max_v: 2420.0\n",
      "lif layer 1 self.abs_max_v: 2510.5\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.765393/  1.927988, val:  39.58%, val_best:  39.58%, tr:  96.94%, tr_best:  96.94%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0383%\n",
      "layer   2  Sparsity: 75.0106%\n",
      "layer   3  Sparsity: 72.3794%\n",
      "total_backward_count 9790 real_backward_count 2230  22.778%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1257.0\n",
      "fc layer 2 self.abs_max_out: 1285.0\n",
      "lif layer 2 self.abs_max_v: 1817.5\n",
      "fc layer 1 self.abs_max_out: 1770.0\n",
      "lif layer 1 self.abs_max_v: 2602.5\n",
      "lif layer 1 self.abs_max_v: 2754.0\n",
      "fc layer 2 self.abs_max_out: 1303.0\n",
      "fc layer 1 self.abs_max_out: 1841.0\n",
      "lif layer 1 self.abs_max_v: 3017.5\n",
      "fc layer 2 self.abs_max_out: 1312.0\n",
      "fc layer 2 self.abs_max_out: 1343.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.653447/  1.865499, val:  53.75%, val_best:  53.75%, tr:  99.39%, tr_best:  99.39%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   2  Sparsity: 74.5271%\n",
      "layer   3  Sparsity: 68.8434%\n",
      "total_backward_count 19580 real_backward_count 3841  19.617%\n",
      "fc layer 1 self.abs_max_out: 1905.0\n",
      "fc layer 3 self.abs_max_out: 438.0\n",
      "lif layer 2 self.abs_max_v: 1818.0\n",
      "lif layer 2 self.abs_max_v: 1847.0\n",
      "lif layer 2 self.abs_max_v: 1940.0\n",
      "lif layer 2 self.abs_max_v: 2042.5\n",
      "fc layer 1 self.abs_max_out: 1958.0\n",
      "fc layer 1 self.abs_max_out: 1979.0\n",
      "lif layer 1 self.abs_max_v: 3280.0\n",
      "lif layer 1 self.abs_max_v: 3396.0\n",
      "fc layer 3 self.abs_max_out: 445.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.617582/  1.900940, val:  51.67%, val_best:  53.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0954%\n",
      "layer   2  Sparsity: 74.1025%\n",
      "layer   3  Sparsity: 67.0903%\n",
      "total_backward_count 29370 real_backward_count 5300  18.046%\n",
      "fc layer 3 self.abs_max_out: 469.0\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "fc layer 3 self.abs_max_out: 529.0\n",
      "fc layer 2 self.abs_max_out: 1351.0\n",
      "fc layer 1 self.abs_max_out: 1983.0\n",
      "fc layer 1 self.abs_max_out: 1998.0\n",
      "fc layer 1 self.abs_max_out: 2136.0\n",
      "lif layer 1 self.abs_max_v: 3467.5\n",
      "lif layer 1 self.abs_max_v: 3598.0\n",
      "lif layer 1 self.abs_max_v: 3665.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.609079/  1.829370, val:  50.42%, val_best:  53.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0876%\n",
      "layer   2  Sparsity: 75.4315%\n",
      "layer   3  Sparsity: 66.7404%\n",
      "total_backward_count 39160 real_backward_count 6661  17.010%\n",
      "lif layer 2 self.abs_max_v: 2044.5\n",
      "lif layer 2 self.abs_max_v: 2049.0\n",
      "lif layer 2 self.abs_max_v: 2063.5\n",
      "lif layer 2 self.abs_max_v: 2140.0\n",
      "fc layer 2 self.abs_max_out: 1391.0\n",
      "fc layer 1 self.abs_max_out: 2149.0\n",
      "fc layer 2 self.abs_max_out: 1396.0\n",
      "fc layer 2 self.abs_max_out: 1445.0\n",
      "fc layer 2 self.abs_max_out: 1464.0\n",
      "fc layer 1 self.abs_max_out: 2165.0\n",
      "fc layer 1 self.abs_max_out: 2185.0\n",
      "lif layer 1 self.abs_max_v: 3956.0\n",
      "fc layer 1 self.abs_max_out: 2204.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.555138/  1.837016, val:  48.75%, val_best:  53.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 74.5796%\n",
      "layer   3  Sparsity: 66.5545%\n",
      "total_backward_count 48950 real_backward_count 8017  16.378%\n",
      "fc layer 2 self.abs_max_out: 1511.0\n",
      "fc layer 2 self.abs_max_out: 1549.0\n",
      "fc layer 2 self.abs_max_out: 1573.0\n",
      "lif layer 2 self.abs_max_v: 2230.5\n",
      "fc layer 2 self.abs_max_out: 1632.0\n",
      "fc layer 1 self.abs_max_out: 2278.0\n",
      "lif layer 1 self.abs_max_v: 3979.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.578872/  1.834388, val:  49.58%, val_best:  53.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0925%\n",
      "layer   2  Sparsity: 74.7165%\n",
      "layer   3  Sparsity: 67.2284%\n",
      "total_backward_count 58740 real_backward_count 9356  15.928%\n",
      "fc layer 2 self.abs_max_out: 1647.0\n",
      "fc layer 3 self.abs_max_out: 543.0\n",
      "fc layer 1 self.abs_max_out: 2509.0\n",
      "fc layer 2 self.abs_max_out: 1655.0\n",
      "fc layer 2 self.abs_max_out: 1671.0\n",
      "fc layer 1 self.abs_max_out: 2550.0\n",
      "lif layer 1 self.abs_max_v: 4192.5\n",
      "lif layer 1 self.abs_max_v: 4486.0\n",
      "lif layer 1 self.abs_max_v: 4541.5\n",
      "lif layer 1 self.abs_max_v: 4562.0\n",
      "lif layer 2 self.abs_max_v: 2231.5\n",
      "lif layer 2 self.abs_max_v: 2271.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.583915/  1.860599, val:  57.50%, val_best:  57.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1120%\n",
      "layer   2  Sparsity: 75.0942%\n",
      "layer   3  Sparsity: 67.3114%\n",
      "total_backward_count 68530 real_backward_count 10690  15.599%\n",
      "fc layer 2 self.abs_max_out: 1738.0\n",
      "fc layer 1 self.abs_max_out: 2579.0\n",
      "fc layer 2 self.abs_max_out: 1790.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.549730/  1.826157, val:  50.00%, val_best:  57.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0851%\n",
      "layer   2  Sparsity: 75.0132%\n",
      "layer   3  Sparsity: 68.4752%\n",
      "total_backward_count 78320 real_backward_count 11934  15.237%\n",
      "fc layer 2 self.abs_max_out: 1791.0\n",
      "fc layer 2 self.abs_max_out: 1808.0\n",
      "fc layer 2 self.abs_max_out: 1817.0\n",
      "fc layer 2 self.abs_max_out: 1852.0\n",
      "fc layer 1 self.abs_max_out: 2604.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.542927/  1.812543, val:  37.50%, val_best:  57.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0624%\n",
      "layer   2  Sparsity: 74.6791%\n",
      "layer   3  Sparsity: 69.0785%\n",
      "total_backward_count 88110 real_backward_count 13154  14.929%\n",
      "fc layer 2 self.abs_max_out: 1887.0\n",
      "lif layer 2 self.abs_max_v: 2290.5\n",
      "fc layer 2 self.abs_max_out: 1896.0\n",
      "fc layer 1 self.abs_max_out: 2608.0\n",
      "fc layer 2 self.abs_max_out: 1911.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.545154/  1.788429, val:  50.42%, val_best:  57.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   2  Sparsity: 74.4802%\n",
      "layer   3  Sparsity: 68.8057%\n",
      "total_backward_count 97900 real_backward_count 14409  14.718%\n",
      "lif layer 2 self.abs_max_v: 2365.5\n",
      "fc layer 3 self.abs_max_out: 549.0\n",
      "lif layer 2 self.abs_max_v: 2415.0\n",
      "lif layer 2 self.abs_max_v: 2446.0\n",
      "lif layer 2 self.abs_max_v: 2515.0\n",
      "fc layer 1 self.abs_max_out: 2670.0\n",
      "fc layer 1 self.abs_max_out: 2792.0\n",
      "lif layer 1 self.abs_max_v: 5008.0\n",
      "lif layer 1 self.abs_max_v: 5133.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.512823/  1.789769, val:  50.42%, val_best:  57.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0859%\n",
      "layer   2  Sparsity: 73.8860%\n",
      "layer   3  Sparsity: 68.3613%\n",
      "total_backward_count 107690 real_backward_count 15638  14.521%\n",
      "lif layer 2 self.abs_max_v: 2557.5\n",
      "fc layer 1 self.abs_max_out: 2847.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.513009/  1.773336, val:  61.67%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   2  Sparsity: 74.4755%\n",
      "layer   3  Sparsity: 69.9163%\n",
      "total_backward_count 117480 real_backward_count 16836  14.331%\n",
      "lif layer 2 self.abs_max_v: 2591.5\n",
      "lif layer 2 self.abs_max_v: 2686.5\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.515818/  1.811081, val:  40.00%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   2  Sparsity: 73.8858%\n",
      "layer   3  Sparsity: 69.8040%\n",
      "total_backward_count 127270 real_backward_count 17971  14.120%\n",
      "fc layer 3 self.abs_max_out: 564.0\n",
      "fc layer 1 self.abs_max_out: 2951.0\n",
      "lif layer 1 self.abs_max_v: 5321.5\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.522977/  1.763381, val:  57.92%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0642%\n",
      "layer   2  Sparsity: 73.6691%\n",
      "layer   3  Sparsity: 70.1940%\n",
      "total_backward_count 137060 real_backward_count 19114  13.946%\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.520874/  1.748142, val:  52.50%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   2  Sparsity: 73.9987%\n",
      "layer   3  Sparsity: 70.0583%\n",
      "total_backward_count 146850 real_backward_count 20290  13.817%\n",
      "lif layer 2 self.abs_max_v: 2728.0\n",
      "fc layer 1 self.abs_max_out: 2964.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.501542/  1.740131, val:  55.00%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0782%\n",
      "layer   2  Sparsity: 73.8888%\n",
      "layer   3  Sparsity: 69.8705%\n",
      "total_backward_count 156640 real_backward_count 21445  13.691%\n",
      "fc layer 1 self.abs_max_out: 2987.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.462097/  1.734407, val:  55.42%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0385%\n",
      "layer   2  Sparsity: 73.6381%\n",
      "layer   3  Sparsity: 69.5790%\n",
      "total_backward_count 166430 real_backward_count 22565  13.558%\n",
      "fc layer 2 self.abs_max_out: 1924.0\n",
      "fc layer 1 self.abs_max_out: 3003.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.479730/  1.707383, val:  57.92%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0667%\n",
      "layer   2  Sparsity: 73.3945%\n",
      "layer   3  Sparsity: 69.8136%\n",
      "total_backward_count 176220 real_backward_count 23680  13.438%\n",
      "fc layer 1 self.abs_max_out: 3016.0\n",
      "fc layer 2 self.abs_max_out: 2038.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.458310/  1.701507, val:  59.58%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0924%\n",
      "layer   2  Sparsity: 73.1243%\n",
      "layer   3  Sparsity: 69.7825%\n",
      "total_backward_count 186010 real_backward_count 24774  13.319%\n",
      "fc layer 2 self.abs_max_out: 2048.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.456911/  1.674707, val:  60.00%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0944%\n",
      "layer   2  Sparsity: 73.4265%\n",
      "layer   3  Sparsity: 70.3608%\n",
      "total_backward_count 195800 real_backward_count 25848  13.201%\n",
      "fc layer 3 self.abs_max_out: 565.0\n",
      "fc layer 1 self.abs_max_out: 3074.0\n",
      "fc layer 2 self.abs_max_out: 2102.0\n",
      "fc layer 1 self.abs_max_out: 3126.0\n",
      "fc layer 3 self.abs_max_out: 591.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.449950/  1.711280, val:  54.17%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1114%\n",
      "layer   2  Sparsity: 73.3784%\n",
      "layer   3  Sparsity: 70.4902%\n",
      "total_backward_count 205590 real_backward_count 26903  13.086%\n",
      "fc layer 2 self.abs_max_out: 2135.0\n",
      "fc layer 2 self.abs_max_out: 2182.0\n",
      "fc layer 1 self.abs_max_out: 3146.0\n",
      "fc layer 3 self.abs_max_out: 608.0\n",
      "fc layer 1 self.abs_max_out: 3173.0\n",
      "lif layer 1 self.abs_max_v: 5498.5\n",
      "lif layer 1 self.abs_max_v: 5554.5\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.446544/  1.665571, val:  68.75%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0632%\n",
      "layer   2  Sparsity: 73.4219%\n",
      "layer   3  Sparsity: 69.6242%\n",
      "total_backward_count 215380 real_backward_count 27897  12.952%\n",
      "fc layer 1 self.abs_max_out: 3322.0\n",
      "lif layer 1 self.abs_max_v: 5687.5\n",
      "lif layer 1 self.abs_max_v: 5719.5\n",
      "lif layer 1 self.abs_max_v: 6100.0\n",
      "lif layer 1 self.abs_max_v: 6120.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.402480/  1.664131, val:  58.33%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0761%\n",
      "layer   2  Sparsity: 73.8000%\n",
      "layer   3  Sparsity: 70.0831%\n",
      "total_backward_count 225170 real_backward_count 28895  12.833%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.402597/  1.637427, val:  65.00%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   2  Sparsity: 73.3621%\n",
      "layer   3  Sparsity: 69.5387%\n",
      "total_backward_count 234960 real_backward_count 29949  12.746%\n",
      "lif layer 2 self.abs_max_v: 2739.5\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.399718/  1.655632, val:  57.92%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0907%\n",
      "layer   2  Sparsity: 72.8326%\n",
      "layer   3  Sparsity: 69.9608%\n",
      "total_backward_count 244750 real_backward_count 30941  12.642%\n",
      "lif layer 2 self.abs_max_v: 2746.0\n",
      "lif layer 2 self.abs_max_v: 2870.5\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.402965/  1.646727, val:  67.92%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   2  Sparsity: 72.7618%\n",
      "layer   3  Sparsity: 69.9854%\n",
      "total_backward_count 254540 real_backward_count 31944  12.550%\n",
      "lif layer 2 self.abs_max_v: 2878.5\n",
      "lif layer 2 self.abs_max_v: 2987.5\n",
      "fc layer 1 self.abs_max_out: 3339.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.395477/  1.640709, val:  64.17%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0799%\n",
      "layer   2  Sparsity: 72.6619%\n",
      "layer   3  Sparsity: 70.1418%\n",
      "total_backward_count 264330 real_backward_count 32941  12.462%\n",
      "fc layer 3 self.abs_max_out: 623.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.379097/  1.605324, val:  68.75%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1016%\n",
      "layer   2  Sparsity: 72.5445%\n",
      "layer   3  Sparsity: 69.7590%\n",
      "total_backward_count 274120 real_backward_count 33931  12.378%\n",
      "fc layer 1 self.abs_max_out: 3468.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.348979/  1.583175, val:  69.58%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.27 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   2  Sparsity: 72.5244%\n",
      "layer   3  Sparsity: 69.2837%\n",
      "total_backward_count 283910 real_backward_count 34842  12.272%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.338415/  1.618202, val:  74.58%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0956%\n",
      "layer   2  Sparsity: 72.6084%\n",
      "layer   3  Sparsity: 68.8141%\n",
      "total_backward_count 293700 real_backward_count 35731  12.166%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.345658/  1.551636, val:  77.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   2  Sparsity: 72.2096%\n",
      "layer   3  Sparsity: 68.8420%\n",
      "total_backward_count 303490 real_backward_count 36613  12.064%\n",
      "fc layer 1 self.abs_max_out: 3535.0\n",
      "fc layer 3 self.abs_max_out: 626.0\n",
      "fc layer 3 self.abs_max_out: 640.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.312657/  1.618149, val:  70.83%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   2  Sparsity: 72.0263%\n",
      "layer   3  Sparsity: 68.6466%\n",
      "total_backward_count 313280 real_backward_count 37492  11.968%\n",
      "fc layer 3 self.abs_max_out: 664.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.313924/  1.549533, val:  75.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   2  Sparsity: 71.7448%\n",
      "layer   3  Sparsity: 68.4140%\n",
      "total_backward_count 323070 real_backward_count 38376  11.879%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.319004/  1.552027, val:  70.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   2  Sparsity: 72.1777%\n",
      "layer   3  Sparsity: 69.7572%\n",
      "total_backward_count 332860 real_backward_count 39263  11.796%\n",
      "lif layer 1 self.abs_max_v: 6163.5\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.312920/  1.551975, val:  65.83%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0631%\n",
      "layer   2  Sparsity: 72.0831%\n",
      "layer   3  Sparsity: 69.7040%\n",
      "total_backward_count 342650 real_backward_count 40159  11.720%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.303805/  1.517594, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0356%\n",
      "layer   2  Sparsity: 72.0934%\n",
      "layer   3  Sparsity: 69.6305%\n",
      "total_backward_count 352440 real_backward_count 41004  11.634%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.299474/  1.559271, val:  60.00%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1082%\n",
      "layer   2  Sparsity: 71.8872%\n",
      "layer   3  Sparsity: 68.4751%\n",
      "total_backward_count 362230 real_backward_count 41825  11.547%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.292873/  1.522871, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0316%\n",
      "layer   2  Sparsity: 71.7475%\n",
      "layer   3  Sparsity: 67.9066%\n",
      "total_backward_count 372020 real_backward_count 42666  11.469%\n",
      "fc layer 3 self.abs_max_out: 690.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.272020/  1.517007, val:  61.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   2  Sparsity: 71.7370%\n",
      "layer   3  Sparsity: 68.0664%\n",
      "total_backward_count 381810 real_backward_count 43459  11.382%\n",
      "lif layer 1 self.abs_max_v: 6240.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.265236/  1.516827, val:  70.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0585%\n",
      "layer   2  Sparsity: 71.7746%\n",
      "layer   3  Sparsity: 68.8150%\n",
      "total_backward_count 391600 real_backward_count 44273  11.306%\n",
      "fc layer 1 self.abs_max_out: 3564.0\n",
      "lif layer 1 self.abs_max_v: 6276.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.259177/  1.490589, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0939%\n",
      "layer   2  Sparsity: 72.1288%\n",
      "layer   3  Sparsity: 68.0286%\n",
      "total_backward_count 401390 real_backward_count 45045  11.222%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.252522/  1.477897, val:  82.08%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0395%\n",
      "layer   2  Sparsity: 71.8342%\n",
      "layer   3  Sparsity: 68.2041%\n",
      "total_backward_count 411180 real_backward_count 45847  11.150%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.235065/  1.486738, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   2  Sparsity: 71.7162%\n",
      "layer   3  Sparsity: 68.0552%\n",
      "total_backward_count 420970 real_backward_count 46595  11.068%\n",
      "fc layer 1 self.abs_max_out: 3591.0\n",
      "fc layer 1 self.abs_max_out: 3791.0\n",
      "fc layer 3 self.abs_max_out: 699.0\n",
      "fc layer 3 self.abs_max_out: 700.0\n",
      "fc layer 3 self.abs_max_out: 743.0\n",
      "fc layer 3 self.abs_max_out: 747.0\n",
      "fc layer 3 self.abs_max_out: 751.0\n",
      "lif layer 2 self.abs_max_v: 3097.5\n",
      "lif layer 1 self.abs_max_v: 6288.0\n",
      "lif layer 1 self.abs_max_v: 6570.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.234877/  1.479688, val:  71.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0418%\n",
      "layer   2  Sparsity: 71.6139%\n",
      "layer   3  Sparsity: 67.8804%\n",
      "total_backward_count 430760 real_backward_count 47336  10.989%\n",
      "fc layer 1 self.abs_max_out: 3834.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.250848/  1.493369, val:  74.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0723%\n",
      "layer   2  Sparsity: 71.8162%\n",
      "layer   3  Sparsity: 68.0061%\n",
      "total_backward_count 440550 real_backward_count 48097  10.917%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.240749/  1.485166, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   2  Sparsity: 71.6001%\n",
      "layer   3  Sparsity: 67.8539%\n",
      "total_backward_count 450340 real_backward_count 48851  10.848%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.215631/  1.445888, val:  80.42%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0993%\n",
      "layer   2  Sparsity: 71.7325%\n",
      "layer   3  Sparsity: 68.0283%\n",
      "total_backward_count 460130 real_backward_count 49527  10.764%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.201080/  1.436440, val:  75.83%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   2  Sparsity: 72.0924%\n",
      "layer   3  Sparsity: 68.6287%\n",
      "total_backward_count 469920 real_backward_count 50243  10.692%\n",
      "fc layer 1 self.abs_max_out: 3836.0\n",
      "lif layer 1 self.abs_max_v: 6733.5\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.193144/  1.418598, val:  78.33%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0758%\n",
      "layer   2  Sparsity: 71.5537%\n",
      "layer   3  Sparsity: 68.3050%\n",
      "total_backward_count 479710 real_backward_count 50960  10.623%\n",
      "fc layer 3 self.abs_max_out: 769.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.177275/  1.438631, val:  71.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1027%\n",
      "layer   2  Sparsity: 71.1893%\n",
      "layer   3  Sparsity: 68.6883%\n",
      "total_backward_count 489500 real_backward_count 51645  10.551%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.191619/  1.427053, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   2  Sparsity: 71.1418%\n",
      "layer   3  Sparsity: 68.4814%\n",
      "total_backward_count 499290 real_backward_count 52370  10.489%\n",
      "fc layer 2 self.abs_max_out: 2190.0\n",
      "lif layer 2 self.abs_max_v: 3107.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.195388/  1.427364, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0476%\n",
      "layer   2  Sparsity: 71.7050%\n",
      "layer   3  Sparsity: 68.4621%\n",
      "total_backward_count 509080 real_backward_count 53057  10.422%\n",
      "lif layer 2 self.abs_max_v: 3178.5\n",
      "lif layer 2 self.abs_max_v: 3231.5\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.184925/  1.444328, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0612%\n",
      "layer   2  Sparsity: 71.2837%\n",
      "layer   3  Sparsity: 67.8893%\n",
      "total_backward_count 518870 real_backward_count 53761  10.361%\n",
      "fc layer 2 self.abs_max_out: 2199.0\n",
      "fc layer 2 self.abs_max_out: 2201.0\n",
      "fc layer 2 self.abs_max_out: 2243.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.166013/  1.408314, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0442%\n",
      "layer   2  Sparsity: 71.1499%\n",
      "layer   3  Sparsity: 67.8356%\n",
      "total_backward_count 528660 real_backward_count 54447  10.299%\n",
      "lif layer 2 self.abs_max_v: 3335.5\n",
      "fc layer 2 self.abs_max_out: 2255.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.163118/  1.424128, val:  67.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0830%\n",
      "layer   2  Sparsity: 71.5064%\n",
      "layer   3  Sparsity: 68.0613%\n",
      "total_backward_count 538450 real_backward_count 55088  10.231%\n",
      "fc layer 2 self.abs_max_out: 2301.0\n",
      "fc layer 1 self.abs_max_out: 3849.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.155612/  1.397133, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   2  Sparsity: 71.4826%\n",
      "layer   3  Sparsity: 67.8484%\n",
      "total_backward_count 548240 real_backward_count 55759  10.171%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.166497/  1.418069, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.31 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0774%\n",
      "layer   2  Sparsity: 71.2486%\n",
      "layer   3  Sparsity: 67.2961%\n",
      "total_backward_count 558030 real_backward_count 56410  10.109%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.163797/  1.391080, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0431%\n",
      "layer   2  Sparsity: 71.3297%\n",
      "layer   3  Sparsity: 68.2101%\n",
      "total_backward_count 567820 real_backward_count 57068  10.050%\n",
      "fc layer 2 self.abs_max_out: 2307.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.150932/  1.377801, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   2  Sparsity: 71.6868%\n",
      "layer   3  Sparsity: 68.3567%\n",
      "total_backward_count 577610 real_backward_count 57745   9.997%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.124032/  1.358424, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0703%\n",
      "layer   2  Sparsity: 71.3287%\n",
      "layer   3  Sparsity: 68.2471%\n",
      "total_backward_count 587400 real_backward_count 58401   9.942%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.144544/  1.369832, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0316%\n",
      "layer   2  Sparsity: 71.1222%\n",
      "layer   3  Sparsity: 67.9813%\n",
      "total_backward_count 597190 real_backward_count 59088   9.894%\n",
      "fc layer 2 self.abs_max_out: 2398.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.140381/  1.423995, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0594%\n",
      "layer   2  Sparsity: 71.2662%\n",
      "layer   3  Sparsity: 68.8457%\n",
      "total_backward_count 606980 real_backward_count 59756   9.845%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.159169/  1.389220, val:  77.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0579%\n",
      "layer   2  Sparsity: 70.8199%\n",
      "layer   3  Sparsity: 69.0195%\n",
      "total_backward_count 616770 real_backward_count 60343   9.784%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.142817/  1.389347, val:  80.00%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   2  Sparsity: 70.6532%\n",
      "layer   3  Sparsity: 68.8897%\n",
      "total_backward_count 626560 real_backward_count 60925   9.724%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.136383/  1.361612, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0116%\n",
      "layer   2  Sparsity: 71.2154%\n",
      "layer   3  Sparsity: 69.0922%\n",
      "total_backward_count 636350 real_backward_count 61506   9.665%\n",
      "fc layer 3 self.abs_max_out: 771.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.128223/  1.358855, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.21 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   2  Sparsity: 71.0829%\n",
      "layer   3  Sparsity: 69.2891%\n",
      "total_backward_count 646140 real_backward_count 62074   9.607%\n",
      "fc layer 3 self.abs_max_out: 794.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.148652/  1.393111, val:  80.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0630%\n",
      "layer   2  Sparsity: 70.8980%\n",
      "layer   3  Sparsity: 69.0198%\n",
      "total_backward_count 655930 real_backward_count 62656   9.552%\n",
      "fc layer 3 self.abs_max_out: 796.0\n",
      "fc layer 3 self.abs_max_out: 803.0\n",
      "fc layer 1 self.abs_max_out: 3866.0\n",
      "lif layer 1 self.abs_max_v: 6753.0\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.118114/  1.414882, val:  75.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   2  Sparsity: 70.9020%\n",
      "layer   3  Sparsity: 69.0932%\n",
      "total_backward_count 665720 real_backward_count 63264   9.503%\n",
      "fc layer 2 self.abs_max_out: 2411.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.122541/  1.375907, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   2  Sparsity: 71.3205%\n",
      "layer   3  Sparsity: 69.2404%\n",
      "total_backward_count 675510 real_backward_count 63838   9.450%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.122244/  1.382052, val:  78.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0509%\n",
      "layer   2  Sparsity: 71.1487%\n",
      "layer   3  Sparsity: 68.9370%\n",
      "total_backward_count 685300 real_backward_count 64422   9.401%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.130669/  1.378224, val:  79.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0891%\n",
      "layer   2  Sparsity: 71.1932%\n",
      "layer   3  Sparsity: 69.0152%\n",
      "total_backward_count 695090 real_backward_count 64999   9.351%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.131629/  1.358801, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0563%\n",
      "layer   2  Sparsity: 71.1144%\n",
      "layer   3  Sparsity: 68.9982%\n",
      "total_backward_count 704880 real_backward_count 65604   9.307%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.121557/  1.343292, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   2  Sparsity: 71.1462%\n",
      "layer   3  Sparsity: 68.9183%\n",
      "total_backward_count 714670 real_backward_count 66144   9.255%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.126400/  1.416074, val:  70.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   2  Sparsity: 70.8140%\n",
      "layer   3  Sparsity: 68.7870%\n",
      "total_backward_count 724460 real_backward_count 66696   9.206%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.110380/  1.394049, val:  69.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   2  Sparsity: 70.9333%\n",
      "layer   3  Sparsity: 69.1238%\n",
      "total_backward_count 734250 real_backward_count 67293   9.165%\n",
      "fc layer 2 self.abs_max_out: 2419.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.113451/  1.365693, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 70.8589%\n",
      "layer   3  Sparsity: 69.1098%\n",
      "total_backward_count 744040 real_backward_count 67824   9.116%\n",
      "fc layer 1 self.abs_max_out: 3917.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.119532/  1.374376, val:  84.58%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   2  Sparsity: 70.9982%\n",
      "layer   3  Sparsity: 68.8319%\n",
      "total_backward_count 753830 real_backward_count 68385   9.072%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.116490/  1.351465, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1108%\n",
      "layer   2  Sparsity: 71.0977%\n",
      "layer   3  Sparsity: 68.8596%\n",
      "total_backward_count 763620 real_backward_count 68904   9.023%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.113299/  1.371757, val:  79.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0917%\n",
      "layer   2  Sparsity: 70.9838%\n",
      "layer   3  Sparsity: 69.1349%\n",
      "total_backward_count 773410 real_backward_count 69443   8.979%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.116759/  1.363209, val:  80.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   2  Sparsity: 71.0281%\n",
      "layer   3  Sparsity: 69.2238%\n",
      "total_backward_count 783200 real_backward_count 69984   8.936%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.112138/  1.386195, val:  77.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0842%\n",
      "layer   2  Sparsity: 70.9179%\n",
      "layer   3  Sparsity: 69.3407%\n",
      "total_backward_count 792990 real_backward_count 70481   8.888%\n",
      "fc layer 1 self.abs_max_out: 4011.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.119927/  1.389945, val:  75.42%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   2  Sparsity: 70.8385%\n",
      "layer   3  Sparsity: 69.2019%\n",
      "total_backward_count 802780 real_backward_count 70989   8.843%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.098427/  1.353776, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0418%\n",
      "layer   2  Sparsity: 70.6156%\n",
      "layer   3  Sparsity: 69.2006%\n",
      "total_backward_count 812570 real_backward_count 71513   8.801%\n",
      "fc layer 2 self.abs_max_out: 2443.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.096655/  1.341866, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0951%\n",
      "layer   2  Sparsity: 70.6818%\n",
      "layer   3  Sparsity: 69.3100%\n",
      "total_backward_count 822360 real_backward_count 72017   8.757%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.088693/  1.384406, val:  77.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1128%\n",
      "layer   2  Sparsity: 70.8043%\n",
      "layer   3  Sparsity: 69.7388%\n",
      "total_backward_count 832150 real_backward_count 72544   8.718%\n",
      "fc layer 1 self.abs_max_out: 4023.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.088785/  1.342451, val:  85.42%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0600%\n",
      "layer   2  Sparsity: 70.7741%\n",
      "layer   3  Sparsity: 69.7569%\n",
      "total_backward_count 841940 real_backward_count 73028   8.674%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.091195/  1.337582, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0963%\n",
      "layer   2  Sparsity: 70.5617%\n",
      "layer   3  Sparsity: 69.4104%\n",
      "total_backward_count 851730 real_backward_count 73481   8.627%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.081218/  1.328860, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0384%\n",
      "layer   2  Sparsity: 70.7952%\n",
      "layer   3  Sparsity: 69.7749%\n",
      "total_backward_count 861520 real_backward_count 73986   8.588%\n",
      "fc layer 1 self.abs_max_out: 4061.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.088391/  1.315693, val:  85.83%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0715%\n",
      "layer   2  Sparsity: 70.8960%\n",
      "layer   3  Sparsity: 69.4920%\n",
      "total_backward_count 871310 real_backward_count 74450   8.545%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.073964/  1.324374, val:  83.75%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0567%\n",
      "layer   2  Sparsity: 70.6658%\n",
      "layer   3  Sparsity: 69.3417%\n",
      "total_backward_count 881100 real_backward_count 74935   8.505%\n",
      "lif layer 1 self.abs_max_v: 6799.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.075290/  1.315825, val:  87.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 70.3166%\n",
      "layer   3  Sparsity: 69.2705%\n",
      "total_backward_count 890890 real_backward_count 75394   8.463%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.066285/  1.324000, val:  80.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0336%\n",
      "layer   2  Sparsity: 70.1104%\n",
      "layer   3  Sparsity: 69.2870%\n",
      "total_backward_count 900680 real_backward_count 75874   8.424%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.068393/  1.304697, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   2  Sparsity: 70.6578%\n",
      "layer   3  Sparsity: 69.7219%\n",
      "total_backward_count 910470 real_backward_count 76319   8.382%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.073730/  1.297729, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   2  Sparsity: 70.7547%\n",
      "layer   3  Sparsity: 69.9700%\n",
      "total_backward_count 920260 real_backward_count 76786   8.344%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.063861/  1.322387, val:  85.83%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0955%\n",
      "layer   2  Sparsity: 70.4886%\n",
      "layer   3  Sparsity: 69.9174%\n",
      "total_backward_count 930050 real_backward_count 77242   8.305%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.079950/  1.324857, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0937%\n",
      "layer   2  Sparsity: 70.5571%\n",
      "layer   3  Sparsity: 69.6105%\n",
      "total_backward_count 939840 real_backward_count 77739   8.272%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.066720/  1.299939, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   2  Sparsity: 70.8126%\n",
      "layer   3  Sparsity: 69.7850%\n",
      "total_backward_count 949630 real_backward_count 78167   8.231%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.045580/  1.334933, val:  77.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   2  Sparsity: 70.4570%\n",
      "layer   3  Sparsity: 69.4908%\n",
      "total_backward_count 959420 real_backward_count 78588   8.191%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.055272/  1.299488, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 70.2567%\n",
      "layer   3  Sparsity: 69.1232%\n",
      "total_backward_count 969210 real_backward_count 79020   8.153%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.068904/  1.353310, val:  71.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0821%\n",
      "layer   2  Sparsity: 70.6434%\n",
      "layer   3  Sparsity: 69.5662%\n",
      "total_backward_count 979000 real_backward_count 79456   8.116%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.075410/  1.300116, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0859%\n",
      "layer   2  Sparsity: 70.2465%\n",
      "layer   3  Sparsity: 69.4636%\n",
      "total_backward_count 988790 real_backward_count 79892   8.080%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.059242/  1.311203, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0880%\n",
      "layer   2  Sparsity: 70.3529%\n",
      "layer   3  Sparsity: 69.2094%\n",
      "total_backward_count 998580 real_backward_count 80359   8.047%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.063420/  1.321707, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1180%\n",
      "layer   2  Sparsity: 70.5552%\n",
      "layer   3  Sparsity: 69.6034%\n",
      "total_backward_count 1008370 real_backward_count 80736   8.007%\n",
      "fc layer 1 self.abs_max_out: 4095.0\n",
      "lif layer 1 self.abs_max_v: 6826.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.057037/  1.308945, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1289%\n",
      "layer   2  Sparsity: 70.5534%\n",
      "layer   3  Sparsity: 69.1490%\n",
      "total_backward_count 1018160 real_backward_count 81127   7.968%\n",
      "fc layer 1 self.abs_max_out: 4131.0\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.046865/  1.294822, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0924%\n",
      "layer   2  Sparsity: 70.5635%\n",
      "layer   3  Sparsity: 69.3202%\n",
      "total_backward_count 1027950 real_backward_count 81537   7.932%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.041374/  1.314309, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   2  Sparsity: 70.6978%\n",
      "layer   3  Sparsity: 69.7908%\n",
      "total_backward_count 1037740 real_backward_count 81908   7.893%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.049009/  1.297512, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0854%\n",
      "layer   2  Sparsity: 70.8178%\n",
      "layer   3  Sparsity: 69.7480%\n",
      "total_backward_count 1047530 real_backward_count 82303   7.857%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.047364/  1.310228, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1076%\n",
      "layer   2  Sparsity: 70.8775%\n",
      "layer   3  Sparsity: 69.7071%\n",
      "total_backward_count 1057320 real_backward_count 82704   7.822%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.037660/  1.289982, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0886%\n",
      "layer   2  Sparsity: 70.5891%\n",
      "layer   3  Sparsity: 70.0393%\n",
      "total_backward_count 1067110 real_backward_count 83118   7.789%\n",
      "fc layer 3 self.abs_max_out: 804.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.041999/  1.299445, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0721%\n",
      "layer   2  Sparsity: 70.5018%\n",
      "layer   3  Sparsity: 69.9707%\n",
      "total_backward_count 1076900 real_backward_count 83507   7.754%\n",
      "fc layer 1 self.abs_max_out: 4144.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.035821/  1.268823, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0512%\n",
      "layer   2  Sparsity: 70.6572%\n",
      "layer   3  Sparsity: 70.0103%\n",
      "total_backward_count 1086690 real_backward_count 83926   7.723%\n",
      "fc layer 1 self.abs_max_out: 4232.0\n",
      "lif layer 1 self.abs_max_v: 6845.0\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.035664/  1.287100, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0636%\n",
      "layer   2  Sparsity: 70.5539%\n",
      "layer   3  Sparsity: 70.1710%\n",
      "total_backward_count 1096480 real_backward_count 84316   7.690%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.022929/  1.292148, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   2  Sparsity: 70.6888%\n",
      "layer   3  Sparsity: 70.0224%\n",
      "total_backward_count 1106270 real_backward_count 84731   7.659%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.030373/  1.285377, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1217%\n",
      "layer   2  Sparsity: 70.6429%\n",
      "layer   3  Sparsity: 69.8322%\n",
      "total_backward_count 1116060 real_backward_count 85094   7.625%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.025725/  1.295883, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0894%\n",
      "layer   2  Sparsity: 70.3576%\n",
      "layer   3  Sparsity: 70.1313%\n",
      "total_backward_count 1125850 real_backward_count 85504   7.595%\n",
      "fc layer 3 self.abs_max_out: 828.0\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.006759/  1.270129, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1036%\n",
      "layer   2  Sparsity: 70.4474%\n",
      "layer   3  Sparsity: 70.0814%\n",
      "total_backward_count 1135640 real_backward_count 85866   7.561%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  0.993605/  1.254954, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   2  Sparsity: 70.5664%\n",
      "layer   3  Sparsity: 70.1788%\n",
      "total_backward_count 1145430 real_backward_count 86210   7.526%\n",
      "lif layer 1 self.abs_max_v: 7099.0\n",
      "lif layer 1 self.abs_max_v: 7099.5\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  0.992838/  1.278984, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1082%\n",
      "layer   2  Sparsity: 70.2791%\n",
      "layer   3  Sparsity: 69.9024%\n",
      "total_backward_count 1155220 real_backward_count 86559   7.493%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  0.994039/  1.278323, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.01 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   2  Sparsity: 70.4650%\n",
      "layer   3  Sparsity: 70.0150%\n",
      "total_backward_count 1165010 real_backward_count 86907   7.460%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  0.998138/  1.292819, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0976%\n",
      "layer   2  Sparsity: 70.2963%\n",
      "layer   3  Sparsity: 69.8635%\n",
      "total_backward_count 1174800 real_backward_count 87249   7.427%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.017468/  1.277435, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 70.3518%\n",
      "layer   3  Sparsity: 70.1266%\n",
      "total_backward_count 1184590 real_backward_count 87596   7.395%\n",
      "fc layer 1 self.abs_max_out: 4248.0\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.001106/  1.264031, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0671%\n",
      "layer   2  Sparsity: 70.3108%\n",
      "layer   3  Sparsity: 70.1812%\n",
      "total_backward_count 1194380 real_backward_count 87943   7.363%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.008264/  1.270373, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0963%\n",
      "layer   2  Sparsity: 70.3036%\n",
      "layer   3  Sparsity: 70.5086%\n",
      "total_backward_count 1204170 real_backward_count 88305   7.333%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.013415/  1.281249, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0974%\n",
      "layer   2  Sparsity: 70.2952%\n",
      "layer   3  Sparsity: 70.5743%\n",
      "total_backward_count 1213960 real_backward_count 88678   7.305%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.013793/  1.297079, val:  80.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0509%\n",
      "layer   2  Sparsity: 70.5601%\n",
      "layer   3  Sparsity: 70.2781%\n",
      "total_backward_count 1223750 real_backward_count 89018   7.274%\n",
      "fc layer 1 self.abs_max_out: 4321.0\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.016411/  1.303209, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1090%\n",
      "layer   2  Sparsity: 70.4304%\n",
      "layer   3  Sparsity: 70.3216%\n",
      "total_backward_count 1233540 real_backward_count 89383   7.246%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.031820/  1.294648, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1140%\n",
      "layer   2  Sparsity: 70.3747%\n",
      "layer   3  Sparsity: 70.3142%\n",
      "total_backward_count 1243330 real_backward_count 89730   7.217%\n",
      "fc layer 1 self.abs_max_out: 4348.0\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.026820/  1.273877, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   2  Sparsity: 70.3353%\n",
      "layer   3  Sparsity: 69.8197%\n",
      "total_backward_count 1253120 real_backward_count 90074   7.188%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.006882/  1.296472, val:  77.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0731%\n",
      "layer   2  Sparsity: 70.6324%\n",
      "layer   3  Sparsity: 69.9935%\n",
      "total_backward_count 1262910 real_backward_count 90406   7.159%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  0.994437/  1.268115, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   2  Sparsity: 70.2519%\n",
      "layer   3  Sparsity: 70.0224%\n",
      "total_backward_count 1272700 real_backward_count 90714   7.128%\n",
      "fc layer 1 self.abs_max_out: 4366.0\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  0.989052/  1.256347, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1104%\n",
      "layer   2  Sparsity: 70.5343%\n",
      "layer   3  Sparsity: 70.0319%\n",
      "total_backward_count 1282490 real_backward_count 91015   7.097%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  0.976107/  1.256115, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0477%\n",
      "layer   2  Sparsity: 70.4658%\n",
      "layer   3  Sparsity: 69.9562%\n",
      "total_backward_count 1292280 real_backward_count 91349   7.069%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  0.984995/  1.239870, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   2  Sparsity: 70.3802%\n",
      "layer   3  Sparsity: 70.0879%\n",
      "total_backward_count 1302070 real_backward_count 91682   7.041%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  0.981486/  1.252276, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   2  Sparsity: 70.4114%\n",
      "layer   3  Sparsity: 69.9960%\n",
      "total_backward_count 1311860 real_backward_count 91991   7.012%\n",
      "fc layer 1 self.abs_max_out: 4381.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  0.974307/  1.249862, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   2  Sparsity: 70.1823%\n",
      "layer   3  Sparsity: 69.9043%\n",
      "total_backward_count 1321650 real_backward_count 92310   6.984%\n",
      "fc layer 1 self.abs_max_out: 4413.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  0.975254/  1.222320, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   2  Sparsity: 70.3598%\n",
      "layer   3  Sparsity: 70.4119%\n",
      "total_backward_count 1331440 real_backward_count 92631   6.957%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  0.963347/  1.211554, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   2  Sparsity: 70.3696%\n",
      "layer   3  Sparsity: 70.5441%\n",
      "total_backward_count 1341230 real_backward_count 92950   6.930%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  0.959366/  1.235536, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0600%\n",
      "layer   2  Sparsity: 70.2313%\n",
      "layer   3  Sparsity: 70.3889%\n",
      "total_backward_count 1351020 real_backward_count 93256   6.903%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  0.983414/  1.246030, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   2  Sparsity: 70.1770%\n",
      "layer   3  Sparsity: 70.1360%\n",
      "total_backward_count 1360810 real_backward_count 93557   6.875%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  0.980421/  1.268929, val:  82.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   2  Sparsity: 70.1219%\n",
      "layer   3  Sparsity: 70.1331%\n",
      "total_backward_count 1370600 real_backward_count 93847   6.847%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  0.977602/  1.246141, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0863%\n",
      "layer   2  Sparsity: 70.3727%\n",
      "layer   3  Sparsity: 70.1322%\n",
      "total_backward_count 1380390 real_backward_count 94166   6.822%\n",
      "fc layer 3 self.abs_max_out: 882.0\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  0.964774/  1.237105, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   2  Sparsity: 70.3628%\n",
      "layer   3  Sparsity: 69.9163%\n",
      "total_backward_count 1390180 real_backward_count 94464   6.795%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  0.965901/  1.233209, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0539%\n",
      "layer   2  Sparsity: 70.4324%\n",
      "layer   3  Sparsity: 69.7810%\n",
      "total_backward_count 1399970 real_backward_count 94756   6.768%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  0.973387/  1.267346, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1025%\n",
      "layer   2  Sparsity: 70.2795%\n",
      "layer   3  Sparsity: 69.5132%\n",
      "total_backward_count 1409760 real_backward_count 95068   6.744%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  0.967900/  1.254717, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 70.2217%\n",
      "layer   3  Sparsity: 69.4772%\n",
      "total_backward_count 1419550 real_backward_count 95335   6.716%\n",
      "fc layer 1 self.abs_max_out: 4452.0\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  0.965828/  1.221923, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0543%\n",
      "layer   2  Sparsity: 70.3491%\n",
      "layer   3  Sparsity: 69.9299%\n",
      "total_backward_count 1429340 real_backward_count 95651   6.692%\n",
      "fc layer 1 self.abs_max_out: 4475.0\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  0.940784/  1.207869, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0392%\n",
      "layer   2  Sparsity: 70.2788%\n",
      "layer   3  Sparsity: 70.3847%\n",
      "total_backward_count 1439130 real_backward_count 95914   6.665%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  0.939388/  1.224619, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0484%\n",
      "layer   2  Sparsity: 70.3878%\n",
      "layer   3  Sparsity: 70.5063%\n",
      "total_backward_count 1448920 real_backward_count 96235   6.642%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  0.944824/  1.220436, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.95 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1083%\n",
      "layer   2  Sparsity: 70.2148%\n",
      "layer   3  Sparsity: 70.2789%\n",
      "total_backward_count 1458710 real_backward_count 96532   6.618%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  0.947184/  1.208775, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0773%\n",
      "layer   2  Sparsity: 70.1674%\n",
      "layer   3  Sparsity: 69.9840%\n",
      "total_backward_count 1468500 real_backward_count 96819   6.593%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  0.933868/  1.210032, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.29 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   2  Sparsity: 70.3586%\n",
      "layer   3  Sparsity: 70.2945%\n",
      "total_backward_count 1478290 real_backward_count 97093   6.568%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  0.944021/  1.213503, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0598%\n",
      "layer   2  Sparsity: 70.2971%\n",
      "layer   3  Sparsity: 70.0608%\n",
      "total_backward_count 1488080 real_backward_count 97385   6.544%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  0.956299/  1.217867, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   2  Sparsity: 70.2311%\n",
      "layer   3  Sparsity: 69.9956%\n",
      "total_backward_count 1497870 real_backward_count 97687   6.522%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  0.936293/  1.220321, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   2  Sparsity: 70.3148%\n",
      "layer   3  Sparsity: 70.0943%\n",
      "total_backward_count 1507660 real_backward_count 97984   6.499%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  0.935189/  1.241284, val:  79.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0969%\n",
      "layer   2  Sparsity: 70.2135%\n",
      "layer   3  Sparsity: 69.7545%\n",
      "total_backward_count 1517450 real_backward_count 98296   6.478%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  0.936802/  1.230642, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0743%\n",
      "layer   2  Sparsity: 70.2389%\n",
      "layer   3  Sparsity: 69.8931%\n",
      "total_backward_count 1527240 real_backward_count 98545   6.452%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  0.945369/  1.223258, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   2  Sparsity: 70.0660%\n",
      "layer   3  Sparsity: 69.7087%\n",
      "total_backward_count 1537030 real_backward_count 98836   6.430%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  0.939126/  1.219647, val:  81.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0642%\n",
      "layer   2  Sparsity: 69.7689%\n",
      "layer   3  Sparsity: 69.5338%\n",
      "total_backward_count 1546820 real_backward_count 99110   6.407%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  0.945038/  1.208817, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1199%\n",
      "layer   2  Sparsity: 70.0957%\n",
      "layer   3  Sparsity: 69.6410%\n",
      "total_backward_count 1556610 real_backward_count 99381   6.384%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  0.942911/  1.205321, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0371%\n",
      "layer   2  Sparsity: 70.2581%\n",
      "layer   3  Sparsity: 70.0214%\n",
      "total_backward_count 1566400 real_backward_count 99646   6.361%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  0.935490/  1.202808, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0983%\n",
      "layer   2  Sparsity: 70.1865%\n",
      "layer   3  Sparsity: 70.1889%\n",
      "total_backward_count 1576190 real_backward_count 99917   6.339%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  0.929804/  1.198967, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0935%\n",
      "layer   2  Sparsity: 70.0836%\n",
      "layer   3  Sparsity: 70.3769%\n",
      "total_backward_count 1585980 real_backward_count 100195   6.318%\n",
      "fc layer 1 self.abs_max_out: 4524.0\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  0.922450/  1.197172, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.21 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0891%\n",
      "layer   2  Sparsity: 70.3840%\n",
      "layer   3  Sparsity: 70.2888%\n",
      "total_backward_count 1595770 real_backward_count 100470   6.296%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  0.922253/  1.173724, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0889%\n",
      "layer   2  Sparsity: 70.4210%\n",
      "layer   3  Sparsity: 70.3021%\n",
      "total_backward_count 1605560 real_backward_count 100727   6.274%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  0.924305/  1.177243, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0614%\n",
      "layer   2  Sparsity: 70.3228%\n",
      "layer   3  Sparsity: 70.6679%\n",
      "total_backward_count 1615350 real_backward_count 101069   6.257%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  0.927046/  1.178143, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1107%\n",
      "layer   2  Sparsity: 70.4657%\n",
      "layer   3  Sparsity: 70.8039%\n",
      "total_backward_count 1625140 real_backward_count 101320   6.235%\n",
      "fc layer 3 self.abs_max_out: 891.0\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  0.938640/  1.194515, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   2  Sparsity: 70.2139%\n",
      "layer   3  Sparsity: 70.3938%\n",
      "total_backward_count 1634930 real_backward_count 101572   6.213%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  0.929273/  1.191137, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1189%\n",
      "layer   2  Sparsity: 70.2083%\n",
      "layer   3  Sparsity: 70.1747%\n",
      "total_backward_count 1644720 real_backward_count 101854   6.193%\n",
      "lif layer 1 self.abs_max_v: 7199.0\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  0.926345/  1.165079, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0865%\n",
      "layer   2  Sparsity: 70.0255%\n",
      "layer   3  Sparsity: 70.8009%\n",
      "total_backward_count 1654510 real_backward_count 102139   6.173%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  0.923465/  1.198883, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1016%\n",
      "layer   2  Sparsity: 69.9818%\n",
      "layer   3  Sparsity: 70.9835%\n",
      "total_backward_count 1664300 real_backward_count 102391   6.152%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  0.930327/  1.215312, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0547%\n",
      "layer   2  Sparsity: 70.0953%\n",
      "layer   3  Sparsity: 70.9269%\n",
      "total_backward_count 1674090 real_backward_count 102642   6.131%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  0.942345/  1.240107, val:  83.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0741%\n",
      "layer   2  Sparsity: 70.2018%\n",
      "layer   3  Sparsity: 70.5884%\n",
      "total_backward_count 1683880 real_backward_count 102908   6.111%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  0.945844/  1.206634, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1097%\n",
      "layer   2  Sparsity: 70.2910%\n",
      "layer   3  Sparsity: 70.7348%\n",
      "total_backward_count 1693670 real_backward_count 103176   6.092%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  0.937976/  1.210296, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   2  Sparsity: 70.0895%\n",
      "layer   3  Sparsity: 71.1920%\n",
      "total_backward_count 1703460 real_backward_count 103417   6.071%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  0.932303/  1.190494, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0720%\n",
      "layer   2  Sparsity: 70.0031%\n",
      "layer   3  Sparsity: 71.0689%\n",
      "total_backward_count 1713250 real_backward_count 103669   6.051%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  0.915935/  1.180831, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   2  Sparsity: 70.1017%\n",
      "layer   3  Sparsity: 70.8002%\n",
      "total_backward_count 1723040 real_backward_count 103902   6.030%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  0.912228/  1.180962, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   2  Sparsity: 70.3276%\n",
      "layer   3  Sparsity: 71.0658%\n",
      "total_backward_count 1732830 real_backward_count 104138   6.010%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  0.918678/  1.178007, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0997%\n",
      "layer   2  Sparsity: 70.1500%\n",
      "layer   3  Sparsity: 70.8366%\n",
      "total_backward_count 1742620 real_backward_count 104361   5.989%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  0.913246/  1.180495, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1270%\n",
      "layer   2  Sparsity: 70.0848%\n",
      "layer   3  Sparsity: 71.0368%\n",
      "total_backward_count 1752410 real_backward_count 104594   5.969%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  0.923516/  1.201558, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0482%\n",
      "layer   2  Sparsity: 69.9945%\n",
      "layer   3  Sparsity: 71.1862%\n",
      "total_backward_count 1762200 real_backward_count 104828   5.949%\n",
      "fc layer 1 self.abs_max_out: 4535.0\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  0.924166/  1.199488, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0824%\n",
      "layer   2  Sparsity: 70.3870%\n",
      "layer   3  Sparsity: 71.4189%\n",
      "total_backward_count 1771990 real_backward_count 105035   5.928%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  0.933589/  1.199133, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   2  Sparsity: 70.2790%\n",
      "layer   3  Sparsity: 71.1015%\n",
      "total_backward_count 1781780 real_backward_count 105261   5.908%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  0.928165/  1.211367, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0681%\n",
      "layer   2  Sparsity: 70.3208%\n",
      "layer   3  Sparsity: 70.9276%\n",
      "total_backward_count 1791570 real_backward_count 105475   5.887%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  0.933663/  1.202894, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0820%\n",
      "layer   2  Sparsity: 70.3098%\n",
      "layer   3  Sparsity: 71.3508%\n",
      "total_backward_count 1801360 real_backward_count 105693   5.867%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  0.933880/  1.205467, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0454%\n",
      "layer   2  Sparsity: 70.2881%\n",
      "layer   3  Sparsity: 71.3467%\n",
      "total_backward_count 1811150 real_backward_count 105896   5.847%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  0.928562/  1.204738, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   2  Sparsity: 70.2222%\n",
      "layer   3  Sparsity: 71.5034%\n",
      "total_backward_count 1820940 real_backward_count 106120   5.828%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  0.920242/  1.188943, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   2  Sparsity: 70.3853%\n",
      "layer   3  Sparsity: 71.1265%\n",
      "total_backward_count 1830730 real_backward_count 106351   5.809%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  0.916561/  1.178310, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0431%\n",
      "layer   2  Sparsity: 70.1797%\n",
      "layer   3  Sparsity: 70.8639%\n",
      "total_backward_count 1840520 real_backward_count 106570   5.790%\n",
      "lif layer 2 self.abs_max_v: 3364.0\n",
      "lif layer 2 self.abs_max_v: 3421.5\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  0.910748/  1.190004, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   2  Sparsity: 70.2891%\n",
      "layer   3  Sparsity: 71.3001%\n",
      "total_backward_count 1850310 real_backward_count 106794   5.772%\n",
      "fc layer 1 self.abs_max_out: 4560.0\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  0.909248/  1.180627, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0960%\n",
      "layer   2  Sparsity: 70.2677%\n",
      "layer   3  Sparsity: 71.1506%\n",
      "total_backward_count 1860100 real_backward_count 107000   5.752%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  0.910672/  1.173181, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0753%\n",
      "layer   2  Sparsity: 70.0610%\n",
      "layer   3  Sparsity: 71.0801%\n",
      "total_backward_count 1869890 real_backward_count 107187   5.732%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  0.899746/  1.158794, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0516%\n",
      "layer   2  Sparsity: 70.3249%\n",
      "layer   3  Sparsity: 71.2527%\n",
      "total_backward_count 1879680 real_backward_count 107405   5.714%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  0.894961/  1.180732, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.37 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   2  Sparsity: 70.3653%\n",
      "layer   3  Sparsity: 71.5043%\n",
      "total_backward_count 1889470 real_backward_count 107614   5.695%\n",
      "fc layer 1 self.abs_max_out: 4571.0\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  0.901013/  1.192372, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0686%\n",
      "layer   2  Sparsity: 70.1754%\n",
      "layer   3  Sparsity: 71.4796%\n",
      "total_backward_count 1899260 real_backward_count 107841   5.678%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  0.901446/  1.180354, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0861%\n",
      "layer   2  Sparsity: 70.2806%\n",
      "layer   3  Sparsity: 71.4745%\n",
      "total_backward_count 1909050 real_backward_count 108044   5.660%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  0.907851/  1.183832, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0549%\n",
      "layer   2  Sparsity: 69.9636%\n",
      "layer   3  Sparsity: 71.2527%\n",
      "total_backward_count 1918840 real_backward_count 108252   5.642%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  0.906204/  1.183668, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   2  Sparsity: 69.9406%\n",
      "layer   3  Sparsity: 71.0084%\n",
      "total_backward_count 1928630 real_backward_count 108456   5.623%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  0.917109/  1.206061, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.26 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0532%\n",
      "layer   2  Sparsity: 69.6288%\n",
      "layer   3  Sparsity: 70.9457%\n",
      "total_backward_count 1938420 real_backward_count 108651   5.605%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  0.915341/  1.192750, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1017%\n",
      "layer   2  Sparsity: 69.7973%\n",
      "layer   3  Sparsity: 70.8151%\n",
      "total_backward_count 1948210 real_backward_count 108870   5.588%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  0.907711/  1.170202, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.16 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0909%\n",
      "layer   2  Sparsity: 70.1575%\n",
      "layer   3  Sparsity: 70.9397%\n",
      "total_backward_count 1958000 real_backward_count 109077   5.571%\n",
      "epoch-200 lr=['0.0019531'], tr/val_loss:  0.911945/  1.157520, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0963%\n",
      "layer   2  Sparsity: 70.0062%\n",
      "layer   3  Sparsity: 70.8185%\n",
      "total_backward_count 1967790 real_backward_count 109274   5.553%\n",
      "epoch-201 lr=['0.0019531'], tr/val_loss:  0.900186/  1.157233, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.07 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1029%\n",
      "layer   2  Sparsity: 70.1779%\n",
      "layer   3  Sparsity: 70.4868%\n",
      "total_backward_count 1977580 real_backward_count 109455   5.535%\n",
      "fc layer 1 self.abs_max_out: 4581.0\n",
      "epoch-202 lr=['0.0019531'], tr/val_loss:  0.907259/  1.178498, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   2  Sparsity: 70.4780%\n",
      "layer   3  Sparsity: 70.6548%\n",
      "total_backward_count 1987370 real_backward_count 109659   5.518%\n",
      "fc layer 1 self.abs_max_out: 4584.0\n",
      "epoch-203 lr=['0.0019531'], tr/val_loss:  0.898076/  1.182483, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1144%\n",
      "layer   2  Sparsity: 70.1661%\n",
      "layer   3  Sparsity: 70.9066%\n",
      "total_backward_count 1997160 real_backward_count 109880   5.502%\n",
      "epoch-204 lr=['0.0019531'], tr/val_loss:  0.901494/  1.181217, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0659%\n",
      "layer   2  Sparsity: 70.0144%\n",
      "layer   3  Sparsity: 71.1272%\n",
      "total_backward_count 2006950 real_backward_count 110058   5.484%\n",
      "epoch-205 lr=['0.0019531'], tr/val_loss:  0.889684/  1.181515, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0575%\n",
      "layer   2  Sparsity: 70.0468%\n",
      "layer   3  Sparsity: 70.9250%\n",
      "total_backward_count 2016740 real_backward_count 110248   5.467%\n",
      "epoch-206 lr=['0.0019531'], tr/val_loss:  0.902703/  1.188704, val:  85.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0489%\n",
      "layer   2  Sparsity: 70.0638%\n",
      "layer   3  Sparsity: 70.9574%\n",
      "total_backward_count 2026530 real_backward_count 110433   5.449%\n",
      "epoch-207 lr=['0.0019531'], tr/val_loss:  0.892924/  1.174843, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   2  Sparsity: 69.8689%\n",
      "layer   3  Sparsity: 71.0564%\n",
      "total_backward_count 2036320 real_backward_count 110599   5.431%\n",
      "epoch-208 lr=['0.0019531'], tr/val_loss:  0.889259/  1.165110, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0227%\n",
      "layer   2  Sparsity: 69.8198%\n",
      "layer   3  Sparsity: 70.8864%\n",
      "total_backward_count 2046110 real_backward_count 110786   5.414%\n",
      "epoch-209 lr=['0.0019531'], tr/val_loss:  0.874640/  1.161765, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   2  Sparsity: 69.8016%\n",
      "layer   3  Sparsity: 71.2180%\n",
      "total_backward_count 2055900 real_backward_count 110989   5.399%\n",
      "epoch-210 lr=['0.0019531'], tr/val_loss:  0.872550/  1.160302, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0850%\n",
      "layer   2  Sparsity: 69.7602%\n",
      "layer   3  Sparsity: 70.9870%\n",
      "total_backward_count 2065690 real_backward_count 111170   5.382%\n",
      "fc layer 1 self.abs_max_out: 4613.0\n",
      "epoch-211 lr=['0.0019531'], tr/val_loss:  0.881748/  1.191433, val:  85.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   2  Sparsity: 69.8607%\n",
      "layer   3  Sparsity: 70.8982%\n",
      "total_backward_count 2075480 real_backward_count 111342   5.365%\n",
      "epoch-212 lr=['0.0019531'], tr/val_loss:  0.877647/  1.160689, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   2  Sparsity: 70.0933%\n",
      "layer   3  Sparsity: 70.6215%\n",
      "total_backward_count 2085270 real_backward_count 111535   5.349%\n",
      "fc layer 1 self.abs_max_out: 4634.0\n",
      "epoch-213 lr=['0.0019531'], tr/val_loss:  0.869208/  1.148429, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0359%\n",
      "layer   2  Sparsity: 70.1378%\n",
      "layer   3  Sparsity: 70.9472%\n",
      "total_backward_count 2095060 real_backward_count 111709   5.332%\n",
      "epoch-214 lr=['0.0019531'], tr/val_loss:  0.876577/  1.156350, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0637%\n",
      "layer   2  Sparsity: 70.2022%\n",
      "layer   3  Sparsity: 71.1912%\n",
      "total_backward_count 2104850 real_backward_count 111887   5.316%\n",
      "epoch-215 lr=['0.0019531'], tr/val_loss:  0.867420/  1.159606, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0754%\n",
      "layer   2  Sparsity: 70.0561%\n",
      "layer   3  Sparsity: 70.9454%\n",
      "total_backward_count 2114640 real_backward_count 112084   5.300%\n",
      "lif layer 1 self.abs_max_v: 7235.0\n",
      "epoch-216 lr=['0.0019531'], tr/val_loss:  0.870432/  1.185560, val:  82.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0101%\n",
      "layer   2  Sparsity: 69.9541%\n",
      "layer   3  Sparsity: 70.6388%\n",
      "total_backward_count 2124430 real_backward_count 112295   5.286%\n",
      "fc layer 3 self.abs_max_out: 910.0\n",
      "epoch-217 lr=['0.0019531'], tr/val_loss:  0.861318/  1.124963, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0659%\n",
      "layer   2  Sparsity: 69.9304%\n",
      "layer   3  Sparsity: 70.7070%\n",
      "total_backward_count 2134220 real_backward_count 112496   5.271%\n",
      "epoch-218 lr=['0.0019531'], tr/val_loss:  0.846556/  1.138977, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0842%\n",
      "layer   2  Sparsity: 70.0476%\n",
      "layer   3  Sparsity: 70.8272%\n",
      "total_backward_count 2144010 real_backward_count 112674   5.255%\n",
      "epoch-219 lr=['0.0019531'], tr/val_loss:  0.850736/  1.147891, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   2  Sparsity: 70.2126%\n",
      "layer   3  Sparsity: 71.3914%\n",
      "total_backward_count 2153800 real_backward_count 112834   5.239%\n",
      "epoch-220 lr=['0.0019531'], tr/val_loss:  0.858586/  1.128136, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   2  Sparsity: 70.3278%\n",
      "layer   3  Sparsity: 71.3015%\n",
      "total_backward_count 2163590 real_backward_count 112992   5.222%\n",
      "fc layer 1 self.abs_max_out: 4684.0\n",
      "epoch-221 lr=['0.0019531'], tr/val_loss:  0.853651/  1.143556, val:  85.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0364%\n",
      "layer   2  Sparsity: 70.2924%\n",
      "layer   3  Sparsity: 71.0736%\n",
      "total_backward_count 2173380 real_backward_count 113165   5.207%\n",
      "epoch-222 lr=['0.0019531'], tr/val_loss:  0.865973/  1.158720, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   2  Sparsity: 70.3114%\n",
      "layer   3  Sparsity: 70.7792%\n",
      "total_backward_count 2183170 real_backward_count 113362   5.193%\n",
      "epoch-223 lr=['0.0019531'], tr/val_loss:  0.868029/  1.129833, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0578%\n",
      "layer   2  Sparsity: 70.1959%\n",
      "layer   3  Sparsity: 70.5856%\n",
      "total_backward_count 2192960 real_backward_count 113558   5.178%\n",
      "epoch-224 lr=['0.0019531'], tr/val_loss:  0.858365/  1.152589, val:  86.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1000%\n",
      "layer   2  Sparsity: 69.9199%\n",
      "layer   3  Sparsity: 70.8852%\n",
      "total_backward_count 2202750 real_backward_count 113737   5.163%\n",
      "epoch-225 lr=['0.0019531'], tr/val_loss:  0.857500/  1.146535, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0484%\n",
      "layer   2  Sparsity: 69.8980%\n",
      "layer   3  Sparsity: 70.8174%\n",
      "total_backward_count 2212540 real_backward_count 113895   5.148%\n",
      "epoch-226 lr=['0.0019531'], tr/val_loss:  0.867322/  1.151538, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0588%\n",
      "layer   2  Sparsity: 69.8738%\n",
      "layer   3  Sparsity: 70.8795%\n",
      "total_backward_count 2222330 real_backward_count 114074   5.133%\n",
      "epoch-227 lr=['0.0019531'], tr/val_loss:  0.861142/  1.158171, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   2  Sparsity: 69.9716%\n",
      "layer   3  Sparsity: 71.2269%\n",
      "total_backward_count 2232120 real_backward_count 114240   5.118%\n",
      "epoch-228 lr=['0.0019531'], tr/val_loss:  0.862773/  1.188467, val:  85.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   2  Sparsity: 69.7731%\n",
      "layer   3  Sparsity: 71.1003%\n",
      "total_backward_count 2241910 real_backward_count 114414   5.103%\n",
      "epoch-229 lr=['0.0019531'], tr/val_loss:  0.863399/  1.153938, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0729%\n",
      "layer   2  Sparsity: 70.1542%\n",
      "layer   3  Sparsity: 70.9446%\n",
      "total_backward_count 2251700 real_backward_count 114582   5.089%\n",
      "epoch-230 lr=['0.0019531'], tr/val_loss:  0.851923/  1.154147, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1106%\n",
      "layer   2  Sparsity: 70.2078%\n",
      "layer   3  Sparsity: 71.0152%\n",
      "total_backward_count 2261490 real_backward_count 114735   5.073%\n",
      "epoch-231 lr=['0.0019531'], tr/val_loss:  0.863712/  1.155649, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0586%\n",
      "layer   2  Sparsity: 69.9138%\n",
      "layer   3  Sparsity: 71.0394%\n",
      "total_backward_count 2271280 real_backward_count 114888   5.058%\n",
      "epoch-232 lr=['0.0019531'], tr/val_loss:  0.857173/  1.136338, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1049%\n",
      "layer   2  Sparsity: 69.9398%\n",
      "layer   3  Sparsity: 71.1721%\n",
      "total_backward_count 2281070 real_backward_count 115051   5.044%\n",
      "epoch-233 lr=['0.0019531'], tr/val_loss:  0.850665/  1.145047, val:  86.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   2  Sparsity: 70.1217%\n",
      "layer   3  Sparsity: 71.5212%\n",
      "total_backward_count 2290860 real_backward_count 115222   5.030%\n",
      "epoch-234 lr=['0.0019531'], tr/val_loss:  0.848484/  1.144983, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0842%\n",
      "layer   2  Sparsity: 69.8138%\n",
      "layer   3  Sparsity: 71.2879%\n",
      "total_backward_count 2300650 real_backward_count 115363   5.014%\n",
      "epoch-235 lr=['0.0019531'], tr/val_loss:  0.858417/  1.154422, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0780%\n",
      "layer   2  Sparsity: 70.1268%\n",
      "layer   3  Sparsity: 71.1780%\n",
      "total_backward_count 2310440 real_backward_count 115535   5.001%\n",
      "epoch-236 lr=['0.0019531'], tr/val_loss:  0.854575/  1.132727, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   2  Sparsity: 70.3280%\n",
      "layer   3  Sparsity: 71.3622%\n",
      "total_backward_count 2320230 real_backward_count 115686   4.986%\n",
      "epoch-237 lr=['0.0019531'], tr/val_loss:  0.847615/  1.148773, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   2  Sparsity: 70.1256%\n",
      "layer   3  Sparsity: 71.1765%\n",
      "total_backward_count 2330020 real_backward_count 115847   4.972%\n",
      "fc layer 1 self.abs_max_out: 4738.0\n",
      "epoch-238 lr=['0.0019531'], tr/val_loss:  0.843501/  1.164906, val:  84.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0986%\n",
      "layer   2  Sparsity: 70.3323%\n",
      "layer   3  Sparsity: 71.6238%\n",
      "total_backward_count 2339810 real_backward_count 116003   4.958%\n",
      "epoch-239 lr=['0.0019531'], tr/val_loss:  0.846517/  1.130489, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   2  Sparsity: 70.1417%\n",
      "layer   3  Sparsity: 71.4547%\n",
      "total_backward_count 2349600 real_backward_count 116163   4.944%\n",
      "epoch-240 lr=['0.0019531'], tr/val_loss:  0.842049/  1.147138, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0333%\n",
      "layer   2  Sparsity: 69.9166%\n",
      "layer   3  Sparsity: 71.4422%\n",
      "total_backward_count 2359390 real_backward_count 116309   4.930%\n",
      "epoch-241 lr=['0.0019531'], tr/val_loss:  0.849505/  1.146288, val:  86.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   2  Sparsity: 69.7125%\n",
      "layer   3  Sparsity: 71.6006%\n",
      "total_backward_count 2369180 real_backward_count 116441   4.915%\n",
      "epoch-242 lr=['0.0019531'], tr/val_loss:  0.842670/  1.150339, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   2  Sparsity: 70.0026%\n",
      "layer   3  Sparsity: 71.6484%\n",
      "total_backward_count 2378970 real_backward_count 116620   4.902%\n",
      "epoch-243 lr=['0.0019531'], tr/val_loss:  0.852954/  1.151332, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   2  Sparsity: 70.0155%\n",
      "layer   3  Sparsity: 71.5695%\n",
      "total_backward_count 2388760 real_backward_count 116805   4.890%\n",
      "epoch-244 lr=['0.0019531'], tr/val_loss:  0.849572/  1.139153, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0963%\n",
      "layer   2  Sparsity: 70.1046%\n",
      "layer   3  Sparsity: 71.6326%\n",
      "total_backward_count 2398550 real_backward_count 116954   4.876%\n",
      "epoch-245 lr=['0.0019531'], tr/val_loss:  0.842005/  1.118362, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.48 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0527%\n",
      "layer   2  Sparsity: 70.1879%\n",
      "layer   3  Sparsity: 71.2829%\n",
      "total_backward_count 2408340 real_backward_count 117087   4.862%\n",
      "epoch-246 lr=['0.0019531'], tr/val_loss:  0.835406/  1.146091, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.63 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0881%\n",
      "layer   2  Sparsity: 70.2304%\n",
      "layer   3  Sparsity: 71.2134%\n",
      "total_backward_count 2418130 real_backward_count 117218   4.847%\n",
      "epoch-247 lr=['0.0019531'], tr/val_loss:  0.835907/  1.127102, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   2  Sparsity: 70.2792%\n",
      "layer   3  Sparsity: 71.2385%\n",
      "total_backward_count 2427920 real_backward_count 117367   4.834%\n",
      "epoch-248 lr=['0.0019531'], tr/val_loss:  0.843835/  1.129663, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0621%\n",
      "layer   2  Sparsity: 70.2537%\n",
      "layer   3  Sparsity: 71.4460%\n",
      "total_backward_count 2437710 real_backward_count 117560   4.823%\n",
      "epoch-249 lr=['0.0019531'], tr/val_loss:  0.831005/  1.116984, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0468%\n",
      "layer   2  Sparsity: 70.1838%\n",
      "layer   3  Sparsity: 71.6301%\n",
      "total_backward_count 2447500 real_backward_count 117711   4.809%\n",
      "fc layer 3 self.abs_max_out: 917.0\n",
      "epoch-250 lr=['0.0019531'], tr/val_loss:  0.829054/  1.135647, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1283%\n",
      "layer   2  Sparsity: 70.2732%\n",
      "layer   3  Sparsity: 71.5633%\n",
      "total_backward_count 2457290 real_backward_count 117833   4.795%\n",
      "lif layer 1 self.abs_max_v: 7493.0\n",
      "epoch-251 lr=['0.0019531'], tr/val_loss:  0.827443/  1.121497, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0983%\n",
      "layer   2  Sparsity: 70.1624%\n",
      "layer   3  Sparsity: 71.3708%\n",
      "total_backward_count 2467080 real_backward_count 117993   4.783%\n",
      "epoch-252 lr=['0.0019531'], tr/val_loss:  0.815779/  1.113156, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   2  Sparsity: 70.0505%\n",
      "layer   3  Sparsity: 71.3463%\n",
      "total_backward_count 2476870 real_backward_count 118125   4.769%\n",
      "epoch-253 lr=['0.0019531'], tr/val_loss:  0.826383/  1.119810, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   2  Sparsity: 70.1436%\n",
      "layer   3  Sparsity: 70.9764%\n",
      "total_backward_count 2486660 real_backward_count 118286   4.757%\n",
      "epoch-254 lr=['0.0019531'], tr/val_loss:  0.827078/  1.148376, val:  86.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0855%\n",
      "layer   2  Sparsity: 70.0807%\n",
      "layer   3  Sparsity: 71.0164%\n",
      "total_backward_count 2496450 real_backward_count 118438   4.744%\n",
      "epoch-255 lr=['0.0019531'], tr/val_loss:  0.827744/  1.138498, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   2  Sparsity: 69.7912%\n",
      "layer   3  Sparsity: 71.2395%\n",
      "total_backward_count 2506240 real_backward_count 118577   4.731%\n",
      "epoch-256 lr=['0.0019531'], tr/val_loss:  0.831014/  1.103081, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0542%\n",
      "layer   2  Sparsity: 70.0302%\n",
      "layer   3  Sparsity: 71.0136%\n",
      "total_backward_count 2516030 real_backward_count 118741   4.719%\n",
      "epoch-257 lr=['0.0019531'], tr/val_loss:  0.816190/  1.094543, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   2  Sparsity: 70.0782%\n",
      "layer   3  Sparsity: 70.9509%\n",
      "total_backward_count 2525820 real_backward_count 118893   4.707%\n",
      "epoch-258 lr=['0.0019531'], tr/val_loss:  0.816783/  1.091433, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0603%\n",
      "layer   2  Sparsity: 70.3450%\n",
      "layer   3  Sparsity: 71.2738%\n",
      "total_backward_count 2535610 real_backward_count 119015   4.694%\n",
      "epoch-259 lr=['0.0019531'], tr/val_loss:  0.817112/  1.114331, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0402%\n",
      "layer   2  Sparsity: 70.1929%\n",
      "layer   3  Sparsity: 71.1706%\n",
      "total_backward_count 2545400 real_backward_count 119164   4.682%\n",
      "epoch-260 lr=['0.0019531'], tr/val_loss:  0.822732/  1.110189, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   2  Sparsity: 70.1169%\n",
      "layer   3  Sparsity: 71.1156%\n",
      "total_backward_count 2555190 real_backward_count 119316   4.670%\n",
      "epoch-261 lr=['0.0019531'], tr/val_loss:  0.820967/  1.115284, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   2  Sparsity: 70.1347%\n",
      "layer   3  Sparsity: 70.8702%\n",
      "total_backward_count 2564980 real_backward_count 119475   4.658%\n",
      "epoch-262 lr=['0.0019531'], tr/val_loss:  0.816250/  1.115748, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   2  Sparsity: 70.2982%\n",
      "layer   3  Sparsity: 70.8617%\n",
      "total_backward_count 2574770 real_backward_count 119605   4.645%\n",
      "epoch-263 lr=['0.0019531'], tr/val_loss:  0.813874/  1.122355, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0435%\n",
      "layer   2  Sparsity: 70.3190%\n",
      "layer   3  Sparsity: 70.9882%\n",
      "total_backward_count 2584560 real_backward_count 119736   4.633%\n",
      "epoch-264 lr=['0.0019531'], tr/val_loss:  0.830304/  1.132776, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0583%\n",
      "layer   2  Sparsity: 70.2640%\n",
      "layer   3  Sparsity: 70.9765%\n",
      "total_backward_count 2594350 real_backward_count 119900   4.622%\n",
      "epoch-265 lr=['0.0019531'], tr/val_loss:  0.817317/  1.104595, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   2  Sparsity: 70.2597%\n",
      "layer   3  Sparsity: 71.2681%\n",
      "total_backward_count 2604140 real_backward_count 120022   4.609%\n",
      "epoch-266 lr=['0.0019531'], tr/val_loss:  0.819493/  1.127161, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0360%\n",
      "layer   2  Sparsity: 70.2467%\n",
      "layer   3  Sparsity: 71.1380%\n",
      "total_backward_count 2613930 real_backward_count 120171   4.597%\n",
      "epoch-267 lr=['0.0019531'], tr/val_loss:  0.825129/  1.131374, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   2  Sparsity: 70.3114%\n",
      "layer   3  Sparsity: 71.2265%\n",
      "total_backward_count 2623720 real_backward_count 120307   4.585%\n",
      "epoch-268 lr=['0.0019531'], tr/val_loss:  0.817421/  1.118679, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0255%\n",
      "layer   2  Sparsity: 70.5125%\n",
      "layer   3  Sparsity: 71.4100%\n",
      "total_backward_count 2633510 real_backward_count 120453   4.574%\n",
      "lif layer 2 self.abs_max_v: 3488.5\n",
      "lif layer 2 self.abs_max_v: 3542.0\n",
      "epoch-269 lr=['0.0019531'], tr/val_loss:  0.830907/  1.123913, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   2  Sparsity: 70.1936%\n",
      "layer   3  Sparsity: 71.3610%\n",
      "total_backward_count 2643300 real_backward_count 120597   4.562%\n",
      "fc layer 3 self.abs_max_out: 948.0\n",
      "fc layer 3 self.abs_max_out: 969.0\n",
      "epoch-270 lr=['0.0019531'], tr/val_loss:  0.820044/  1.124306, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0743%\n",
      "layer   2  Sparsity: 70.1233%\n",
      "layer   3  Sparsity: 71.5259%\n",
      "total_backward_count 2653090 real_backward_count 120710   4.550%\n",
      "fc layer 1 self.abs_max_out: 4883.0\n",
      "lif layer 2 self.abs_max_v: 3592.5\n",
      "epoch-271 lr=['0.0019531'], tr/val_loss:  0.812009/  1.125291, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   2  Sparsity: 70.4491%\n",
      "layer   3  Sparsity: 71.8846%\n",
      "total_backward_count 2662880 real_backward_count 120816   4.537%\n",
      "lif layer 2 self.abs_max_v: 3672.5\n",
      "epoch-272 lr=['0.0019531'], tr/val_loss:  0.813090/  1.116071, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   2  Sparsity: 70.3223%\n",
      "layer   3  Sparsity: 71.4114%\n",
      "total_backward_count 2672670 real_backward_count 120930   4.525%\n",
      "fc layer 1 self.abs_max_out: 4937.0\n",
      "epoch-273 lr=['0.0019531'], tr/val_loss:  0.821451/  1.109833, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0801%\n",
      "layer   2  Sparsity: 70.2124%\n",
      "layer   3  Sparsity: 70.8996%\n",
      "total_backward_count 2682460 real_backward_count 121101   4.515%\n",
      "epoch-274 lr=['0.0019531'], tr/val_loss:  0.820010/  1.122450, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   2  Sparsity: 70.0649%\n",
      "layer   3  Sparsity: 70.9325%\n",
      "total_backward_count 2692250 real_backward_count 121237   4.503%\n",
      "epoch-275 lr=['0.0019531'], tr/val_loss:  0.818092/  1.117427, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1168%\n",
      "layer   2  Sparsity: 70.2092%\n",
      "layer   3  Sparsity: 71.0591%\n",
      "total_backward_count 2702040 real_backward_count 121385   4.492%\n",
      "epoch-276 lr=['0.0019531'], tr/val_loss:  0.818246/  1.107216, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 70.4007%\n",
      "layer   3  Sparsity: 71.0927%\n",
      "total_backward_count 2711830 real_backward_count 121530   4.481%\n",
      "epoch-277 lr=['0.0019531'], tr/val_loss:  0.820826/  1.099533, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0313%\n",
      "layer   2  Sparsity: 70.2043%\n",
      "layer   3  Sparsity: 70.9789%\n",
      "total_backward_count 2721620 real_backward_count 121667   4.470%\n",
      "epoch-278 lr=['0.0019531'], tr/val_loss:  0.810652/  1.091767, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   2  Sparsity: 70.1808%\n",
      "layer   3  Sparsity: 70.9779%\n",
      "total_backward_count 2731410 real_backward_count 121805   4.459%\n",
      "epoch-279 lr=['0.0019531'], tr/val_loss:  0.801671/  1.103094, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   2  Sparsity: 70.0652%\n",
      "layer   3  Sparsity: 71.2738%\n",
      "total_backward_count 2741200 real_backward_count 121955   4.449%\n",
      "epoch-280 lr=['0.0019531'], tr/val_loss:  0.806387/  1.117829, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0845%\n",
      "layer   2  Sparsity: 70.1077%\n",
      "layer   3  Sparsity: 71.3582%\n",
      "total_backward_count 2750990 real_backward_count 122081   4.438%\n",
      "epoch-281 lr=['0.0019531'], tr/val_loss:  0.808023/  1.127421, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.22 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0702%\n",
      "layer   2  Sparsity: 70.3858%\n",
      "layer   3  Sparsity: 71.4009%\n",
      "total_backward_count 2760780 real_backward_count 122216   4.427%\n",
      "epoch-282 lr=['0.0019531'], tr/val_loss:  0.801025/  1.119797, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1131%\n",
      "layer   2  Sparsity: 70.5044%\n",
      "layer   3  Sparsity: 71.3732%\n",
      "total_backward_count 2770570 real_backward_count 122331   4.415%\n",
      "epoch-283 lr=['0.0019531'], tr/val_loss:  0.795907/  1.096821, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   2  Sparsity: 70.3494%\n",
      "layer   3  Sparsity: 71.2952%\n",
      "total_backward_count 2780360 real_backward_count 122426   4.403%\n",
      "epoch-284 lr=['0.0019531'], tr/val_loss:  0.798940/  1.105077, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0811%\n",
      "layer   2  Sparsity: 69.9050%\n",
      "layer   3  Sparsity: 71.2029%\n",
      "total_backward_count 2790150 real_backward_count 122551   4.392%\n",
      "epoch-285 lr=['0.0019531'], tr/val_loss:  0.792505/  1.104302, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   2  Sparsity: 70.0417%\n",
      "layer   3  Sparsity: 71.4585%\n",
      "total_backward_count 2799940 real_backward_count 122644   4.380%\n",
      "epoch-286 lr=['0.0019531'], tr/val_loss:  0.788789/  1.097993, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   2  Sparsity: 70.2240%\n",
      "layer   3  Sparsity: 71.7710%\n",
      "total_backward_count 2809730 real_backward_count 122753   4.369%\n",
      "epoch-287 lr=['0.0019531'], tr/val_loss:  0.797022/  1.102178, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0938%\n",
      "layer   2  Sparsity: 70.0139%\n",
      "layer   3  Sparsity: 71.7563%\n",
      "total_backward_count 2819520 real_backward_count 122865   4.358%\n",
      "epoch-288 lr=['0.0019531'], tr/val_loss:  0.792303/  1.086294, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   2  Sparsity: 69.9866%\n",
      "layer   3  Sparsity: 71.3595%\n",
      "total_backward_count 2829310 real_backward_count 122972   4.346%\n",
      "epoch-289 lr=['0.0019531'], tr/val_loss:  0.797791/  1.114555, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0574%\n",
      "layer   2  Sparsity: 69.9862%\n",
      "layer   3  Sparsity: 71.0260%\n",
      "total_backward_count 2839100 real_backward_count 123103   4.336%\n",
      "epoch-290 lr=['0.0019531'], tr/val_loss:  0.783279/  1.086230, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0467%\n",
      "layer   2  Sparsity: 70.2040%\n",
      "layer   3  Sparsity: 71.0860%\n",
      "total_backward_count 2848890 real_backward_count 123211   4.325%\n",
      "lif layer 2 self.abs_max_v: 3674.5\n",
      "fc layer 2 self.abs_max_out: 2447.0\n",
      "epoch-291 lr=['0.0019531'], tr/val_loss:  0.782574/  1.093309, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.30 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   2  Sparsity: 70.2751%\n",
      "layer   3  Sparsity: 71.0491%\n",
      "total_backward_count 2858680 real_backward_count 123340   4.315%\n",
      "epoch-292 lr=['0.0019531'], tr/val_loss:  0.785324/  1.089348, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0507%\n",
      "layer   2  Sparsity: 70.3887%\n",
      "layer   3  Sparsity: 71.0305%\n",
      "total_backward_count 2868470 real_backward_count 123447   4.304%\n",
      "epoch-293 lr=['0.0019531'], tr/val_loss:  0.788576/  1.079718, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   2  Sparsity: 70.2356%\n",
      "layer   3  Sparsity: 70.9899%\n",
      "total_backward_count 2878260 real_backward_count 123575   4.293%\n",
      "lif layer 2 self.abs_max_v: 3816.5\n",
      "epoch-294 lr=['0.0019531'], tr/val_loss:  0.780635/  1.082382, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1051%\n",
      "layer   2  Sparsity: 70.4283%\n",
      "layer   3  Sparsity: 71.2603%\n",
      "total_backward_count 2888050 real_backward_count 123701   4.283%\n",
      "epoch-295 lr=['0.0019531'], tr/val_loss:  0.786077/  1.084057, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1131%\n",
      "layer   2  Sparsity: 70.1335%\n",
      "layer   3  Sparsity: 71.3091%\n",
      "total_backward_count 2897840 real_backward_count 123826   4.273%\n",
      "epoch-296 lr=['0.0019531'], tr/val_loss:  0.787112/  1.096854, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0431%\n",
      "layer   2  Sparsity: 69.9296%\n",
      "layer   3  Sparsity: 71.3571%\n",
      "total_backward_count 2907630 real_backward_count 123955   4.263%\n",
      "epoch-297 lr=['0.0019531'], tr/val_loss:  0.785798/  1.099889, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0703%\n",
      "layer   2  Sparsity: 69.9194%\n",
      "layer   3  Sparsity: 71.5187%\n",
      "total_backward_count 2917420 real_backward_count 124059   4.252%\n",
      "epoch-298 lr=['0.0019531'], tr/val_loss:  0.787962/  1.066426, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0416%\n",
      "layer   2  Sparsity: 70.0841%\n",
      "layer   3  Sparsity: 71.4393%\n",
      "total_backward_count 2927210 real_backward_count 124185   4.242%\n",
      "epoch-299 lr=['0.0019531'], tr/val_loss:  0.791007/  1.086524, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   2  Sparsity: 70.2028%\n",
      "layer   3  Sparsity: 71.1934%\n",
      "total_backward_count 2937000 real_backward_count 124324   4.233%\n",
      "epoch-300 lr=['0.0019531'], tr/val_loss:  0.784558/  1.077352, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0954%\n",
      "layer   2  Sparsity: 70.0720%\n",
      "layer   3  Sparsity: 71.1062%\n",
      "total_backward_count 2946790 real_backward_count 124420   4.222%\n",
      "epoch-301 lr=['0.0019531'], tr/val_loss:  0.779502/  1.080669, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0546%\n",
      "layer   2  Sparsity: 70.0954%\n",
      "layer   3  Sparsity: 71.2213%\n",
      "total_backward_count 2956580 real_backward_count 124523   4.212%\n",
      "epoch-302 lr=['0.0019531'], tr/val_loss:  0.795892/  1.089424, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1104%\n",
      "layer   2  Sparsity: 70.1396%\n",
      "layer   3  Sparsity: 70.9759%\n",
      "total_backward_count 2966370 real_backward_count 124641   4.202%\n",
      "epoch-303 lr=['0.0019531'], tr/val_loss:  0.786650/  1.080975, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0731%\n",
      "layer   2  Sparsity: 70.1477%\n",
      "layer   3  Sparsity: 71.4062%\n",
      "total_backward_count 2976160 real_backward_count 124745   4.191%\n",
      "epoch-304 lr=['0.0019531'], tr/val_loss:  0.785449/  1.097124, val:  85.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.92 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0553%\n",
      "layer   2  Sparsity: 70.0959%\n",
      "layer   3  Sparsity: 71.2650%\n",
      "total_backward_count 2985950 real_backward_count 124859   4.182%\n",
      "epoch-305 lr=['0.0019531'], tr/val_loss:  0.783672/  1.069384, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.83 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0472%\n",
      "layer   2  Sparsity: 70.0850%\n",
      "layer   3  Sparsity: 71.5378%\n",
      "total_backward_count 2995740 real_backward_count 124984   4.172%\n",
      "epoch-306 lr=['0.0019531'], tr/val_loss:  0.788693/  1.105315, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0946%\n",
      "layer   2  Sparsity: 70.2361%\n",
      "layer   3  Sparsity: 71.2373%\n",
      "total_backward_count 3005530 real_backward_count 125120   4.163%\n",
      "epoch-307 lr=['0.0019531'], tr/val_loss:  0.795444/  1.113566, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1109%\n",
      "layer   2  Sparsity: 70.4238%\n",
      "layer   3  Sparsity: 71.3771%\n",
      "total_backward_count 3015320 real_backward_count 125210   4.152%\n",
      "epoch-308 lr=['0.0019531'], tr/val_loss:  0.790346/  1.116455, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0378%\n",
      "layer   2  Sparsity: 70.4693%\n",
      "layer   3  Sparsity: 71.6638%\n",
      "total_backward_count 3025110 real_backward_count 125338   4.143%\n",
      "epoch-309 lr=['0.0019531'], tr/val_loss:  0.802819/  1.090860, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0783%\n",
      "layer   2  Sparsity: 70.6205%\n",
      "layer   3  Sparsity: 71.3627%\n",
      "total_backward_count 3034900 real_backward_count 125478   4.135%\n",
      "fc layer 3 self.abs_max_out: 974.0\n",
      "epoch-310 lr=['0.0019531'], tr/val_loss:  0.790018/  1.084219, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.21 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0741%\n",
      "layer   2  Sparsity: 70.5843%\n",
      "layer   3  Sparsity: 71.4685%\n",
      "total_backward_count 3044690 real_backward_count 125572   4.124%\n",
      "fc layer 3 self.abs_max_out: 1007.0\n",
      "epoch-311 lr=['0.0019531'], tr/val_loss:  0.780952/  1.070406, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1525%\n",
      "layer   2  Sparsity: 70.5742%\n",
      "layer   3  Sparsity: 71.5176%\n",
      "total_backward_count 3054480 real_backward_count 125692   4.115%\n",
      "epoch-312 lr=['0.0019531'], tr/val_loss:  0.776504/  1.071306, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0979%\n",
      "layer   2  Sparsity: 70.4059%\n",
      "layer   3  Sparsity: 71.3475%\n",
      "total_backward_count 3064270 real_backward_count 125794   4.105%\n",
      "epoch-313 lr=['0.0019531'], tr/val_loss:  0.780646/  1.080759, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   2  Sparsity: 70.2609%\n",
      "layer   3  Sparsity: 71.2808%\n",
      "total_backward_count 3074060 real_backward_count 125906   4.096%\n",
      "fc layer 2 self.abs_max_out: 2456.0\n",
      "epoch-314 lr=['0.0019531'], tr/val_loss:  0.785277/  1.108391, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   2  Sparsity: 70.1511%\n",
      "layer   3  Sparsity: 71.4906%\n",
      "total_backward_count 3083850 real_backward_count 126040   4.087%\n",
      "epoch-315 lr=['0.0019531'], tr/val_loss:  0.788400/  1.124109, val:  85.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.34 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   2  Sparsity: 70.1190%\n",
      "layer   3  Sparsity: 71.5626%\n",
      "total_backward_count 3093640 real_backward_count 126152   4.078%\n",
      "epoch-316 lr=['0.0019531'], tr/val_loss:  0.795851/  1.093989, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   2  Sparsity: 70.3146%\n",
      "layer   3  Sparsity: 71.5822%\n",
      "total_backward_count 3103430 real_backward_count 126250   4.068%\n",
      "epoch-317 lr=['0.0019531'], tr/val_loss:  0.778501/  1.081164, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   2  Sparsity: 70.1810%\n",
      "layer   3  Sparsity: 71.4279%\n",
      "total_backward_count 3113220 real_backward_count 126350   4.058%\n",
      "epoch-318 lr=['0.0019531'], tr/val_loss:  0.774217/  1.076769, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0637%\n",
      "layer   2  Sparsity: 70.1237%\n",
      "layer   3  Sparsity: 71.4595%\n",
      "total_backward_count 3123010 real_backward_count 126441   4.049%\n",
      "epoch-319 lr=['0.0019531'], tr/val_loss:  0.773613/  1.098544, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0318%\n",
      "layer   2  Sparsity: 70.1724%\n",
      "layer   3  Sparsity: 71.7173%\n",
      "total_backward_count 3132800 real_backward_count 126538   4.039%\n",
      "epoch-320 lr=['0.0019531'], tr/val_loss:  0.786099/  1.074829, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   2  Sparsity: 70.3363%\n",
      "layer   3  Sparsity: 71.7870%\n",
      "total_backward_count 3142590 real_backward_count 126672   4.031%\n",
      "epoch-321 lr=['0.0019531'], tr/val_loss:  0.769765/  1.065715, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0932%\n",
      "layer   2  Sparsity: 70.2750%\n",
      "layer   3  Sparsity: 71.7180%\n",
      "total_backward_count 3152380 real_backward_count 126784   4.022%\n",
      "epoch-322 lr=['0.0019531'], tr/val_loss:  0.768914/  1.085723, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   2  Sparsity: 70.2074%\n",
      "layer   3  Sparsity: 71.9674%\n",
      "total_backward_count 3162170 real_backward_count 126894   4.013%\n",
      "epoch-323 lr=['0.0019531'], tr/val_loss:  0.774492/  1.092555, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1161%\n",
      "layer   2  Sparsity: 70.1583%\n",
      "layer   3  Sparsity: 71.9054%\n",
      "total_backward_count 3171960 real_backward_count 126988   4.003%\n",
      "epoch-324 lr=['0.0019531'], tr/val_loss:  0.779585/  1.102475, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.05 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0677%\n",
      "layer   2  Sparsity: 70.0496%\n",
      "layer   3  Sparsity: 72.0428%\n",
      "total_backward_count 3181750 real_backward_count 127085   3.994%\n",
      "epoch-325 lr=['0.0019531'], tr/val_loss:  0.774936/  1.082669, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0409%\n",
      "layer   2  Sparsity: 69.9717%\n",
      "layer   3  Sparsity: 72.1637%\n",
      "total_backward_count 3191540 real_backward_count 127194   3.985%\n",
      "epoch-326 lr=['0.0019531'], tr/val_loss:  0.783016/  1.065008, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   2  Sparsity: 70.0251%\n",
      "layer   3  Sparsity: 71.9967%\n",
      "total_backward_count 3201330 real_backward_count 127293   3.976%\n",
      "epoch-327 lr=['0.0019531'], tr/val_loss:  0.771361/  1.059186, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0881%\n",
      "layer   2  Sparsity: 69.9602%\n",
      "layer   3  Sparsity: 71.5439%\n",
      "total_backward_count 3211120 real_backward_count 127383   3.967%\n",
      "epoch-328 lr=['0.0019531'], tr/val_loss:  0.761553/  1.064618, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0665%\n",
      "layer   2  Sparsity: 69.9454%\n",
      "layer   3  Sparsity: 71.6315%\n",
      "total_backward_count 3220910 real_backward_count 127475   3.958%\n",
      "epoch-329 lr=['0.0019531'], tr/val_loss:  0.769501/  1.057182, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0676%\n",
      "layer   2  Sparsity: 69.9832%\n",
      "layer   3  Sparsity: 72.0160%\n",
      "total_backward_count 3230700 real_backward_count 127584   3.949%\n",
      "epoch-330 lr=['0.0019531'], tr/val_loss:  0.768317/  1.068011, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0660%\n",
      "layer   2  Sparsity: 70.0431%\n",
      "layer   3  Sparsity: 72.0982%\n",
      "total_backward_count 3240490 real_backward_count 127659   3.939%\n",
      "epoch-331 lr=['0.0019531'], tr/val_loss:  0.773869/  1.085475, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0951%\n",
      "layer   2  Sparsity: 70.2167%\n",
      "layer   3  Sparsity: 71.8298%\n",
      "total_backward_count 3250280 real_backward_count 127760   3.931%\n",
      "epoch-332 lr=['0.0019531'], tr/val_loss:  0.768000/  1.080073, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0685%\n",
      "layer   2  Sparsity: 70.4211%\n",
      "layer   3  Sparsity: 71.7785%\n",
      "total_backward_count 3260070 real_backward_count 127835   3.921%\n",
      "epoch-333 lr=['0.0019531'], tr/val_loss:  0.767444/  1.082496, val:  85.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1188%\n",
      "layer   2  Sparsity: 70.1916%\n",
      "layer   3  Sparsity: 71.7353%\n",
      "total_backward_count 3269860 real_backward_count 127939   3.913%\n",
      "epoch-334 lr=['0.0019531'], tr/val_loss:  0.760165/  1.073966, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1105%\n",
      "layer   2  Sparsity: 70.2163%\n",
      "layer   3  Sparsity: 71.8819%\n",
      "total_backward_count 3279650 real_backward_count 128020   3.903%\n",
      "epoch-335 lr=['0.0019531'], tr/val_loss:  0.757560/  1.061899, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0877%\n",
      "layer   2  Sparsity: 70.2812%\n",
      "layer   3  Sparsity: 72.0450%\n",
      "total_backward_count 3289440 real_backward_count 128124   3.895%\n",
      "epoch-336 lr=['0.0019531'], tr/val_loss:  0.758783/  1.063522, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0823%\n",
      "layer   2  Sparsity: 70.1658%\n",
      "layer   3  Sparsity: 71.9604%\n",
      "total_backward_count 3299230 real_backward_count 128207   3.886%\n",
      "epoch-337 lr=['0.0019531'], tr/val_loss:  0.765337/  1.048941, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0568%\n",
      "layer   2  Sparsity: 70.0729%\n",
      "layer   3  Sparsity: 71.8795%\n",
      "total_backward_count 3309020 real_backward_count 128296   3.877%\n",
      "epoch-338 lr=['0.0019531'], tr/val_loss:  0.761730/  1.071363, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   2  Sparsity: 70.2590%\n",
      "layer   3  Sparsity: 71.7861%\n",
      "total_backward_count 3318810 real_backward_count 128404   3.869%\n",
      "epoch-339 lr=['0.0019531'], tr/val_loss:  0.757552/  1.068258, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   2  Sparsity: 70.2687%\n",
      "layer   3  Sparsity: 72.1061%\n",
      "total_backward_count 3328600 real_backward_count 128505   3.861%\n",
      "epoch-340 lr=['0.0019531'], tr/val_loss:  0.762142/  1.067521, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0364%\n",
      "layer   2  Sparsity: 70.1550%\n",
      "layer   3  Sparsity: 72.0445%\n",
      "total_backward_count 3338390 real_backward_count 128593   3.852%\n",
      "epoch-341 lr=['0.0019531'], tr/val_loss:  0.756322/  1.051849, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0966%\n",
      "layer   2  Sparsity: 70.0831%\n",
      "layer   3  Sparsity: 72.2324%\n",
      "total_backward_count 3348180 real_backward_count 128678   3.843%\n",
      "epoch-342 lr=['0.0019531'], tr/val_loss:  0.759962/  1.085072, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1047%\n",
      "layer   2  Sparsity: 70.0182%\n",
      "layer   3  Sparsity: 72.2187%\n",
      "total_backward_count 3357970 real_backward_count 128747   3.834%\n",
      "epoch-343 lr=['0.0019531'], tr/val_loss:  0.763907/  1.072001, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   2  Sparsity: 70.0196%\n",
      "layer   3  Sparsity: 72.0220%\n",
      "total_backward_count 3367760 real_backward_count 128839   3.826%\n",
      "epoch-344 lr=['0.0019531'], tr/val_loss:  0.759296/  1.064164, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0821%\n",
      "layer   2  Sparsity: 70.0952%\n",
      "layer   3  Sparsity: 72.1704%\n",
      "total_backward_count 3377550 real_backward_count 128906   3.817%\n",
      "epoch-345 lr=['0.0019531'], tr/val_loss:  0.757414/  1.077479, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0319%\n",
      "layer   2  Sparsity: 70.0387%\n",
      "layer   3  Sparsity: 72.1112%\n",
      "total_backward_count 3387340 real_backward_count 128977   3.808%\n",
      "epoch-346 lr=['0.0019531'], tr/val_loss:  0.757225/  1.055347, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0394%\n",
      "layer   2  Sparsity: 70.2217%\n",
      "layer   3  Sparsity: 71.8704%\n",
      "total_backward_count 3397130 real_backward_count 129065   3.799%\n",
      "epoch-347 lr=['0.0019531'], tr/val_loss:  0.758473/  1.074505, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   2  Sparsity: 70.1889%\n",
      "layer   3  Sparsity: 71.9603%\n",
      "total_backward_count 3406920 real_backward_count 129164   3.791%\n",
      "epoch-348 lr=['0.0019531'], tr/val_loss:  0.756984/  1.058834, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1042%\n",
      "layer   2  Sparsity: 70.0366%\n",
      "layer   3  Sparsity: 71.7955%\n",
      "total_backward_count 3416710 real_backward_count 129234   3.782%\n",
      "epoch-349 lr=['0.0019531'], tr/val_loss:  0.761923/  1.055259, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0396%\n",
      "layer   2  Sparsity: 69.9759%\n",
      "layer   3  Sparsity: 71.6833%\n",
      "total_backward_count 3426500 real_backward_count 129316   3.774%\n",
      "epoch-350 lr=['0.0019531'], tr/val_loss:  0.750962/  1.061683, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   2  Sparsity: 69.9691%\n",
      "layer   3  Sparsity: 71.6595%\n",
      "total_backward_count 3436290 real_backward_count 129421   3.766%\n",
      "epoch-351 lr=['0.0019531'], tr/val_loss:  0.757832/  1.072366, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   2  Sparsity: 70.1749%\n",
      "layer   3  Sparsity: 71.5959%\n",
      "total_backward_count 3446080 real_backward_count 129523   3.759%\n",
      "epoch-352 lr=['0.0019531'], tr/val_loss:  0.756371/  1.064200, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0555%\n",
      "layer   2  Sparsity: 70.1306%\n",
      "layer   3  Sparsity: 71.9264%\n",
      "total_backward_count 3455870 real_backward_count 129612   3.750%\n",
      "epoch-353 lr=['0.0019531'], tr/val_loss:  0.757361/  1.065060, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.32 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0672%\n",
      "layer   2  Sparsity: 69.9391%\n",
      "layer   3  Sparsity: 71.7943%\n",
      "total_backward_count 3465660 real_backward_count 129692   3.742%\n",
      "epoch-354 lr=['0.0019531'], tr/val_loss:  0.754386/  1.074510, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1152%\n",
      "layer   2  Sparsity: 70.1916%\n",
      "layer   3  Sparsity: 71.7107%\n",
      "total_backward_count 3475450 real_backward_count 129776   3.734%\n",
      "fc layer 1 self.abs_max_out: 4938.0\n",
      "epoch-355 lr=['0.0019531'], tr/val_loss:  0.756584/  1.069444, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1004%\n",
      "layer   2  Sparsity: 70.2247%\n",
      "layer   3  Sparsity: 71.3349%\n",
      "total_backward_count 3485240 real_backward_count 129892   3.727%\n",
      "epoch-356 lr=['0.0019531'], tr/val_loss:  0.757646/  1.054515, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0443%\n",
      "layer   2  Sparsity: 70.3594%\n",
      "layer   3  Sparsity: 71.3103%\n",
      "total_backward_count 3495030 real_backward_count 129968   3.719%\n",
      "epoch-357 lr=['0.0019531'], tr/val_loss:  0.754926/  1.096826, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0629%\n",
      "layer   2  Sparsity: 70.0970%\n",
      "layer   3  Sparsity: 71.4295%\n",
      "total_backward_count 3504820 real_backward_count 130068   3.711%\n",
      "epoch-358 lr=['0.0019531'], tr/val_loss:  0.747754/  1.061175, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0818%\n",
      "layer   2  Sparsity: 70.0954%\n",
      "layer   3  Sparsity: 71.6356%\n",
      "total_backward_count 3514610 real_backward_count 130160   3.703%\n",
      "epoch-359 lr=['0.0019531'], tr/val_loss:  0.749650/  1.060555, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0889%\n",
      "layer   2  Sparsity: 70.1977%\n",
      "layer   3  Sparsity: 71.5472%\n",
      "total_backward_count 3524400 real_backward_count 130245   3.696%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'random', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        # \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.25]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [4.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/512]}, \n",
    "        \"epoch_num\": {\"values\": [400]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [-8]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"0\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'vdepwll5'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
