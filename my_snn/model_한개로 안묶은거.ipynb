{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1b9803edb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메인 셀\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1,2,3,4,5\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SYNAPSE_FC_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, spike_one_time, spike_now, weight, bias):\n",
    "        ctx.save_for_backward(spike_one_time, spike_now, weight, bias)\n",
    "        return F.linear(spike_one_time, weight, bias=bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_current):\n",
    "        #############밑에부터 수정해라#######\n",
    "        spike_one_time, spike_now, weight, bias = ctx.saved_tensors\n",
    "        \n",
    "        ## 이거 클론해야되는지 모르겠음!!!!\n",
    "        grad_output_current_clone = grad_output_current.clone()\n",
    "\n",
    "        grad_input_spike = grad_weight = grad_bias = None\n",
    "\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input_spike = grad_output_current_clone @ weight\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_weight = grad_output_current_clone.t() @ spike_now\n",
    "        if bias is not None and ctx.needs_input_grad[3]:\n",
    "            grad_bias = grad_output_current_clone.sum(0)\n",
    "\n",
    "        return grad_input_spike, None, grad_weight, grad_bias\n",
    "\n",
    "     \n",
    "class SYNAPSE_FC(nn.Module):\n",
    "    def __init__(self, in_features, out_features, trace_const1=1, trace_const2=0.7):\n",
    "        super(SYNAPSE_FC, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.trace_const1 = trace_const1\n",
    "        self.trace_const2 = trace_const2\n",
    "\n",
    "        # self.weight = torch.randn(self.out_features, self.in_features, requires_grad=True)\n",
    "        # self.bias = torch.randn(self.out_features, requires_grad=True)\n",
    "        self.weight = nn.Parameter(torch.randn(self.out_features, self.in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(self.out_features))\n",
    "\n",
    "    def forward(self, spike):\n",
    "        # spike: [Time, Batch, Features]   \n",
    "        Time = spike.shape[0]\n",
    "        Batch = spike.shape[1] \n",
    "        output_current = torch.zeros(Time, Batch, self.out_features, device=spike.device)\n",
    "\n",
    "        # spike_detach = spike.detach().clone()\n",
    "        spike_detach = spike.detach()\n",
    "        spike_past = torch.zeros_like(spike_detach[0], device=spike.device)\n",
    "        spike_now = torch.zeros_like(spike_detach[0], device=spike.device)\n",
    "\n",
    "        for t in range(Time):\n",
    "            spike_now = self.trace_const1*spike_detach[t] + self.trace_const2*spike_past\n",
    "            output_current[t]= SYNAPSE_FC_METHOD.apply(spike[t], spike_now, self.weight, self.bias) \n",
    "            spike_past = spike_now\n",
    "\n",
    "        return output_current \n",
    "\n",
    "\n",
    "\n",
    "class SYNAPSE_CONV_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, spike_one_time, spike_now, weight, bias, stride=1, padding=1):\n",
    "        ctx.save_for_backward(spike_one_time, spike_now, weight, bias, torch.tensor([stride], requires_grad=False), torch.tensor([padding], requires_grad=False))\n",
    "        return F.conv2d(spike_one_time, weight, bias=bias, stride=stride, padding=padding)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_current):\n",
    "        spike_one_time, spike_now, weight, bias, stride, padding = ctx.saved_tensors\n",
    "        stride=stride.item()\n",
    "        padding=padding.item()\n",
    "        \n",
    "        ## 이거 클론해야되는지 모르겠음!!!!\n",
    "        grad_output_current_clone = grad_output_current.clone()\n",
    "\n",
    "        grad_input_spike = grad_weight = grad_bias = None\n",
    "\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input_spike = F.conv_transpose2d(grad_output_current_clone, weight, stride=stride, padding=padding)\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_weight = torch.nn.grad.conv2d_weight(spike_now, weight.shape, grad_output_current_clone,\n",
    "                                                      stride=stride, padding=padding)\n",
    "        if bias is not None and ctx.needs_input_grad[3]:\n",
    "            grad_bias = grad_output_current_clone.sum((0, -1, -2))\n",
    "\n",
    "        return grad_input_spike, None, grad_weight, grad_bias, None, None\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "class SYNAPSE_CONV(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, trace_const1=1, trace_const2=0.7):\n",
    "        super(SYNAPSE_CONV, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.trace_const1 = trace_const1\n",
    "        self.trace_const2 = trace_const2\n",
    "\n",
    "        # self.weight = torch.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size, requires_grad=True)\n",
    "        # self.bias = torch.randn(self.out_channels, requires_grad=True)\n",
    "        self.weight = nn.Parameter(torch.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(self.out_channels))\n",
    "\n",
    "    def forward(self, spike):\n",
    "        # spike: [Time, Batch, Channel, Height, Width]   \n",
    "        # print('spike.shape', spike.shape)\n",
    "        Time = spike.shape[0]\n",
    "        Batch = spike.shape[1] \n",
    "        Channel = self.out_channels\n",
    "        Height = (spike.shape[3] + self.padding*2 - self.kernel_size) // self.stride + 1\n",
    "        Width = (spike.shape[4] + self.padding*2 - self.kernel_size) // self.stride + 1\n",
    "        output_current = torch.zeros(Time, Batch, Channel, Height, Width, device=spike.device)\n",
    "\n",
    "        # spike_detach = spike.detach().clone()\n",
    "        spike_detach = spike.detach()\n",
    "        spike_past = torch.zeros_like(spike_detach[0])\n",
    "        spike_now = torch.zeros_like(spike_detach[0])\n",
    "\n",
    "        for t in range(Time):\n",
    "            spike_now = self.trace_const1*spike_detach[t] + self.trace_const2*spike_past\n",
    "            output_current[t]= SYNAPSE_CONV_METHOD.apply(spike[t], spike_now, self.weight, self.bias, self.stride, self.padding) \n",
    "            spike_past = spike_now\n",
    "\n",
    "        return output_current\n",
    "\n",
    "\n",
    "\n",
    "class LIF_METHOD(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_current_one_time, v_one_time, v_decay, v_threshold, v_reset, sg_width):\n",
    "        v_one_time = v_one_time * v_decay + input_current_one_time # leak + pre-synaptic current integrate\n",
    "        spike = (v_one_time >= v_threshold).float() #fire\n",
    "        ctx.save_for_backward(v_one_time, torch.tensor([v_decay], requires_grad=False), \n",
    "                              torch.tensor([v_threshold], requires_grad=False), \n",
    "                              torch.tensor([v_reset], requires_grad=False), \n",
    "                              torch.tensor([sg_width], requires_grad=False)) # save before reset\n",
    "        v_one_time = (v_one_time - spike * v_threshold).clamp_min(0) # reset\n",
    "        return spike, v_one_time\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_spike, grad_output_v):\n",
    "        v_one_time, v_decay, v_threshold, v_reset, sg_width = ctx.saved_tensors\n",
    "        v_decay=v_decay.item()\n",
    "        v_threshold=v_threshold.item()\n",
    "        v_reset=v_reset.item()\n",
    "        sg_width=sg_width.item()\n",
    "\n",
    "        grad_input_current = grad_output_spike.clone()\n",
    "        # grad_temp_v = grad_output_v.clone() # not used\n",
    "\n",
    "        ################ select one of the following surrogate gradient functions ################\n",
    "        #===========surrogate gradient function (rectangle)\n",
    "        grad_input_current = grad_input_current * ((v_one_time - v_threshold).abs() < sg_width/2).float() / sg_width\n",
    "\n",
    "        #===========surrogate gradient function (sigmoid)\n",
    "        # sig = torch.sigmoid((v_one_time - v_threshold))\n",
    "        # grad_input_current =  sig*(1-sig)*grad_input_current\n",
    "\n",
    "        #===========surrogate gradient function (rough rectangle)\n",
    "        # v_minus_th = (v_one_time - v_threshold)\n",
    "        # grad_input_current[v_minus_th <= -.5] = 0\n",
    "        # grad_input_current[v_minus_th > .5] = 0\n",
    "        ###########################################################################################\n",
    "        return grad_input_current, None, None, None, None, None\n",
    "\n",
    "class LIF_layer(nn.Module):\n",
    "    def __init__ (self, v_init = 0.0, v_decay = 0.8, v_threshold = 0.5, v_reset = 0.0, sg_width = 1):\n",
    "        super(LIF_layer, self).__init__()\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.sg_width = sg_width\n",
    "\n",
    "    def forward(self, input_current):\n",
    "        v = torch.full_like(input_current, fill_value = self.v_init, dtype = torch.float) # v (membrane potential) init\n",
    "        post_spike = torch.full_like(input_current, fill_value = self.v_init, device=input_current.device, dtype = torch.float) # v (membrane potential) init\n",
    "        # i와 v와 post_spike size는 여기서 다 같음: [Time, Batch, Channel, Height, Width] \n",
    "\n",
    "        Time = v.shape[0]\n",
    "        for t in range(Time):\n",
    "            # leaky하고 input_current 더하고 fire하고 reset까지 (backward직접처리)\n",
    "            post_spike[t], v[t] = LIF_METHOD.apply(input_current[t], v[t], \n",
    "                                            self.v_decay, self.v_threshold, self.v_reset, self.sg_width) \n",
    "\n",
    "        return post_spike   \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "IMAGE_PIXEL_CHANNEL: 3\n",
      "CLASS_NUM: 10\n",
      "torch.Size([256, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# HEPER PARAMETER\n",
    "TIME = 8\n",
    "BATCH = 256\n",
    "# IMAGE_PIXEL_CHANNEL = 3\n",
    "IMAGE_SIZE = 32\n",
    "# CLASS_NUM = 10\n",
    "\n",
    "## 데이터셋 선택하세요 #########\n",
    "# which_data = 'MNIST'\n",
    "which_data = 'CIFAR10'\n",
    "################################\n",
    "\n",
    "rate_coding = True\n",
    "\n",
    "if (which_data == 'MNIST'):\n",
    "    data_path = '/data2'\n",
    "\n",
    "    if rate_coding :\n",
    "        transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "    else : \n",
    "        transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transform)\n",
    "\n",
    "\n",
    "    testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(trainset,\n",
    "                            batch_size =BATCH,\n",
    "                            shuffle = True,\n",
    "                            num_workers =2)\n",
    "    test_loader = DataLoader(testset,\n",
    "                            batch_size =BATCH,\n",
    "                            shuffle = False,\n",
    "                            num_workers =2)\n",
    "\n",
    "\n",
    "if (which_data == 'CIFAR10'):\n",
    "    data_path = '/data2/cifar10'\n",
    "\n",
    "    if rate_coding :\n",
    "        transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            transforms.ToTensor()])\n",
    "\n",
    "        transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                            transforms.ToTensor()])\n",
    "    \n",
    "    else :\n",
    "        transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "        transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transform_train)\n",
    "\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=transform_test)\n",
    "\n",
    "    train_loader = DataLoader(trainset,\n",
    "                            batch_size =BATCH,\n",
    "                            shuffle = True,\n",
    "                            num_workers =2)\n",
    "    test_loader = DataLoader(testset,\n",
    "                            batch_size =BATCH,\n",
    "                            shuffle = False,\n",
    "                            num_workers =2)\n",
    "\n",
    "    '''\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "            'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "    '''\n",
    "\n",
    "\n",
    "# 데이터 로더에서 첫 번째 배치를 가져옵니다.\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "# 채널 수와 클래스 개수를 확인합니다.\n",
    "IMAGE_PIXEL_CHANNEL = images.shape[1]\n",
    "CLASS_NUM = len(torch.unique(labels))\n",
    "print('IMAGE_PIXEL_CHANNEL:', IMAGE_PIXEL_CHANNEL)\n",
    "print('CLASS_NUM:', CLASS_NUM)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIF_layer 레이어의 하이퍼파라미터\n",
    "lif_layer_v_init = 0.0\n",
    "lif_layer_v_decay = 0.6 \n",
    "lif_layer_v_threshold = 1.2\n",
    "lif_layer_v_reset = 0.0\n",
    "lif_layer_sg_width = 1\n",
    "\n",
    "## SYNAPSE_CONV 레이어의 하이퍼파라미터\n",
    "synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL\n",
    "# synapse_conv_out_channels = layer별 지정\n",
    "synapse_conv_kernel_size = 3\n",
    "synapse_conv_stride = 1\n",
    "synapse_conv_padding = 1\n",
    "synapse_conv_trace_const1 = 1\n",
    "synapse_conv_trace_const2 = lif_layer_v_decay\n",
    "\n",
    "## SYNAPSE_FC 레이어의 하이퍼파라미터\n",
    "# synapse_fc_in_features = 마지막CONV_OUT_CHANNEL * H * W\n",
    "synapse_fc_out_features = CLASS_NUM\n",
    "synapse_fc_trace_const1 = 1\n",
    "synapse_fc_trace_const2 = lif_layer_v_decay\n",
    "\n",
    "\n",
    "def make_layers_conv(cfg, in_c=3):\n",
    "    layers = []\n",
    "    img_size_var = IMAGE_SIZE\n",
    "    in_channels = in_c\n",
    "    for which in cfg:\n",
    "        if which == 'P':\n",
    "            # layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            layers += [nn.AvgPool2d(kernel_size=2, stride=2)]\n",
    "            img_size_var = img_size_var // 2\n",
    "        else:\n",
    "            out_channels = which\n",
    "            layers += [SYNAPSE_CONV(in_channels=in_channels,\n",
    "                                    out_channels=out_channels, \n",
    "                                    kernel_size=synapse_conv_kernel_size, \n",
    "                                    stride=synapse_conv_stride, \n",
    "                                    padding=synapse_conv_padding, \n",
    "                                    trace_const1=synapse_conv_trace_const1, \n",
    "                                    trace_const2=synapse_conv_trace_const2)]\n",
    "            img_size_var = (img_size_var - synapse_conv_kernel_size + 2*synapse_conv_padding)//synapse_conv_stride + 1\n",
    "\n",
    "            layers += [LIF_layer(v_init=lif_layer_v_init, \n",
    "                                    v_decay=lif_layer_v_decay, \n",
    "                                    v_threshold=lif_layer_v_threshold, \n",
    "                                    v_reset=lif_layer_v_reset, \n",
    "                                    sg_width=lif_layer_sg_width)]\n",
    "            in_channels = which\n",
    "\n",
    "    return nn.Sequential(*layers), in_channels, img_size_var\n",
    "\n",
    "\n",
    "\n",
    "class MY_SNN_CONV(nn.Module):\n",
    "    def __init__(self, cfg, inc = 3):\n",
    "        super(MY_SNN_CONV, self).__init__()\n",
    "\n",
    "        self.layers, self.conv_last_channels, self.img_size_var = make_layers_conv(cfg, in_c=inc)\n",
    "\n",
    "        self.synapse_FC = SYNAPSE_FC(in_features=self.conv_last_channels*self.img_size_var*self.img_size_var,  # 마지막CONV의 OUT_CHANNEL * H * W\n",
    "                                      out_features=synapse_fc_out_features, \n",
    "                                      trace_const1=synapse_fc_trace_const1, \n",
    "                                      trace_const2=synapse_fc_trace_const2)\n",
    "        \n",
    "\n",
    "    def forward(self, spike_input):\n",
    "        # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "        spike_input = spike_input.permute(1, 0, 2, 3, 4)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "        spike_input = self.layers(spike_input)\n",
    "\n",
    "        spike_input = spike_input.view(spike_input.size(0), spike_input.size(1), -1)\n",
    "        \n",
    "        spike_input = self.synapse_FC(spike_input)\n",
    "\n",
    "        spike_input = spike_input.sum(axis=0)\n",
    "\n",
    "        return spike_input\n",
    "\n",
    "\n",
    "\n",
    "def make_layers_fc(cfg, in_c=3, img_size = 32, out_c=10):\n",
    "    layers = []\n",
    "    in_channels = in_c * img_size * img_size\n",
    "    class_num = out_c\n",
    "    for which in cfg:\n",
    "        out_channels = which\n",
    "        layers += [SYNAPSE_FC(in_features=in_channels,  # 마지막CONV의 OUT_CHANNEL * H * W\n",
    "                                      out_features=out_channels, \n",
    "                                      trace_const1=synapse_fc_trace_const1, \n",
    "                                      trace_const2=synapse_fc_trace_const2)]\n",
    "\n",
    "        layers += [LIF_layer(v_init=lif_layer_v_init, \n",
    "                                v_decay=lif_layer_v_decay, \n",
    "                                v_threshold=lif_layer_v_threshold, \n",
    "                                v_reset=lif_layer_v_reset, \n",
    "                                sg_width=lif_layer_sg_width)]\n",
    "        in_channels = which\n",
    "\n",
    "    \n",
    "    out_channels = class_num\n",
    "    layers += [SYNAPSE_FC(in_features=in_channels,  \n",
    "                                    out_features=out_channels, \n",
    "                                    trace_const1=synapse_fc_trace_const1, \n",
    "                                    trace_const2=synapse_fc_trace_const2)]\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class MY_SNN_FC(nn.Module):\n",
    "    def __init__(self, cfg, inc = 3):\n",
    "        super(MY_SNN_FC, self).__init__()\n",
    "\n",
    "        self.layers = make_layers_fc(cfg, in_c=inc, img_size = IMAGE_SIZE, out_c=synapse_fc_out_features)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, spike_input):\n",
    "        # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "        spike_input = spike_input.permute(1, 0, 2, 3, 4)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "        spike_input = spike_input.view(spike_input.size(0), spike_input.size(1), -1)\n",
    "        \n",
    "        spike_input = self.layers(spike_input)\n",
    "\n",
    "        spike_input = spike_input.sum(axis=0)\n",
    "\n",
    "        return spike_input\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "training acc: 15.62%\n",
      "validation acc: 19.11%\n",
      "iter_one_val_time: 5.670857667922974 seconds\n",
      "training acc: 19.92%\n",
      "validation acc: 11.95%\n",
      "iter_one_val_time: 5.922300815582275 seconds\n",
      "epoch_time: 79.34645533561707 seconds\n",
      "\n",
      "\n",
      "epoch 1\n",
      "training acc: 15.23%\n",
      "validation acc: 16.85%\n",
      "iter_one_val_time: 5.420670032501221 seconds\n",
      "training acc: 14.45%\n",
      "validation acc: 14.13%\n",
      "iter_one_val_time: 5.785154104232788 seconds\n",
      "epoch_time: 62.71440100669861 seconds\n",
      "\n",
      "\n",
      "epoch 2\n",
      "training acc: 11.33%\n",
      "validation acc: 14.35%\n",
      "iter_one_val_time: 5.680572032928467 seconds\n",
      "training acc: 10.16%\n",
      "validation acc: 14.70%\n",
      "iter_one_val_time: 5.542245626449585 seconds\n",
      "epoch_time: 61.480024337768555 seconds\n",
      "\n",
      "\n",
      "epoch 3\n",
      "training acc: 13.67%\n",
      "validation acc: 13.54%\n",
      "iter_one_val_time: 5.651238679885864 seconds\n",
      "training acc: 13.67%\n",
      "validation acc: 13.40%\n",
      "iter_one_val_time: 5.23375129699707 seconds\n",
      "epoch_time: 61.69606018066406 seconds\n",
      "\n",
      "\n",
      "epoch 4\n",
      "training acc: 17.19%\n",
      "validation acc: 17.43%\n",
      "iter_one_val_time: 5.714173078536987 seconds\n",
      "training acc: 9.77%\n",
      "validation acc: 14.01%\n",
      "iter_one_val_time: 5.105064868927002 seconds\n",
      "epoch_time: 61.67701721191406 seconds\n",
      "\n",
      "\n",
      "epoch 5\n",
      "training acc: 16.80%\n",
      "validation acc: 13.63%\n",
      "iter_one_val_time: 5.575453996658325 seconds\n",
      "training acc: 12.89%\n",
      "validation acc: 16.40%\n",
      "iter_one_val_time: 5.402896404266357 seconds\n",
      "epoch_time: 61.64238476753235 seconds\n",
      "\n",
      "\n",
      "epoch 6\n",
      "training acc: 25.78%\n",
      "validation acc: 14.60%\n",
      "iter_one_val_time: 5.380134582519531 seconds\n",
      "training acc: 20.70%\n",
      "validation acc: 18.34%\n",
      "iter_one_val_time: 5.593449592590332 seconds\n",
      "epoch_time: 61.74367904663086 seconds\n",
      "\n",
      "\n",
      "epoch 7\n",
      "training acc: 16.41%\n",
      "validation acc: 20.47%\n",
      "iter_one_val_time: 5.115158319473267 seconds\n",
      "training acc: 16.02%\n",
      "validation acc: 16.37%\n",
      "iter_one_val_time: 5.7324182987213135 seconds\n",
      "epoch_time: 61.452231645584106 seconds\n",
      "\n",
      "\n",
      "epoch 8\n",
      "training acc: 16.02%\n",
      "validation acc: 20.15%\n",
      "iter_one_val_time: 5.922641754150391 seconds\n",
      "training acc: 14.45%\n",
      "validation acc: 19.43%\n",
      "iter_one_val_time: 5.349294424057007 seconds\n",
      "epoch_time: 62.31295418739319 seconds\n",
      "\n",
      "\n",
      "epoch 9\n",
      "training acc: 17.58%\n",
      "validation acc: 14.80%\n",
      "iter_one_val_time: 5.6560399532318115 seconds\n",
      "training acc: 16.41%\n",
      "validation acc: 15.21%\n",
      "iter_one_val_time: 5.4620749950408936 seconds\n",
      "epoch_time: 61.47597002983093 seconds\n",
      "\n",
      "\n",
      "epoch 10\n",
      "training acc: 16.41%\n",
      "validation acc: 16.03%\n",
      "iter_one_val_time: 5.450291872024536 seconds\n",
      "training acc: 14.84%\n",
      "validation acc: 13.44%\n",
      "iter_one_val_time: 5.466668605804443 seconds\n",
      "epoch_time: 61.67426252365112 seconds\n",
      "\n",
      "\n",
      "epoch 11\n",
      "training acc: 10.94%\n",
      "validation acc: 10.43%\n",
      "iter_one_val_time: 5.828914403915405 seconds\n",
      "training acc: 16.41%\n",
      "validation acc: 13.29%\n",
      "iter_one_val_time: 5.484521150588989 seconds\n",
      "epoch_time: 61.863717555999756 seconds\n",
      "\n",
      "\n",
      "epoch 12\n",
      "training acc: 16.02%\n",
      "validation acc: 18.70%\n",
      "iter_one_val_time: 5.330973148345947 seconds\n",
      "training acc: 19.53%\n",
      "validation acc: 19.74%\n",
      "iter_one_val_time: 5.340704441070557 seconds\n",
      "epoch_time: 62.161407232284546 seconds\n",
      "\n",
      "\n",
      "epoch 13\n",
      "training acc: 15.62%\n",
      "validation acc: 18.37%\n",
      "iter_one_val_time: 5.494093656539917 seconds\n",
      "training acc: 17.97%\n",
      "validation acc: 18.76%\n",
      "iter_one_val_time: 5.34489631652832 seconds\n",
      "epoch_time: 61.77773356437683 seconds\n",
      "\n",
      "\n",
      "epoch 14\n",
      "training acc: 12.11%\n",
      "validation acc: 16.42%\n",
      "iter_one_val_time: 5.7254228591918945 seconds\n",
      "training acc: 18.75%\n",
      "validation acc: 14.01%\n",
      "iter_one_val_time: 5.4990434646606445 seconds\n",
      "epoch_time: 62.46935796737671 seconds\n",
      "\n",
      "\n",
      "epoch 15\n",
      "training acc: 18.75%\n",
      "validation acc: 18.37%\n",
      "iter_one_val_time: 5.645202398300171 seconds\n",
      "training acc: 14.45%\n",
      "validation acc: 12.36%\n",
      "iter_one_val_time: 5.408628463745117 seconds\n",
      "epoch_time: 62.14653754234314 seconds\n",
      "\n",
      "\n",
      "epoch 16\n",
      "training acc: 21.09%\n",
      "validation acc: 17.71%\n",
      "iter_one_val_time: 5.539357662200928 seconds\n",
      "training acc: 12.89%\n",
      "validation acc: 16.29%\n",
      "iter_one_val_time: 5.4397454261779785 seconds\n",
      "epoch_time: 62.45430374145508 seconds\n",
      "\n",
      "\n",
      "epoch 17\n",
      "training acc: 14.06%\n",
      "validation acc: 13.79%\n",
      "iter_one_val_time: 5.841844320297241 seconds\n",
      "training acc: 14.84%\n",
      "validation acc: 19.36%\n",
      "iter_one_val_time: 5.643965005874634 seconds\n",
      "epoch_time: 62.473188161849976 seconds\n",
      "\n",
      "\n",
      "epoch 18\n",
      "training acc: 17.58%\n",
      "validation acc: 17.49%\n",
      "iter_one_val_time: 5.582069158554077 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "#########################\n",
    "##### net 선택 #############\n",
    "\n",
    "cfg = {\n",
    "    'A': [64, 64], \n",
    "    'B': [64, 64, 64, 64], \n",
    "    'C': [64, 128, 256],\n",
    "    'D': [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512],\n",
    "    'K': [64, 64],\n",
    "}\n",
    "\n",
    "#### 새로 net 만들기 ####\n",
    "which_cfg = 'D'\n",
    "\n",
    "if (which_cfg >= 'K'):\n",
    "    net = MY_SNN_FC(cfg[which_cfg], inc = synapse_conv_in_channels).to(device)\n",
    "else:\n",
    "    net = MY_SNN_CONV(cfg[which_cfg], inc = synapse_conv_in_channels).to(device)\n",
    "net = torch.nn.DataParallel(net)\n",
    "val_acc = 0\n",
    "########################\n",
    "\n",
    "#### model 저장해놨던거 쓰기##############\n",
    "net = torch.load(\"net_save/save_now_net.pth\")\n",
    "# net = torch.load(\"net_save/mnist97.pth\")\n",
    "##########################################\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    print('epoch', epoch)\n",
    "    epoch_start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        net.train()\n",
    "\n",
    "        # print('\\niter', i)\n",
    "        iter_one_train_time_start = time.time()\n",
    "\n",
    "        inputs, labels = data\n",
    "\n",
    "        if rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "        \n",
    "        # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함.\n",
    "        # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "\n",
    "\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        batch = BATCH \n",
    "        if labels.size(0) != BATCH: \n",
    "            batch = labels.size(0)\n",
    "\n",
    "\n",
    "        ####### training accruacy ######\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted[0:batch] == labels).sum().item()\n",
    "        if i % 100 == 9:\n",
    "            print(f'training acc: {100 * correct / total:.2f}%')\n",
    "        ################################\n",
    "\n",
    "        loss = criterion(outputs[0:batch,:], labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # optimizer.zero_grad()와 loss.backward() 호출 후에 실행해야 합니다.\n",
    "        # if (i % 100 == 9):\n",
    "        #     print('\\n\\nepoch', epoch, 'iter', i)\n",
    "        #     for name, param in net.named_parameters():\n",
    "        #         if param.requires_grad:\n",
    "        #             print('\\n\\n\\n\\n' , name, param.grad)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # print(\"Epoch: {}, Iter: {}, Loss: {}\".format(epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "        iter_one_train_time_end = time.time()\n",
    "        elapsed_time = iter_one_train_time_end - iter_one_train_time_start  # 실행 시간 계산\n",
    "        # print(f\"iter_one_train_time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "        if i % 100 == 9:\n",
    "            iter_one_val_time_start = time.time()\n",
    "            \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "                how_many_val_image=0\n",
    "                for data in test_loader:\n",
    "                    how_many_val_image += 1\n",
    "                    inputs, labels = data\n",
    "        \n",
    "                    if rate_coding == True :\n",
    "                        inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "                    else :\n",
    "                        inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "\n",
    "                    \n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = net(inputs.permute(1, 0, 2, 3, 4))\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    batch = BATCH \n",
    "                    if labels.size(0) != BATCH: \n",
    "                        batch = labels.size(0)\n",
    "                    correct += (predicted[0:batch] == labels).sum().item()\n",
    "                print(f'validation acc: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "            iter_one_val_time_end = time.time()\n",
    "            elapsed_time = iter_one_val_time_end - iter_one_val_time_start  # 실행 시간 계산\n",
    "            print(f\"iter_one_val_time: {elapsed_time} seconds\")\n",
    "            if val_acc < correct / total:\n",
    "                val_acc = correct / total\n",
    "                torch.save(net.state_dict(), \"net_save/save_now_net_weights.pth\")\n",
    "                torch.save(net, \"net_save/save_now_net.pth\")\n",
    "                torch.save(net.module.state_dict(), \"net_save/save_now_net_weights2.pth\")\n",
    "                torch.save(net.module, \"net_save/save_now_net2.pth\")\n",
    "    epoch_time_end = time.time()\n",
    "    epoch_time = epoch_time_end - epoch_start_time  # 실행 시간 계산\n",
    "    \n",
    "    print(f\"epoch_time: {epoch_time} seconds\")\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 1., 0., 1.],\n",
       "        [1., 0., 0., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 1., 0., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "data_path = '/data2'\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((28, 28)),\n",
    "                            transforms.ToTensor()])\n",
    "\n",
    "# OR\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((28, 28)),\n",
    "                                transforms.Lambda(lambda x: torch.from_numpy(np.array(x)))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(trainset,\n",
    "                        batch_size =64,\n",
    "                        shuffle = True,\n",
    "                        num_workers =2)\n",
    "\n",
    "from snntorch import spikegen\n",
    "torch.set_printoptions(threshold=torch.inf)\n",
    "\n",
    "# Iterate through minibatches\n",
    "data = iter(train_loader)\n",
    "data_it, targets_it = next(data)\n",
    "# Spiking Data\n",
    "# data_it = spikegen.rate(data_it, num_steps=8)\n",
    "print(data_it.shape)\n",
    "\n",
    "# print(data_it[0,0,0,:,:])\n",
    "\n",
    "# print(data_it[0])\n",
    "# print(targets_it[0])\n",
    "\n",
    "\n",
    "a = torch.Tensor([1, .2, .7, 1])\n",
    "a=spikegen.rate(a, num_steps=8)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): MY_SNN_CONV(\n",
       "    (layers): Sequential(\n",
       "      (0): SYNAPSE_CONV()\n",
       "      (1): LIF_layer()\n",
       "      (2): SYNAPSE_CONV()\n",
       "      (3): LIF_layer()\n",
       "    )\n",
       "    (synapse_FC): SYNAPSE_FC()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    my_seed = 42,\n",
    "                    TIME = 8,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "                    synapse_conv_trace_const1 = 1,\n",
    "                    synapse_conv_trace_const2 = lif_layer_v_decay,\n",
    "\n",
    "                    synapse_fc_out_features = CLASS_NUM,\n",
    "                    synapse_fc_trace_const1 = 1,\n",
    "                    synapse_fc_trace_const2 = lif_layer_v_decay,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "                    cfg = [64, 64],\n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 100,\n",
    "                    verbose_interval = 100\n",
    "                  ):\n",
    "    \n",
    "    import sys\n",
    "    import os\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.backends.cudnn as cudnn\n",
    "\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "    import torchvision\n",
    "    import torchvision.datasets\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    import time\n",
    "\n",
    "    from snntorch import spikegen\n",
    "    import matplotlib.pyplot as plt\n",
    "    import snntorch.spikeplot as splt\n",
    "    from IPython.display import HTML\n",
    "\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "\n",
    "    \n",
    "    torch.manual_seed(my_seed)\n",
    "\n",
    "\n",
    "    class SYNAPSE_FC_METHOD(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, spike_one_time, spike_now, weight, bias):\n",
    "            ctx.save_for_backward(spike_one_time, spike_now, weight, bias)\n",
    "            return F.linear(spike_one_time, weight, bias=bias)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output_current):\n",
    "            #############밑에부터 수정해라#######\n",
    "            spike_one_time, spike_now, weight, bias = ctx.saved_tensors\n",
    "            \n",
    "            ## 이거 클론해야되는지 모르겠음!!!!\n",
    "            grad_output_current_clone = grad_output_current.clone()\n",
    "\n",
    "            grad_input_spike = grad_weight = grad_bias = None\n",
    "\n",
    "\n",
    "            if ctx.needs_input_grad[0]:\n",
    "                grad_input_spike = grad_output_current_clone @ weight\n",
    "            if ctx.needs_input_grad[2]:\n",
    "                grad_weight = grad_output_current_clone.t() @ spike_now\n",
    "            if bias is not None and ctx.needs_input_grad[3]:\n",
    "                grad_bias = grad_output_current_clone.sum(0)\n",
    "\n",
    "            return grad_input_spike, None, grad_weight, grad_bias\n",
    "\n",
    "     \n",
    "    class SYNAPSE_FC(nn.Module):\n",
    "        def __init__(self, in_features, out_features, trace_const1=1, trace_const2=0.7):\n",
    "            super(SYNAPSE_FC, self).__init__()\n",
    "            self.in_features = in_features\n",
    "            self.out_features = out_features\n",
    "            self.trace_const1 = trace_const1\n",
    "            self.trace_const2 = trace_const2\n",
    "\n",
    "            # self.weight = torch.randn(self.out_features, self.in_features, requires_grad=True)\n",
    "            # self.bias = torch.randn(self.out_features, requires_grad=True)\n",
    "            self.weight = nn.Parameter(torch.randn(self.out_features, self.in_features))\n",
    "            self.bias = nn.Parameter(torch.randn(self.out_features))\n",
    "\n",
    "        def forward(self, spike):\n",
    "            # spike: [Time, Batch, Features]   \n",
    "            Time = spike.shape[0]\n",
    "            Batch = spike.shape[1] \n",
    "            output_current = torch.zeros(Time, Batch, self.out_features, device=spike.device)\n",
    "\n",
    "            # spike_detach = spike.detach().clone()\n",
    "            spike_detach = spike.detach()\n",
    "            spike_past = torch.zeros_like(spike_detach[0], device=spike.device)\n",
    "            spike_now = torch.zeros_like(spike_detach[0], device=spike.device)\n",
    "\n",
    "            for t in range(Time):\n",
    "                spike_now = self.trace_const1*spike_detach[t] + self.trace_const2*spike_past\n",
    "                output_current[t]= SYNAPSE_FC_METHOD.apply(spike[t], spike_now, self.weight, self.bias) \n",
    "                spike_past = spike_now\n",
    "\n",
    "            return output_current \n",
    "\n",
    "\n",
    "\n",
    "    class SYNAPSE_CONV_METHOD(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, spike_one_time, spike_now, weight, bias, stride=1, padding=1):\n",
    "            ctx.save_for_backward(spike_one_time, spike_now, weight, bias, torch.tensor([stride], requires_grad=False), torch.tensor([padding], requires_grad=False))\n",
    "            return F.conv2d(spike_one_time, weight, bias=bias, stride=stride, padding=padding)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output_current):\n",
    "            spike_one_time, spike_now, weight, bias, stride, padding = ctx.saved_tensors\n",
    "            stride=stride.item()\n",
    "            padding=padding.item()\n",
    "            \n",
    "            ## 이거 클론해야되는지 모르겠음!!!!\n",
    "            grad_output_current_clone = grad_output_current.clone()\n",
    "\n",
    "            grad_input_spike = grad_weight = grad_bias = None\n",
    "\n",
    "\n",
    "            if ctx.needs_input_grad[0]:\n",
    "                grad_input_spike = F.conv_transpose2d(grad_output_current_clone, weight, stride=stride, padding=padding)\n",
    "            if ctx.needs_input_grad[2]:\n",
    "                grad_weight = torch.nn.grad.conv2d_weight(spike_now, weight.shape, grad_output_current_clone,\n",
    "                                                        stride=stride, padding=padding)\n",
    "            if bias is not None and ctx.needs_input_grad[3]:\n",
    "                grad_bias = grad_output_current_clone.sum((0, -1, -2))\n",
    "\n",
    "            return grad_input_spike, None, grad_weight, grad_bias, None, None\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    class SYNAPSE_CONV(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, trace_const1=1, trace_const2=0.7):\n",
    "            super(SYNAPSE_CONV, self).__init__()\n",
    "            self.in_channels = in_channels\n",
    "            self.out_channels = out_channels\n",
    "            self.kernel_size = kernel_size\n",
    "            self.stride = stride\n",
    "            self.padding = padding\n",
    "            self.trace_const1 = trace_const1\n",
    "            self.trace_const2 = trace_const2\n",
    "\n",
    "            # self.weight = torch.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size, requires_grad=True)\n",
    "            # self.bias = torch.randn(self.out_channels, requires_grad=True)\n",
    "            self.weight = nn.Parameter(torch.randn(self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))\n",
    "            self.bias = nn.Parameter(torch.randn(self.out_channels))\n",
    "\n",
    "        def forward(self, spike):\n",
    "            # spike: [Time, Batch, Channel, Height, Width]   \n",
    "            # print('spike.shape', spike.shape)\n",
    "            Time = spike.shape[0]\n",
    "            Batch = spike.shape[1] \n",
    "            Channel = self.out_channels\n",
    "            Height = (spike.shape[3] + self.padding*2 - self.kernel_size) // self.stride + 1\n",
    "            Width = (spike.shape[4] + self.padding*2 - self.kernel_size) // self.stride + 1\n",
    "            output_current = torch.zeros(Time, Batch, Channel, Height, Width, device=spike.device)\n",
    "\n",
    "            # spike_detach = spike.detach().clone()\n",
    "            spike_detach = spike.detach()\n",
    "            spike_past = torch.zeros_like(spike_detach[0])\n",
    "            spike_now = torch.zeros_like(spike_detach[0])\n",
    "\n",
    "            for t in range(Time):\n",
    "                spike_now = self.trace_const1*spike_detach[t] + self.trace_const2*spike_past\n",
    "                output_current[t]= SYNAPSE_CONV_METHOD.apply(spike[t], spike_now, self.weight, self.bias, self.stride, self.padding) \n",
    "                spike_past = spike_now\n",
    "\n",
    "            return output_current\n",
    "\n",
    "\n",
    "\n",
    "    class LIF_METHOD(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input_current_one_time, v_one_time, v_decay, v_threshold, v_reset, sg_width):\n",
    "            v_one_time = v_one_time * v_decay + input_current_one_time # leak + pre-synaptic current integrate\n",
    "            spike = (v_one_time >= v_threshold).float() #fire\n",
    "            ctx.save_for_backward(v_one_time, torch.tensor([v_decay], requires_grad=False), \n",
    "                                torch.tensor([v_threshold], requires_grad=False), \n",
    "                                torch.tensor([v_reset], requires_grad=False), \n",
    "                                torch.tensor([sg_width], requires_grad=False)) # save before reset\n",
    "            v_one_time = (v_one_time - spike * v_threshold).clamp_min(0) # reset\n",
    "            return spike, v_one_time\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output_spike, grad_output_v):\n",
    "            v_one_time, v_decay, v_threshold, v_reset, sg_width = ctx.saved_tensors\n",
    "            v_decay=v_decay.item()\n",
    "            v_threshold=v_threshold.item()\n",
    "            v_reset=v_reset.item()\n",
    "            sg_width=sg_width.item()\n",
    "\n",
    "            grad_input_current = grad_output_spike.clone()\n",
    "            # grad_temp_v = grad_output_v.clone() # not used\n",
    "\n",
    "            ################ select one of the following surrogate gradient functions ################\n",
    "            #===========surrogate gradient function (rectangle)\n",
    "            grad_input_current = grad_input_current * ((v_one_time - v_threshold).abs() < sg_width/2).float() / sg_width\n",
    "\n",
    "            #===========surrogate gradient function (sigmoid)\n",
    "            # sig = torch.sigmoid((v_one_time - v_threshold))\n",
    "            # grad_input_current =  sig*(1-sig)*grad_input_current\n",
    "\n",
    "            #===========surrogate gradient function (rough rectangle)\n",
    "            # v_minus_th = (v_one_time - v_threshold)\n",
    "            # grad_input_current[v_minus_th <= -.5] = 0\n",
    "            # grad_input_current[v_minus_th > .5] = 0\n",
    "            ###########################################################################################\n",
    "            return grad_input_current, None, None, None, None, None\n",
    "\n",
    "    class LIF_layer(nn.Module):\n",
    "        def __init__ (self, v_init = 0.0, v_decay = 0.8, v_threshold = 0.5, v_reset = 0.0, sg_width = 1):\n",
    "            super(LIF_layer, self).__init__()\n",
    "            self.v_init = v_init\n",
    "            self.v_decay = v_decay\n",
    "            self.v_threshold = v_threshold\n",
    "            self.v_reset = v_reset\n",
    "            self.sg_width = sg_width\n",
    "\n",
    "        def forward(self, input_current):\n",
    "            v = torch.full_like(input_current, fill_value = self.v_init, dtype = torch.float) # v (membrane potential) init\n",
    "            post_spike = torch.full_like(input_current, fill_value = self.v_init, device=input_current.device, dtype = torch.float) # v (membrane potential) init\n",
    "            # i와 v와 post_spike size는 여기서 다 같음: [Time, Batch, Channel, Height, Width] \n",
    "\n",
    "            Time = v.shape[0]\n",
    "            for t in range(Time):\n",
    "                # leaky하고 input_current 더하고 fire하고 reset까지 (backward직접처리)\n",
    "                post_spike[t], v[t] = LIF_METHOD.apply(input_current[t], v[t], \n",
    "                                                self.v_decay, self.v_threshold, self.v_reset, self.sg_width) \n",
    "\n",
    "            return post_spike \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if (which_data == 'MNIST'):\n",
    "        data_path = '/data2'\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "\n",
    "\n",
    "    if (which_data == 'CIFAR10'):\n",
    "        data_path = '/data2/cifar10'\n",
    "\n",
    "        if rate_coding :\n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def make_layers_conv(cfg, in_c=3):\n",
    "        layers = []\n",
    "        img_size_var = IMAGE_SIZE\n",
    "        in_channels = in_c\n",
    "        for which in cfg:\n",
    "            if which == 'P':\n",
    "                # layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "                layers += [nn.AvgPool2d(kernel_size=2, stride=2)]\n",
    "                img_size_var = img_size_var // 2\n",
    "            else:\n",
    "                out_channels = which\n",
    "                layers += [SYNAPSE_CONV(in_channels=in_channels,\n",
    "                                        out_channels=out_channels, \n",
    "                                        kernel_size=synapse_conv_kernel_size, \n",
    "                                        stride=synapse_conv_stride, \n",
    "                                        padding=synapse_conv_padding, \n",
    "                                        trace_const1=synapse_conv_trace_const1, \n",
    "                                        trace_const2=synapse_conv_trace_const2)]\n",
    "                img_size_var = (img_size_var - synapse_conv_kernel_size + 2*synapse_conv_padding)//synapse_conv_stride + 1\n",
    "\n",
    "                layers += [LIF_layer(v_init=lif_layer_v_init, \n",
    "                                        v_decay=lif_layer_v_decay, \n",
    "                                        v_threshold=lif_layer_v_threshold, \n",
    "                                        v_reset=lif_layer_v_reset, \n",
    "                                        sg_width=lif_layer_sg_width)]\n",
    "                in_channels = which\n",
    "\n",
    "        return nn.Sequential(*layers), in_channels, img_size_var\n",
    "\n",
    "\n",
    "\n",
    "    class MY_SNN_CONV(nn.Module):\n",
    "        def __init__(self, cfg, inc = 3):\n",
    "            super(MY_SNN_CONV, self).__init__()\n",
    "\n",
    "            self.layers, self.conv_last_channels, self.img_size_var = make_layers_conv(cfg, in_c=inc)\n",
    "\n",
    "            self.synapse_FC = SYNAPSE_FC(in_features=self.conv_last_channels*self.img_size_var*self.img_size_var,  # 마지막CONV의 OUT_CHANNEL * H * W\n",
    "                                        out_features=synapse_fc_out_features, \n",
    "                                        trace_const1=synapse_fc_trace_const1, \n",
    "                                        trace_const2=synapse_fc_trace_const2)\n",
    "            \n",
    "\n",
    "        def forward(self, spike_input):\n",
    "            # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "            spike_input = spike_input.permute(1, 0, 2, 3, 4)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "            spike_input = self.layers(spike_input)\n",
    "\n",
    "            spike_input = spike_input.view(spike_input.size(0), spike_input.size(1), -1)\n",
    "            \n",
    "            spike_input = self.synapse_FC(spike_input)\n",
    "\n",
    "            spike_input = spike_input.sum(axis=0)\n",
    "\n",
    "            return spike_input\n",
    "\n",
    "\n",
    "\n",
    "    def make_layers_fc(cfg, in_c=3, img_size = 32, out_c=10):\n",
    "        layers = []\n",
    "        in_channels = in_c * img_size * img_size\n",
    "        class_num = out_c\n",
    "        for which in cfg:\n",
    "            out_channels = which\n",
    "            layers += [SYNAPSE_FC(in_features=in_channels,  # 마지막CONV의 OUT_CHANNEL * H * W\n",
    "                                        out_features=out_channels, \n",
    "                                        trace_const1=synapse_fc_trace_const1, \n",
    "                                        trace_const2=synapse_fc_trace_const2)]\n",
    "\n",
    "            layers += [LIF_layer(v_init=lif_layer_v_init, \n",
    "                                    v_decay=lif_layer_v_decay, \n",
    "                                    v_threshold=lif_layer_v_threshold, \n",
    "                                    v_reset=lif_layer_v_reset, \n",
    "                                    sg_width=lif_layer_sg_width)]\n",
    "            in_channels = which\n",
    "\n",
    "        \n",
    "        out_channels = class_num\n",
    "        layers += [SYNAPSE_FC(in_features=in_channels,  \n",
    "                                        out_features=out_channels, \n",
    "                                        trace_const1=synapse_fc_trace_const1, \n",
    "                                        trace_const2=synapse_fc_trace_const2)]\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    class MY_SNN_FC(nn.Module):\n",
    "        def __init__(self, cfg, inc = 3):\n",
    "            super(MY_SNN_FC, self).__init__()\n",
    "\n",
    "            self.layers = make_layers_fc(cfg, in_c=inc, img_size = IMAGE_SIZE, out_c=synapse_fc_out_features)\n",
    "\n",
    "            \n",
    "\n",
    "        def forward(self, spike_input):\n",
    "            # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "            spike_input = spike_input.permute(1, 0, 2, 3, 4)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "            spike_input = spike_input.view(spike_input.size(0), spike_input.size(1), -1)\n",
    "            \n",
    "            spike_input = self.layers(spike_input)\n",
    "\n",
    "            spike_input = spike_input.sum(axis=0)\n",
    "\n",
    "            return spike_input\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    if pre_trained == False:\n",
    "        if (convTrue_fcFalse == False):\n",
    "            net = MY_SNN_FC(cfg, inc = synapse_conv_in_channels).to(device)\n",
    "        else:\n",
    "            net = MY_SNN_CONV(cfg, inc = synapse_conv_in_channels).to(device)\n",
    "        net = torch.nn.DataParallel(net)\n",
    "    else:\n",
    "        net = torch.load(pre_trained_path)\n",
    "\n",
    "    val_acc = 0\n",
    "\n",
    "    net = net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        print('epoch', epoch)\n",
    "        epoch_start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            net.train()\n",
    "\n",
    "            # print('\\niter', i)\n",
    "            iter_one_train_time_start = time.time()\n",
    "\n",
    "            inputs, labels = data\n",
    "\n",
    "            if rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            \n",
    "            # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함.\n",
    "            # inputs: [Batch, Time, Channel, Height, Width]   \n",
    "\n",
    "\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            batch = BATCH \n",
    "            if labels.size(0) != BATCH: \n",
    "                batch = labels.size(0)\n",
    "\n",
    "\n",
    "            ####### training accruacy ######\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted[0:batch] == labels).sum().item()\n",
    "            if i % verbose_interval == 9:\n",
    "                print(f'training acc: {100 * correct / total:.2f}%')\n",
    "            ################################\n",
    "\n",
    "            loss = criterion(outputs[0:batch,:], labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # optimizer.zero_grad()와 loss.backward() 호출 후에 실행해야 합니다.\n",
    "            # if (i % 100 == 9):\n",
    "            #     print('\\n\\nepoch', epoch, 'iter', i)\n",
    "            #     for name, param in net.named_parameters():\n",
    "            #         if param.requires_grad:\n",
    "            #             print('\\n\\n\\n\\n' , name, param.grad)\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            # print(\"Epoch: {}, Iter: {}, Loss: {}\".format(epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "            iter_one_train_time_end = time.time()\n",
    "            elapsed_time = iter_one_train_time_end - iter_one_train_time_start  # 실행 시간 계산\n",
    "            # print(f\"iter_one_train_time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "            if i % verbose_interval == 9:\n",
    "                iter_one_val_time_start = time.time()\n",
    "                \n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    net.eval()\n",
    "                    how_many_val_image=0\n",
    "                    for data in test_loader:\n",
    "                        how_many_val_image += 1\n",
    "                        inputs, labels = data\n",
    "            \n",
    "                        if rate_coding == True :\n",
    "                            inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "\n",
    "                        \n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        outputs = net(inputs.permute(1, 0, 2, 3, 4))\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        batch = BATCH \n",
    "                        if labels.size(0) != BATCH: \n",
    "                            batch = labels.size(0)\n",
    "                        correct += (predicted[0:batch] == labels).sum().item()\n",
    "                    print(f'validation acc: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "                iter_one_val_time_end = time.time()\n",
    "                elapsed_time = iter_one_val_time_end - iter_one_val_time_start  # 실행 시간 계산\n",
    "                print(f\"iter_one_val_time: {elapsed_time} seconds\")\n",
    "                if val_acc < correct / total:\n",
    "                    val_acc = correct / total\n",
    "                    torch.save(net.state_dict(), \"net_save/save_now_net_weights.pth\")\n",
    "                    torch.save(net, \"net_save/save_now_net.pth\")\n",
    "                    torch.save(net.module.state_dict(), \"net_save/save_now_net_weights2.pth\")\n",
    "                    torch.save(net.module, \"net_save/save_now_net2.pth\")\n",
    "        epoch_time_end = time.time()\n",
    "        epoch_time = epoch_time_end - epoch_start_time  # 실행 시간 계산\n",
    "        \n",
    "        print(f\"epoch_time: {epoch_time} seconds\")\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    cfg = {\n",
    "    'A': [64, 64], \n",
    "    'B': [64, 64, 64, 64], \n",
    "    'C': [64, 128, 256],\n",
    "    'D': [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512],\n",
    "    'K': [64, 64],\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
