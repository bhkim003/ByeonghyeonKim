{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15806/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    \n",
    "                    random_select_ratio = 4,\n",
    "                    leaky_temporal_filter= 1.0,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                this_sample_total_tw = inputs.shape[1]\n",
    "                # Í∞Å timestepÎ≥Ñ Ìï© Í≥ÑÏÇ∞\n",
    "                time_sums = inputs.sum(dim=(0, 2, 3, 4))\n",
    "                \n",
    "                # time_sums ÏÉÅÏúÑ NÍ∞ú Ïù∏Îç±Ïä§ Ï∂îÏ∂ú (Ïòà: N = TIME * 2)\n",
    "                N = min(this_sample_total_tw, round(temporal_filter*TIME * random_select_ratio))\n",
    "                topk_vals, topk_idx = torch.topk(time_sums, k=N)\n",
    "\n",
    "                # ÏÉÅÏúÑ NÍ∞ú Ï§ëÏóêÏÑú TIMEÍ∞ú ÎûúÎç§ ÏÑ†ÌÉù\n",
    "                chosen_idx = topk_idx[torch.randperm(N)[:temporal_filter*TIME]]\n",
    "\n",
    "                # ÏÑ†ÌÉùÌïú Ïù∏Îç±Ïä§ Ï†ïÎ†¨ (ÏãúÍ∞Ñ ÏàúÏÑú Ïú†ÏßÄ)\n",
    "                chosen_idx, _ = torch.sort(chosen_idx)\n",
    " \n",
    "                # ÏÑ†ÌÉùÌïú Ïù∏Îç±Ïä§Î°ú Ïä¨ÎùºÏù¥Ïã±\n",
    "                inputs = inputs[:,chosen_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if temporal_filter_accumulation == False:\n",
    "                    if dvs_clipping != 0:\n",
    "                        inputs[inputs<dvs_clipping] = 0.0\n",
    "                        inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        for ttt in range(temporal_filter):\n",
    "                            if ttt == 0:\n",
    "                                pass\n",
    "                            else:\n",
    "                                slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] = slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] + leaky_temporal_filter * slice_concat[..., shape_temp[-1] * (ttt-1) : shape_temp[-1] * (ttt)]\n",
    "                        slice_bucket.append(slice_concat)\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True:\n",
    "                    if dvs_clipping != 0:\n",
    "                        inputs[inputs<dvs_clipping] = 0.0\n",
    "                        inputs[inputs>=dvs_clipping] = 1.0\n",
    "                # if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                #     inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if temporal_filter_accumulation == False:\n",
    "                                if dvs_clipping != 0:\n",
    "                                    inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                    inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    for ttt in range(temporal_filter):\n",
    "                                        if ttt == 0:\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] = slice_concat[..., shape_temp[-1] * (ttt) : shape_temp[-1] * (ttt+1)] + leaky_temporal_filter * slice_concat[..., shape_temp[-1] * (ttt-1) : shape_temp[-1] * (ttt)]\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True:\n",
    "                                if dvs_clipping != 0:\n",
    "                                    inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                    inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "                            # if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                            #     inputs_val = (inputs_val != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "                        # ##############################################################################################\n",
    "                        # dvs_visualization(inputs_val, labels_val, TIME, BATCH, my_seed)\n",
    "                        # #####################################################################################################\n",
    "\n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "                \n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"2\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.25,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = True, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-9,-9],[-9,-9],[-8,-8]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 random_select_ratio = 4,\n",
    "#                 leaky_temporal_filter= 0.25,\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tj3a2tzt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251114_144405-tj3a2tzt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tj3a2tzt' target=\"_blank\">firm-sweep-46</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tj3a2tzt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tj3a2tzt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251114_144412_055', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 2, 'leaky_temporal_filter': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 659.0\n",
      "lif layer 1 self.abs_max_v: 659.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 599.0\n",
      "lif layer 2 self.abs_max_v: 599.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 193.0\n",
      "lif layer 1 self.abs_max_v: 870.5\n",
      "fc layer 2 self.abs_max_out: 866.0\n",
      "lif layer 2 self.abs_max_v: 932.5\n",
      "fc layer 3 self.abs_max_out: 396.0\n",
      "fc layer 1 self.abs_max_out: 779.0\n",
      "lif layer 1 self.abs_max_v: 1158.5\n",
      "lif layer 2 self.abs_max_v: 1082.0\n",
      "fc layer 1 self.abs_max_out: 990.0\n",
      "fc layer 2 self.abs_max_out: 891.0\n",
      "lif layer 2 self.abs_max_v: 1326.0\n",
      "fc layer 1 self.abs_max_out: 1584.0\n",
      "lif layer 1 self.abs_max_v: 1584.0\n",
      "fc layer 1 self.abs_max_out: 2127.0\n",
      "lif layer 1 self.abs_max_v: 2127.0\n",
      "fc layer 1 self.abs_max_out: 2601.0\n",
      "lif layer 1 self.abs_max_v: 2601.0\n",
      "fc layer 1 self.abs_max_out: 2729.0\n",
      "lif layer 1 self.abs_max_v: 2729.0\n",
      "fc layer 1 self.abs_max_out: 2838.0\n",
      "lif layer 1 self.abs_max_v: 2838.0\n",
      "fc layer 2 self.abs_max_out: 914.0\n",
      "fc layer 1 self.abs_max_out: 3098.0\n",
      "lif layer 1 self.abs_max_v: 3098.0\n",
      "fc layer 2 self.abs_max_out: 967.0\n",
      "lif layer 2 self.abs_max_v: 1377.5\n",
      "fc layer 1 self.abs_max_out: 3308.0\n",
      "lif layer 1 self.abs_max_v: 3308.0\n",
      "fc layer 3 self.abs_max_out: 494.0\n",
      "fc layer 2 self.abs_max_out: 994.0\n",
      "lif layer 2 self.abs_max_v: 1381.5\n",
      "lif layer 2 self.abs_max_v: 1662.0\n",
      "lif layer 2 self.abs_max_v: 1680.0\n",
      "fc layer 2 self.abs_max_out: 1210.0\n",
      "lif layer 1 self.abs_max_v: 3948.5\n",
      "lif layer 1 self.abs_max_v: 4781.5\n",
      "fc layer 2 self.abs_max_out: 1282.0\n",
      "fc layer 3 self.abs_max_out: 535.0\n",
      "lif layer 2 self.abs_max_v: 1851.5\n",
      "fc layer 2 self.abs_max_out: 1349.0\n",
      "fc layer 2 self.abs_max_out: 1418.0\n",
      "fc layer 2 self.abs_max_out: 1580.0\n",
      "fc layer 1 self.abs_max_out: 4435.0\n",
      "fc layer 1 self.abs_max_out: 4691.0\n",
      "lif layer 2 self.abs_max_v: 2000.5\n",
      "fc layer 2 self.abs_max_out: 1618.0\n",
      "fc layer 3 self.abs_max_out: 563.0\n",
      "lif layer 2 self.abs_max_v: 2128.0\n",
      "lif layer 2 self.abs_max_v: 2141.0\n",
      "lif layer 1 self.abs_max_v: 4805.0\n",
      "fc layer 3 self.abs_max_out: 566.0\n",
      "lif layer 1 self.abs_max_v: 4843.0\n",
      "fc layer 2 self.abs_max_out: 1620.0\n",
      "fc layer 2 self.abs_max_out: 1651.0\n",
      "fc layer 2 self.abs_max_out: 1654.0\n",
      "fc layer 2 self.abs_max_out: 1662.0\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "lif layer 2 self.abs_max_v: 2163.0\n",
      "lif layer 2 self.abs_max_v: 2192.0\n",
      "fc layer 2 self.abs_max_out: 1674.0\n",
      "lif layer 1 self.abs_max_v: 5037.5\n",
      "lif layer 1 self.abs_max_v: 5095.0\n",
      "fc layer 3 self.abs_max_out: 588.0\n",
      "fc layer 3 self.abs_max_out: 698.0\n",
      "fc layer 3 self.abs_max_out: 733.0\n",
      "lif layer 1 self.abs_max_v: 5243.5\n",
      "fc layer 1 self.abs_max_out: 5182.0\n",
      "lif layer 1 self.abs_max_v: 5289.0\n",
      "lif layer 1 self.abs_max_v: 5618.0\n",
      "lif layer 1 self.abs_max_v: 5693.5\n",
      "lif layer 1 self.abs_max_v: 5793.0\n",
      "lif layer 1 self.abs_max_v: 5981.5\n",
      "lif layer 1 self.abs_max_v: 6016.0\n",
      "fc layer 2 self.abs_max_out: 1679.0\n",
      "fc layer 2 self.abs_max_out: 1681.0\n",
      "fc layer 2 self.abs_max_out: 1718.0\n",
      "fc layer 2 self.abs_max_out: 1721.0\n",
      "fc layer 2 self.abs_max_out: 1841.0\n",
      "lif layer 2 self.abs_max_v: 2303.5\n",
      "lif layer 2 self.abs_max_v: 2326.5\n",
      "lif layer 1 self.abs_max_v: 6183.5\n",
      "lif layer 1 self.abs_max_v: 6186.0\n",
      "fc layer 1 self.abs_max_out: 5482.0\n",
      "lif layer 1 self.abs_max_v: 6367.0\n",
      "lif layer 1 self.abs_max_v: 6426.5\n",
      "lif layer 1 self.abs_max_v: 6526.0\n",
      "fc layer 1 self.abs_max_out: 5512.0\n",
      "lif layer 1 self.abs_max_v: 6917.0\n",
      "lif layer 1 self.abs_max_v: 7291.5\n",
      "lif layer 1 self.abs_max_v: 7493.0\n",
      "lif layer 1 self.abs_max_v: 7581.5\n",
      "lif layer 2 self.abs_max_v: 2365.0\n",
      "lif layer 2 self.abs_max_v: 2380.0\n",
      "lif layer 2 self.abs_max_v: 2442.0\n",
      "lif layer 2 self.abs_max_v: 2458.0\n",
      "lif layer 2 self.abs_max_v: 2596.5\n",
      "lif layer 1 self.abs_max_v: 7617.5\n",
      "fc layer 3 self.abs_max_out: 745.0\n",
      "fc layer 3 self.abs_max_out: 761.0\n",
      "lif layer 2 self.abs_max_v: 2715.5\n",
      "lif layer 2 self.abs_max_v: 2773.5\n",
      "lif layer 2 self.abs_max_v: 2824.0\n",
      "lif layer 2 self.abs_max_v: 2861.0\n",
      "lif layer 2 self.abs_max_v: 2996.0\n",
      "lif layer 2 self.abs_max_v: 3096.5\n",
      "lif layer 2 self.abs_max_v: 3146.5\n",
      "lif layer 2 self.abs_max_v: 3244.5\n",
      "lif layer 1 self.abs_max_v: 7669.5\n",
      "lif layer 2 self.abs_max_v: 3253.5\n",
      "lif layer 2 self.abs_max_v: 3254.0\n",
      "lif layer 1 self.abs_max_v: 8135.0\n",
      "lif layer 1 self.abs_max_v: 8351.5\n",
      "fc layer 2 self.abs_max_out: 1884.0\n",
      "fc layer 2 self.abs_max_out: 1907.0\n",
      "fc layer 3 self.abs_max_out: 813.0\n",
      "lif layer 1 self.abs_max_v: 8435.5\n",
      "lif layer 1 self.abs_max_v: 8822.5\n",
      "lif layer 1 self.abs_max_v: 9516.5\n",
      "lif layer 1 self.abs_max_v: 10179.5\n",
      "lif layer 2 self.abs_max_v: 3270.5\n",
      "lif layer 2 self.abs_max_v: 3386.5\n",
      "fc layer 3 self.abs_max_out: 890.0\n",
      "fc layer 3 self.abs_max_out: 907.0\n",
      "fc layer 3 self.abs_max_out: 917.0\n",
      "lif layer 2 self.abs_max_v: 3411.0\n",
      "lif layer 2 self.abs_max_v: 3437.5\n",
      "fc layer 1 self.abs_max_out: 5574.0\n",
      "fc layer 1 self.abs_max_out: 5636.0\n",
      "lif layer 1 self.abs_max_v: 10489.5\n",
      "lif layer 1 self.abs_max_v: 10623.0\n",
      "lif layer 1 self.abs_max_v: 10706.5\n",
      "lif layer 1 self.abs_max_v: 10722.0\n",
      "fc layer 1 self.abs_max_out: 6047.0\n",
      "lif layer 1 self.abs_max_v: 10769.5\n",
      "fc layer 1 self.abs_max_out: 6214.0\n",
      "lif layer 1 self.abs_max_v: 10980.0\n",
      "fc layer 3 self.abs_max_out: 956.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.653730/  2.004586, val:  28.33%, val_best:  28.33%, tr:  99.39%, tr_best:  99.39%, epoch time: 91.60 seconds, 1.53 minutes\n",
      "layer   1  Sparsity: 80.9127%\n",
      "layer   2  Sparsity: 73.7793%\n",
      "layer   3  Sparsity: 70.5291%\n",
      "total_backward_count 9790 real_backward_count 1299  13.269%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 11007.5\n",
      "lif layer 1 self.abs_max_v: 11041.5\n",
      "fc layer 2 self.abs_max_out: 1913.0\n",
      "lif layer 1 self.abs_max_v: 11249.0\n",
      "fc layer 2 self.abs_max_out: 1916.0\n",
      "lif layer 2 self.abs_max_v: 3480.0\n",
      "lif layer 2 self.abs_max_v: 3535.5\n",
      "lif layer 2 self.abs_max_v: 3580.0\n",
      "lif layer 2 self.abs_max_v: 3635.5\n",
      "fc layer 2 self.abs_max_out: 1969.0\n",
      "fc layer 2 self.abs_max_out: 1976.0\n",
      "lif layer 2 self.abs_max_v: 3747.0\n",
      "fc layer 3 self.abs_max_out: 1040.0\n",
      "lif layer 2 self.abs_max_v: 3778.0\n",
      "fc layer 2 self.abs_max_out: 1977.0\n",
      "fc layer 2 self.abs_max_out: 2008.0\n",
      "fc layer 2 self.abs_max_out: 2011.0\n",
      "fc layer 2 self.abs_max_out: 2155.0\n",
      "lif layer 1 self.abs_max_v: 11593.5\n",
      "lif layer 2 self.abs_max_v: 3814.0\n",
      "lif layer 2 self.abs_max_v: 3817.0\n",
      "lif layer 2 self.abs_max_v: 3821.5\n",
      "lif layer 2 self.abs_max_v: 3841.0\n",
      "fc layer 1 self.abs_max_out: 6498.0\n",
      "lif layer 1 self.abs_max_v: 11724.5\n",
      "lif layer 2 self.abs_max_v: 3857.5\n",
      "lif layer 2 self.abs_max_v: 3867.5\n",
      "lif layer 2 self.abs_max_v: 3925.0\n",
      "lif layer 2 self.abs_max_v: 3926.5\n",
      "fc layer 3 self.abs_max_out: 1051.0\n",
      "lif layer 2 self.abs_max_v: 3998.5\n",
      "lif layer 2 self.abs_max_v: 4102.5\n",
      "fc layer 3 self.abs_max_out: 1077.0\n",
      "fc layer 2 self.abs_max_out: 2173.0\n",
      "lif layer 1 self.abs_max_v: 11906.5\n",
      "fc layer 1 self.abs_max_out: 6634.0\n",
      "lif layer 1 self.abs_max_v: 12218.0\n",
      "fc layer 2 self.abs_max_out: 2176.0\n",
      "fc layer 1 self.abs_max_out: 6698.0\n",
      "fc layer 1 self.abs_max_out: 7037.0\n",
      "lif layer 1 self.abs_max_v: 12762.5\n",
      "lif layer 1 self.abs_max_v: 13275.5\n",
      "fc layer 2 self.abs_max_out: 2185.0\n",
      "lif layer 2 self.abs_max_v: 4165.5\n",
      "lif layer 2 self.abs_max_v: 4222.0\n",
      "fc layer 1 self.abs_max_out: 7353.0\n",
      "lif layer 1 self.abs_max_v: 13413.5\n",
      "lif layer 1 self.abs_max_v: 13447.0\n",
      "lif layer 1 self.abs_max_v: 13656.5\n",
      "lif layer 1 self.abs_max_v: 13759.5\n",
      "fc layer 2 self.abs_max_out: 2300.0\n",
      "fc layer 2 self.abs_max_out: 2422.0\n",
      "lif layer 2 self.abs_max_v: 4333.0\n",
      "lif layer 2 self.abs_max_v: 4452.5\n",
      "lif layer 2 self.abs_max_v: 4477.5\n",
      "lif layer 2 self.abs_max_v: 4478.0\n",
      "lif layer 2 self.abs_max_v: 4638.0\n",
      "fc layer 2 self.abs_max_out: 2424.0\n",
      "lif layer 1 self.abs_max_v: 13763.5\n",
      "fc layer 2 self.abs_max_out: 2465.0\n",
      "fc layer 2 self.abs_max_out: 2534.0\n",
      "lif layer 2 self.abs_max_v: 4830.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.502450/  1.871625, val:  30.83%, val_best:  30.83%, tr:  99.59%, tr_best:  99.59%, epoch time: 93.80 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 80.9040%\n",
      "layer   2  Sparsity: 76.7525%\n",
      "layer   3  Sparsity: 73.0057%\n",
      "total_backward_count 19580 real_backward_count 2530  12.921%\n",
      "fc layer 3 self.abs_max_out: 1142.0\n",
      "lif layer 2 self.abs_max_v: 4853.5\n",
      "fc layer 3 self.abs_max_out: 1240.0\n",
      "lif layer 1 self.abs_max_v: 13820.0\n",
      "fc layer 1 self.abs_max_out: 7570.0\n",
      "lif layer 1 self.abs_max_v: 14480.0\n",
      "fc layer 2 self.abs_max_out: 2605.0\n",
      "lif layer 2 self.abs_max_v: 4877.0\n",
      "fc layer 2 self.abs_max_out: 2716.0\n",
      "lif layer 2 self.abs_max_v: 5154.5\n",
      "fc layer 1 self.abs_max_out: 7748.0\n",
      "fc layer 1 self.abs_max_out: 8047.0\n",
      "fc layer 1 self.abs_max_out: 8049.0\n",
      "fc layer 1 self.abs_max_out: 9005.0\n",
      "lif layer 1 self.abs_max_v: 16221.0\n",
      "fc layer 1 self.abs_max_out: 9156.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.431049/  1.838228, val:  42.08%, val_best:  42.08%, tr:  99.28%, tr_best:  99.59%, epoch time: 100.11 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 80.9075%\n",
      "layer   2  Sparsity: 78.5153%\n",
      "layer   3  Sparsity: 72.8783%\n",
      "total_backward_count 29370 real_backward_count 3744  12.748%\n",
      "fc layer 3 self.abs_max_out: 1244.0\n",
      "fc layer 3 self.abs_max_out: 1286.0\n",
      "fc layer 3 self.abs_max_out: 1329.0\n",
      "lif layer 1 self.abs_max_v: 16368.5\n",
      "fc layer 1 self.abs_max_out: 9552.0\n",
      "lif layer 1 self.abs_max_v: 16486.5\n",
      "fc layer 1 self.abs_max_out: 9869.0\n",
      "lif layer 1 self.abs_max_v: 18112.5\n",
      "fc layer 1 self.abs_max_out: 10023.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.420735/  1.869898, val:  36.67%, val_best:  42.08%, tr:  99.39%, tr_best:  99.59%, epoch time: 98.16 seconds, 1.64 minutes\n",
      "layer   1  Sparsity: 80.9204%\n",
      "layer   2  Sparsity: 80.1826%\n",
      "layer   3  Sparsity: 74.1333%\n",
      "total_backward_count 39160 real_backward_count 4987  12.735%\n",
      "lif layer 1 self.abs_max_v: 18320.5\n",
      "fc layer 3 self.abs_max_out: 1394.0\n",
      "fc layer 1 self.abs_max_out: 10507.0\n",
      "lif layer 1 self.abs_max_v: 19150.5\n",
      "lif layer 2 self.abs_max_v: 5218.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.444469/  1.811720, val:  40.42%, val_best:  42.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 108.70 seconds, 1.81 minutes\n",
      "layer   1  Sparsity: 80.9166%\n",
      "layer   2  Sparsity: 80.1593%\n",
      "layer   3  Sparsity: 76.7466%\n",
      "total_backward_count 48950 real_backward_count 6239  12.746%\n",
      "fc layer 1 self.abs_max_out: 10530.0\n",
      "fc layer 1 self.abs_max_out: 10887.0\n",
      "lif layer 1 self.abs_max_v: 20189.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.443881/  1.847677, val:  35.00%, val_best:  42.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 111.42 seconds, 1.86 minutes\n",
      "layer   1  Sparsity: 80.9148%\n",
      "layer   2  Sparsity: 81.4436%\n",
      "layer   3  Sparsity: 77.9236%\n",
      "total_backward_count 58740 real_backward_count 7526  12.812%\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.391024/  1.831531, val:  40.83%, val_best:  42.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 110.17 seconds, 1.84 minutes\n",
      "layer   1  Sparsity: 80.9235%\n",
      "layer   2  Sparsity: 81.4296%\n",
      "layer   3  Sparsity: 75.8169%\n",
      "total_backward_count 68530 real_backward_count 8770  12.797%\n",
      "fc layer 3 self.abs_max_out: 1531.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.332839/  1.674516, val:  46.25%, val_best:  46.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 112.66 seconds, 1.88 minutes\n",
      "layer   1  Sparsity: 80.9202%\n",
      "layer   2  Sparsity: 81.0644%\n",
      "layer   3  Sparsity: 74.6180%\n",
      "total_backward_count 78320 real_backward_count 9944  12.697%\n",
      "fc layer 2 self.abs_max_out: 2800.0\n",
      "lif layer 2 self.abs_max_v: 5261.0\n",
      "lif layer 2 self.abs_max_v: 5299.5\n",
      "lif layer 2 self.abs_max_v: 5381.0\n",
      "fc layer 1 self.abs_max_out: 11226.0\n",
      "lif layer 1 self.abs_max_v: 20755.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.331868/  1.744370, val:  35.83%, val_best:  46.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 114.31 seconds, 1.91 minutes\n",
      "layer   1  Sparsity: 80.9065%\n",
      "layer   2  Sparsity: 79.0924%\n",
      "layer   3  Sparsity: 74.6642%\n",
      "total_backward_count 88110 real_backward_count 11188  12.698%\n",
      "fc layer 1 self.abs_max_out: 11358.0\n",
      "lif layer 1 self.abs_max_v: 21043.0\n",
      "fc layer 2 self.abs_max_out: 2818.0\n",
      "fc layer 2 self.abs_max_out: 2898.0\n",
      "lif layer 2 self.abs_max_v: 5403.0\n",
      "fc layer 2 self.abs_max_out: 2947.0\n",
      "lif layer 2 self.abs_max_v: 5485.5\n",
      "fc layer 2 self.abs_max_out: 3028.0\n",
      "lif layer 2 self.abs_max_v: 5771.0\n",
      "fc layer 2 self.abs_max_out: 3241.0\n",
      "lif layer 2 self.abs_max_v: 6117.5\n",
      "lif layer 2 self.abs_max_v: 6130.0\n",
      "lif layer 2 self.abs_max_v: 6156.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.372297/  1.827051, val:  42.08%, val_best:  46.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 115.54 seconds, 1.93 minutes\n",
      "layer   1  Sparsity: 80.9035%\n",
      "layer   2  Sparsity: 77.8236%\n",
      "layer   3  Sparsity: 75.4224%\n",
      "total_backward_count 97900 real_backward_count 12416  12.682%\n",
      "fc layer 2 self.abs_max_out: 3249.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.402045/  1.784656, val:  42.50%, val_best:  46.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 113.92 seconds, 1.90 minutes\n",
      "layer   1  Sparsity: 80.9188%\n",
      "layer   2  Sparsity: 77.9384%\n",
      "layer   3  Sparsity: 76.2928%\n",
      "total_backward_count 107690 real_backward_count 13621  12.648%\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.390852/  1.811710, val:  33.33%, val_best:  46.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 114.74 seconds, 1.91 minutes\n",
      "layer   1  Sparsity: 80.9067%\n",
      "layer   2  Sparsity: 77.5511%\n",
      "layer   3  Sparsity: 76.2198%\n",
      "total_backward_count 117480 real_backward_count 14809  12.606%\n",
      "fc layer 3 self.abs_max_out: 1566.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.434798/  1.837343, val:  37.50%, val_best:  46.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 111.58 seconds, 1.86 minutes\n",
      "layer   1  Sparsity: 80.9387%\n",
      "layer   2  Sparsity: 78.8506%\n",
      "layer   3  Sparsity: 78.0766%\n",
      "total_backward_count 127270 real_backward_count 15990  12.564%\n",
      "fc layer 3 self.abs_max_out: 1720.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.440674/  1.823680, val:  29.58%, val_best:  46.25%, tr:  99.69%, tr_best:  99.90%, epoch time: 110.47 seconds, 1.84 minutes\n",
      "layer   1  Sparsity: 80.9194%\n",
      "layer   2  Sparsity: 78.2740%\n",
      "layer   3  Sparsity: 79.8414%\n",
      "total_backward_count 137060 real_backward_count 17199  12.549%\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.397505/  1.755301, val:  40.42%, val_best:  46.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 110.98 seconds, 1.85 minutes\n",
      "layer   1  Sparsity: 80.9249%\n",
      "layer   2  Sparsity: 78.0236%\n",
      "layer   3  Sparsity: 79.5647%\n",
      "total_backward_count 146850 real_backward_count 18390  12.523%\n",
      "fc layer 1 self.abs_max_out: 11430.0\n",
      "fc layer 1 self.abs_max_out: 11479.0\n",
      "fc layer 1 self.abs_max_out: 12459.0\n",
      "lif layer 1 self.abs_max_v: 21234.5\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.403555/  1.766359, val:  44.58%, val_best:  46.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 115.92 seconds, 1.93 minutes\n",
      "layer   1  Sparsity: 80.9153%\n",
      "layer   2  Sparsity: 78.9187%\n",
      "layer   3  Sparsity: 77.4340%\n",
      "total_backward_count 156640 real_backward_count 19568  12.492%\n",
      "lif layer 1 self.abs_max_v: 21297.0\n",
      "fc layer 1 self.abs_max_out: 12492.0\n",
      "lif layer 1 self.abs_max_v: 21314.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.375500/  1.799041, val:  54.17%, val_best:  54.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 113.54 seconds, 1.89 minutes\n",
      "layer   1  Sparsity: 80.9302%\n",
      "layer   2  Sparsity: 78.7459%\n",
      "layer   3  Sparsity: 76.4714%\n",
      "total_backward_count 166430 real_backward_count 20752  12.469%\n",
      "lif layer 1 self.abs_max_v: 21701.5\n",
      "lif layer 1 self.abs_max_v: 22198.5\n",
      "lif layer 1 self.abs_max_v: 22691.5\n",
      "lif layer 2 self.abs_max_v: 6243.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.392210/  1.740677, val:  49.17%, val_best:  54.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 113.20 seconds, 1.89 minutes\n",
      "layer   1  Sparsity: 80.9207%\n",
      "layer   2  Sparsity: 78.4030%\n",
      "layer   3  Sparsity: 77.6979%\n",
      "total_backward_count 176220 real_backward_count 21874  12.413%\n",
      "lif layer 1 self.abs_max_v: 22730.5\n",
      "lif layer 1 self.abs_max_v: 22809.5\n",
      "lif layer 1 self.abs_max_v: 23377.5\n",
      "fc layer 2 self.abs_max_out: 3263.0\n",
      "fc layer 2 self.abs_max_out: 3281.0\n",
      "lif layer 2 self.abs_max_v: 6316.5\n",
      "fc layer 2 self.abs_max_out: 3289.0\n",
      "fc layer 2 self.abs_max_out: 3378.0\n",
      "lif layer 2 self.abs_max_v: 6323.5\n",
      "lif layer 2 self.abs_max_v: 6430.0\n",
      "lif layer 2 self.abs_max_v: 6502.0\n",
      "fc layer 2 self.abs_max_out: 3412.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.354463/  1.771869, val:  48.75%, val_best:  54.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 113.88 seconds, 1.90 minutes\n",
      "layer   1  Sparsity: 80.9147%\n",
      "layer   2  Sparsity: 78.0675%\n",
      "layer   3  Sparsity: 77.0302%\n",
      "total_backward_count 186010 real_backward_count 23079  12.407%\n",
      "fc layer 2 self.abs_max_out: 3423.0\n",
      "fc layer 1 self.abs_max_out: 12902.0\n",
      "lif layer 1 self.abs_max_v: 23649.5\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.363690/  1.684778, val:  44.17%, val_best:  54.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 112.90 seconds, 1.88 minutes\n",
      "layer   1  Sparsity: 80.9321%\n",
      "layer   2  Sparsity: 77.9288%\n",
      "layer   3  Sparsity: 77.6117%\n",
      "total_backward_count 195800 real_backward_count 24189  12.354%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.347215/  1.791819, val:  44.17%, val_best:  54.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 111.86 seconds, 1.86 minutes\n",
      "layer   1  Sparsity: 80.9299%\n",
      "layer   2  Sparsity: 78.5349%\n",
      "layer   3  Sparsity: 78.7837%\n",
      "total_backward_count 205590 real_backward_count 25349  12.330%\n",
      "fc layer 1 self.abs_max_out: 12975.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.388103/  1.808582, val:  43.75%, val_best:  54.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 112.28 seconds, 1.87 minutes\n",
      "layer   1  Sparsity: 80.9165%\n",
      "layer   2  Sparsity: 79.1109%\n",
      "layer   3  Sparsity: 78.8757%\n",
      "total_backward_count 215380 real_backward_count 26517  12.312%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.427508/  1.704203, val:  49.17%, val_best:  54.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 113.24 seconds, 1.89 minutes\n",
      "layer   1  Sparsity: 80.9234%\n",
      "layer   2  Sparsity: 79.9148%\n",
      "layer   3  Sparsity: 80.2863%\n",
      "total_backward_count 225170 real_backward_count 27726  12.313%\n",
      "lif layer 1 self.abs_max_v: 23969.0\n",
      "fc layer 1 self.abs_max_out: 13479.0\n",
      "lif layer 1 self.abs_max_v: 24436.5\n",
      "lif layer 1 self.abs_max_v: 24931.5\n",
      "lif layer 1 self.abs_max_v: 24992.5\n",
      "lif layer 1 self.abs_max_v: 25156.5\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.391990/  1.738171, val:  57.08%, val_best:  57.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 115.07 seconds, 1.92 minutes\n",
      "layer   1  Sparsity: 80.9368%\n",
      "layer   2  Sparsity: 80.3859%\n",
      "layer   3  Sparsity: 80.0189%\n",
      "total_backward_count 234960 real_backward_count 28934  12.314%\n",
      "fc layer 1 self.abs_max_out: 13548.0\n",
      "lif layer 1 self.abs_max_v: 26111.0\n",
      "lif layer 1 self.abs_max_v: 26153.5\n",
      "fc layer 1 self.abs_max_out: 13818.0\n",
      "fc layer 1 self.abs_max_out: 14003.0\n",
      "lif layer 1 self.abs_max_v: 26243.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.327315/  1.672869, val:  56.25%, val_best:  57.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 114.87 seconds, 1.91 minutes\n",
      "layer   1  Sparsity: 80.9204%\n",
      "layer   2  Sparsity: 79.4483%\n",
      "layer   3  Sparsity: 77.4606%\n",
      "total_backward_count 244750 real_backward_count 30110  12.302%\n",
      "fc layer 2 self.abs_max_out: 3432.0\n",
      "fc layer 2 self.abs_max_out: 3433.0\n",
      "lif layer 2 self.abs_max_v: 6531.5\n",
      "lif layer 2 self.abs_max_v: 6572.0\n",
      "lif layer 2 self.abs_max_v: 6572.5\n",
      "fc layer 1 self.abs_max_out: 14596.0\n",
      "lif layer 1 self.abs_max_v: 26260.0\n",
      "lif layer 1 self.abs_max_v: 26343.5\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.337551/  1.700819, val:  53.75%, val_best:  57.08%, tr:  99.39%, tr_best: 100.00%, epoch time: 114.28 seconds, 1.90 minutes\n",
      "layer   1  Sparsity: 80.9165%\n",
      "layer   2  Sparsity: 79.1877%\n",
      "layer   3  Sparsity: 79.1533%\n",
      "total_backward_count 254540 real_backward_count 31303  12.298%\n",
      "fc layer 2 self.abs_max_out: 3444.0\n",
      "lif layer 1 self.abs_max_v: 27087.5\n",
      "lif layer 1 self.abs_max_v: 27884.0\n",
      "lif layer 1 self.abs_max_v: 28410.0\n",
      "fc layer 1 self.abs_max_out: 14737.0\n",
      "fc layer 1 self.abs_max_out: 14872.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.363601/  1.733027, val:  47.50%, val_best:  57.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 114.75 seconds, 1.91 minutes\n",
      "layer   1  Sparsity: 80.9211%\n",
      "layer   2  Sparsity: 79.1728%\n",
      "layer   3  Sparsity: 79.2983%\n",
      "total_backward_count 264330 real_backward_count 32486  12.290%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.374225/  1.713902, val:  54.58%, val_best:  57.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 114.08 seconds, 1.90 minutes\n",
      "layer   1  Sparsity: 80.9122%\n",
      "layer   2  Sparsity: 78.7756%\n",
      "layer   3  Sparsity: 79.0135%\n",
      "total_backward_count 274120 real_backward_count 33659  12.279%\n",
      "fc layer 1 self.abs_max_out: 15758.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.340949/  1.732313, val:  51.25%, val_best:  57.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 115.10 seconds, 1.92 minutes\n",
      "layer   1  Sparsity: 80.9079%\n",
      "layer   2  Sparsity: 78.7386%\n",
      "layer   3  Sparsity: 78.8325%\n",
      "total_backward_count 283910 real_backward_count 34763  12.244%\n",
      "fc layer 2 self.abs_max_out: 3456.0\n",
      "fc layer 2 self.abs_max_out: 3717.0\n",
      "lif layer 2 self.abs_max_v: 6611.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.330464/  1.680939, val:  48.33%, val_best:  57.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 114.09 seconds, 1.90 minutes\n",
      "layer   1  Sparsity: 80.9249%\n",
      "layer   2  Sparsity: 77.8278%\n",
      "layer   3  Sparsity: 79.1870%\n",
      "total_backward_count 293700 real_backward_count 35967  12.246%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.340376/  1.762944, val:  48.75%, val_best:  57.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 114.09 seconds, 1.90 minutes\n",
      "layer   1  Sparsity: 80.9079%\n",
      "layer   2  Sparsity: 77.4616%\n",
      "layer   3  Sparsity: 77.7153%\n",
      "total_backward_count 303490 real_backward_count 37106  12.226%\n",
      "lif layer 1 self.abs_max_v: 28772.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.332534/  1.745373, val:  47.50%, val_best:  57.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 113.63 seconds, 1.89 minutes\n",
      "layer   1  Sparsity: 80.9193%\n",
      "layer   2  Sparsity: 78.5777%\n",
      "layer   3  Sparsity: 78.1014%\n",
      "total_backward_count 313280 real_backward_count 38254  12.211%\n",
      "lif layer 1 self.abs_max_v: 28952.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.309436/  1.674136, val:  54.17%, val_best:  57.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 114.34 seconds, 1.91 minutes\n",
      "layer   1  Sparsity: 80.9172%\n",
      "layer   2  Sparsity: 80.4354%\n",
      "layer   3  Sparsity: 78.5763%\n",
      "total_backward_count 323070 real_backward_count 39426  12.204%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.313309/  1.675293, val:  59.58%, val_best:  59.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 115.33 seconds, 1.92 minutes\n",
      "layer   1  Sparsity: 80.9043%\n",
      "layer   2  Sparsity: 79.8630%\n",
      "layer   3  Sparsity: 78.4649%\n",
      "total_backward_count 332860 real_backward_count 40646  12.211%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.315012/  1.718754, val:  49.58%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 115.04 seconds, 1.92 minutes\n",
      "layer   1  Sparsity: 80.9032%\n",
      "layer   2  Sparsity: 79.4653%\n",
      "layer   3  Sparsity: 79.9176%\n",
      "total_backward_count 342650 real_backward_count 41799  12.199%\n",
      "lif layer 2 self.abs_max_v: 6889.5\n",
      "fc layer 2 self.abs_max_out: 3873.0\n",
      "fc layer 2 self.abs_max_out: 3880.0\n",
      "lif layer 2 self.abs_max_v: 7007.5\n",
      "lif layer 2 self.abs_max_v: 7052.0\n",
      "lif layer 2 self.abs_max_v: 7101.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.359278/  1.713798, val:  57.08%, val_best:  59.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 112.09 seconds, 1.87 minutes\n",
      "layer   1  Sparsity: 80.9186%\n",
      "layer   2  Sparsity: 79.0491%\n",
      "layer   3  Sparsity: 79.5303%\n",
      "total_backward_count 352440 real_backward_count 42991  12.198%\n",
      "lif layer 1 self.abs_max_v: 29047.5\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.391721/  1.703069, val:  56.25%, val_best:  59.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 111.89 seconds, 1.86 minutes\n",
      "layer   1  Sparsity: 80.9130%\n",
      "layer   2  Sparsity: 78.1726%\n",
      "layer   3  Sparsity: 79.4785%\n",
      "total_backward_count 362230 real_backward_count 44166  12.193%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.317410/  1.706417, val:  49.17%, val_best:  59.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 111.48 seconds, 1.86 minutes\n",
      "layer   1  Sparsity: 80.9151%\n",
      "layer   2  Sparsity: 78.5180%\n",
      "layer   3  Sparsity: 79.3983%\n",
      "total_backward_count 372020 real_backward_count 45268  12.168%\n",
      "lif layer 2 self.abs_max_v: 7111.0\n",
      "fc layer 1 self.abs_max_out: 16019.0\n",
      "lif layer 1 self.abs_max_v: 29110.5\n",
      "lif layer 1 self.abs_max_v: 29388.5\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.344118/  1.668519, val:  50.00%, val_best:  59.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 111.78 seconds, 1.86 minutes\n",
      "layer   1  Sparsity: 80.9162%\n",
      "layer   2  Sparsity: 78.0784%\n",
      "layer   3  Sparsity: 79.2128%\n",
      "total_backward_count 381810 real_backward_count 46510  12.181%\n",
      "fc layer 2 self.abs_max_out: 3885.0\n",
      "lif layer 2 self.abs_max_v: 7278.5\n",
      "lif layer 1 self.abs_max_v: 29555.5\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.232319/  1.649734, val:  49.17%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 113.19 seconds, 1.89 minutes\n",
      "layer   1  Sparsity: 80.9043%\n",
      "layer   2  Sparsity: 78.1160%\n",
      "layer   3  Sparsity: 76.0239%\n",
      "total_backward_count 391600 real_backward_count 47666  12.172%\n",
      "fc layer 2 self.abs_max_out: 3903.0\n",
      "lif layer 2 self.abs_max_v: 7305.0\n",
      "lif layer 2 self.abs_max_v: 7449.5\n",
      "lif layer 1 self.abs_max_v: 29753.5\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.216893/  1.699867, val:  50.00%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 112.36 seconds, 1.87 minutes\n",
      "layer   1  Sparsity: 80.9192%\n",
      "layer   2  Sparsity: 78.0879%\n",
      "layer   3  Sparsity: 76.4457%\n",
      "total_backward_count 401390 real_backward_count 48812  12.161%\n",
      "lif layer 1 self.abs_max_v: 29835.0\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.262275/  1.613911, val:  47.08%, val_best:  59.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 113.17 seconds, 1.89 minutes\n",
      "layer   1  Sparsity: 80.9234%\n",
      "layer   2  Sparsity: 79.3758%\n",
      "layer   3  Sparsity: 77.4774%\n",
      "total_backward_count 411180 real_backward_count 49999  12.160%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.222815/  1.651574, val:  52.08%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 113.49 seconds, 1.89 minutes\n",
      "layer   1  Sparsity: 80.9180%\n",
      "layer   2  Sparsity: 79.3485%\n",
      "layer   3  Sparsity: 76.5572%\n",
      "total_backward_count 420970 real_backward_count 51086  12.135%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.185178/  1.658283, val:  50.42%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 115.35 seconds, 1.92 minutes\n",
      "layer   1  Sparsity: 80.9198%\n",
      "layer   2  Sparsity: 78.8518%\n",
      "layer   3  Sparsity: 75.2557%\n",
      "total_backward_count 430760 real_backward_count 52206  12.120%\n",
      "fc layer 1 self.abs_max_out: 16377.0\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.204943/  1.706743, val:  43.75%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 113.43 seconds, 1.89 minutes\n",
      "layer   1  Sparsity: 80.9226%\n",
      "layer   2  Sparsity: 78.6506%\n",
      "layer   3  Sparsity: 75.6753%\n",
      "total_backward_count 440550 real_backward_count 53303  12.099%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.238050/  1.615851, val:  53.75%, val_best:  59.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 113.54 seconds, 1.89 minutes\n",
      "layer   1  Sparsity: 80.9345%\n",
      "layer   2  Sparsity: 79.0975%\n",
      "layer   3  Sparsity: 76.1573%\n",
      "total_backward_count 450340 real_backward_count 54419  12.084%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.235020/  1.655787, val:  47.08%, val_best:  59.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 115.21 seconds, 1.92 minutes\n",
      "layer   1  Sparsity: 80.8986%\n",
      "layer   2  Sparsity: 78.7174%\n",
      "layer   3  Sparsity: 77.1916%\n",
      "total_backward_count 460130 real_backward_count 55567  12.076%\n",
      "fc layer 1 self.abs_max_out: 17074.0\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.284841/  1.731703, val:  40.00%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 114.40 seconds, 1.91 minutes\n",
      "layer   1  Sparsity: 80.9190%\n",
      "layer   2  Sparsity: 77.7905%\n",
      "layer   3  Sparsity: 78.3027%\n",
      "total_backward_count 469920 real_backward_count 56746  12.076%\n",
      "lif layer 1 self.abs_max_v: 30304.0\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.226423/  1.672254, val:  47.92%, val_best:  59.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 112.90 seconds, 1.88 minutes\n",
      "layer   1  Sparsity: 80.9095%\n",
      "layer   2  Sparsity: 77.8992%\n",
      "layer   3  Sparsity: 77.9473%\n",
      "total_backward_count 479710 real_backward_count 57873  12.064%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.270582/  1.701589, val:  53.75%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 113.32 seconds, 1.89 minutes\n",
      "layer   1  Sparsity: 80.9194%\n",
      "layer   2  Sparsity: 79.0921%\n",
      "layer   3  Sparsity: 78.6830%\n",
      "total_backward_count 489500 real_backward_count 58978  12.049%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.310856/  1.665916, val:  45.83%, val_best:  59.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 112.95 seconds, 1.88 minutes\n",
      "layer   1  Sparsity: 80.9034%\n",
      "layer   2  Sparsity: 79.6660%\n",
      "layer   3  Sparsity: 79.6259%\n",
      "total_backward_count 499290 real_backward_count 60118  12.041%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.229123/  1.661965, val:  47.50%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 115.54 seconds, 1.93 minutes\n",
      "layer   1  Sparsity: 80.9209%\n",
      "layer   2  Sparsity: 80.1114%\n",
      "layer   3  Sparsity: 78.3975%\n",
      "total_backward_count 509080 real_backward_count 61188  12.019%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.233657/  1.690725, val:  53.33%, val_best:  59.58%, tr:  99.28%, tr_best: 100.00%, epoch time: 120.40 seconds, 2.01 minutes\n",
      "layer   1  Sparsity: 80.9177%\n",
      "layer   2  Sparsity: 79.4781%\n",
      "layer   3  Sparsity: 76.9389%\n",
      "total_backward_count 518870 real_backward_count 62343  12.015%\n",
      "fc layer 2 self.abs_max_out: 3968.0\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.231009/  1.656535, val:  53.33%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 119.26 seconds, 1.99 minutes\n",
      "layer   1  Sparsity: 80.9168%\n",
      "layer   2  Sparsity: 78.0797%\n",
      "layer   3  Sparsity: 77.4870%\n",
      "total_backward_count 528660 real_backward_count 63491  12.010%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.274325/  1.669247, val:  55.83%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 120.74 seconds, 2.01 minutes\n",
      "layer   1  Sparsity: 80.9192%\n",
      "layer   2  Sparsity: 78.0256%\n",
      "layer   3  Sparsity: 78.1184%\n",
      "total_backward_count 538450 real_backward_count 64609  11.999%\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.261148/  1.584580, val:  56.25%, val_best:  59.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 117.61 seconds, 1.96 minutes\n",
      "layer   1  Sparsity: 80.9203%\n",
      "layer   2  Sparsity: 78.6658%\n",
      "layer   3  Sparsity: 78.0217%\n",
      "total_backward_count 548240 real_backward_count 65718  11.987%\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.222890/  1.688556, val:  42.08%, val_best:  59.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 118.00 seconds, 1.97 minutes\n",
      "layer   1  Sparsity: 80.9278%\n",
      "layer   2  Sparsity: 78.4304%\n",
      "layer   3  Sparsity: 78.2164%\n",
      "total_backward_count 558030 real_backward_count 66859  11.981%\n",
      "fc layer 2 self.abs_max_out: 4110.0\n",
      "lif layer 2 self.abs_max_v: 7531.5\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.256826/  1.661817, val:  55.42%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 115.27 seconds, 1.92 minutes\n",
      "layer   1  Sparsity: 80.9090%\n",
      "layer   2  Sparsity: 78.2451%\n",
      "layer   3  Sparsity: 78.8678%\n",
      "total_backward_count 567820 real_backward_count 67987  11.973%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.235766/  1.656277, val:  50.83%, val_best:  59.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 116.65 seconds, 1.94 minutes\n",
      "layer   1  Sparsity: 80.9017%\n",
      "layer   2  Sparsity: 79.2135%\n",
      "layer   3  Sparsity: 78.0679%\n",
      "total_backward_count 577610 real_backward_count 69092  11.962%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.228352/  1.586594, val:  52.92%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 116.64 seconds, 1.94 minutes\n",
      "layer   1  Sparsity: 80.9222%\n",
      "layer   2  Sparsity: 79.9784%\n",
      "layer   3  Sparsity: 77.1225%\n",
      "total_backward_count 587400 real_backward_count 70209  11.953%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.187539/  1.603349, val:  58.75%, val_best:  59.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 117.28 seconds, 1.95 minutes\n",
      "layer   1  Sparsity: 80.9032%\n",
      "layer   2  Sparsity: 79.5145%\n",
      "layer   3  Sparsity: 76.4779%\n",
      "total_backward_count 597190 real_backward_count 71311  11.941%\n",
      "lif layer 2 self.abs_max_v: 7609.0\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.216295/  1.614721, val:  57.50%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 117.71 seconds, 1.96 minutes\n",
      "layer   1  Sparsity: 80.9138%\n",
      "layer   2  Sparsity: 78.4178%\n",
      "layer   3  Sparsity: 77.0822%\n",
      "total_backward_count 606980 real_backward_count 72462  11.938%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.210405/  1.637027, val:  47.08%, val_best:  59.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 119.36 seconds, 1.99 minutes\n",
      "layer   1  Sparsity: 80.9095%\n",
      "layer   2  Sparsity: 78.9720%\n",
      "layer   3  Sparsity: 78.4612%\n",
      "total_backward_count 616770 real_backward_count 73617  11.936%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.190711/  1.606225, val:  56.67%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 119.41 seconds, 1.99 minutes\n",
      "layer   1  Sparsity: 80.9189%\n",
      "layer   2  Sparsity: 78.5249%\n",
      "layer   3  Sparsity: 77.2921%\n",
      "total_backward_count 626560 real_backward_count 74677  11.919%\n",
      "fc layer 1 self.abs_max_out: 17337.0\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.239235/  1.660982, val:  49.58%, val_best:  59.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 117.80 seconds, 1.96 minutes\n",
      "layer   1  Sparsity: 80.9072%\n",
      "layer   2  Sparsity: 78.9815%\n",
      "layer   3  Sparsity: 77.7689%\n",
      "total_backward_count 636350 real_backward_count 75786  11.909%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.202667/  1.581451, val:  59.17%, val_best:  59.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 119.15 seconds, 1.99 minutes\n",
      "layer   1  Sparsity: 80.9214%\n",
      "layer   2  Sparsity: 79.1610%\n",
      "layer   3  Sparsity: 78.2118%\n",
      "total_backward_count 646140 real_backward_count 76861  11.895%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.244422/  1.702435, val:  47.92%, val_best:  59.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 117.95 seconds, 1.97 minutes\n",
      "layer   1  Sparsity: 80.9113%\n",
      "layer   2  Sparsity: 78.6109%\n",
      "layer   3  Sparsity: 78.0352%\n",
      "total_backward_count 655930 real_backward_count 77947  11.883%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.268980/  1.695946, val:  49.58%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 118.06 seconds, 1.97 minutes\n",
      "layer   1  Sparsity: 80.9044%\n",
      "layer   2  Sparsity: 78.0592%\n",
      "layer   3  Sparsity: 78.2365%\n",
      "total_backward_count 665720 real_backward_count 79042  11.873%\n",
      "lif layer 1 self.abs_max_v: 30341.5\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.239292/  1.636754, val:  54.58%, val_best:  59.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 118.83 seconds, 1.98 minutes\n",
      "layer   1  Sparsity: 80.9080%\n",
      "layer   2  Sparsity: 78.5002%\n",
      "layer   3  Sparsity: 77.5993%\n",
      "total_backward_count 675510 real_backward_count 80152  11.865%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.239060/  1.689891, val:  48.75%, val_best:  59.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 118.27 seconds, 1.97 minutes\n",
      "layer   1  Sparsity: 80.9437%\n",
      "layer   2  Sparsity: 78.9446%\n",
      "layer   3  Sparsity: 77.2439%\n",
      "total_backward_count 685300 real_backward_count 81293  11.862%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.211758/  1.591277, val:  54.58%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 117.54 seconds, 1.96 minutes\n",
      "layer   1  Sparsity: 80.9090%\n",
      "layer   2  Sparsity: 79.0265%\n",
      "layer   3  Sparsity: 75.9672%\n",
      "total_backward_count 695090 real_backward_count 82466  11.864%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.223285/  1.642002, val:  53.33%, val_best:  59.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 117.27 seconds, 1.95 minutes\n",
      "layer   1  Sparsity: 80.8938%\n",
      "layer   2  Sparsity: 78.8578%\n",
      "layer   3  Sparsity: 77.2868%\n",
      "total_backward_count 704880 real_backward_count 83584  11.858%\n",
      "fc layer 2 self.abs_max_out: 4119.0\n",
      "fc layer 2 self.abs_max_out: 4149.0\n",
      "lif layer 1 self.abs_max_v: 30402.0\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.201569/  1.589193, val:  57.08%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 119.69 seconds, 1.99 minutes\n",
      "layer   1  Sparsity: 80.9147%\n",
      "layer   2  Sparsity: 79.2715%\n",
      "layer   3  Sparsity: 76.6879%\n",
      "total_backward_count 714670 real_backward_count 84694  11.851%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.186387/  1.614434, val:  50.42%, val_best:  59.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 117.75 seconds, 1.96 minutes\n",
      "layer   1  Sparsity: 80.9234%\n",
      "layer   2  Sparsity: 80.2184%\n",
      "layer   3  Sparsity: 77.2208%\n",
      "total_backward_count 724460 real_backward_count 85790  11.842%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.218937/  1.643443, val:  45.83%, val_best:  59.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 117.48 seconds, 1.96 minutes\n",
      "layer   1  Sparsity: 80.9247%\n",
      "layer   2  Sparsity: 80.3213%\n",
      "layer   3  Sparsity: 78.6698%\n",
      "total_backward_count 734250 real_backward_count 86927  11.839%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.240741/  1.616619, val:  56.67%, val_best:  59.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 119.36 seconds, 1.99 minutes\n",
      "layer   1  Sparsity: 80.9044%\n",
      "layer   2  Sparsity: 80.8053%\n",
      "layer   3  Sparsity: 77.0302%\n",
      "total_backward_count 744040 real_backward_count 88084  11.839%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.198341/  1.634139, val:  55.42%, val_best:  59.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 118.50 seconds, 1.98 minutes\n",
      "layer   1  Sparsity: 80.9306%\n",
      "layer   2  Sparsity: 80.5178%\n",
      "layer   3  Sparsity: 77.3264%\n",
      "total_backward_count 753830 real_backward_count 89259  11.841%\n",
      "lif layer 2 self.abs_max_v: 7683.0\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.218973/  1.605958, val:  49.17%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 118.08 seconds, 1.97 minutes\n",
      "layer   1  Sparsity: 80.9018%\n",
      "layer   2  Sparsity: 80.0754%\n",
      "layer   3  Sparsity: 78.4281%\n",
      "total_backward_count 763620 real_backward_count 90382  11.836%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.247832/  1.705272, val:  44.58%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 116.55 seconds, 1.94 minutes\n",
      "layer   1  Sparsity: 80.9157%\n",
      "layer   2  Sparsity: 80.3904%\n",
      "layer   3  Sparsity: 80.0047%\n",
      "total_backward_count 773410 real_backward_count 91505  11.831%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.272655/  1.651252, val:  54.17%, val_best:  59.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 118.60 seconds, 1.98 minutes\n",
      "layer   1  Sparsity: 80.9146%\n",
      "layer   2  Sparsity: 79.9517%\n",
      "layer   3  Sparsity: 79.4839%\n",
      "total_backward_count 783200 real_backward_count 92635  11.828%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.246460/  1.608317, val:  55.42%, val_best:  59.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 117.50 seconds, 1.96 minutes\n",
      "layer   1  Sparsity: 80.9315%\n",
      "layer   2  Sparsity: 79.5183%\n",
      "layer   3  Sparsity: 79.3064%\n",
      "total_backward_count 792990 real_backward_count 93774  11.825%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.278375/  1.712362, val:  45.83%, val_best:  59.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 119.89 seconds, 2.00 minutes\n",
      "layer   1  Sparsity: 80.9193%\n",
      "layer   2  Sparsity: 80.0918%\n",
      "layer   3  Sparsity: 80.5142%\n",
      "total_backward_count 802780 real_backward_count 94908  11.822%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.310302/  1.693129, val:  50.83%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 121.25 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.9076%\n",
      "layer   2  Sparsity: 79.0957%\n",
      "layer   3  Sparsity: 80.0191%\n",
      "total_backward_count 812570 real_backward_count 96083  11.825%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.273804/  1.694979, val:  43.33%, val_best:  59.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 119.18 seconds, 1.99 minutes\n",
      "layer   1  Sparsity: 80.9235%\n",
      "layer   2  Sparsity: 79.1545%\n",
      "layer   3  Sparsity: 79.5007%\n",
      "total_backward_count 822360 real_backward_count 97260  11.827%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.281457/  1.690550, val:  46.67%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 118.37 seconds, 1.97 minutes\n",
      "layer   1  Sparsity: 80.9225%\n",
      "layer   2  Sparsity: 79.0456%\n",
      "layer   3  Sparsity: 79.4541%\n",
      "total_backward_count 832150 real_backward_count 98421  11.827%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.313599/  1.734201, val:  52.50%, val_best:  59.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 117.24 seconds, 1.95 minutes\n",
      "layer   1  Sparsity: 80.9059%\n",
      "layer   2  Sparsity: 79.2633%\n",
      "layer   3  Sparsity: 78.9513%\n",
      "total_backward_count 841940 real_backward_count 99540  11.823%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.276243/  1.662099, val:  51.25%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 117.42 seconds, 1.96 minutes\n",
      "layer   1  Sparsity: 80.9222%\n",
      "layer   2  Sparsity: 78.8976%\n",
      "layer   3  Sparsity: 79.6445%\n",
      "total_backward_count 851730 real_backward_count 100660  11.818%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.267517/  1.683056, val:  40.00%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 117.01 seconds, 1.95 minutes\n",
      "layer   1  Sparsity: 80.9158%\n",
      "layer   2  Sparsity: 78.8260%\n",
      "layer   3  Sparsity: 79.3571%\n",
      "total_backward_count 861520 real_backward_count 101787  11.815%\n",
      "lif layer 2 self.abs_max_v: 7747.5\n",
      "fc layer 2 self.abs_max_out: 4155.0\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.248651/  1.645668, val:  54.58%, val_best:  59.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 117.36 seconds, 1.96 minutes\n",
      "layer   1  Sparsity: 80.9433%\n",
      "layer   2  Sparsity: 77.8742%\n",
      "layer   3  Sparsity: 78.9359%\n",
      "total_backward_count 871310 real_backward_count 102901  11.810%\n",
      "fc layer 2 self.abs_max_out: 4210.0\n",
      "lif layer 2 self.abs_max_v: 7786.5\n",
      "lif layer 2 self.abs_max_v: 7825.5\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.255522/  1.623668, val:  54.17%, val_best:  59.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 117.71 seconds, 1.96 minutes\n",
      "layer   1  Sparsity: 80.9183%\n",
      "layer   2  Sparsity: 78.1647%\n",
      "layer   3  Sparsity: 79.4126%\n",
      "total_backward_count 881100 real_backward_count 104062  11.810%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.235047/  1.647975, val:  47.50%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 118.57 seconds, 1.98 minutes\n",
      "layer   1  Sparsity: 80.9226%\n",
      "layer   2  Sparsity: 77.7527%\n",
      "layer   3  Sparsity: 79.0174%\n",
      "total_backward_count 890890 real_backward_count 105154  11.803%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.247774/  1.611136, val:  58.75%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 118.89 seconds, 1.98 minutes\n",
      "layer   1  Sparsity: 80.9300%\n",
      "layer   2  Sparsity: 77.2697%\n",
      "layer   3  Sparsity: 78.6498%\n",
      "total_backward_count 900680 real_backward_count 106268  11.799%\n",
      "fc layer 2 self.abs_max_out: 4216.0\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.245536/  1.620461, val:  48.75%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 121.85 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9176%\n",
      "layer   2  Sparsity: 76.9749%\n",
      "layer   3  Sparsity: 77.9505%\n",
      "total_backward_count 910470 real_backward_count 107374  11.793%\n",
      "fc layer 2 self.abs_max_out: 4235.0\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.234627/  1.635441, val:  52.92%, val_best:  59.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 122.24 seconds, 2.04 minutes\n",
      "layer   1  Sparsity: 80.9275%\n",
      "layer   2  Sparsity: 75.6824%\n",
      "layer   3  Sparsity: 77.9901%\n",
      "total_backward_count 920260 real_backward_count 108449  11.785%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.193148/  1.600869, val:  55.00%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 122.52 seconds, 2.04 minutes\n",
      "layer   1  Sparsity: 80.9211%\n",
      "layer   2  Sparsity: 75.7112%\n",
      "layer   3  Sparsity: 77.6452%\n",
      "total_backward_count 930050 real_backward_count 109520  11.776%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.197669/  1.611925, val:  58.75%, val_best:  59.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 122.69 seconds, 2.04 minutes\n",
      "layer   1  Sparsity: 80.9277%\n",
      "layer   2  Sparsity: 76.2404%\n",
      "layer   3  Sparsity: 76.3603%\n",
      "total_backward_count 939840 real_backward_count 110588  11.767%\n",
      "fc layer 3 self.abs_max_out: 1723.0\n",
      "lif layer 1 self.abs_max_v: 30502.0\n",
      "lif layer 1 self.abs_max_v: 31059.0\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.188158/  1.687383, val:  37.08%, val_best:  59.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 122.14 seconds, 2.04 minutes\n",
      "layer   1  Sparsity: 80.9186%\n",
      "layer   2  Sparsity: 76.2416%\n",
      "layer   3  Sparsity: 75.8261%\n",
      "total_backward_count 949630 real_backward_count 111642  11.756%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.152548/  1.561935, val:  52.08%, val_best:  59.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 123.85 seconds, 2.06 minutes\n",
      "layer   1  Sparsity: 80.9078%\n",
      "layer   2  Sparsity: 76.4031%\n",
      "layer   3  Sparsity: 75.2834%\n",
      "total_backward_count 959420 real_backward_count 112713  11.748%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.190774/  1.698280, val:  45.42%, val_best:  59.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 121.49 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.9110%\n",
      "layer   2  Sparsity: 76.2746%\n",
      "layer   3  Sparsity: 77.0234%\n",
      "total_backward_count 969210 real_backward_count 113802  11.742%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.174467/  1.533206, val:  60.00%, val_best:  60.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 121.49 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.9266%\n",
      "layer   2  Sparsity: 76.2420%\n",
      "layer   3  Sparsity: 77.1112%\n",
      "total_backward_count 979000 real_backward_count 114894  11.736%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.135340/  1.639248, val:  43.75%, val_best:  60.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 124.15 seconds, 2.07 minutes\n",
      "layer   1  Sparsity: 80.9163%\n",
      "layer   2  Sparsity: 76.5763%\n",
      "layer   3  Sparsity: 76.7037%\n",
      "total_backward_count 988790 real_backward_count 115998  11.731%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.106996/  1.524169, val:  56.67%, val_best:  60.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 123.35 seconds, 2.06 minutes\n",
      "layer   1  Sparsity: 80.9146%\n",
      "layer   2  Sparsity: 76.5253%\n",
      "layer   3  Sparsity: 75.4842%\n",
      "total_backward_count 998580 real_backward_count 117114  11.728%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.103020/  1.526545, val:  53.75%, val_best:  60.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 120.95 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.8994%\n",
      "layer   2  Sparsity: 75.5770%\n",
      "layer   3  Sparsity: 75.9733%\n",
      "total_backward_count 1008370 real_backward_count 118228  11.725%\n",
      "fc layer 2 self.abs_max_out: 4254.0\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.143202/  1.540907, val:  55.00%, val_best:  60.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 120.78 seconds, 2.01 minutes\n",
      "layer   1  Sparsity: 80.9148%\n",
      "layer   2  Sparsity: 75.7537%\n",
      "layer   3  Sparsity: 77.2090%\n",
      "total_backward_count 1018160 real_backward_count 119335  11.721%\n",
      "fc layer 2 self.abs_max_out: 4316.0\n",
      "lif layer 2 self.abs_max_v: 8207.0\n",
      "fc layer 2 self.abs_max_out: 4337.0\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.156510/  1.620895, val:  47.50%, val_best:  60.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 121.97 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9266%\n",
      "layer   2  Sparsity: 75.0785%\n",
      "layer   3  Sparsity: 77.8067%\n",
      "total_backward_count 1027950 real_backward_count 120447  11.717%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.164016/  1.576900, val:  49.58%, val_best:  60.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 123.43 seconds, 2.06 minutes\n",
      "layer   1  Sparsity: 80.9100%\n",
      "layer   2  Sparsity: 75.2131%\n",
      "layer   3  Sparsity: 76.9265%\n",
      "total_backward_count 1037740 real_backward_count 121560  11.714%\n",
      "lif layer 2 self.abs_max_v: 8208.5\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.122734/  1.612859, val:  43.75%, val_best:  60.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 121.13 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.9096%\n",
      "layer   2  Sparsity: 75.3067%\n",
      "layer   3  Sparsity: 75.9482%\n",
      "total_backward_count 1047530 real_backward_count 122610  11.705%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.130556/  1.600042, val:  57.92%, val_best:  60.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 122.15 seconds, 2.04 minutes\n",
      "layer   1  Sparsity: 80.9296%\n",
      "layer   2  Sparsity: 75.6678%\n",
      "layer   3  Sparsity: 75.4791%\n",
      "total_backward_count 1057320 real_backward_count 123700  11.699%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.143010/  1.577439, val:  56.67%, val_best:  60.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 121.16 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.8866%\n",
      "layer   2  Sparsity: 75.7530%\n",
      "layer   3  Sparsity: 76.7237%\n",
      "total_backward_count 1067110 real_backward_count 124786  11.694%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.082301/  1.499124, val:  54.58%, val_best:  60.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 120.24 seconds, 2.00 minutes\n",
      "layer   1  Sparsity: 80.9211%\n",
      "layer   2  Sparsity: 75.5679%\n",
      "layer   3  Sparsity: 75.3995%\n",
      "total_backward_count 1076900 real_backward_count 125874  11.689%\n",
      "fc layer 3 self.abs_max_out: 1797.0\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.118914/  1.522766, val:  60.83%, val_best:  60.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 121.03 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.9096%\n",
      "layer   2  Sparsity: 75.4915%\n",
      "layer   3  Sparsity: 74.9492%\n",
      "total_backward_count 1086690 real_backward_count 126908  11.678%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.135384/  1.609537, val:  50.83%, val_best:  60.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 121.44 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.8949%\n",
      "layer   2  Sparsity: 75.4401%\n",
      "layer   3  Sparsity: 76.1705%\n",
      "total_backward_count 1096480 real_backward_count 127998  11.674%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.165828/  1.547323, val:  60.00%, val_best:  60.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 121.78 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9212%\n",
      "layer   2  Sparsity: 76.3012%\n",
      "layer   3  Sparsity: 77.4307%\n",
      "total_backward_count 1106270 real_backward_count 129103  11.670%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.119202/  1.612663, val:  44.58%, val_best:  60.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 121.17 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.9105%\n",
      "layer   2  Sparsity: 76.9628%\n",
      "layer   3  Sparsity: 76.5498%\n",
      "total_backward_count 1116060 real_backward_count 130159  11.662%\n",
      "fc layer 1 self.abs_max_out: 17518.0\n",
      "lif layer 1 self.abs_max_v: 31490.5\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.112100/  1.598054, val:  46.67%, val_best:  60.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 122.57 seconds, 2.04 minutes\n",
      "layer   1  Sparsity: 80.9076%\n",
      "layer   2  Sparsity: 76.9591%\n",
      "layer   3  Sparsity: 77.1039%\n",
      "total_backward_count 1125850 real_backward_count 131192  11.653%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.130050/  1.593518, val:  48.75%, val_best:  60.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 121.04 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.9026%\n",
      "layer   2  Sparsity: 77.3447%\n",
      "layer   3  Sparsity: 76.8901%\n",
      "total_backward_count 1135640 real_backward_count 132246  11.645%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.135526/  1.515459, val:  55.83%, val_best:  60.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 120.70 seconds, 2.01 minutes\n",
      "layer   1  Sparsity: 80.9161%\n",
      "layer   2  Sparsity: 77.3347%\n",
      "layer   3  Sparsity: 76.7680%\n",
      "total_backward_count 1145430 real_backward_count 133355  11.642%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.118546/  1.530842, val:  52.92%, val_best:  60.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 121.52 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9139%\n",
      "layer   2  Sparsity: 77.6007%\n",
      "layer   3  Sparsity: 76.3252%\n",
      "total_backward_count 1155220 real_backward_count 134395  11.634%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.060439/  1.475275, val:  56.25%, val_best:  60.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 122.48 seconds, 2.04 minutes\n",
      "layer   1  Sparsity: 80.9311%\n",
      "layer   2  Sparsity: 77.2109%\n",
      "layer   3  Sparsity: 74.4621%\n",
      "total_backward_count 1165010 real_backward_count 135396  11.622%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.085502/  1.532628, val:  55.83%, val_best:  60.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 122.42 seconds, 2.04 minutes\n",
      "layer   1  Sparsity: 80.8990%\n",
      "layer   2  Sparsity: 76.0803%\n",
      "layer   3  Sparsity: 74.3866%\n",
      "total_backward_count 1174800 real_backward_count 136402  11.611%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.049797/  1.480884, val:  61.67%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 126.00 seconds, 2.10 minutes\n",
      "layer   1  Sparsity: 80.8955%\n",
      "layer   2  Sparsity: 76.1662%\n",
      "layer   3  Sparsity: 73.6336%\n",
      "total_backward_count 1184590 real_backward_count 137388  11.598%\n",
      "fc layer 2 self.abs_max_out: 4354.0\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.054985/  1.542615, val:  55.00%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 124.92 seconds, 2.08 minutes\n",
      "layer   1  Sparsity: 80.9145%\n",
      "layer   2  Sparsity: 77.0526%\n",
      "layer   3  Sparsity: 75.1986%\n",
      "total_backward_count 1194380 real_backward_count 138406  11.588%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.061338/  1.549470, val:  53.33%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 123.03 seconds, 2.05 minutes\n",
      "layer   1  Sparsity: 80.9249%\n",
      "layer   2  Sparsity: 76.9502%\n",
      "layer   3  Sparsity: 76.3014%\n",
      "total_backward_count 1204170 real_backward_count 139500  11.585%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.055144/  1.526506, val:  54.58%, val_best:  61.67%, tr:  99.39%, tr_best: 100.00%, epoch time: 123.19 seconds, 2.05 minutes\n",
      "layer   1  Sparsity: 80.9192%\n",
      "layer   2  Sparsity: 76.6800%\n",
      "layer   3  Sparsity: 75.6952%\n",
      "total_backward_count 1213960 real_backward_count 140594  11.581%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.066502/  1.513581, val:  59.17%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 124.00 seconds, 2.07 minutes\n",
      "layer   1  Sparsity: 80.9123%\n",
      "layer   2  Sparsity: 76.3920%\n",
      "layer   3  Sparsity: 74.8192%\n",
      "total_backward_count 1223750 real_backward_count 141667  11.576%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.062736/  1.578165, val:  47.92%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 125.23 seconds, 2.09 minutes\n",
      "layer   1  Sparsity: 80.9228%\n",
      "layer   2  Sparsity: 76.2594%\n",
      "layer   3  Sparsity: 75.3960%\n",
      "total_backward_count 1233540 real_backward_count 142754  11.573%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.067855/  1.572965, val:  50.42%, val_best:  61.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 121.65 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9224%\n",
      "layer   2  Sparsity: 76.3973%\n",
      "layer   3  Sparsity: 75.7374%\n",
      "total_backward_count 1243330 real_backward_count 143801  11.566%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.082032/  1.529114, val:  57.08%, val_best:  61.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 121.20 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.9131%\n",
      "layer   2  Sparsity: 76.7865%\n",
      "layer   3  Sparsity: 75.7774%\n",
      "total_backward_count 1253120 real_backward_count 144824  11.557%\n",
      "fc layer 1 self.abs_max_out: 18366.0\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.099296/  1.587808, val:  53.75%, val_best:  61.67%, tr:  99.39%, tr_best: 100.00%, epoch time: 121.39 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.9161%\n",
      "layer   2  Sparsity: 76.2053%\n",
      "layer   3  Sparsity: 76.8951%\n",
      "total_backward_count 1262910 real_backward_count 145890  11.552%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.117239/  1.524912, val:  57.92%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 121.80 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9141%\n",
      "layer   2  Sparsity: 75.9927%\n",
      "layer   3  Sparsity: 77.5194%\n",
      "total_backward_count 1272700 real_backward_count 146924  11.544%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.098596/  1.597088, val:  42.50%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 123.50 seconds, 2.06 minutes\n",
      "layer   1  Sparsity: 80.9250%\n",
      "layer   2  Sparsity: 75.1841%\n",
      "layer   3  Sparsity: 76.1664%\n",
      "total_backward_count 1282490 real_backward_count 147955  11.537%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.108537/  1.591793, val:  52.50%, val_best:  61.67%, tr:  99.39%, tr_best: 100.00%, epoch time: 123.64 seconds, 2.06 minutes\n",
      "layer   1  Sparsity: 80.9293%\n",
      "layer   2  Sparsity: 74.1421%\n",
      "layer   3  Sparsity: 76.0643%\n",
      "total_backward_count 1292280 real_backward_count 149048  11.534%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.116324/  1.573237, val:  51.25%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 122.49 seconds, 2.04 minutes\n",
      "layer   1  Sparsity: 80.9217%\n",
      "layer   2  Sparsity: 74.3232%\n",
      "layer   3  Sparsity: 76.3900%\n",
      "total_backward_count 1302070 real_backward_count 150105  11.528%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.116824/  1.584622, val:  51.25%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 121.79 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9143%\n",
      "layer   2  Sparsity: 75.3363%\n",
      "layer   3  Sparsity: 77.0683%\n",
      "total_backward_count 1311860 real_backward_count 151142  11.521%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.118791/  1.577316, val:  50.83%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 121.18 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.9291%\n",
      "layer   2  Sparsity: 74.3154%\n",
      "layer   3  Sparsity: 76.0614%\n",
      "total_backward_count 1321650 real_backward_count 152209  11.517%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.124898/  1.502526, val:  56.25%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 120.66 seconds, 2.01 minutes\n",
      "layer   1  Sparsity: 80.9177%\n",
      "layer   2  Sparsity: 73.8825%\n",
      "layer   3  Sparsity: 76.3999%\n",
      "total_backward_count 1331440 real_backward_count 153309  11.515%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.136918/  1.580483, val:  57.50%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 121.75 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9325%\n",
      "layer   2  Sparsity: 73.9818%\n",
      "layer   3  Sparsity: 77.4089%\n",
      "total_backward_count 1341230 real_backward_count 154338  11.507%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.145137/  1.547644, val:  52.08%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 120.80 seconds, 2.01 minutes\n",
      "layer   1  Sparsity: 80.9474%\n",
      "layer   2  Sparsity: 74.3776%\n",
      "layer   3  Sparsity: 76.9121%\n",
      "total_backward_count 1351020 real_backward_count 155399  11.502%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.149389/  1.541981, val:  48.75%, val_best:  61.67%, tr:  99.39%, tr_best: 100.00%, epoch time: 122.78 seconds, 2.05 minutes\n",
      "layer   1  Sparsity: 80.9130%\n",
      "layer   2  Sparsity: 75.1589%\n",
      "layer   3  Sparsity: 77.0927%\n",
      "total_backward_count 1360810 real_backward_count 156471  11.498%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.137769/  1.569175, val:  56.25%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 122.33 seconds, 2.04 minutes\n",
      "layer   1  Sparsity: 80.9248%\n",
      "layer   2  Sparsity: 75.3872%\n",
      "layer   3  Sparsity: 77.5877%\n",
      "total_backward_count 1370600 real_backward_count 157551  11.495%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.149263/  1.629599, val:  49.17%, val_best:  61.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 121.28 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.9181%\n",
      "layer   2  Sparsity: 74.9997%\n",
      "layer   3  Sparsity: 76.7527%\n",
      "total_backward_count 1380390 real_backward_count 158562  11.487%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.147772/  1.543563, val:  56.67%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 122.10 seconds, 2.04 minutes\n",
      "layer   1  Sparsity: 80.9182%\n",
      "layer   2  Sparsity: 75.1334%\n",
      "layer   3  Sparsity: 75.3818%\n",
      "total_backward_count 1390180 real_backward_count 159581  11.479%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.192010/  1.558536, val:  60.83%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 121.97 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9296%\n",
      "layer   2  Sparsity: 74.9413%\n",
      "layer   3  Sparsity: 77.6871%\n",
      "total_backward_count 1399970 real_backward_count 160651  11.475%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.190288/  1.582680, val:  53.33%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 122.89 seconds, 2.05 minutes\n",
      "layer   1  Sparsity: 80.8937%\n",
      "layer   2  Sparsity: 75.0358%\n",
      "layer   3  Sparsity: 77.1890%\n",
      "total_backward_count 1409760 real_backward_count 161709  11.471%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.166461/  1.553130, val:  56.25%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 121.63 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9076%\n",
      "layer   2  Sparsity: 75.4167%\n",
      "layer   3  Sparsity: 77.0754%\n",
      "total_backward_count 1419550 real_backward_count 162713  11.462%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.151496/  1.624817, val:  45.83%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 121.78 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9003%\n",
      "layer   2  Sparsity: 75.8024%\n",
      "layer   3  Sparsity: 76.9725%\n",
      "total_backward_count 1429340 real_backward_count 163739  11.456%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.153565/  1.491072, val:  59.17%, val_best:  61.67%, tr:  99.49%, tr_best: 100.00%, epoch time: 121.56 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9321%\n",
      "layer   2  Sparsity: 76.6666%\n",
      "layer   3  Sparsity: 76.7260%\n",
      "total_backward_count 1439130 real_backward_count 164834  11.454%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.125973/  1.701334, val:  37.08%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 123.34 seconds, 2.06 minutes\n",
      "layer   1  Sparsity: 80.9130%\n",
      "layer   2  Sparsity: 76.6386%\n",
      "layer   3  Sparsity: 75.4865%\n",
      "total_backward_count 1448920 real_backward_count 165856  11.447%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.106021/  1.528639, val:  52.92%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 121.77 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9036%\n",
      "layer   2  Sparsity: 76.2627%\n",
      "layer   3  Sparsity: 76.1792%\n",
      "total_backward_count 1458710 real_backward_count 166950  11.445%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.124852/  1.510166, val:  60.83%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 120.82 seconds, 2.01 minutes\n",
      "layer   1  Sparsity: 80.9169%\n",
      "layer   2  Sparsity: 75.9128%\n",
      "layer   3  Sparsity: 76.5299%\n",
      "total_backward_count 1468500 real_backward_count 168027  11.442%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.051203/  1.568927, val:  52.50%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 123.50 seconds, 2.06 minutes\n",
      "layer   1  Sparsity: 80.9324%\n",
      "layer   2  Sparsity: 76.0411%\n",
      "layer   3  Sparsity: 75.7709%\n",
      "total_backward_count 1478290 real_backward_count 169052  11.436%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.094085/  1.474249, val:  53.75%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 123.07 seconds, 2.05 minutes\n",
      "layer   1  Sparsity: 80.9296%\n",
      "layer   2  Sparsity: 75.5766%\n",
      "layer   3  Sparsity: 75.9252%\n",
      "total_backward_count 1488080 real_backward_count 170101  11.431%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.087510/  1.445293, val:  59.17%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 122.02 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.8917%\n",
      "layer   2  Sparsity: 76.1863%\n",
      "layer   3  Sparsity: 75.8423%\n",
      "total_backward_count 1497870 real_backward_count 171145  11.426%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.093331/  1.543751, val:  54.58%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 123.27 seconds, 2.05 minutes\n",
      "layer   1  Sparsity: 80.9305%\n",
      "layer   2  Sparsity: 76.4029%\n",
      "layer   3  Sparsity: 76.2438%\n",
      "total_backward_count 1507660 real_backward_count 172254  11.425%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.121586/  1.567057, val:  56.25%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 121.73 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9124%\n",
      "layer   2  Sparsity: 76.5704%\n",
      "layer   3  Sparsity: 77.1174%\n",
      "total_backward_count 1517450 real_backward_count 173277  11.419%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.154103/  1.498131, val:  61.67%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 121.55 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9187%\n",
      "layer   2  Sparsity: 76.5596%\n",
      "layer   3  Sparsity: 77.7936%\n",
      "total_backward_count 1527240 real_backward_count 174317  11.414%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.119251/  1.603245, val:  51.67%, val_best:  61.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 120.48 seconds, 2.01 minutes\n",
      "layer   1  Sparsity: 80.9084%\n",
      "layer   2  Sparsity: 76.5392%\n",
      "layer   3  Sparsity: 78.1565%\n",
      "total_backward_count 1537030 real_backward_count 175377  11.410%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.137802/  1.530524, val:  61.67%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 122.18 seconds, 2.04 minutes\n",
      "layer   1  Sparsity: 80.9266%\n",
      "layer   2  Sparsity: 75.5658%\n",
      "layer   3  Sparsity: 77.0848%\n",
      "total_backward_count 1546820 real_backward_count 176374  11.402%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.127787/  1.558484, val:  52.50%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 122.07 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9172%\n",
      "layer   2  Sparsity: 75.0534%\n",
      "layer   3  Sparsity: 75.4427%\n",
      "total_backward_count 1556610 real_backward_count 177432  11.399%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.102809/  1.508180, val:  55.83%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 121.29 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.9111%\n",
      "layer   2  Sparsity: 75.1994%\n",
      "layer   3  Sparsity: 74.0098%\n",
      "total_backward_count 1566400 real_backward_count 178424  11.391%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.084874/  1.471772, val:  60.00%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 120.57 seconds, 2.01 minutes\n",
      "layer   1  Sparsity: 80.9337%\n",
      "layer   2  Sparsity: 75.1873%\n",
      "layer   3  Sparsity: 74.2966%\n",
      "total_backward_count 1576190 real_backward_count 179427  11.384%\n",
      "fc layer 2 self.abs_max_out: 4402.0\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.057745/  1.493392, val:  56.25%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 121.39 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.9107%\n",
      "layer   2  Sparsity: 75.4631%\n",
      "layer   3  Sparsity: 75.1992%\n",
      "total_backward_count 1585980 real_backward_count 180417  11.376%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.084324/  1.525097, val:  52.50%, val_best:  61.67%, tr:  99.39%, tr_best: 100.00%, epoch time: 122.10 seconds, 2.04 minutes\n",
      "layer   1  Sparsity: 80.9261%\n",
      "layer   2  Sparsity: 75.1482%\n",
      "layer   3  Sparsity: 76.0868%\n",
      "total_backward_count 1595770 real_backward_count 181453  11.371%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.083451/  1.550468, val:  54.17%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 118.76 seconds, 1.98 minutes\n",
      "layer   1  Sparsity: 80.9128%\n",
      "layer   2  Sparsity: 74.5648%\n",
      "layer   3  Sparsity: 74.6934%\n",
      "total_backward_count 1605560 real_backward_count 182445  11.363%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.067579/  1.519073, val:  50.00%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 121.82 seconds, 2.03 minutes\n",
      "layer   1  Sparsity: 80.9206%\n",
      "layer   2  Sparsity: 73.6363%\n",
      "layer   3  Sparsity: 74.5570%\n",
      "total_backward_count 1615350 real_backward_count 183469  11.358%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.084650/  1.501931, val:  56.67%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 122.32 seconds, 2.04 minutes\n",
      "layer   1  Sparsity: 80.9091%\n",
      "layer   2  Sparsity: 74.2833%\n",
      "layer   3  Sparsity: 74.4094%\n",
      "total_backward_count 1625140 real_backward_count 184551  11.356%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.052419/  1.502599, val:  52.08%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 120.07 seconds, 2.00 minutes\n",
      "layer   1  Sparsity: 80.9370%\n",
      "layer   2  Sparsity: 74.6075%\n",
      "layer   3  Sparsity: 74.4317%\n",
      "total_backward_count 1634930 real_backward_count 185567  11.350%\n",
      "fc layer 2 self.abs_max_out: 4421.0\n",
      "fc layer 3 self.abs_max_out: 1862.0\n",
      "fc layer 3 self.abs_max_out: 1898.0\n",
      "fc layer 3 self.abs_max_out: 1938.0\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.047920/  1.579184, val:  44.17%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 121.39 seconds, 2.02 minutes\n",
      "layer   1  Sparsity: 80.9296%\n",
      "layer   2  Sparsity: 74.3977%\n",
      "layer   3  Sparsity: 73.4329%\n",
      "total_backward_count 1644720 real_backward_count 186611  11.346%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.059201/  1.554169, val:  50.42%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 122.48 seconds, 2.04 minutes\n",
      "layer   1  Sparsity: 80.9049%\n",
      "layer   2  Sparsity: 74.0994%\n",
      "layer   3  Sparsity: 73.4750%\n",
      "total_backward_count 1654510 real_backward_count 187662  11.342%\n",
      "lif layer 2 self.abs_max_v: 8347.5\n",
      "lif layer 2 self.abs_max_v: 8465.0\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.994865/  1.430628, val:  52.08%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 123.44 seconds, 2.06 minutes\n",
      "layer   1  Sparsity: 80.9205%\n",
      "layer   2  Sparsity: 74.1910%\n",
      "layer   3  Sparsity: 72.5118%\n",
      "total_backward_count 1664300 real_backward_count 188672  11.336%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.979283/  1.504905, val:  49.17%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 122.48 seconds, 2.04 minutes\n",
      "layer   1  Sparsity: 80.9195%\n",
      "layer   2  Sparsity: 73.8430%\n",
      "layer   3  Sparsity: 73.0822%\n",
      "total_backward_count 1674090 real_backward_count 189639  11.328%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.015299/  1.592534, val:  48.75%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 119.41 seconds, 1.99 minutes\n",
      "layer   1  Sparsity: 80.9187%\n",
      "layer   2  Sparsity: 75.1063%\n",
      "layer   3  Sparsity: 73.9547%\n",
      "total_backward_count 1683880 real_backward_count 190671  11.323%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.029549/  1.475464, val:  59.17%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 115.86 seconds, 1.93 minutes\n",
      "layer   1  Sparsity: 80.8899%\n",
      "layer   2  Sparsity: 75.3005%\n",
      "layer   3  Sparsity: 73.4567%\n",
      "total_backward_count 1693670 real_backward_count 191705  11.319%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.983105/  1.561198, val:  47.50%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 113.44 seconds, 1.89 minutes\n",
      "layer   1  Sparsity: 80.9120%\n",
      "layer   2  Sparsity: 75.4337%\n",
      "layer   3  Sparsity: 73.3211%\n",
      "total_backward_count 1703460 real_backward_count 192676  11.311%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.073529/  1.516424, val:  57.50%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 110.24 seconds, 1.84 minutes\n",
      "layer   1  Sparsity: 80.9201%\n",
      "layer   2  Sparsity: 75.2302%\n",
      "layer   3  Sparsity: 74.5108%\n",
      "total_backward_count 1713250 real_backward_count 193705  11.306%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.076027/  1.495832, val:  61.25%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 111.83 seconds, 1.86 minutes\n",
      "layer   1  Sparsity: 80.9106%\n",
      "layer   2  Sparsity: 74.5632%\n",
      "layer   3  Sparsity: 75.2705%\n",
      "total_backward_count 1723040 real_backward_count 194708  11.300%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.105958/  1.514281, val:  53.33%, val_best:  61.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 107.20 seconds, 1.79 minutes\n",
      "layer   1  Sparsity: 80.9063%\n",
      "layer   2  Sparsity: 75.0595%\n",
      "layer   3  Sparsity: 74.5942%\n",
      "total_backward_count 1732830 real_backward_count 195747  11.296%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.074881/  1.610698, val:  56.25%, val_best:  61.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 104.78 seconds, 1.75 minutes\n",
      "layer   1  Sparsity: 80.8967%\n",
      "layer   2  Sparsity: 74.9321%\n",
      "layer   3  Sparsity: 74.8558%\n",
      "total_backward_count 1742620 real_backward_count 196717  11.289%\n",
      "fc layer 2 self.abs_max_out: 4681.0\n",
      "lif layer 2 self.abs_max_v: 8769.5\n",
      "lif layer 2 self.abs_max_v: 8861.0\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.090762/  1.607900, val:  51.25%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 102.71 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 80.9210%\n",
      "layer   2  Sparsity: 74.2717%\n",
      "layer   3  Sparsity: 75.8447%\n",
      "total_backward_count 1752410 real_backward_count 197699  11.282%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.086541/  1.570137, val:  47.92%, val_best:  61.67%, tr:  99.49%, tr_best: 100.00%, epoch time: 101.97 seconds, 1.70 minutes\n",
      "layer   1  Sparsity: 80.9229%\n",
      "layer   2  Sparsity: 73.4997%\n",
      "layer   3  Sparsity: 75.8619%\n",
      "total_backward_count 1762200 real_backward_count 198717  11.277%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.060864/  1.534736, val:  50.42%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 102.38 seconds, 1.71 minutes\n",
      "layer   1  Sparsity: 80.9287%\n",
      "layer   2  Sparsity: 73.8997%\n",
      "layer   3  Sparsity: 75.5846%\n",
      "total_backward_count 1771990 real_backward_count 199679  11.269%\n",
      "fc layer 3 self.abs_max_out: 2062.0\n",
      "fc layer 3 self.abs_max_out: 2067.0\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.075734/  1.499258, val:  59.17%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 101.21 seconds, 1.69 minutes\n",
      "layer   1  Sparsity: 80.9001%\n",
      "layer   2  Sparsity: 73.7045%\n",
      "layer   3  Sparsity: 76.1763%\n",
      "total_backward_count 1781780 real_backward_count 200639  11.261%\n",
      "fc layer 3 self.abs_max_out: 2107.0\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.105719/  1.576014, val:  54.17%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 100.21 seconds, 1.67 minutes\n",
      "layer   1  Sparsity: 80.9085%\n",
      "layer   2  Sparsity: 73.6202%\n",
      "layer   3  Sparsity: 76.9459%\n",
      "total_backward_count 1791570 real_backward_count 201690  11.258%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.096391/  1.552011, val:  51.25%, val_best:  61.67%, tr:  99.49%, tr_best: 100.00%, epoch time: 97.74 seconds, 1.63 minutes\n",
      "layer   1  Sparsity: 80.9215%\n",
      "layer   2  Sparsity: 74.4557%\n",
      "layer   3  Sparsity: 76.2562%\n",
      "total_backward_count 1801360 real_backward_count 202744  11.255%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.043362/  1.614853, val:  41.67%, val_best:  61.67%, tr:  99.49%, tr_best: 100.00%, epoch time: 93.76 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 80.9204%\n",
      "layer   2  Sparsity: 75.5509%\n",
      "layer   3  Sparsity: 75.8342%\n",
      "total_backward_count 1811150 real_backward_count 203743  11.249%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.049320/  1.488621, val:  54.58%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 93.78 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 80.9088%\n",
      "layer   2  Sparsity: 75.1110%\n",
      "layer   3  Sparsity: 76.0911%\n",
      "total_backward_count 1820940 real_backward_count 204780  11.246%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.052644/  1.486989, val:  57.08%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 93.95 seconds, 1.57 minutes\n",
      "layer   1  Sparsity: 80.9377%\n",
      "layer   2  Sparsity: 75.8394%\n",
      "layer   3  Sparsity: 75.7805%\n",
      "total_backward_count 1830730 real_backward_count 205844  11.244%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.080122/  1.547478, val:  54.58%, val_best:  61.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 92.48 seconds, 1.54 minutes\n",
      "layer   1  Sparsity: 80.9204%\n",
      "layer   2  Sparsity: 75.7320%\n",
      "layer   3  Sparsity: 76.9522%\n",
      "total_backward_count 1840520 real_backward_count 206886  11.241%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.101207/  1.572696, val:  55.42%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 91.97 seconds, 1.53 minutes\n",
      "layer   1  Sparsity: 80.9186%\n",
      "layer   2  Sparsity: 75.9980%\n",
      "layer   3  Sparsity: 75.9164%\n",
      "total_backward_count 1850310 real_backward_count 207873  11.234%\n",
      "fc layer 2 self.abs_max_out: 4929.0\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.102315/  1.563250, val:  47.08%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 93.45 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 80.9149%\n",
      "layer   2  Sparsity: 75.5263%\n",
      "layer   3  Sparsity: 76.3922%\n",
      "total_backward_count 1860100 real_backward_count 208881  11.230%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.097061/  1.534496, val:  60.83%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 93.21 seconds, 1.55 minutes\n",
      "layer   1  Sparsity: 80.9182%\n",
      "layer   2  Sparsity: 75.8962%\n",
      "layer   3  Sparsity: 76.8254%\n",
      "total_backward_count 1869890 real_backward_count 209888  11.225%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.122921/  1.671769, val:  38.75%, val_best:  61.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 93.55 seconds, 1.56 minutes\n",
      "layer   1  Sparsity: 80.8987%\n",
      "layer   2  Sparsity: 74.9120%\n",
      "layer   3  Sparsity: 77.4174%\n",
      "total_backward_count 1879680 real_backward_count 210933  11.222%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.134523/  1.577989, val:  57.08%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 91.96 seconds, 1.53 minutes\n",
      "layer   1  Sparsity: 80.9181%\n",
      "layer   2  Sparsity: 75.1117%\n",
      "layer   3  Sparsity: 77.3603%\n",
      "total_backward_count 1889470 real_backward_count 211953  11.218%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.148342/  1.628705, val:  44.58%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 93.07 seconds, 1.55 minutes\n",
      "layer   1  Sparsity: 80.9251%\n",
      "layer   2  Sparsity: 75.7966%\n",
      "layer   3  Sparsity: 77.5592%\n",
      "total_backward_count 1899260 real_backward_count 212966  11.213%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.144954/  1.511309, val:  58.75%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 93.29 seconds, 1.55 minutes\n",
      "layer   1  Sparsity: 80.9098%\n",
      "layer   2  Sparsity: 76.2920%\n",
      "layer   3  Sparsity: 77.5587%\n",
      "total_backward_count 1909050 real_backward_count 213937  11.206%\n",
      "lif layer 1 self.abs_max_v: 31668.0\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.104560/  1.564507, val:  48.33%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 90.68 seconds, 1.51 minutes\n",
      "layer   1  Sparsity: 80.8942%\n",
      "layer   2  Sparsity: 76.1331%\n",
      "layer   3  Sparsity: 77.3815%\n",
      "total_backward_count 1918840 real_backward_count 214958  11.202%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.089746/  1.478882, val:  61.25%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.76 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 80.8998%\n",
      "layer   2  Sparsity: 75.7811%\n",
      "layer   3  Sparsity: 77.1365%\n",
      "total_backward_count 1928630 real_backward_count 216010  11.200%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.094249/  1.549375, val:  53.33%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 82.76 seconds, 1.38 minutes\n",
      "layer   1  Sparsity: 80.9054%\n",
      "layer   2  Sparsity: 76.0942%\n",
      "layer   3  Sparsity: 75.6075%\n",
      "total_backward_count 1938420 real_backward_count 217046  11.197%\n",
      "fc layer 1 self.abs_max_out: 18841.0\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.110069/  1.585548, val:  51.25%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 84.19 seconds, 1.40 minutes\n",
      "layer   1  Sparsity: 80.9265%\n",
      "layer   2  Sparsity: 76.5822%\n",
      "layer   3  Sparsity: 76.3997%\n",
      "total_backward_count 1948210 real_backward_count 218130  11.196%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.111829/  1.516757, val:  59.17%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 84.46 seconds, 1.41 minutes\n",
      "layer   1  Sparsity: 80.9204%\n",
      "layer   2  Sparsity: 77.4937%\n",
      "layer   3  Sparsity: 76.6544%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5e523b91f14d30abd5a9e0ffa0f1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñá‚ñÉ‚ñÉ‚ñÖ‚ñÅ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñà‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñá‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>1.11183</td></tr><tr><td>val_acc_best</td><td>0.61667</td></tr><tr><td>val_acc_now</td><td>0.59167</td></tr><tr><td>val_loss</td><td>1.51676</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">firm-sweep-46</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tj3a2tzt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/tj3a2tzt</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251114_144405-tj3a2tzt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yaiug1nn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251114_211125-yaiug1nn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yaiug1nn' target=\"_blank\">fragrant-sweep-53</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yaiug1nn' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yaiug1nn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251114_211135_136', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 8, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 237.0\n",
      "lif layer 1 self.abs_max_v: 237.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 436.0\n",
      "lif layer 2 self.abs_max_v: 436.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 244.0\n",
      "fc layer 2 self.abs_max_out: 438.0\n",
      "lif layer 2 self.abs_max_v: 518.5\n",
      "fc layer 1 self.abs_max_out: 283.0\n",
      "lif layer 1 self.abs_max_v: 385.5\n",
      "lif layer 2 self.abs_max_v: 634.5\n",
      "fc layer 2 self.abs_max_out: 490.0\n",
      "lif layer 2 self.abs_max_v: 807.5\n",
      "lif layer 2 self.abs_max_v: 854.5\n",
      "fc layer 2 self.abs_max_out: 525.0\n",
      "lif layer 2 self.abs_max_v: 906.5\n",
      "lif layer 1 self.abs_max_v: 389.0\n",
      "fc layer 1 self.abs_max_out: 343.0\n",
      "fc layer 2 self.abs_max_out: 550.0\n",
      "lif layer 1 self.abs_max_v: 446.0\n",
      "fc layer 2 self.abs_max_out: 654.0\n",
      "fc layer 1 self.abs_max_out: 347.0\n",
      "fc layer 1 self.abs_max_out: 383.0\n",
      "lif layer 1 self.abs_max_v: 499.5\n",
      "lif layer 1 self.abs_max_v: 545.0\n",
      "fc layer 3 self.abs_max_out: 247.0\n",
      "lif layer 2 self.abs_max_v: 920.0\n",
      "fc layer 1 self.abs_max_out: 415.0\n",
      "fc layer 2 self.abs_max_out: 685.0\n",
      "fc layer 3 self.abs_max_out: 301.0\n",
      "fc layer 1 self.abs_max_out: 442.0\n",
      "lif layer 1 self.abs_max_v: 569.5\n",
      "lif layer 1 self.abs_max_v: 584.0\n",
      "lif layer 1 self.abs_max_v: 713.0\n",
      "lif layer 1 self.abs_max_v: 744.5\n",
      "lif layer 1 self.abs_max_v: 774.5\n",
      "lif layer 2 self.abs_max_v: 984.5\n",
      "fc layer 1 self.abs_max_out: 445.0\n",
      "lif layer 2 self.abs_max_v: 1000.0\n",
      "lif layer 2 self.abs_max_v: 1016.0\n",
      "fc layer 1 self.abs_max_out: 448.0\n",
      "lif layer 2 self.abs_max_v: 1020.5\n",
      "lif layer 2 self.abs_max_v: 1118.0\n",
      "fc layer 1 self.abs_max_out: 457.0\n",
      "fc layer 1 self.abs_max_out: 477.0\n",
      "fc layer 1 self.abs_max_out: 521.0\n",
      "lif layer 1 self.abs_max_v: 795.5\n",
      "fc layer 2 self.abs_max_out: 692.0\n",
      "fc layer 1 self.abs_max_out: 539.0\n",
      "lif layer 1 self.abs_max_v: 820.5\n",
      "lif layer 2 self.abs_max_v: 1133.5\n",
      "fc layer 2 self.abs_max_out: 714.0\n",
      "fc layer 2 self.abs_max_out: 848.0\n",
      "lif layer 2 self.abs_max_v: 1245.5\n",
      "fc layer 3 self.abs_max_out: 302.0\n",
      "fc layer 3 self.abs_max_out: 317.0\n",
      "lif layer 1 self.abs_max_v: 859.5\n",
      "lif layer 1 self.abs_max_v: 898.5\n",
      "fc layer 1 self.abs_max_out: 563.0\n",
      "fc layer 3 self.abs_max_out: 377.0\n",
      "fc layer 1 self.abs_max_out: 579.0\n",
      "lif layer 1 self.abs_max_v: 988.0\n",
      "lif layer 1 self.abs_max_v: 1014.0\n",
      "lif layer 2 self.abs_max_v: 1302.0\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "lif layer 2 self.abs_max_v: 1309.0\n",
      "fc layer 1 self.abs_max_out: 596.0\n",
      "fc layer 1 self.abs_max_out: 615.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.355872/  2.356119, val:  15.83%, val_best:  15.83%, tr:  13.28%, tr_best:  13.28%, epoch time: 80.71 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 89.6930%\n",
      "layer   2  Sparsity: 65.7211%\n",
      "layer   3  Sparsity: 66.2607%\n",
      "total_backward_count 9790 real_backward_count 8551  87.344%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 622.0\n",
      "lif layer 1 self.abs_max_v: 1061.0\n",
      "fc layer 1 self.abs_max_out: 680.0\n",
      "lif layer 1 self.abs_max_v: 1131.0\n",
      "lif layer 1 self.abs_max_v: 1163.5\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.358010/  2.356119, val:  15.83%, val_best:  15.83%, tr:  13.07%, tr_best:  13.28%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 89.7054%\n",
      "layer   2  Sparsity: 65.8044%\n",
      "layer   3  Sparsity: 66.2807%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c20a9338714a01ab000b45d918c1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñà‚ñÅ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñà</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.13075</td></tr><tr><td>tr_epoch_loss</td><td>2.35801</td></tr><tr><td>val_acc_best</td><td>0.15833</td></tr><tr><td>val_acc_now</td><td>0.15833</td></tr><tr><td>val_loss</td><td>2.35612</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fragrant-sweep-53</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yaiug1nn' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yaiug1nn</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251114_211125-yaiug1nn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run yaiug1nn errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_15806/2189894352.py\", line 114, in hyper_iter\n",
      "    my_snn_system(\n",
      "  File \"/tmp/ipykernel_15806/256537533.py\", line 952, in my_snn_system\n",
      "    assert val_acc_best > 0.2\n",
      "AssertionError\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run yaiug1nn errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_15806/2189894352.py\", line 114, in hyper_iter\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     my_snn_system(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_15806/256537533.py\", line 952, in my_snn_system\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     assert val_acc_best > 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AssertionError\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mk4a21qf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251114_211434-mk4a21qf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mk4a21qf' target=\"_blank\">efficient-sweep-54</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mk4a21qf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mk4a21qf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251114_211443_699', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 8, 'leaky_temporal_filter': 0} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 540.0\n",
      "lif layer 1 self.abs_max_v: 540.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1005.0\n",
      "lif layer 2 self.abs_max_v: 1005.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 662.0\n",
      "fc layer 1 self.abs_max_out: 565.0\n",
      "fc layer 2 self.abs_max_out: 1083.0\n",
      "lif layer 2 self.abs_max_v: 1070.0\n",
      "fc layer 1 self.abs_max_out: 607.0\n",
      "lif layer 1 self.abs_max_v: 831.5\n",
      "fc layer 2 self.abs_max_out: 1098.0\n",
      "lif layer 2 self.abs_max_v: 1268.0\n",
      "fc layer 3 self.abs_max_out: 664.0\n",
      "fc layer 2 self.abs_max_out: 1151.0\n",
      "lif layer 2 self.abs_max_v: 1494.0\n",
      "fc layer 1 self.abs_max_out: 654.0\n",
      "lif layer 1 self.abs_max_v: 1028.5\n",
      "fc layer 2 self.abs_max_out: 1391.0\n",
      "lif layer 2 self.abs_max_v: 1662.5\n",
      "lif layer 2 self.abs_max_v: 2001.5\n",
      "fc layer 1 self.abs_max_out: 822.0\n",
      "fc layer 2 self.abs_max_out: 1750.0\n",
      "fc layer 1 self.abs_max_out: 855.0\n",
      "fc layer 1 self.abs_max_out: 979.0\n",
      "lif layer 1 self.abs_max_v: 1372.0\n",
      "lif layer 1 self.abs_max_v: 1478.0\n",
      "lif layer 2 self.abs_max_v: 2259.5\n",
      "fc layer 1 self.abs_max_out: 1000.0\n",
      "lif layer 2 self.abs_max_v: 2272.0\n",
      "lif layer 1 self.abs_max_v: 1621.5\n",
      "fc layer 2 self.abs_max_out: 1784.0\n",
      "fc layer 3 self.abs_max_out: 913.0\n",
      "fc layer 2 self.abs_max_out: 1990.0\n",
      "fc layer 1 self.abs_max_out: 1042.0\n",
      "fc layer 1 self.abs_max_out: 1158.0\n",
      "lif layer 2 self.abs_max_v: 2668.5\n",
      "fc layer 1 self.abs_max_out: 1257.0\n",
      "fc layer 2 self.abs_max_out: 2031.0\n",
      "lif layer 2 self.abs_max_v: 2783.0\n",
      "lif layer 2 self.abs_max_v: 2911.5\n",
      "lif layer 1 self.abs_max_v: 1787.0\n",
      "fc layer 1 self.abs_max_out: 1316.0\n",
      "lif layer 1 self.abs_max_v: 2143.5\n",
      "fc layer 2 self.abs_max_out: 2290.0\n",
      "lif layer 2 self.abs_max_v: 2972.5\n",
      "fc layer 3 self.abs_max_out: 943.0\n",
      "lif layer 2 self.abs_max_v: 3046.0\n",
      "lif layer 2 self.abs_max_v: 3054.0\n",
      "lif layer 2 self.abs_max_v: 3380.5\n",
      "fc layer 2 self.abs_max_out: 2460.0\n",
      "fc layer 1 self.abs_max_out: 1333.0\n",
      "fc layer 1 self.abs_max_out: 1523.0\n",
      "fc layer 2 self.abs_max_out: 2894.0\n",
      "fc layer 1 self.abs_max_out: 1548.0\n",
      "lif layer 2 self.abs_max_v: 3381.5\n",
      "fc layer 1 self.abs_max_out: 1820.0\n",
      "lif layer 2 self.abs_max_v: 3491.0\n",
      "lif layer 2 self.abs_max_v: 3624.0\n",
      "fc layer 1 self.abs_max_out: 1865.0\n",
      "fc layer 3 self.abs_max_out: 1033.0\n",
      "lif layer 2 self.abs_max_v: 3785.0\n",
      "fc layer 1 self.abs_max_out: 1987.0\n",
      "lif layer 1 self.abs_max_v: 2233.5\n",
      "lif layer 1 self.abs_max_v: 2321.0\n",
      "lif layer 2 self.abs_max_v: 3837.5\n",
      "lif layer 2 self.abs_max_v: 3953.0\n",
      "fc layer 1 self.abs_max_out: 2004.0\n",
      "lif layer 1 self.abs_max_v: 2613.0\n",
      "fc layer 1 self.abs_max_out: 2033.0\n",
      "fc layer 1 self.abs_max_out: 2747.0\n",
      "lif layer 1 self.abs_max_v: 2747.0\n",
      "fc layer 3 self.abs_max_out: 1059.0\n",
      "fc layer 3 self.abs_max_out: 1085.0\n",
      "lif layer 1 self.abs_max_v: 2783.0\n",
      "lif layer 2 self.abs_max_v: 3960.5\n",
      "fc layer 3 self.abs_max_out: 1185.0\n",
      "fc layer 3 self.abs_max_out: 1263.0\n",
      "lif layer 1 self.abs_max_v: 2929.5\n",
      "lif layer 2 self.abs_max_v: 4046.5\n",
      "lif layer 2 self.abs_max_v: 4090.5\n",
      "lif layer 2 self.abs_max_v: 4214.0\n",
      "lif layer 2 self.abs_max_v: 4272.0\n",
      "lif layer 2 self.abs_max_v: 4364.5\n",
      "lif layer 2 self.abs_max_v: 4564.5\n",
      "lif layer 1 self.abs_max_v: 2967.5\n",
      "fc layer 1 self.abs_max_out: 2828.0\n",
      "fc layer 2 self.abs_max_out: 2969.0\n",
      "lif layer 1 self.abs_max_v: 3198.5\n",
      "lif layer 1 self.abs_max_v: 3362.5\n",
      "fc layer 2 self.abs_max_out: 3050.0\n",
      "lif layer 2 self.abs_max_v: 4804.5\n",
      "fc layer 3 self.abs_max_out: 1276.0\n",
      "fc layer 3 self.abs_max_out: 1307.0\n",
      "fc layer 2 self.abs_max_out: 3118.0\n",
      "fc layer 2 self.abs_max_out: 3373.0\n",
      "lif layer 1 self.abs_max_v: 3439.5\n",
      "lif layer 1 self.abs_max_v: 3451.5\n",
      "lif layer 1 self.abs_max_v: 3643.0\n",
      "lif layer 1 self.abs_max_v: 3804.5\n",
      "fc layer 1 self.abs_max_out: 2958.0\n",
      "lif layer 1 self.abs_max_v: 3828.5\n",
      "lif layer 1 self.abs_max_v: 3880.0\n",
      "lif layer 1 self.abs_max_v: 4026.5\n",
      "lif layer 1 self.abs_max_v: 4937.5\n",
      "fc layer 1 self.abs_max_out: 2991.0\n",
      "lif layer 1 self.abs_max_v: 5460.0\n",
      "fc layer 1 self.abs_max_out: 3151.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  1.941457/  2.012389, val:  50.42%, val_best:  50.42%, tr:  78.96%, tr_best:  78.96%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0891%\n",
      "layer   2  Sparsity: 76.4249%\n",
      "layer   3  Sparsity: 72.6710%\n",
      "total_backward_count 9790 real_backward_count 3666  37.446%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 1353.0\n",
      "fc layer 1 self.abs_max_out: 3206.0\n",
      "fc layer 1 self.abs_max_out: 3219.0\n",
      "lif layer 2 self.abs_max_v: 4949.5\n",
      "fc layer 3 self.abs_max_out: 1439.0\n",
      "fc layer 3 self.abs_max_out: 1462.0\n",
      "fc layer 1 self.abs_max_out: 3585.0\n",
      "lif layer 1 self.abs_max_v: 5550.0\n",
      "lif layer 1 self.abs_max_v: 6198.0\n",
      "lif layer 1 self.abs_max_v: 6277.0\n",
      "fc layer 1 self.abs_max_out: 3696.0\n",
      "fc layer 2 self.abs_max_out: 3386.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.875933/  2.015906, val:  54.17%, val_best:  54.17%, tr:  92.75%, tr_best:  92.75%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0923%\n",
      "layer   2  Sparsity: 74.8370%\n",
      "layer   3  Sparsity: 71.7440%\n",
      "total_backward_count 19580 real_backward_count 5969  30.485%\n",
      "lif layer 2 self.abs_max_v: 5001.0\n",
      "lif layer 2 self.abs_max_v: 5069.5\n",
      "lif layer 2 self.abs_max_v: 5070.5\n",
      "lif layer 2 self.abs_max_v: 5273.0\n",
      "lif layer 2 self.abs_max_v: 5373.5\n",
      "lif layer 2 self.abs_max_v: 5420.0\n",
      "fc layer 2 self.abs_max_out: 3446.0\n",
      "lif layer 1 self.abs_max_v: 6379.5\n",
      "lif layer 1 self.abs_max_v: 6449.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.881698/  2.001713, val:  54.58%, val_best:  54.58%, tr:  96.73%, tr_best:  96.73%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0936%\n",
      "layer   2  Sparsity: 74.6483%\n",
      "layer   3  Sparsity: 70.6880%\n",
      "total_backward_count 29370 real_backward_count 7851  26.731%\n",
      "fc layer 2 self.abs_max_out: 3475.0\n",
      "fc layer 2 self.abs_max_out: 3485.0\n",
      "lif layer 2 self.abs_max_v: 5543.5\n",
      "fc layer 2 self.abs_max_out: 3512.0\n",
      "fc layer 2 self.abs_max_out: 3515.0\n",
      "fc layer 2 self.abs_max_out: 3602.0\n",
      "lif layer 1 self.abs_max_v: 6465.0\n",
      "lif layer 1 self.abs_max_v: 6539.5\n",
      "fc layer 1 self.abs_max_out: 3725.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  1.893446/  2.021339, val:  52.92%, val_best:  54.58%, tr:  97.85%, tr_best:  97.85%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0838%\n",
      "layer   2  Sparsity: 74.3652%\n",
      "layer   3  Sparsity: 70.2947%\n",
      "total_backward_count 39160 real_backward_count 9477  24.201%\n",
      "fc layer 2 self.abs_max_out: 3629.0\n",
      "fc layer 2 self.abs_max_out: 3883.0\n",
      "fc layer 1 self.abs_max_out: 3940.0\n",
      "lif layer 1 self.abs_max_v: 6669.0\n",
      "lif layer 1 self.abs_max_v: 6731.5\n",
      "fc layer 1 self.abs_max_out: 3972.0\n",
      "fc layer 1 self.abs_max_out: 3989.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  1.908312/  2.019911, val:  59.58%, val_best:  59.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0886%\n",
      "layer   2  Sparsity: 73.9466%\n",
      "layer   3  Sparsity: 70.3221%\n",
      "total_backward_count 48950 real_backward_count 10977  22.425%\n",
      "fc layer 1 self.abs_max_out: 3991.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  1.906201/  2.027775, val:  57.50%, val_best:  59.58%, tr:  98.77%, tr_best:  99.49%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0886%\n",
      "layer   2  Sparsity: 74.0847%\n",
      "layer   3  Sparsity: 70.2896%\n",
      "total_backward_count 58740 real_backward_count 12375  21.067%\n",
      "fc layer 1 self.abs_max_out: 4026.0\n",
      "lif layer 2 self.abs_max_v: 5677.0\n",
      "lif layer 2 self.abs_max_v: 6234.0\n",
      "lif layer 1 self.abs_max_v: 7010.0\n",
      "lif layer 1 self.abs_max_v: 7202.0\n",
      "fc layer 1 self.abs_max_out: 4190.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  1.914041/  2.030792, val:  55.00%, val_best:  59.58%, tr:  99.18%, tr_best:  99.49%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0828%\n",
      "layer   2  Sparsity: 74.0465%\n",
      "layer   3  Sparsity: 69.7735%\n",
      "total_backward_count 68530 real_backward_count 13738  20.047%\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  1.917247/  2.024398, val:  56.67%, val_best:  59.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0888%\n",
      "layer   2  Sparsity: 73.9297%\n",
      "layer   3  Sparsity: 69.8861%\n",
      "total_backward_count 78320 real_backward_count 15007  19.161%\n",
      "fc layer 1 self.abs_max_out: 4378.0\n",
      "lif layer 1 self.abs_max_v: 7315.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  1.918555/  2.030950, val:  64.58%, val_best:  64.58%, tr:  99.49%, tr_best:  99.59%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0823%\n",
      "layer   2  Sparsity: 73.7022%\n",
      "layer   3  Sparsity: 69.7123%\n",
      "total_backward_count 88110 real_backward_count 16272  18.468%\n",
      "fc layer 1 self.abs_max_out: 4736.0\n",
      "lif layer 1 self.abs_max_v: 7654.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  1.917076/  2.031933, val:  54.17%, val_best:  64.58%, tr:  99.49%, tr_best:  99.59%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0942%\n",
      "layer   2  Sparsity: 73.3641%\n",
      "layer   3  Sparsity: 69.2580%\n",
      "total_backward_count 97900 real_backward_count 17442  17.816%\n",
      "fc layer 2 self.abs_max_out: 3997.0\n",
      "lif layer 1 self.abs_max_v: 7951.5\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  1.924492/  2.038900, val:  53.75%, val_best:  64.58%, tr:  99.39%, tr_best:  99.59%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0865%\n",
      "layer   2  Sparsity: 73.5425%\n",
      "layer   3  Sparsity: 69.4395%\n",
      "total_backward_count 107690 real_backward_count 18604  17.276%\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  1.918873/  2.025258, val:  65.42%, val_best:  65.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.50 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0789%\n",
      "layer   2  Sparsity: 73.5047%\n",
      "layer   3  Sparsity: 68.3667%\n",
      "total_backward_count 117480 real_backward_count 19769  16.828%\n",
      "fc layer 2 self.abs_max_out: 4087.0\n",
      "lif layer 1 self.abs_max_v: 8214.5\n",
      "lif layer 1 self.abs_max_v: 8235.5\n",
      "fc layer 1 self.abs_max_out: 5068.0\n",
      "lif layer 1 self.abs_max_v: 8714.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  1.917359/  2.036646, val:  57.08%, val_best:  65.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0829%\n",
      "layer   2  Sparsity: 73.7285%\n",
      "layer   3  Sparsity: 69.2228%\n",
      "total_backward_count 127270 real_backward_count 20819  16.358%\n",
      "fc layer 1 self.abs_max_out: 5118.0\n",
      "lif layer 1 self.abs_max_v: 8779.5\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  1.931449/  2.039587, val:  55.00%, val_best:  65.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.90 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0959%\n",
      "layer   2  Sparsity: 73.4910%\n",
      "layer   3  Sparsity: 69.1623%\n",
      "total_backward_count 137060 real_backward_count 21894  15.974%\n",
      "fc layer 2 self.abs_max_out: 4160.0\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  1.930434/  2.035998, val:  60.83%, val_best:  65.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0911%\n",
      "layer   2  Sparsity: 73.3328%\n",
      "layer   3  Sparsity: 68.9155%\n",
      "total_backward_count 146850 real_backward_count 22972  15.643%\n",
      "fc layer 1 self.abs_max_out: 5446.0\n",
      "lif layer 1 self.abs_max_v: 9475.5\n",
      "fc layer 2 self.abs_max_out: 4161.0\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  1.942422/  2.059708, val:  53.75%, val_best:  65.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0852%\n",
      "layer   2  Sparsity: 73.4736%\n",
      "layer   3  Sparsity: 68.8783%\n",
      "total_backward_count 156640 real_backward_count 24011  15.329%\n",
      "fc layer 2 self.abs_max_out: 4210.0\n",
      "fc layer 2 self.abs_max_out: 4292.0\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  1.948519/  2.048257, val:  73.33%, val_best:  73.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0952%\n",
      "layer   2  Sparsity: 73.3893%\n",
      "layer   3  Sparsity: 69.2769%\n",
      "total_backward_count 166430 real_backward_count 25012  15.029%\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  1.944285/  2.044430, val:  66.67%, val_best:  73.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0894%\n",
      "layer   2  Sparsity: 73.2762%\n",
      "layer   3  Sparsity: 69.3865%\n",
      "total_backward_count 176220 real_backward_count 26041  14.778%\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  1.943073/  2.040604, val:  50.00%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0815%\n",
      "layer   2  Sparsity: 72.8109%\n",
      "layer   3  Sparsity: 68.8095%\n",
      "total_backward_count 186010 real_backward_count 27026  14.529%\n",
      "fc layer 2 self.abs_max_out: 4656.0\n",
      "fc layer 1 self.abs_max_out: 5517.0\n",
      "lif layer 1 self.abs_max_v: 9505.5\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  1.938284/  2.046788, val:  61.25%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0922%\n",
      "layer   2  Sparsity: 72.5608%\n",
      "layer   3  Sparsity: 68.9477%\n",
      "total_backward_count 195800 real_backward_count 27972  14.286%\n",
      "fc layer 2 self.abs_max_out: 4680.0\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  1.940512/  2.051898, val:  57.50%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0835%\n",
      "layer   2  Sparsity: 73.1279%\n",
      "layer   3  Sparsity: 69.5773%\n",
      "total_backward_count 205590 real_backward_count 28892  14.053%\n",
      "fc layer 1 self.abs_max_out: 5552.0\n",
      "lif layer 1 self.abs_max_v: 9507.5\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  1.943607/  2.048181, val:  68.33%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.38 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 95.0899%\n",
      "layer   2  Sparsity: 72.9223%\n",
      "layer   3  Sparsity: 69.2441%\n",
      "total_backward_count 215380 real_backward_count 29830  13.850%\n",
      "fc layer 1 self.abs_max_out: 5557.0\n",
      "lif layer 1 self.abs_max_v: 9508.0\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  1.946367/  2.032564, val:  65.83%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0839%\n",
      "layer   2  Sparsity: 72.8384%\n",
      "layer   3  Sparsity: 69.3418%\n",
      "total_backward_count 225170 real_backward_count 30812  13.684%\n",
      "fc layer 1 self.abs_max_out: 5638.0\n",
      "lif layer 1 self.abs_max_v: 9673.5\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  1.943107/  2.036342, val:  65.42%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0850%\n",
      "layer   2  Sparsity: 72.6872%\n",
      "layer   3  Sparsity: 69.3846%\n",
      "total_backward_count 234960 real_backward_count 31704  13.493%\n",
      "fc layer 2 self.abs_max_out: 5034.0\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  1.943799/  2.048664, val:  67.50%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0915%\n",
      "layer   2  Sparsity: 72.7033%\n",
      "layer   3  Sparsity: 69.6858%\n",
      "total_backward_count 244750 real_backward_count 32610  13.324%\n",
      "fc layer 1 self.abs_max_out: 5931.0\n",
      "lif layer 1 self.abs_max_v: 10227.5\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  1.954596/  2.040939, val:  70.83%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0808%\n",
      "layer   2  Sparsity: 72.8158%\n",
      "layer   3  Sparsity: 69.7757%\n",
      "total_backward_count 254540 real_backward_count 33548  13.180%\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  1.951747/  2.037997, val:  75.42%, val_best:  75.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.00 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0908%\n",
      "layer   2  Sparsity: 72.8928%\n",
      "layer   3  Sparsity: 70.2836%\n",
      "total_backward_count 264330 real_backward_count 34431  13.026%\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  1.941722/  2.037101, val:  80.42%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0966%\n",
      "layer   2  Sparsity: 72.8829%\n",
      "layer   3  Sparsity: 69.8272%\n",
      "total_backward_count 274120 real_backward_count 35339  12.892%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  1.943850/  2.043632, val:  64.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0910%\n",
      "layer   2  Sparsity: 72.9104%\n",
      "layer   3  Sparsity: 69.5325%\n",
      "total_backward_count 283910 real_backward_count 36217  12.757%\n",
      "fc layer 1 self.abs_max_out: 6185.0\n",
      "lif layer 1 self.abs_max_v: 10685.5\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  1.949458/  2.043896, val:  62.50%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0852%\n",
      "layer   2  Sparsity: 72.8943%\n",
      "layer   3  Sparsity: 69.6941%\n",
      "total_backward_count 293700 real_backward_count 37067  12.621%\n",
      "fc layer 1 self.abs_max_out: 6194.0\n",
      "lif layer 1 self.abs_max_v: 10692.0\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  1.955186/  2.047358, val:  64.58%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0858%\n",
      "layer   2  Sparsity: 72.8459%\n",
      "layer   3  Sparsity: 69.2869%\n",
      "total_backward_count 303490 real_backward_count 37901  12.488%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  1.955414/  2.042020, val:  67.08%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0907%\n",
      "layer   2  Sparsity: 73.1165%\n",
      "layer   3  Sparsity: 69.2749%\n",
      "total_backward_count 313280 real_backward_count 38779  12.378%\n",
      "fc layer 1 self.abs_max_out: 6619.0\n",
      "lif layer 1 self.abs_max_v: 11487.0\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  1.950986/  2.049142, val:  60.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0809%\n",
      "layer   2  Sparsity: 72.7892%\n",
      "layer   3  Sparsity: 69.3148%\n",
      "total_backward_count 323070 real_backward_count 39616  12.262%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  1.955388/  2.042220, val:  70.00%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0897%\n",
      "layer   2  Sparsity: 72.5991%\n",
      "layer   3  Sparsity: 68.9256%\n",
      "total_backward_count 332860 real_backward_count 40476  12.160%\n",
      "fc layer 1 self.abs_max_out: 6662.0\n",
      "lif layer 1 self.abs_max_v: 11562.5\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  1.948137/  2.045217, val:  66.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0740%\n",
      "layer   2  Sparsity: 72.4106%\n",
      "layer   3  Sparsity: 69.3196%\n",
      "total_backward_count 342650 real_backward_count 41280  12.047%\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  1.950629/  2.041268, val:  70.00%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.27 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 95.0906%\n",
      "layer   2  Sparsity: 72.1956%\n",
      "layer   3  Sparsity: 69.5335%\n",
      "total_backward_count 352440 real_backward_count 42099  11.945%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  1.948119/  2.031811, val:  72.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0903%\n",
      "layer   2  Sparsity: 72.1073%\n",
      "layer   3  Sparsity: 69.1113%\n",
      "total_backward_count 362230 real_backward_count 42894  11.842%\n",
      "fc layer 1 self.abs_max_out: 6792.0\n",
      "lif layer 1 self.abs_max_v: 11743.5\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  1.943557/  2.042440, val:  75.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0783%\n",
      "layer   2  Sparsity: 72.4407%\n",
      "layer   3  Sparsity: 68.3879%\n",
      "total_backward_count 372020 real_backward_count 43693  11.745%\n",
      "fc layer 2 self.abs_max_out: 5161.0\n",
      "fc layer 1 self.abs_max_out: 6830.0\n",
      "lif layer 1 self.abs_max_v: 11788.5\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  1.945449/  2.039910, val:  67.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0890%\n",
      "layer   2  Sparsity: 72.8151%\n",
      "layer   3  Sparsity: 68.5493%\n",
      "total_backward_count 381810 real_backward_count 44479  11.650%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  1.944202/  2.038650, val:  71.25%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.08 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0714%\n",
      "layer   2  Sparsity: 72.7070%\n",
      "layer   3  Sparsity: 68.7084%\n",
      "total_backward_count 391600 real_backward_count 45258  11.557%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  1.943510/  2.045122, val:  59.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0861%\n",
      "layer   2  Sparsity: 72.6205%\n",
      "layer   3  Sparsity: 68.4476%\n",
      "total_backward_count 401390 real_backward_count 46056  11.474%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  1.951308/  2.046119, val:  71.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0848%\n",
      "layer   2  Sparsity: 72.3857%\n",
      "layer   3  Sparsity: 68.7703%\n",
      "total_backward_count 411180 real_backward_count 46820  11.387%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  1.948033/  2.035000, val:  75.00%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0866%\n",
      "layer   2  Sparsity: 72.3236%\n",
      "layer   3  Sparsity: 68.5017%\n",
      "total_backward_count 420970 real_backward_count 47612  11.310%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  1.942065/  2.040448, val:  75.42%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0863%\n",
      "layer   2  Sparsity: 72.2269%\n",
      "layer   3  Sparsity: 68.5514%\n",
      "total_backward_count 430760 real_backward_count 48407  11.238%\n",
      "fc layer 1 self.abs_max_out: 6884.0\n",
      "lif layer 1 self.abs_max_v: 11900.0\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  1.944927/  2.040788, val:  65.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0882%\n",
      "layer   2  Sparsity: 72.0979%\n",
      "layer   3  Sparsity: 68.0355%\n",
      "total_backward_count 440550 real_backward_count 49143  11.155%\n",
      "fc layer 1 self.abs_max_out: 7056.0\n",
      "lif layer 1 self.abs_max_v: 12255.0\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  1.944243/  2.033915, val:  77.92%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0911%\n",
      "layer   2  Sparsity: 72.0182%\n",
      "layer   3  Sparsity: 68.6267%\n",
      "total_backward_count 450340 real_backward_count 49922  11.085%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  1.943048/  2.027830, val:  65.42%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0891%\n",
      "layer   2  Sparsity: 72.2668%\n",
      "layer   3  Sparsity: 68.5268%\n",
      "total_backward_count 460130 real_backward_count 50658  11.009%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  1.934627/  2.039327, val:  70.83%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0978%\n",
      "layer   2  Sparsity: 72.4325%\n",
      "layer   3  Sparsity: 68.4801%\n",
      "total_backward_count 469920 real_backward_count 51393  10.937%\n",
      "fc layer 1 self.abs_max_out: 7255.0\n",
      "lif layer 1 self.abs_max_v: 12624.0\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  1.943579/  2.034596, val:  72.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0923%\n",
      "layer   2  Sparsity: 72.4228%\n",
      "layer   3  Sparsity: 68.5922%\n",
      "total_backward_count 479710 real_backward_count 52124  10.866%\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  1.939009/  2.039774, val:  67.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0810%\n",
      "layer   2  Sparsity: 71.9292%\n",
      "layer   3  Sparsity: 68.7868%\n",
      "total_backward_count 489500 real_backward_count 52854  10.798%\n",
      "fc layer 1 self.abs_max_out: 7599.0\n",
      "lif layer 1 self.abs_max_v: 13326.0\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  1.943479/  2.038848, val:  66.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0977%\n",
      "layer   2  Sparsity: 72.0690%\n",
      "layer   3  Sparsity: 68.7459%\n",
      "total_backward_count 499290 real_backward_count 53595  10.734%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  1.948408/  2.045202, val:  73.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0834%\n",
      "layer   2  Sparsity: 72.0807%\n",
      "layer   3  Sparsity: 68.0406%\n",
      "total_backward_count 509080 real_backward_count 54314  10.669%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  1.943033/  2.032482, val:  71.25%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0855%\n",
      "layer   2  Sparsity: 72.1483%\n",
      "layer   3  Sparsity: 68.1415%\n",
      "total_backward_count 518870 real_backward_count 55011  10.602%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  1.942942/  2.045272, val:  62.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0841%\n",
      "layer   2  Sparsity: 72.1361%\n",
      "layer   3  Sparsity: 68.1615%\n",
      "total_backward_count 528660 real_backward_count 55700  10.536%\n",
      "fc layer 2 self.abs_max_out: 5241.0\n",
      "lif layer 2 self.abs_max_v: 6426.5\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  1.947576/  2.045238, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0886%\n",
      "layer   2  Sparsity: 72.2379%\n",
      "layer   3  Sparsity: 68.7263%\n",
      "total_backward_count 538450 real_backward_count 56402  10.475%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.953302/  2.049425, val:  75.83%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0853%\n",
      "layer   2  Sparsity: 72.3668%\n",
      "layer   3  Sparsity: 68.7946%\n",
      "total_backward_count 548240 real_backward_count 57128  10.420%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.964033/  2.056081, val:  74.58%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.88 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0849%\n",
      "layer   2  Sparsity: 72.5084%\n",
      "layer   3  Sparsity: 68.5123%\n",
      "total_backward_count 558030 real_backward_count 57827  10.363%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.964253/  2.057403, val:  75.00%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0810%\n",
      "layer   2  Sparsity: 72.3498%\n",
      "layer   3  Sparsity: 68.4050%\n",
      "total_backward_count 567820 real_backward_count 58536  10.309%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.959427/  2.057700, val:  70.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0826%\n",
      "layer   2  Sparsity: 72.0700%\n",
      "layer   3  Sparsity: 68.6133%\n",
      "total_backward_count 577610 real_backward_count 59227  10.254%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.959883/  2.057143, val:  58.75%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0924%\n",
      "layer   2  Sparsity: 72.0836%\n",
      "layer   3  Sparsity: 68.6459%\n",
      "total_backward_count 587400 real_backward_count 59885  10.195%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.958093/  2.048528, val:  64.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0827%\n",
      "layer   2  Sparsity: 71.8269%\n",
      "layer   3  Sparsity: 68.6463%\n",
      "total_backward_count 597190 real_backward_count 60577  10.144%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.951161/  2.034932, val:  71.25%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0800%\n",
      "layer   2  Sparsity: 71.6738%\n",
      "layer   3  Sparsity: 68.1158%\n",
      "total_backward_count 606980 real_backward_count 61240  10.089%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.951629/  2.055135, val:  74.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0892%\n",
      "layer   2  Sparsity: 71.7140%\n",
      "layer   3  Sparsity: 68.3995%\n",
      "total_backward_count 616770 real_backward_count 61953  10.045%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.962610/  2.052434, val:  71.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.09 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0921%\n",
      "layer   2  Sparsity: 72.0671%\n",
      "layer   3  Sparsity: 68.4100%\n",
      "total_backward_count 626560 real_backward_count 62637   9.997%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.965233/  2.047742, val:  76.25%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0929%\n",
      "layer   2  Sparsity: 72.1453%\n",
      "layer   3  Sparsity: 68.5391%\n",
      "total_backward_count 636350 real_backward_count 63331   9.952%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.951401/  2.034920, val:  74.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0865%\n",
      "layer   2  Sparsity: 72.2164%\n",
      "layer   3  Sparsity: 68.3837%\n",
      "total_backward_count 646140 real_backward_count 63988   9.903%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.940906/  2.036929, val:  75.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0840%\n",
      "layer   2  Sparsity: 72.1728%\n",
      "layer   3  Sparsity: 68.5721%\n",
      "total_backward_count 655930 real_backward_count 64690   9.862%\n",
      "fc layer 2 self.abs_max_out: 5399.0\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.943938/  2.041560, val:  72.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0879%\n",
      "layer   2  Sparsity: 72.1661%\n",
      "layer   3  Sparsity: 69.6448%\n",
      "total_backward_count 665720 real_backward_count 65395   9.823%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.951746/  2.049635, val:  75.42%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0810%\n",
      "layer   2  Sparsity: 71.7916%\n",
      "layer   3  Sparsity: 69.6164%\n",
      "total_backward_count 675510 real_backward_count 66078   9.782%\n",
      "fc layer 1 self.abs_max_out: 7735.0\n",
      "lif layer 1 self.abs_max_v: 13477.5\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.950779/  2.044087, val:  65.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0905%\n",
      "layer   2  Sparsity: 71.9378%\n",
      "layer   3  Sparsity: 69.3198%\n",
      "total_backward_count 685300 real_backward_count 66725   9.737%\n",
      "fc layer 1 self.abs_max_out: 7844.0\n",
      "lif layer 1 self.abs_max_v: 13665.0\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.943786/  2.031778, val:  69.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0885%\n",
      "layer   2  Sparsity: 71.7342%\n",
      "layer   3  Sparsity: 69.2081%\n",
      "total_backward_count 695090 real_backward_count 67412   9.698%\n",
      "fc layer 2 self.abs_max_out: 5724.0\n",
      "fc layer 1 self.abs_max_out: 7956.0\n",
      "lif layer 1 self.abs_max_v: 13881.5\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.947946/  2.048310, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0943%\n",
      "layer   2  Sparsity: 71.8588%\n",
      "layer   3  Sparsity: 69.5908%\n",
      "total_backward_count 704880 real_backward_count 68084   9.659%\n",
      "fc layer 1 self.abs_max_out: 7958.0\n",
      "lif layer 1 self.abs_max_v: 13921.0\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.950652/  2.043115, val:  70.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0922%\n",
      "layer   2  Sparsity: 71.9757%\n",
      "layer   3  Sparsity: 69.9720%\n",
      "total_backward_count 714670 real_backward_count 68728   9.617%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.951319/  2.039014, val:  61.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.49 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 95.0867%\n",
      "layer   2  Sparsity: 71.8087%\n",
      "layer   3  Sparsity: 69.7918%\n",
      "total_backward_count 724460 real_backward_count 69368   9.575%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.947867/  2.038872, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0847%\n",
      "layer   2  Sparsity: 71.8409%\n",
      "layer   3  Sparsity: 69.9825%\n",
      "total_backward_count 734250 real_backward_count 70052   9.541%\n",
      "lif layer 2 self.abs_max_v: 6574.5\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.952133/  2.041673, val:  76.25%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0809%\n",
      "layer   2  Sparsity: 71.8582%\n",
      "layer   3  Sparsity: 69.9000%\n",
      "total_backward_count 744040 real_backward_count 70700   9.502%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.948709/  2.043070, val:  79.58%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0929%\n",
      "layer   2  Sparsity: 71.7109%\n",
      "layer   3  Sparsity: 70.0084%\n",
      "total_backward_count 753830 real_backward_count 71336   9.463%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.949193/  2.036325, val:  70.00%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0878%\n",
      "layer   2  Sparsity: 71.6292%\n",
      "layer   3  Sparsity: 69.8360%\n",
      "total_backward_count 763620 real_backward_count 72019   9.431%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.952126/  2.041427, val:  70.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.95 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0894%\n",
      "layer   2  Sparsity: 71.4795%\n",
      "layer   3  Sparsity: 69.4103%\n",
      "total_backward_count 773410 real_backward_count 72737   9.405%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.949107/  2.046135, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0837%\n",
      "layer   2  Sparsity: 71.4706%\n",
      "layer   3  Sparsity: 69.5737%\n",
      "total_backward_count 783200 real_backward_count 73365   9.367%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.956565/  2.045284, val:  68.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0884%\n",
      "layer   2  Sparsity: 71.5434%\n",
      "layer   3  Sparsity: 70.0430%\n",
      "total_backward_count 792990 real_backward_count 74025   9.335%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.953625/  2.040328, val:  68.75%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0888%\n",
      "layer   2  Sparsity: 71.7875%\n",
      "layer   3  Sparsity: 69.9431%\n",
      "total_backward_count 802780 real_backward_count 74701   9.305%\n",
      "lif layer 2 self.abs_max_v: 6661.5\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.953304/  2.044362, val:  65.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0865%\n",
      "layer   2  Sparsity: 71.4856%\n",
      "layer   3  Sparsity: 69.7414%\n",
      "total_backward_count 812570 real_backward_count 75344   9.272%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.958688/  2.052579, val:  70.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0929%\n",
      "layer   2  Sparsity: 71.2938%\n",
      "layer   3  Sparsity: 69.4543%\n",
      "total_backward_count 822360 real_backward_count 76021   9.244%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.957518/  2.042021, val:  82.92%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.97 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0892%\n",
      "layer   2  Sparsity: 71.3088%\n",
      "layer   3  Sparsity: 68.7943%\n",
      "total_backward_count 832150 real_backward_count 76709   9.218%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.950654/  2.040798, val:  61.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0875%\n",
      "layer   2  Sparsity: 71.5462%\n",
      "layer   3  Sparsity: 69.5436%\n",
      "total_backward_count 841940 real_backward_count 77347   9.187%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.955513/  2.046861, val:  73.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0781%\n",
      "layer   2  Sparsity: 71.5232%\n",
      "layer   3  Sparsity: 70.6619%\n",
      "total_backward_count 851730 real_backward_count 77997   9.157%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.954601/  2.043700, val:  80.83%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0909%\n",
      "layer   2  Sparsity: 71.4094%\n",
      "layer   3  Sparsity: 70.6082%\n",
      "total_backward_count 861520 real_backward_count 78638   9.128%\n",
      "lif layer 2 self.abs_max_v: 6864.0\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.951797/  2.046628, val:  63.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0901%\n",
      "layer   2  Sparsity: 71.4372%\n",
      "layer   3  Sparsity: 69.8957%\n",
      "total_backward_count 871310 real_backward_count 79282   9.099%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.954424/  2.043673, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0830%\n",
      "layer   2  Sparsity: 71.2887%\n",
      "layer   3  Sparsity: 69.8605%\n",
      "total_backward_count 881100 real_backward_count 79911   9.069%\n",
      "fc layer 1 self.abs_max_out: 8110.0\n",
      "lif layer 1 self.abs_max_v: 14177.5\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.957947/  2.043603, val:  70.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0868%\n",
      "layer   2  Sparsity: 71.1605%\n",
      "layer   3  Sparsity: 69.8585%\n",
      "total_backward_count 890890 real_backward_count 80562   9.043%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.957427/  2.036652, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0859%\n",
      "layer   2  Sparsity: 71.2347%\n",
      "layer   3  Sparsity: 69.9828%\n",
      "total_backward_count 900680 real_backward_count 81171   9.012%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.951794/  2.032871, val:  77.50%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0859%\n",
      "layer   2  Sparsity: 71.0772%\n",
      "layer   3  Sparsity: 69.0406%\n",
      "total_backward_count 910470 real_backward_count 81780   8.982%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.952987/  2.034640, val:  75.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0906%\n",
      "layer   2  Sparsity: 70.9993%\n",
      "layer   3  Sparsity: 69.2970%\n",
      "total_backward_count 920260 real_backward_count 82401   8.954%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.954005/  2.037654, val:  77.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0834%\n",
      "layer   2  Sparsity: 71.1116%\n",
      "layer   3  Sparsity: 69.9475%\n",
      "total_backward_count 930050 real_backward_count 82986   8.923%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.948655/  2.038947, val:  75.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0824%\n",
      "layer   2  Sparsity: 71.0359%\n",
      "layer   3  Sparsity: 70.4346%\n",
      "total_backward_count 939840 real_backward_count 83594   8.894%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.955504/  2.038870, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.80 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0906%\n",
      "layer   2  Sparsity: 71.2205%\n",
      "layer   3  Sparsity: 70.7906%\n",
      "total_backward_count 949630 real_backward_count 84184   8.865%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.954473/  2.034389, val:  71.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0813%\n",
      "layer   2  Sparsity: 71.2840%\n",
      "layer   3  Sparsity: 70.1884%\n",
      "total_backward_count 959420 real_backward_count 84816   8.840%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.953211/  2.038041, val:  69.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0771%\n",
      "layer   2  Sparsity: 71.1511%\n",
      "layer   3  Sparsity: 70.4947%\n",
      "total_backward_count 969210 real_backward_count 85436   8.815%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.939635/  2.033576, val:  69.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0939%\n",
      "layer   2  Sparsity: 71.3181%\n",
      "layer   3  Sparsity: 70.5348%\n",
      "total_backward_count 979000 real_backward_count 86077   8.792%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.947262/  2.029518, val:  79.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0921%\n",
      "layer   2  Sparsity: 71.2572%\n",
      "layer   3  Sparsity: 70.0428%\n",
      "total_backward_count 988790 real_backward_count 86660   8.764%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.948060/  2.033484, val:  78.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0897%\n",
      "layer   2  Sparsity: 71.4215%\n",
      "layer   3  Sparsity: 70.3758%\n",
      "total_backward_count 998580 real_backward_count 87280   8.740%\n",
      "fc layer 1 self.abs_max_out: 8239.0\n",
      "lif layer 1 self.abs_max_v: 14423.5\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.953329/  2.030175, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0844%\n",
      "layer   2  Sparsity: 71.5568%\n",
      "layer   3  Sparsity: 70.0902%\n",
      "total_backward_count 1008370 real_backward_count 87868   8.714%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.951197/  2.034055, val:  74.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.95 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0840%\n",
      "layer   2  Sparsity: 71.5439%\n",
      "layer   3  Sparsity: 69.7759%\n",
      "total_backward_count 1018160 real_backward_count 88474   8.690%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.958018/  2.039440, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0978%\n",
      "layer   2  Sparsity: 71.3781%\n",
      "layer   3  Sparsity: 70.0366%\n",
      "total_backward_count 1027950 real_backward_count 89110   8.669%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.956935/  2.044241, val:  70.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0849%\n",
      "layer   2  Sparsity: 71.3713%\n",
      "layer   3  Sparsity: 69.8940%\n",
      "total_backward_count 1037740 real_backward_count 89688   8.643%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.953729/  2.038769, val:  79.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0899%\n",
      "layer   2  Sparsity: 71.2797%\n",
      "layer   3  Sparsity: 69.8872%\n",
      "total_backward_count 1047530 real_backward_count 90305   8.621%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.961303/  2.039798, val:  71.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0769%\n",
      "layer   2  Sparsity: 71.1856%\n",
      "layer   3  Sparsity: 69.7764%\n",
      "total_backward_count 1057320 real_backward_count 90938   8.601%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.962144/  2.049240, val:  77.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0997%\n",
      "layer   2  Sparsity: 71.1114%\n",
      "layer   3  Sparsity: 69.7808%\n",
      "total_backward_count 1067110 real_backward_count 91507   8.575%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.967684/  2.043048, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0847%\n",
      "layer   2  Sparsity: 71.0726%\n",
      "layer   3  Sparsity: 69.3831%\n",
      "total_backward_count 1076900 real_backward_count 92086   8.551%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.960041/  2.043763, val:  74.17%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0825%\n",
      "layer   2  Sparsity: 71.0283%\n",
      "layer   3  Sparsity: 69.6286%\n",
      "total_backward_count 1086690 real_backward_count 92677   8.528%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.955160/  2.036511, val:  72.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0831%\n",
      "layer   2  Sparsity: 70.8973%\n",
      "layer   3  Sparsity: 69.2878%\n",
      "total_backward_count 1096480 real_backward_count 93230   8.503%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.946642/  2.033942, val:  79.58%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0797%\n",
      "layer   2  Sparsity: 70.8975%\n",
      "layer   3  Sparsity: 68.9235%\n",
      "total_backward_count 1106270 real_backward_count 93832   8.482%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.948670/  2.034837, val:  64.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0876%\n",
      "layer   2  Sparsity: 71.0160%\n",
      "layer   3  Sparsity: 68.8405%\n",
      "total_backward_count 1116060 real_backward_count 94433   8.461%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.943996/  2.030766, val:  74.17%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0892%\n",
      "layer   2  Sparsity: 71.0219%\n",
      "layer   3  Sparsity: 69.2963%\n",
      "total_backward_count 1125850 real_backward_count 95015   8.439%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.946395/  2.037868, val:  75.42%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0900%\n",
      "layer   2  Sparsity: 71.0836%\n",
      "layer   3  Sparsity: 69.2161%\n",
      "total_backward_count 1135640 real_backward_count 95585   8.417%\n",
      "lif layer 1 self.abs_max_v: 14615.5\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.949365/  2.029867, val:  65.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0825%\n",
      "layer   2  Sparsity: 70.8338%\n",
      "layer   3  Sparsity: 69.4522%\n",
      "total_backward_count 1145430 real_backward_count 96187   8.397%\n",
      "fc layer 1 self.abs_max_out: 8272.0\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.948949/  2.033804, val:  72.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0945%\n",
      "layer   2  Sparsity: 70.8756%\n",
      "layer   3  Sparsity: 70.0961%\n",
      "total_backward_count 1155220 real_backward_count 96743   8.374%\n",
      "fc layer 1 self.abs_max_out: 8435.0\n",
      "lif layer 1 self.abs_max_v: 14713.0\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.952773/  2.044810, val:  68.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0840%\n",
      "layer   2  Sparsity: 70.6684%\n",
      "layer   3  Sparsity: 69.4217%\n",
      "total_backward_count 1165010 real_backward_count 97321   8.354%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.954779/  2.037436, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.88 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0982%\n",
      "layer   2  Sparsity: 70.7701%\n",
      "layer   3  Sparsity: 69.5748%\n",
      "total_backward_count 1174800 real_backward_count 97935   8.336%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.949760/  2.042316, val:  76.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0940%\n",
      "layer   2  Sparsity: 70.9142%\n",
      "layer   3  Sparsity: 70.1140%\n",
      "total_backward_count 1184590 real_backward_count 98543   8.319%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.956528/  2.041824, val:  77.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0810%\n",
      "layer   2  Sparsity: 71.0377%\n",
      "layer   3  Sparsity: 69.8745%\n",
      "total_backward_count 1194380 real_backward_count 99144   8.301%\n",
      "fc layer 1 self.abs_max_out: 8478.0\n",
      "lif layer 1 self.abs_max_v: 14830.0\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.953495/  2.036511, val:  67.50%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0916%\n",
      "layer   2  Sparsity: 70.8274%\n",
      "layer   3  Sparsity: 69.5411%\n",
      "total_backward_count 1204170 real_backward_count 99762   8.285%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.951821/  2.036525, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0847%\n",
      "layer   2  Sparsity: 70.8165%\n",
      "layer   3  Sparsity: 70.3667%\n",
      "total_backward_count 1213960 real_backward_count 100361   8.267%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.951287/  2.026251, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0883%\n",
      "layer   2  Sparsity: 70.8756%\n",
      "layer   3  Sparsity: 70.1874%\n",
      "total_backward_count 1223750 real_backward_count 100913   8.246%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.957691/  2.043004, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0870%\n",
      "layer   2  Sparsity: 70.7370%\n",
      "layer   3  Sparsity: 70.1864%\n",
      "total_backward_count 1233540 real_backward_count 101514   8.229%\n",
      "fc layer 1 self.abs_max_out: 8639.0\n",
      "lif layer 1 self.abs_max_v: 15158.5\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.963340/  2.044417, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0866%\n",
      "layer   2  Sparsity: 70.9053%\n",
      "layer   3  Sparsity: 70.5194%\n",
      "total_backward_count 1243330 real_backward_count 102058   8.208%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.959112/  2.047609, val:  75.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0877%\n",
      "layer   2  Sparsity: 70.9084%\n",
      "layer   3  Sparsity: 70.4613%\n",
      "total_backward_count 1253120 real_backward_count 102661   8.192%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.958123/  2.038150, val:  73.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0814%\n",
      "layer   2  Sparsity: 70.8143%\n",
      "layer   3  Sparsity: 70.6580%\n",
      "total_backward_count 1262910 real_backward_count 103254   8.176%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.951497/  2.033989, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0908%\n",
      "layer   2  Sparsity: 70.7402%\n",
      "layer   3  Sparsity: 70.7735%\n",
      "total_backward_count 1272700 real_backward_count 103844   8.159%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.952173/  2.035754, val:  76.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0875%\n",
      "layer   2  Sparsity: 70.7596%\n",
      "layer   3  Sparsity: 70.4730%\n",
      "total_backward_count 1282490 real_backward_count 104413   8.141%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.947181/  2.035467, val:  78.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0919%\n",
      "layer   2  Sparsity: 70.8628%\n",
      "layer   3  Sparsity: 70.5197%\n",
      "total_backward_count 1292280 real_backward_count 105009   8.126%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.945424/  2.028655, val:  66.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0938%\n",
      "layer   2  Sparsity: 70.7529%\n",
      "layer   3  Sparsity: 70.5726%\n",
      "total_backward_count 1302070 real_backward_count 105548   8.106%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.944493/  2.028581, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0871%\n",
      "layer   2  Sparsity: 70.7211%\n",
      "layer   3  Sparsity: 70.8317%\n",
      "total_backward_count 1311860 real_backward_count 106113   8.089%\n",
      "fc layer 2 self.abs_max_out: 5833.0\n",
      "fc layer 1 self.abs_max_out: 8774.0\n",
      "lif layer 1 self.abs_max_v: 15342.0\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.938925/  2.025477, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0802%\n",
      "layer   2  Sparsity: 70.7019%\n",
      "layer   3  Sparsity: 70.2406%\n",
      "total_backward_count 1321650 real_backward_count 106700   8.073%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.946343/  2.029965, val:  75.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0876%\n",
      "layer   2  Sparsity: 70.7992%\n",
      "layer   3  Sparsity: 70.4345%\n",
      "total_backward_count 1331440 real_backward_count 107283   8.058%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.948846/  2.034999, val:  74.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0867%\n",
      "layer   2  Sparsity: 70.6659%\n",
      "layer   3  Sparsity: 70.7040%\n",
      "total_backward_count 1341230 real_backward_count 107847   8.041%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.947543/  2.035593, val:  81.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0916%\n",
      "layer   2  Sparsity: 70.8471%\n",
      "layer   3  Sparsity: 70.7499%\n",
      "total_backward_count 1351020 real_backward_count 108428   8.026%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.947896/  2.027696, val:  72.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0902%\n",
      "layer   2  Sparsity: 70.8684%\n",
      "layer   3  Sparsity: 70.5239%\n",
      "total_backward_count 1360810 real_backward_count 108992   8.009%\n",
      "fc layer 1 self.abs_max_out: 8903.0\n",
      "lif layer 1 self.abs_max_v: 15611.0\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.945881/  2.029758, val:  79.58%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0867%\n",
      "layer   2  Sparsity: 70.8969%\n",
      "layer   3  Sparsity: 70.1592%\n",
      "total_backward_count 1370600 real_backward_count 109539   7.992%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.945788/  2.025921, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0826%\n",
      "layer   2  Sparsity: 70.8231%\n",
      "layer   3  Sparsity: 69.9209%\n",
      "total_backward_count 1380390 real_backward_count 110090   7.975%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.940873/  2.027587, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0827%\n",
      "layer   2  Sparsity: 70.7426%\n",
      "layer   3  Sparsity: 70.0384%\n",
      "total_backward_count 1390180 real_backward_count 110666   7.961%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.943157/  2.029370, val:  67.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0908%\n",
      "layer   2  Sparsity: 70.7048%\n",
      "layer   3  Sparsity: 69.9811%\n",
      "total_backward_count 1399970 real_backward_count 111249   7.947%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.944377/  2.027700, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0983%\n",
      "layer   2  Sparsity: 70.8201%\n",
      "layer   3  Sparsity: 70.3356%\n",
      "total_backward_count 1409760 real_backward_count 111799   7.930%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.940611/  2.023384, val:  78.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0804%\n",
      "layer   2  Sparsity: 70.7048%\n",
      "layer   3  Sparsity: 69.6269%\n",
      "total_backward_count 1419550 real_backward_count 112378   7.916%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.944794/  2.031259, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0889%\n",
      "layer   2  Sparsity: 70.7062%\n",
      "layer   3  Sparsity: 69.6637%\n",
      "total_backward_count 1429340 real_backward_count 112977   7.904%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.952833/  2.040150, val:  70.42%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0967%\n",
      "layer   2  Sparsity: 70.6883%\n",
      "layer   3  Sparsity: 70.0711%\n",
      "total_backward_count 1439130 real_backward_count 113521   7.888%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.949703/  2.036606, val:  68.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0962%\n",
      "layer   2  Sparsity: 70.5178%\n",
      "layer   3  Sparsity: 70.2614%\n",
      "total_backward_count 1448920 real_backward_count 114039   7.871%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.948827/  2.030589, val:  74.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0836%\n",
      "layer   2  Sparsity: 70.5952%\n",
      "layer   3  Sparsity: 69.7692%\n",
      "total_backward_count 1458710 real_backward_count 114594   7.856%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.941857/  2.021565, val:  68.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0851%\n",
      "layer   2  Sparsity: 70.7804%\n",
      "layer   3  Sparsity: 69.5651%\n",
      "total_backward_count 1468500 real_backward_count 115167   7.842%\n",
      "fc layer 1 self.abs_max_out: 8978.0\n",
      "lif layer 1 self.abs_max_v: 15785.5\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.936125/  2.025587, val:  71.25%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0827%\n",
      "layer   2  Sparsity: 70.7109%\n",
      "layer   3  Sparsity: 70.0594%\n",
      "total_backward_count 1478290 real_backward_count 115736   7.829%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.944638/  2.030590, val:  77.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0837%\n",
      "layer   2  Sparsity: 70.5997%\n",
      "layer   3  Sparsity: 70.3404%\n",
      "total_backward_count 1488080 real_backward_count 116279   7.814%\n",
      "fc layer 2 self.abs_max_out: 5880.0\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.948153/  2.027578, val:  77.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0865%\n",
      "layer   2  Sparsity: 70.6881%\n",
      "layer   3  Sparsity: 70.1532%\n",
      "total_backward_count 1497870 real_backward_count 116862   7.802%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.955088/  2.033150, val:  77.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0879%\n",
      "layer   2  Sparsity: 70.5691%\n",
      "layer   3  Sparsity: 70.8835%\n",
      "total_backward_count 1507660 real_backward_count 117422   7.788%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.950014/  2.036798, val:  75.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0859%\n",
      "layer   2  Sparsity: 70.6102%\n",
      "layer   3  Sparsity: 70.4505%\n",
      "total_backward_count 1517450 real_backward_count 118005   7.777%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.953387/  2.041014, val:  75.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0927%\n",
      "layer   2  Sparsity: 70.6159%\n",
      "layer   3  Sparsity: 70.2521%\n",
      "total_backward_count 1527240 real_backward_count 118566   7.763%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.959387/  2.036447, val:  72.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0820%\n",
      "layer   2  Sparsity: 70.6412%\n",
      "layer   3  Sparsity: 70.3162%\n",
      "total_backward_count 1537030 real_backward_count 119136   7.751%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.947197/  2.033321, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0880%\n",
      "layer   2  Sparsity: 70.6569%\n",
      "layer   3  Sparsity: 69.7679%\n",
      "total_backward_count 1546820 real_backward_count 119685   7.737%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.945210/  2.028743, val:  74.58%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0884%\n",
      "layer   2  Sparsity: 70.8756%\n",
      "layer   3  Sparsity: 70.3650%\n",
      "total_backward_count 1556610 real_backward_count 120224   7.723%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.950042/  2.035251, val:  70.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 95.0882%\n",
      "layer   2  Sparsity: 70.6763%\n",
      "layer   3  Sparsity: 70.5107%\n",
      "total_backward_count 1566400 real_backward_count 120757   7.709%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.949338/  2.030089, val:  73.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0873%\n",
      "layer   2  Sparsity: 70.7005%\n",
      "layer   3  Sparsity: 70.5482%\n",
      "total_backward_count 1576190 real_backward_count 121301   7.696%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.953745/  2.036294, val:  77.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0824%\n",
      "layer   2  Sparsity: 70.7781%\n",
      "layer   3  Sparsity: 70.6799%\n",
      "total_backward_count 1585980 real_backward_count 121884   7.685%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.956028/  2.042657, val:  69.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 95.0992%\n",
      "layer   2  Sparsity: 71.0258%\n",
      "layer   3  Sparsity: 70.2853%\n",
      "total_backward_count 1595770 real_backward_count 122426   7.672%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.952817/  2.035099, val:  76.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0871%\n",
      "layer   2  Sparsity: 71.0674%\n",
      "layer   3  Sparsity: 70.0447%\n",
      "total_backward_count 1605560 real_backward_count 122920   7.656%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.954358/  2.028472, val:  75.83%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0892%\n",
      "layer   2  Sparsity: 70.9186%\n",
      "layer   3  Sparsity: 69.8347%\n",
      "total_backward_count 1615350 real_backward_count 123475   7.644%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.945848/  2.027421, val:  73.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0853%\n",
      "layer   2  Sparsity: 70.7441%\n",
      "layer   3  Sparsity: 70.2520%\n",
      "total_backward_count 1625140 real_backward_count 124006   7.630%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.945598/  2.031011, val:  74.17%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0911%\n",
      "layer   2  Sparsity: 70.7796%\n",
      "layer   3  Sparsity: 70.0789%\n",
      "total_backward_count 1634930 real_backward_count 124518   7.616%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.950382/  2.034351, val:  73.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0910%\n",
      "layer   2  Sparsity: 70.9394%\n",
      "layer   3  Sparsity: 69.9623%\n",
      "total_backward_count 1644720 real_backward_count 125059   7.604%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.951110/  2.038417, val:  79.58%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0832%\n",
      "layer   2  Sparsity: 71.0402%\n",
      "layer   3  Sparsity: 69.4715%\n",
      "total_backward_count 1654510 real_backward_count 125596   7.591%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.955971/  2.028621, val:  71.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0947%\n",
      "layer   2  Sparsity: 70.9750%\n",
      "layer   3  Sparsity: 69.6921%\n",
      "total_backward_count 1664300 real_backward_count 126159   7.580%\n",
      "lif layer 2 self.abs_max_v: 7148.5\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.947561/  2.033400, val:  69.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0897%\n",
      "layer   2  Sparsity: 70.9470%\n",
      "layer   3  Sparsity: 69.8523%\n",
      "total_backward_count 1674090 real_backward_count 126687   7.568%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.952049/  2.037560, val:  60.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0900%\n",
      "layer   2  Sparsity: 70.9509%\n",
      "layer   3  Sparsity: 69.5416%\n",
      "total_backward_count 1683880 real_backward_count 127237   7.556%\n",
      "fc layer 2 self.abs_max_out: 5948.0\n",
      "fc layer 2 self.abs_max_out: 5964.0\n",
      "fc layer 1 self.abs_max_out: 9028.0\n",
      "lif layer 1 self.abs_max_v: 15786.5\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.955133/  2.036662, val:  70.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0905%\n",
      "layer   2  Sparsity: 70.7449%\n",
      "layer   3  Sparsity: 69.9900%\n",
      "total_backward_count 1693670 real_backward_count 127800   7.546%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.954034/  2.032572, val:  74.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0810%\n",
      "layer   2  Sparsity: 70.5842%\n",
      "layer   3  Sparsity: 69.3878%\n",
      "total_backward_count 1703460 real_backward_count 128339   7.534%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.952871/  2.036824, val:  77.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0893%\n",
      "layer   2  Sparsity: 70.4902%\n",
      "layer   3  Sparsity: 69.4736%\n",
      "total_backward_count 1713250 real_backward_count 128887   7.523%\n",
      "fc layer 2 self.abs_max_out: 6006.0\n",
      "fc layer 2 self.abs_max_out: 6075.0\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.954518/  2.032382, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0797%\n",
      "layer   2  Sparsity: 70.5752%\n",
      "layer   3  Sparsity: 70.0007%\n",
      "total_backward_count 1723040 real_backward_count 129418   7.511%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.956000/  2.034518, val:  77.92%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0988%\n",
      "layer   2  Sparsity: 70.4660%\n",
      "layer   3  Sparsity: 69.9837%\n",
      "total_backward_count 1732830 real_backward_count 129987   7.501%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.958105/  2.037988, val:  70.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0888%\n",
      "layer   2  Sparsity: 70.5840%\n",
      "layer   3  Sparsity: 70.5505%\n",
      "total_backward_count 1742620 real_backward_count 130531   7.491%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.958393/  2.040858, val:  71.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.43 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 95.0777%\n",
      "layer   2  Sparsity: 70.5800%\n",
      "layer   3  Sparsity: 70.2005%\n",
      "total_backward_count 1752410 real_backward_count 131045   7.478%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.951624/  2.035778, val:  80.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0918%\n",
      "layer   2  Sparsity: 70.6554%\n",
      "layer   3  Sparsity: 69.6316%\n",
      "total_backward_count 1762200 real_backward_count 131592   7.467%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.954720/  2.038111, val:  80.42%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0860%\n",
      "layer   2  Sparsity: 70.6062%\n",
      "layer   3  Sparsity: 69.9305%\n",
      "total_backward_count 1771990 real_backward_count 132160   7.458%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.955933/  2.031621, val:  75.83%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0810%\n",
      "layer   2  Sparsity: 70.3571%\n",
      "layer   3  Sparsity: 69.8845%\n",
      "total_backward_count 1781780 real_backward_count 132697   7.447%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.954726/  2.032093, val:  84.58%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.90 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0888%\n",
      "layer   2  Sparsity: 70.2465%\n",
      "layer   3  Sparsity: 70.1259%\n",
      "total_backward_count 1791570 real_backward_count 133202   7.435%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.953326/  2.034855, val:  68.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0869%\n",
      "layer   2  Sparsity: 70.1579%\n",
      "layer   3  Sparsity: 69.6599%\n",
      "total_backward_count 1801360 real_backward_count 133734   7.424%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.956076/  2.043427, val:  77.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0835%\n",
      "layer   2  Sparsity: 70.3208%\n",
      "layer   3  Sparsity: 70.5003%\n",
      "total_backward_count 1811150 real_backward_count 134254   7.413%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.960665/  2.034664, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 95.0880%\n",
      "layer   2  Sparsity: 70.5427%\n",
      "layer   3  Sparsity: 70.5774%\n",
      "total_backward_count 1820940 real_backward_count 134793   7.402%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.954631/  2.038677, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.20 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 95.0918%\n",
      "layer   2  Sparsity: 70.5548%\n",
      "layer   3  Sparsity: 70.2847%\n",
      "total_backward_count 1830730 real_backward_count 135310   7.391%\n",
      "fc layer 1 self.abs_max_out: 9038.0\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.958141/  2.033744, val:  76.25%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0838%\n",
      "layer   2  Sparsity: 70.3802%\n",
      "layer   3  Sparsity: 69.7356%\n",
      "total_backward_count 1840520 real_backward_count 135854   7.381%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.952989/  2.032514, val:  72.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0819%\n",
      "layer   2  Sparsity: 70.4644%\n",
      "layer   3  Sparsity: 69.7941%\n",
      "total_backward_count 1850310 real_backward_count 136417   7.373%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.951731/  2.027200, val:  80.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0813%\n",
      "layer   2  Sparsity: 70.4555%\n",
      "layer   3  Sparsity: 69.8003%\n",
      "total_backward_count 1860100 real_backward_count 136954   7.363%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.950334/  2.034106, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.40 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 95.0924%\n",
      "layer   2  Sparsity: 70.1475%\n",
      "layer   3  Sparsity: 69.4961%\n",
      "total_backward_count 1869890 real_backward_count 137472   7.352%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.958432/  2.033267, val:  65.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0856%\n",
      "layer   2  Sparsity: 70.0272%\n",
      "layer   3  Sparsity: 69.3967%\n",
      "total_backward_count 1879680 real_backward_count 138049   7.344%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.951970/  2.033459, val:  75.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0818%\n",
      "layer   2  Sparsity: 70.1950%\n",
      "layer   3  Sparsity: 69.8427%\n",
      "total_backward_count 1889470 real_backward_count 138609   7.336%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.948785/  2.030285, val:  76.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0943%\n",
      "layer   2  Sparsity: 70.3836%\n",
      "layer   3  Sparsity: 70.2336%\n",
      "total_backward_count 1899260 real_backward_count 139187   7.328%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.944913/  2.020447, val:  70.83%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 95.0957%\n",
      "layer   2  Sparsity: 70.1467%\n",
      "layer   3  Sparsity: 70.0222%\n",
      "total_backward_count 1909050 real_backward_count 139728   7.319%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.941905/  2.024608, val:  71.67%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0944%\n",
      "layer   2  Sparsity: 70.1575%\n",
      "layer   3  Sparsity: 70.0543%\n",
      "total_backward_count 1918840 real_backward_count 140256   7.309%\n",
      "fc layer 2 self.abs_max_out: 6091.0\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.950995/  2.032358, val:  83.33%, val_best:  84.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0823%\n",
      "layer   2  Sparsity: 70.2466%\n",
      "layer   3  Sparsity: 70.1925%\n",
      "total_backward_count 1928630 real_backward_count 140813   7.301%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.951194/  2.029295, val:  73.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0889%\n",
      "layer   2  Sparsity: 70.3217%\n",
      "layer   3  Sparsity: 69.4109%\n",
      "total_backward_count 1938420 real_backward_count 141371   7.293%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.952584/  2.032729, val:  82.50%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 95.0869%\n",
      "layer   2  Sparsity: 70.2985%\n",
      "layer   3  Sparsity: 69.1700%\n",
      "total_backward_count 1948210 real_backward_count 141883   7.283%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.952141/  2.036885, val:  70.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 95.0881%\n",
      "layer   2  Sparsity: 70.3543%\n",
      "layer   3  Sparsity: 69.8379%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ac61a689f44c818da88780f4af0ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñá‚ñà‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñá‚ñà‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.95214</td></tr><tr><td>val_acc_best</td><td>0.84583</td></tr><tr><td>val_acc_now</td><td>0.7</td></tr><tr><td>val_loss</td><td>2.03689</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-sweep-54</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mk4a21qf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mk4a21qf</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251114_211434-mk4a21qf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rccfvxb2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_014008-rccfvxb2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rccfvxb2' target=\"_blank\">good-sweep-59</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rccfvxb2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rccfvxb2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251115_014017_273', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 911.0\n",
      "lif layer 1 self.abs_max_v: 911.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1338.0\n",
      "lif layer 2 self.abs_max_v: 1338.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 653.0\n",
      "lif layer 1 self.abs_max_v: 1081.0\n",
      "fc layer 2 self.abs_max_out: 1425.0\n",
      "lif layer 2 self.abs_max_v: 1954.0\n",
      "fc layer 1 self.abs_max_out: 965.0\n",
      "lif layer 1 self.abs_max_v: 1475.5\n",
      "lif layer 2 self.abs_max_v: 2140.0\n",
      "fc layer 2 self.abs_max_out: 2078.0\n",
      "lif layer 2 self.abs_max_v: 2635.0\n",
      "lif layer 1 self.abs_max_v: 1596.0\n",
      "fc layer 1 self.abs_max_out: 1030.0\n",
      "lif layer 2 self.abs_max_v: 2771.0\n",
      "fc layer 3 self.abs_max_out: 664.0\n",
      "fc layer 1 self.abs_max_out: 1238.0\n",
      "fc layer 1 self.abs_max_out: 1329.0\n",
      "lif layer 1 self.abs_max_v: 1744.0\n",
      "lif layer 2 self.abs_max_v: 2784.0\n",
      "fc layer 1 self.abs_max_out: 1470.0\n",
      "lif layer 1 self.abs_max_v: 2022.0\n",
      "fc layer 3 self.abs_max_out: 836.0\n",
      "fc layer 1 self.abs_max_out: 1630.0\n",
      "fc layer 1 self.abs_max_out: 1632.0\n",
      "fc layer 2 self.abs_max_out: 2133.0\n",
      "lif layer 2 self.abs_max_v: 2785.0\n",
      "fc layer 1 self.abs_max_out: 1825.0\n",
      "fc layer 1 self.abs_max_out: 2163.0\n",
      "lif layer 1 self.abs_max_v: 2625.5\n",
      "lif layer 1 self.abs_max_v: 2742.0\n",
      "lif layer 2 self.abs_max_v: 2786.0\n",
      "lif layer 2 self.abs_max_v: 2983.0\n",
      "lif layer 2 self.abs_max_v: 3154.0\n",
      "fc layer 1 self.abs_max_out: 2181.0\n",
      "lif layer 2 self.abs_max_v: 3201.0\n",
      "fc layer 3 self.abs_max_out: 847.0\n",
      "lif layer 1 self.abs_max_v: 2875.0\n",
      "lif layer 2 self.abs_max_v: 3411.0\n",
      "fc layer 2 self.abs_max_out: 2196.0\n",
      "fc layer 3 self.abs_max_out: 878.0\n",
      "lif layer 1 self.abs_max_v: 2914.5\n",
      "fc layer 3 self.abs_max_out: 959.0\n",
      "fc layer 2 self.abs_max_out: 2411.0\n",
      "lif layer 1 self.abs_max_v: 3151.0\n",
      "lif layer 2 self.abs_max_v: 3636.5\n",
      "fc layer 2 self.abs_max_out: 2430.0\n",
      "lif layer 2 self.abs_max_v: 3646.5\n",
      "fc layer 1 self.abs_max_out: 2263.0\n",
      "fc layer 1 self.abs_max_out: 2468.0\n",
      "lif layer 2 self.abs_max_v: 3676.0\n",
      "lif layer 1 self.abs_max_v: 3202.0\n",
      "lif layer 1 self.abs_max_v: 3256.0\n",
      "lif layer 2 self.abs_max_v: 3747.5\n",
      "lif layer 1 self.abs_max_v: 3578.5\n",
      "fc layer 1 self.abs_max_out: 2721.0\n",
      "fc layer 1 self.abs_max_out: 2833.0\n",
      "fc layer 1 self.abs_max_out: 2885.0\n",
      "lif layer 1 self.abs_max_v: 3672.0\n",
      "fc layer 2 self.abs_max_out: 2587.0\n",
      "fc layer 3 self.abs_max_out: 988.0\n",
      "lif layer 2 self.abs_max_v: 4102.5\n",
      "fc layer 2 self.abs_max_out: 2603.0\n",
      "lif layer 2 self.abs_max_v: 4353.5\n",
      "fc layer 2 self.abs_max_out: 2830.0\n",
      "lif layer 2 self.abs_max_v: 5007.0\n",
      "fc layer 2 self.abs_max_out: 3015.0\n",
      "lif layer 2 self.abs_max_v: 5461.0\n",
      "fc layer 1 self.abs_max_out: 2965.0\n",
      "fc layer 1 self.abs_max_out: 3170.0\n",
      "lif layer 1 self.abs_max_v: 3765.0\n",
      "fc layer 1 self.abs_max_out: 3303.0\n",
      "lif layer 1 self.abs_max_v: 3917.5\n",
      "lif layer 1 self.abs_max_v: 4324.5\n",
      "lif layer 1 self.abs_max_v: 4893.5\n",
      "fc layer 1 self.abs_max_out: 3440.0\n",
      "fc layer 1 self.abs_max_out: 3722.0\n",
      "fc layer 3 self.abs_max_out: 1024.0\n",
      "lif layer 1 self.abs_max_v: 5261.5\n",
      "lif layer 1 self.abs_max_v: 5485.0\n",
      "fc layer 1 self.abs_max_out: 3833.0\n",
      "lif layer 1 self.abs_max_v: 5564.0\n",
      "fc layer 1 self.abs_max_out: 3861.0\n",
      "fc layer 1 self.abs_max_out: 3890.0\n",
      "fc layer 3 self.abs_max_out: 1040.0\n",
      "fc layer 3 self.abs_max_out: 1117.0\n",
      "lif layer 1 self.abs_max_v: 5763.5\n",
      "lif layer 1 self.abs_max_v: 5834.5\n",
      "fc layer 2 self.abs_max_out: 3123.0\n",
      "fc layer 1 self.abs_max_out: 4296.0\n",
      "lif layer 1 self.abs_max_v: 6422.0\n",
      "lif layer 1 self.abs_max_v: 6447.0\n",
      "fc layer 2 self.abs_max_out: 3210.0\n",
      "fc layer 3 self.abs_max_out: 1125.0\n",
      "fc layer 3 self.abs_max_out: 1235.0\n",
      "lif layer 1 self.abs_max_v: 7067.5\n",
      "lif layer 1 self.abs_max_v: 7351.5\n",
      "fc layer 2 self.abs_max_out: 3474.0\n",
      "fc layer 1 self.abs_max_out: 4480.0\n",
      "lif layer 1 self.abs_max_v: 7464.5\n",
      "lif layer 2 self.abs_max_v: 5464.0\n",
      "lif layer 2 self.abs_max_v: 5740.0\n",
      "lif layer 2 self.abs_max_v: 5883.0\n",
      "fc layer 1 self.abs_max_out: 4660.0\n",
      "lif layer 1 self.abs_max_v: 7681.5\n",
      "fc layer 2 self.abs_max_out: 3596.0\n",
      "lif layer 2 self.abs_max_v: 5971.0\n",
      "fc layer 2 self.abs_max_out: 3602.0\n",
      "fc layer 1 self.abs_max_out: 4739.0\n",
      "lif layer 1 self.abs_max_v: 7702.0\n",
      "lif layer 1 self.abs_max_v: 8132.0\n",
      "lif layer 1 self.abs_max_v: 8267.0\n",
      "lif layer 1 self.abs_max_v: 8460.5\n",
      "lif layer 1 self.abs_max_v: 8723.5\n",
      "fc layer 1 self.abs_max_out: 4849.0\n",
      "fc layer 2 self.abs_max_out: 3621.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  1.945657/  2.055798, val:  41.67%, val_best:  41.67%, tr:  83.04%, tr_best:  83.04%, epoch time: 79.80 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3472%\n",
      "layer   2  Sparsity: 70.2098%\n",
      "layer   3  Sparsity: 68.4386%\n",
      "total_backward_count 9790 real_backward_count 3303  33.739%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 5028.0\n",
      "fc layer 2 self.abs_max_out: 3661.0\n",
      "lif layer 2 self.abs_max_v: 6031.5\n",
      "lif layer 2 self.abs_max_v: 6137.0\n",
      "lif layer 1 self.abs_max_v: 8884.0\n",
      "lif layer 1 self.abs_max_v: 9063.0\n",
      "lif layer 1 self.abs_max_v: 9338.5\n",
      "lif layer 1 self.abs_max_v: 9549.5\n",
      "fc layer 1 self.abs_max_out: 5315.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.902068/  2.037574, val:  46.25%, val_best:  46.25%, tr:  93.87%, tr_best:  93.87%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3554%\n",
      "layer   2  Sparsity: 72.2223%\n",
      "layer   3  Sparsity: 68.6144%\n",
      "total_backward_count 19580 real_backward_count 5505  28.115%\n",
      "fc layer 1 self.abs_max_out: 5527.0\n",
      "lif layer 1 self.abs_max_v: 9864.5\n",
      "fc layer 2 self.abs_max_out: 3813.0\n",
      "fc layer 1 self.abs_max_out: 6295.0\n",
      "lif layer 1 self.abs_max_v: 10564.5\n",
      "fc layer 1 self.abs_max_out: 6330.0\n",
      "lif layer 1 self.abs_max_v: 11612.5\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.894934/  2.018839, val:  46.67%, val_best:  46.67%, tr:  95.81%, tr_best:  95.81%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3537%\n",
      "layer   2  Sparsity: 72.6689%\n",
      "layer   3  Sparsity: 67.4739%\n",
      "total_backward_count 29370 real_backward_count 7403  25.206%\n",
      "fc layer 2 self.abs_max_out: 4148.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  1.892371/  2.030530, val:  51.25%, val_best:  51.25%, tr:  97.45%, tr_best:  97.45%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3628%\n",
      "layer   2  Sparsity: 72.5976%\n",
      "layer   3  Sparsity: 66.1523%\n",
      "total_backward_count 39160 real_backward_count 9069  23.159%\n",
      "fc layer 2 self.abs_max_out: 4186.0\n",
      "fc layer 1 self.abs_max_out: 6388.0\n",
      "fc layer 1 self.abs_max_out: 6396.0\n",
      "lif layer 1 self.abs_max_v: 11700.5\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  1.886430/  2.025566, val:  55.42%, val_best:  55.42%, tr:  99.28%, tr_best:  99.28%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3501%\n",
      "layer   2  Sparsity: 70.5188%\n",
      "layer   3  Sparsity: 65.7513%\n",
      "total_backward_count 48950 real_backward_count 10561  21.575%\n",
      "lif layer 2 self.abs_max_v: 6240.5\n",
      "fc layer 1 self.abs_max_out: 6634.0\n",
      "lif layer 1 self.abs_max_v: 12111.5\n",
      "lif layer 1 self.abs_max_v: 12118.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  1.877627/  2.023505, val:  53.75%, val_best:  55.42%, tr:  98.26%, tr_best:  99.28%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3611%\n",
      "layer   2  Sparsity: 71.3485%\n",
      "layer   3  Sparsity: 66.2009%\n",
      "total_backward_count 58740 real_backward_count 12045  20.506%\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  1.884924/  2.015868, val:  55.00%, val_best:  55.42%, tr:  99.18%, tr_best:  99.28%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3579%\n",
      "layer   2  Sparsity: 71.1062%\n",
      "layer   3  Sparsity: 64.4993%\n",
      "total_backward_count 68530 real_backward_count 13444  19.618%\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  1.883912/  2.001815, val:  50.83%, val_best:  55.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3642%\n",
      "layer   2  Sparsity: 71.6609%\n",
      "layer   3  Sparsity: 63.8411%\n",
      "total_backward_count 78320 real_backward_count 14647  18.701%\n",
      "lif layer 2 self.abs_max_v: 6286.5\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  1.890489/  2.001670, val:  68.75%, val_best:  68.75%, tr:  99.28%, tr_best:  99.80%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3418%\n",
      "layer   2  Sparsity: 71.4071%\n",
      "layer   3  Sparsity: 64.0480%\n",
      "total_backward_count 88110 real_backward_count 15941  18.092%\n",
      "lif layer 2 self.abs_max_v: 6340.5\n",
      "fc layer 1 self.abs_max_out: 6704.0\n",
      "lif layer 2 self.abs_max_v: 6527.5\n",
      "fc layer 1 self.abs_max_out: 6970.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  1.883288/  2.027398, val:  51.25%, val_best:  68.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 80.22 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3557%\n",
      "layer   2  Sparsity: 70.6243%\n",
      "layer   3  Sparsity: 63.9349%\n",
      "total_backward_count 97900 real_backward_count 17196  17.565%\n",
      "fc layer 1 self.abs_max_out: 7129.0\n",
      "lif layer 1 self.abs_max_v: 12783.0\n",
      "lif layer 1 self.abs_max_v: 12822.5\n",
      "lif layer 1 self.abs_max_v: 12997.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  1.891513/  2.027489, val:  50.00%, val_best:  68.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3549%\n",
      "layer   2  Sparsity: 70.9943%\n",
      "layer   3  Sparsity: 65.0271%\n",
      "total_backward_count 107690 real_backward_count 18377  17.065%\n",
      "fc layer 1 self.abs_max_out: 7216.0\n",
      "lif layer 1 self.abs_max_v: 13147.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  1.896208/  2.016260, val:  59.17%, val_best:  68.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3623%\n",
      "layer   2  Sparsity: 71.0582%\n",
      "layer   3  Sparsity: 65.8297%\n",
      "total_backward_count 117480 real_backward_count 19562  16.651%\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  1.888355/  2.018451, val:  59.58%, val_best:  68.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 80.43 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3611%\n",
      "layer   2  Sparsity: 71.1094%\n",
      "layer   3  Sparsity: 66.1619%\n",
      "total_backward_count 127270 real_backward_count 20621  16.203%\n",
      "fc layer 1 self.abs_max_out: 7319.0\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  1.896369/  2.027692, val:  55.00%, val_best:  68.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 80.24 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3526%\n",
      "layer   2  Sparsity: 70.8335%\n",
      "layer   3  Sparsity: 64.7359%\n",
      "total_backward_count 137060 real_backward_count 21703  15.835%\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  1.897812/  2.004843, val:  57.92%, val_best:  68.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 80.18 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3577%\n",
      "layer   2  Sparsity: 70.8090%\n",
      "layer   3  Sparsity: 65.1981%\n",
      "total_backward_count 146850 real_backward_count 22730  15.478%\n",
      "lif layer 2 self.abs_max_v: 6532.0\n",
      "lif layer 2 self.abs_max_v: 6976.0\n",
      "fc layer 1 self.abs_max_out: 7451.0\n",
      "fc layer 1 self.abs_max_out: 7524.0\n",
      "lif layer 1 self.abs_max_v: 13535.0\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  1.891042/  2.020859, val:  59.17%, val_best:  68.75%, tr:  99.49%, tr_best:  99.80%, epoch time: 80.09 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3598%\n",
      "layer   2  Sparsity: 70.3700%\n",
      "layer   3  Sparsity: 64.7301%\n",
      "total_backward_count 156640 real_backward_count 23829  15.213%\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  1.897243/  2.020717, val:  60.42%, val_best:  68.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3636%\n",
      "layer   2  Sparsity: 70.4725%\n",
      "layer   3  Sparsity: 65.9872%\n",
      "total_backward_count 166430 real_backward_count 24894  14.958%\n",
      "fc layer 1 self.abs_max_out: 7586.0\n",
      "lif layer 1 self.abs_max_v: 13550.0\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  1.904986/  2.025178, val:  57.92%, val_best:  68.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3591%\n",
      "layer   2  Sparsity: 70.3585%\n",
      "layer   3  Sparsity: 66.4494%\n",
      "total_backward_count 176220 real_backward_count 25918  14.708%\n",
      "fc layer 1 self.abs_max_out: 7654.0\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  1.902881/  2.015026, val:  61.25%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3608%\n",
      "layer   2  Sparsity: 70.5724%\n",
      "layer   3  Sparsity: 65.3967%\n",
      "total_backward_count 186010 real_backward_count 26912  14.468%\n",
      "lif layer 1 self.abs_max_v: 13607.0\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  1.903697/  2.034313, val:  49.17%, val_best:  68.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 80.35 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3567%\n",
      "layer   2  Sparsity: 70.3175%\n",
      "layer   3  Sparsity: 66.6295%\n",
      "total_backward_count 195800 real_backward_count 27924  14.261%\n",
      "fc layer 1 self.abs_max_out: 7724.0\n",
      "fc layer 1 self.abs_max_out: 7882.0\n",
      "fc layer 1 self.abs_max_out: 8173.0\n",
      "lif layer 1 self.abs_max_v: 13686.0\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  1.892191/  2.026606, val:  50.00%, val_best:  68.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3566%\n",
      "layer   2  Sparsity: 69.3418%\n",
      "layer   3  Sparsity: 65.7533%\n",
      "total_backward_count 205590 real_backward_count 28880  14.047%\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  1.893318/  2.024136, val:  69.58%, val_best:  69.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3509%\n",
      "layer   2  Sparsity: 70.0219%\n",
      "layer   3  Sparsity: 67.0155%\n",
      "total_backward_count 215380 real_backward_count 29867  13.867%\n",
      "lif layer 1 self.abs_max_v: 13855.0\n",
      "lif layer 1 self.abs_max_v: 13886.5\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  1.905091/  2.022518, val:  60.83%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.54 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3626%\n",
      "layer   2  Sparsity: 70.0426%\n",
      "layer   3  Sparsity: 67.3135%\n",
      "total_backward_count 225170 real_backward_count 30845  13.699%\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  1.908485/  2.020197, val:  66.25%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3636%\n",
      "layer   2  Sparsity: 69.5590%\n",
      "layer   3  Sparsity: 67.3770%\n",
      "total_backward_count 234960 real_backward_count 31776  13.524%\n",
      "fc layer 2 self.abs_max_out: 4215.0\n",
      "fc layer 2 self.abs_max_out: 4329.0\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  1.905367/  2.023264, val:  60.83%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3564%\n",
      "layer   2  Sparsity: 69.7450%\n",
      "layer   3  Sparsity: 67.2412%\n",
      "total_backward_count 244750 real_backward_count 32722  13.370%\n",
      "lif layer 1 self.abs_max_v: 14029.0\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  1.911095/  2.025013, val:  65.00%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3501%\n",
      "layer   2  Sparsity: 69.7260%\n",
      "layer   3  Sparsity: 67.5218%\n",
      "total_backward_count 254540 real_backward_count 33710  13.243%\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  1.911892/  2.025287, val:  63.75%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3582%\n",
      "layer   2  Sparsity: 70.2558%\n",
      "layer   3  Sparsity: 67.4872%\n",
      "total_backward_count 264330 real_backward_count 34595  13.088%\n",
      "lif layer 1 self.abs_max_v: 14042.0\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  1.919574/  2.027122, val:  62.50%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3494%\n",
      "layer   2  Sparsity: 69.8453%\n",
      "layer   3  Sparsity: 68.4909%\n",
      "total_backward_count 274120 real_backward_count 35524  12.959%\n",
      "lif layer 1 self.abs_max_v: 14396.5\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  1.912465/  2.025137, val:  64.58%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3481%\n",
      "layer   2  Sparsity: 69.3832%\n",
      "layer   3  Sparsity: 67.8137%\n",
      "total_backward_count 283910 real_backward_count 36387  12.816%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  1.913814/  2.014034, val:  64.58%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3575%\n",
      "layer   2  Sparsity: 69.6890%\n",
      "layer   3  Sparsity: 67.5247%\n",
      "total_backward_count 293700 real_backward_count 37240  12.680%\n",
      "fc layer 1 self.abs_max_out: 8413.0\n",
      "lif layer 1 self.abs_max_v: 14448.0\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  1.907351/  2.019051, val:  65.42%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3547%\n",
      "layer   2  Sparsity: 69.7598%\n",
      "layer   3  Sparsity: 67.3489%\n",
      "total_backward_count 303490 real_backward_count 38119  12.560%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  1.910093/  2.017732, val:  62.50%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3556%\n",
      "layer   2  Sparsity: 69.8198%\n",
      "layer   3  Sparsity: 68.0539%\n",
      "total_backward_count 313280 real_backward_count 39008  12.451%\n",
      "fc layer 1 self.abs_max_out: 9008.0\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  1.904241/  2.032277, val:  60.83%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3574%\n",
      "layer   2  Sparsity: 69.8789%\n",
      "layer   3  Sparsity: 68.1108%\n",
      "total_backward_count 323070 real_backward_count 39869  12.341%\n",
      "lif layer 1 self.abs_max_v: 14470.5\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  1.912131/  2.024620, val:  67.08%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3481%\n",
      "layer   2  Sparsity: 70.2878%\n",
      "layer   3  Sparsity: 68.5131%\n",
      "total_backward_count 332860 real_backward_count 40699  12.227%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  1.913859/  2.024747, val:  60.83%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3412%\n",
      "layer   2  Sparsity: 69.9557%\n",
      "layer   3  Sparsity: 68.1690%\n",
      "total_backward_count 342650 real_backward_count 41540  12.123%\n",
      "lif layer 2 self.abs_max_v: 7053.0\n",
      "lif layer 1 self.abs_max_v: 14566.5\n",
      "lif layer 1 self.abs_max_v: 14688.5\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  1.917349/  2.031222, val:  63.75%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3559%\n",
      "layer   2  Sparsity: 70.0482%\n",
      "layer   3  Sparsity: 68.5454%\n",
      "total_backward_count 352440 real_backward_count 42379  12.024%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  1.911716/  2.012484, val:  75.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3507%\n",
      "layer   2  Sparsity: 70.1326%\n",
      "layer   3  Sparsity: 68.9151%\n",
      "total_backward_count 362230 real_backward_count 43179  11.920%\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  1.918742/  2.030625, val:  60.83%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3515%\n",
      "layer   2  Sparsity: 69.7064%\n",
      "layer   3  Sparsity: 69.3694%\n",
      "total_backward_count 372020 real_backward_count 43954  11.815%\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  1.920000/  2.025235, val:  57.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3575%\n",
      "layer   2  Sparsity: 69.9519%\n",
      "layer   3  Sparsity: 69.3237%\n",
      "total_backward_count 381810 real_backward_count 44791  11.731%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  1.911554/  2.020220, val:  67.08%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3611%\n",
      "layer   2  Sparsity: 69.6908%\n",
      "layer   3  Sparsity: 68.7942%\n",
      "total_backward_count 391600 real_backward_count 45571  11.637%\n",
      "fc layer 1 self.abs_max_out: 9083.0\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  1.905734/  2.013739, val:  67.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3619%\n",
      "layer   2  Sparsity: 69.7375%\n",
      "layer   3  Sparsity: 68.7102%\n",
      "total_backward_count 401390 real_backward_count 46366  11.551%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  1.904674/  2.016774, val:  71.67%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3545%\n",
      "layer   2  Sparsity: 69.1933%\n",
      "layer   3  Sparsity: 67.2593%\n",
      "total_backward_count 411180 real_backward_count 47123  11.460%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  1.906377/  2.018022, val:  70.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3631%\n",
      "layer   2  Sparsity: 69.9968%\n",
      "layer   3  Sparsity: 68.6773%\n",
      "total_backward_count 420970 real_backward_count 47916  11.382%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  1.911879/  2.028049, val:  61.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3491%\n",
      "layer   2  Sparsity: 69.2876%\n",
      "layer   3  Sparsity: 69.1032%\n",
      "total_backward_count 430760 real_backward_count 48714  11.309%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  1.918358/  2.017225, val:  71.67%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.37 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3557%\n",
      "layer   2  Sparsity: 69.7527%\n",
      "layer   3  Sparsity: 69.1644%\n",
      "total_backward_count 440550 real_backward_count 49523  11.241%\n",
      "lif layer 2 self.abs_max_v: 7075.5\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  1.931506/  2.030896, val:  60.83%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3529%\n",
      "layer   2  Sparsity: 70.1650%\n",
      "layer   3  Sparsity: 69.8611%\n",
      "total_backward_count 450340 real_backward_count 50291  11.167%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  1.928216/  2.029339, val:  68.75%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.13 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3452%\n",
      "layer   2  Sparsity: 69.4621%\n",
      "layer   3  Sparsity: 69.5271%\n",
      "total_backward_count 460130 real_backward_count 51079  11.101%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  1.924670/  2.031429, val:  66.25%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3596%\n",
      "layer   2  Sparsity: 69.1459%\n",
      "layer   3  Sparsity: 69.4017%\n",
      "total_backward_count 469920 real_backward_count 51835  11.031%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  1.927446/  2.025543, val:  70.42%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3511%\n",
      "layer   2  Sparsity: 69.1442%\n",
      "layer   3  Sparsity: 68.7308%\n",
      "total_backward_count 479710 real_backward_count 52583  10.961%\n",
      "lif layer 1 self.abs_max_v: 15563.5\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  1.919339/  2.021689, val:  70.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.77 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.3556%\n",
      "layer   2  Sparsity: 69.2600%\n",
      "layer   3  Sparsity: 69.3480%\n",
      "total_backward_count 489500 real_backward_count 53318  10.892%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  1.928210/  2.032860, val:  65.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3562%\n",
      "layer   2  Sparsity: 69.5147%\n",
      "layer   3  Sparsity: 69.3498%\n",
      "total_backward_count 499290 real_backward_count 54067  10.829%\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  1.920649/  2.033691, val:  65.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3557%\n",
      "layer   2  Sparsity: 69.1615%\n",
      "layer   3  Sparsity: 68.8558%\n",
      "total_backward_count 509080 real_backward_count 54844  10.773%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  1.919574/  2.020633, val:  66.67%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3599%\n",
      "layer   2  Sparsity: 69.1864%\n",
      "layer   3  Sparsity: 68.9361%\n",
      "total_backward_count 518870 real_backward_count 55601  10.716%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  1.915342/  2.007999, val:  71.25%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3522%\n",
      "layer   2  Sparsity: 69.0637%\n",
      "layer   3  Sparsity: 68.7195%\n",
      "total_backward_count 528660 real_backward_count 56330  10.655%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  1.915896/  2.016974, val:  65.83%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3566%\n",
      "layer   2  Sparsity: 68.8029%\n",
      "layer   3  Sparsity: 69.4649%\n",
      "total_backward_count 538450 real_backward_count 57053  10.596%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.923380/  2.032932, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3548%\n",
      "layer   2  Sparsity: 68.7630%\n",
      "layer   3  Sparsity: 69.0968%\n",
      "total_backward_count 548240 real_backward_count 57775  10.538%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.924744/  2.035511, val:  65.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3539%\n",
      "layer   2  Sparsity: 68.7333%\n",
      "layer   3  Sparsity: 68.2551%\n",
      "total_backward_count 558030 real_backward_count 58439  10.472%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.923814/  2.033477, val:  75.42%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3549%\n",
      "layer   2  Sparsity: 68.8434%\n",
      "layer   3  Sparsity: 68.8524%\n",
      "total_backward_count 567820 real_backward_count 59170  10.421%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.915340/  2.013150, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3530%\n",
      "layer   2  Sparsity: 68.8349%\n",
      "layer   3  Sparsity: 68.4906%\n",
      "total_backward_count 577610 real_backward_count 59842  10.360%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.909640/  2.011564, val:  69.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3536%\n",
      "layer   2  Sparsity: 69.1324%\n",
      "layer   3  Sparsity: 68.4100%\n",
      "total_backward_count 587400 real_backward_count 60528  10.304%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.905637/  2.008442, val:  61.25%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3507%\n",
      "layer   2  Sparsity: 69.3553%\n",
      "layer   3  Sparsity: 68.9553%\n",
      "total_backward_count 597190 real_backward_count 61222  10.252%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.907115/  2.013770, val:  70.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3504%\n",
      "layer   2  Sparsity: 68.9656%\n",
      "layer   3  Sparsity: 68.9845%\n",
      "total_backward_count 606980 real_backward_count 61948  10.206%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.913004/  2.021936, val:  64.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3607%\n",
      "layer   2  Sparsity: 68.7002%\n",
      "layer   3  Sparsity: 69.3555%\n",
      "total_backward_count 616770 real_backward_count 62641  10.156%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.914490/  2.024450, val:  70.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3604%\n",
      "layer   2  Sparsity: 68.8908%\n",
      "layer   3  Sparsity: 68.9213%\n",
      "total_backward_count 626560 real_backward_count 63291  10.101%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.917998/  2.033240, val:  72.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3645%\n",
      "layer   2  Sparsity: 68.7647%\n",
      "layer   3  Sparsity: 69.2502%\n",
      "total_backward_count 636350 real_backward_count 63978  10.054%\n",
      "lif layer 1 self.abs_max_v: 15907.5\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.916696/  2.027855, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3497%\n",
      "layer   2  Sparsity: 68.9153%\n",
      "layer   3  Sparsity: 69.5965%\n",
      "total_backward_count 646140 real_backward_count 64641  10.004%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.919148/  2.029989, val:  67.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3484%\n",
      "layer   2  Sparsity: 68.6656%\n",
      "layer   3  Sparsity: 69.3381%\n",
      "total_backward_count 655930 real_backward_count 65301   9.955%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.914044/  2.013223, val:  72.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3440%\n",
      "layer   2  Sparsity: 68.8884%\n",
      "layer   3  Sparsity: 68.9017%\n",
      "total_backward_count 665720 real_backward_count 65975   9.910%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.921063/  2.030443, val:  71.67%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3486%\n",
      "layer   2  Sparsity: 68.4950%\n",
      "layer   3  Sparsity: 69.3250%\n",
      "total_backward_count 675510 real_backward_count 66681   9.871%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.917677/  2.026615, val:  72.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3569%\n",
      "layer   2  Sparsity: 68.2718%\n",
      "layer   3  Sparsity: 69.1688%\n",
      "total_backward_count 685300 real_backward_count 67366   9.830%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.919337/  2.020370, val:  69.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3576%\n",
      "layer   2  Sparsity: 68.0956%\n",
      "layer   3  Sparsity: 69.3454%\n",
      "total_backward_count 695090 real_backward_count 68060   9.792%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.916212/  2.025426, val:  69.58%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.97 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3555%\n",
      "layer   2  Sparsity: 68.3586%\n",
      "layer   3  Sparsity: 69.5711%\n",
      "total_backward_count 704880 real_backward_count 68731   9.751%\n",
      "fc layer 1 self.abs_max_out: 9457.0\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.915516/  2.019038, val:  71.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3486%\n",
      "layer   2  Sparsity: 67.9309%\n",
      "layer   3  Sparsity: 68.9959%\n",
      "total_backward_count 714670 real_backward_count 69409   9.712%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.920718/  2.025552, val:  71.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3470%\n",
      "layer   2  Sparsity: 68.0874%\n",
      "layer   3  Sparsity: 70.1282%\n",
      "total_backward_count 724460 real_backward_count 70072   9.672%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.921234/  2.031013, val:  64.58%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3600%\n",
      "layer   2  Sparsity: 68.2062%\n",
      "layer   3  Sparsity: 69.7191%\n",
      "total_backward_count 734250 real_backward_count 70755   9.636%\n",
      "lif layer 1 self.abs_max_v: 16031.5\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.927053/  2.023091, val:  71.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3561%\n",
      "layer   2  Sparsity: 68.2071%\n",
      "layer   3  Sparsity: 69.2646%\n",
      "total_backward_count 744040 real_backward_count 71364   9.591%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.922326/  2.019723, val:  76.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3566%\n",
      "layer   2  Sparsity: 68.1850%\n",
      "layer   3  Sparsity: 69.2308%\n",
      "total_backward_count 753830 real_backward_count 72036   9.556%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.920577/  2.023559, val:  62.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3607%\n",
      "layer   2  Sparsity: 68.3041%\n",
      "layer   3  Sparsity: 69.2856%\n",
      "total_backward_count 763620 real_backward_count 72677   9.517%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.918200/  2.024983, val:  72.50%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3559%\n",
      "layer   2  Sparsity: 67.9677%\n",
      "layer   3  Sparsity: 69.1507%\n",
      "total_backward_count 773410 real_backward_count 73300   9.478%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.913666/  2.010668, val:  75.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3498%\n",
      "layer   2  Sparsity: 68.5363%\n",
      "layer   3  Sparsity: 69.0253%\n",
      "total_backward_count 783200 real_backward_count 73892   9.435%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.911648/  2.018390, val:  73.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3658%\n",
      "layer   2  Sparsity: 68.4663%\n",
      "layer   3  Sparsity: 68.3226%\n",
      "total_backward_count 792990 real_backward_count 74525   9.398%\n",
      "lif layer 1 self.abs_max_v: 16166.0\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.909120/  2.015677, val:  67.08%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3519%\n",
      "layer   2  Sparsity: 68.9953%\n",
      "layer   3  Sparsity: 68.8758%\n",
      "total_backward_count 802780 real_backward_count 75168   9.363%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.913269/  2.016762, val:  66.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3536%\n",
      "layer   2  Sparsity: 68.8543%\n",
      "layer   3  Sparsity: 68.8846%\n",
      "total_backward_count 812570 real_backward_count 75779   9.326%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.912605/  2.022676, val:  65.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3634%\n",
      "layer   2  Sparsity: 68.3678%\n",
      "layer   3  Sparsity: 69.0844%\n",
      "total_backward_count 822360 real_backward_count 76418   9.293%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.919040/  2.024218, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3480%\n",
      "layer   2  Sparsity: 68.6844%\n",
      "layer   3  Sparsity: 69.3855%\n",
      "total_backward_count 832150 real_backward_count 77050   9.259%\n",
      "lif layer 1 self.abs_max_v: 16299.5\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.917202/  2.024512, val:  69.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3540%\n",
      "layer   2  Sparsity: 68.6345%\n",
      "layer   3  Sparsity: 68.8326%\n",
      "total_backward_count 841940 real_backward_count 77697   9.228%\n",
      "lif layer 1 self.abs_max_v: 16477.0\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.907758/  2.011419, val:  72.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.57 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3578%\n",
      "layer   2  Sparsity: 68.6191%\n",
      "layer   3  Sparsity: 69.1721%\n",
      "total_backward_count 851730 real_backward_count 78312   9.194%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.910826/  2.024842, val:  65.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3562%\n",
      "layer   2  Sparsity: 68.0921%\n",
      "layer   3  Sparsity: 69.3097%\n",
      "total_backward_count 861520 real_backward_count 79009   9.171%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.912171/  2.016876, val:  72.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3645%\n",
      "layer   2  Sparsity: 67.8424%\n",
      "layer   3  Sparsity: 68.7261%\n",
      "total_backward_count 871310 real_backward_count 79665   9.143%\n",
      "fc layer 1 self.abs_max_out: 9624.0\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.905905/  2.014447, val:  68.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3603%\n",
      "layer   2  Sparsity: 68.1553%\n",
      "layer   3  Sparsity: 68.3795%\n",
      "total_backward_count 881100 real_backward_count 80329   9.117%\n",
      "fc layer 1 self.abs_max_out: 10003.0\n",
      "lif layer 1 self.abs_max_v: 16520.0\n",
      "lif layer 1 self.abs_max_v: 17804.0\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.904606/  2.018159, val:  77.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3619%\n",
      "layer   2  Sparsity: 68.3016%\n",
      "layer   3  Sparsity: 68.2056%\n",
      "total_backward_count 890890 real_backward_count 80952   9.087%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.910166/  2.007715, val:  81.25%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3537%\n",
      "layer   2  Sparsity: 68.3343%\n",
      "layer   3  Sparsity: 68.7151%\n",
      "total_backward_count 900680 real_backward_count 81556   9.055%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.913271/  2.021765, val:  70.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.37 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3549%\n",
      "layer   2  Sparsity: 68.3850%\n",
      "layer   3  Sparsity: 68.8916%\n",
      "total_backward_count 910470 real_backward_count 82147   9.022%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.919842/  2.018690, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.00 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3539%\n",
      "layer   2  Sparsity: 68.1757%\n",
      "layer   3  Sparsity: 68.2883%\n",
      "total_backward_count 920260 real_backward_count 82790   8.996%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.912863/  2.010604, val:  70.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3506%\n",
      "layer   2  Sparsity: 67.8584%\n",
      "layer   3  Sparsity: 67.9808%\n",
      "total_backward_count 930050 real_backward_count 83382   8.965%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.905251/  2.018108, val:  75.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3534%\n",
      "layer   2  Sparsity: 67.7322%\n",
      "layer   3  Sparsity: 67.8887%\n",
      "total_backward_count 939840 real_backward_count 84012   8.939%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.911960/  2.031712, val:  65.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3596%\n",
      "layer   2  Sparsity: 67.8262%\n",
      "layer   3  Sparsity: 67.8306%\n",
      "total_backward_count 949630 real_backward_count 84615   8.910%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.910719/  2.016926, val:  70.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3536%\n",
      "layer   2  Sparsity: 68.1411%\n",
      "layer   3  Sparsity: 67.8423%\n",
      "total_backward_count 959420 real_backward_count 85217   8.882%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.906887/  2.032376, val:  65.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3482%\n",
      "layer   2  Sparsity: 67.9943%\n",
      "layer   3  Sparsity: 67.5556%\n",
      "total_backward_count 969210 real_backward_count 85844   8.857%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.909269/  2.012176, val:  74.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3577%\n",
      "layer   2  Sparsity: 68.1360%\n",
      "layer   3  Sparsity: 67.8068%\n",
      "total_backward_count 979000 real_backward_count 86420   8.827%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.899165/  2.007359, val:  75.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3498%\n",
      "layer   2  Sparsity: 68.3299%\n",
      "layer   3  Sparsity: 67.7337%\n",
      "total_backward_count 988790 real_backward_count 87011   8.800%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.905147/  2.025775, val:  68.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3599%\n",
      "layer   2  Sparsity: 68.2632%\n",
      "layer   3  Sparsity: 68.0095%\n",
      "total_backward_count 998580 real_backward_count 87588   8.771%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.909185/  2.012400, val:  71.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3513%\n",
      "layer   2  Sparsity: 68.4612%\n",
      "layer   3  Sparsity: 68.9594%\n",
      "total_backward_count 1008370 real_backward_count 88158   8.743%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.901286/  2.009586, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3570%\n",
      "layer   2  Sparsity: 68.4642%\n",
      "layer   3  Sparsity: 68.4233%\n",
      "total_backward_count 1018160 real_backward_count 88738   8.716%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.899376/  2.004555, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3633%\n",
      "layer   2  Sparsity: 68.1895%\n",
      "layer   3  Sparsity: 68.5802%\n",
      "total_backward_count 1027950 real_backward_count 89336   8.691%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.901477/  2.014761, val:  75.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.26 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3633%\n",
      "layer   2  Sparsity: 68.2045%\n",
      "layer   3  Sparsity: 68.3938%\n",
      "total_backward_count 1037740 real_backward_count 89928   8.666%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.903067/  2.008876, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3516%\n",
      "layer   2  Sparsity: 67.6486%\n",
      "layer   3  Sparsity: 68.1109%\n",
      "total_backward_count 1047530 real_backward_count 90523   8.642%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.907336/  2.008420, val:  71.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.00 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3568%\n",
      "layer   2  Sparsity: 67.6850%\n",
      "layer   3  Sparsity: 68.1525%\n",
      "total_backward_count 1057320 real_backward_count 91150   8.621%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.904491/  2.011462, val:  71.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3577%\n",
      "layer   2  Sparsity: 67.8519%\n",
      "layer   3  Sparsity: 67.9540%\n",
      "total_backward_count 1067110 real_backward_count 91759   8.599%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.905537/  2.002905, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3571%\n",
      "layer   2  Sparsity: 67.7955%\n",
      "layer   3  Sparsity: 68.5245%\n",
      "total_backward_count 1076900 real_backward_count 92306   8.571%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.904159/  2.010893, val:  73.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3522%\n",
      "layer   2  Sparsity: 68.0213%\n",
      "layer   3  Sparsity: 68.4765%\n",
      "total_backward_count 1086690 real_backward_count 92909   8.550%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.911601/  2.004401, val:  72.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3579%\n",
      "layer   2  Sparsity: 67.8060%\n",
      "layer   3  Sparsity: 68.5694%\n",
      "total_backward_count 1096480 real_backward_count 93493   8.527%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.908780/  2.019622, val:  66.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3598%\n",
      "layer   2  Sparsity: 67.9005%\n",
      "layer   3  Sparsity: 69.1772%\n",
      "total_backward_count 1106270 real_backward_count 94074   8.504%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.908841/  2.023335, val:  67.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3643%\n",
      "layer   2  Sparsity: 67.9044%\n",
      "layer   3  Sparsity: 69.1699%\n",
      "total_backward_count 1116060 real_backward_count 94670   8.483%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.909002/  2.010872, val:  74.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3599%\n",
      "layer   2  Sparsity: 67.8082%\n",
      "layer   3  Sparsity: 69.3228%\n",
      "total_backward_count 1125850 real_backward_count 95260   8.461%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.905854/  2.018344, val:  71.25%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3474%\n",
      "layer   2  Sparsity: 68.1836%\n",
      "layer   3  Sparsity: 69.1885%\n",
      "total_backward_count 1135640 real_backward_count 95813   8.437%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.915533/  2.023686, val:  69.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3585%\n",
      "layer   2  Sparsity: 68.3922%\n",
      "layer   3  Sparsity: 68.9805%\n",
      "total_backward_count 1145430 real_backward_count 96402   8.416%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.920940/  2.018084, val:  70.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.16 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3512%\n",
      "layer   2  Sparsity: 68.3944%\n",
      "layer   3  Sparsity: 69.4767%\n",
      "total_backward_count 1155220 real_backward_count 96971   8.394%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.908779/  2.012724, val:  63.33%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3561%\n",
      "layer   2  Sparsity: 68.1847%\n",
      "layer   3  Sparsity: 68.5785%\n",
      "total_backward_count 1165010 real_backward_count 97552   8.373%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.907870/  2.012389, val:  75.00%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3587%\n",
      "layer   2  Sparsity: 68.4092%\n",
      "layer   3  Sparsity: 68.2639%\n",
      "total_backward_count 1174800 real_backward_count 98106   8.351%\n",
      "fc layer 1 self.abs_max_out: 10213.0\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.909327/  2.014268, val:  70.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3441%\n",
      "layer   2  Sparsity: 67.9970%\n",
      "layer   3  Sparsity: 68.4115%\n",
      "total_backward_count 1184590 real_backward_count 98612   8.325%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.909858/  2.018283, val:  75.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3540%\n",
      "layer   2  Sparsity: 67.5662%\n",
      "layer   3  Sparsity: 68.5869%\n",
      "total_backward_count 1194380 real_backward_count 99186   8.304%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.907044/  2.010660, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.13 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3563%\n",
      "layer   2  Sparsity: 67.6984%\n",
      "layer   3  Sparsity: 68.9782%\n",
      "total_backward_count 1204170 real_backward_count 99758   8.284%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.902214/  2.003360, val:  80.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3518%\n",
      "layer   2  Sparsity: 67.7775%\n",
      "layer   3  Sparsity: 69.1870%\n",
      "total_backward_count 1213960 real_backward_count 100320   8.264%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.901078/  2.003664, val:  70.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.36 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3569%\n",
      "layer   2  Sparsity: 67.9083%\n",
      "layer   3  Sparsity: 68.4990%\n",
      "total_backward_count 1223750 real_backward_count 100887   8.244%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.906154/  2.002114, val:  69.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.42 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3587%\n",
      "layer   2  Sparsity: 67.7374%\n",
      "layer   3  Sparsity: 68.2457%\n",
      "total_backward_count 1233540 real_backward_count 101475   8.226%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.904341/  2.009066, val:  67.50%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3647%\n",
      "layer   2  Sparsity: 67.6760%\n",
      "layer   3  Sparsity: 68.7390%\n",
      "total_backward_count 1243330 real_backward_count 102062   8.209%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.898626/  2.006844, val:  73.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3538%\n",
      "layer   2  Sparsity: 67.9741%\n",
      "layer   3  Sparsity: 68.8186%\n",
      "total_backward_count 1253120 real_backward_count 102653   8.192%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.894664/  2.004719, val:  77.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3610%\n",
      "layer   2  Sparsity: 68.1306%\n",
      "layer   3  Sparsity: 68.1525%\n",
      "total_backward_count 1262910 real_backward_count 103228   8.174%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.902872/  2.004790, val:  79.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3473%\n",
      "layer   2  Sparsity: 67.8322%\n",
      "layer   3  Sparsity: 67.8628%\n",
      "total_backward_count 1272700 real_backward_count 103815   8.157%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.897800/  2.007272, val:  72.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3640%\n",
      "layer   2  Sparsity: 67.7506%\n",
      "layer   3  Sparsity: 67.9916%\n",
      "total_backward_count 1282490 real_backward_count 104344   8.136%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.901776/  2.009919, val:  69.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3555%\n",
      "layer   2  Sparsity: 67.6970%\n",
      "layer   3  Sparsity: 67.4453%\n",
      "total_backward_count 1292280 real_backward_count 104939   8.120%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.900401/  2.007794, val:  72.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3580%\n",
      "layer   2  Sparsity: 67.6790%\n",
      "layer   3  Sparsity: 68.2075%\n",
      "total_backward_count 1302070 real_backward_count 105493   8.102%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.900785/  2.008583, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3551%\n",
      "layer   2  Sparsity: 67.9276%\n",
      "layer   3  Sparsity: 68.7500%\n",
      "total_backward_count 1311860 real_backward_count 106063   8.085%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.898981/  2.010382, val:  70.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3488%\n",
      "layer   2  Sparsity: 67.8378%\n",
      "layer   3  Sparsity: 68.7592%\n",
      "total_backward_count 1321650 real_backward_count 106618   8.067%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.894908/  2.000936, val:  77.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3606%\n",
      "layer   2  Sparsity: 67.9747%\n",
      "layer   3  Sparsity: 67.6151%\n",
      "total_backward_count 1331440 real_backward_count 107177   8.050%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.890411/  1.992129, val:  75.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3488%\n",
      "layer   2  Sparsity: 67.7667%\n",
      "layer   3  Sparsity: 67.8944%\n",
      "total_backward_count 1341230 real_backward_count 107700   8.030%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.892777/  1.999765, val:  75.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3696%\n",
      "layer   2  Sparsity: 67.7130%\n",
      "layer   3  Sparsity: 68.0345%\n",
      "total_backward_count 1351020 real_backward_count 108231   8.011%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.895075/  2.012606, val:  76.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3505%\n",
      "layer   2  Sparsity: 67.7526%\n",
      "layer   3  Sparsity: 68.5818%\n",
      "total_backward_count 1360810 real_backward_count 108802   7.995%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.904420/  2.016704, val:  74.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.15 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3557%\n",
      "layer   2  Sparsity: 67.8081%\n",
      "layer   3  Sparsity: 68.6243%\n",
      "total_backward_count 1370600 real_backward_count 109362   7.979%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.899235/  2.001052, val:  63.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.26 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3539%\n",
      "layer   2  Sparsity: 67.7077%\n",
      "layer   3  Sparsity: 68.5938%\n",
      "total_backward_count 1380390 real_backward_count 109909   7.962%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.890494/  1.999817, val:  69.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3574%\n",
      "layer   2  Sparsity: 68.0546%\n",
      "layer   3  Sparsity: 68.4139%\n",
      "total_backward_count 1390180 real_backward_count 110427   7.943%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.886851/  2.001179, val:  74.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3586%\n",
      "layer   2  Sparsity: 68.0522%\n",
      "layer   3  Sparsity: 68.8588%\n",
      "total_backward_count 1399970 real_backward_count 110986   7.928%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.888947/  1.997275, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3519%\n",
      "layer   2  Sparsity: 68.2037%\n",
      "layer   3  Sparsity: 69.7911%\n",
      "total_backward_count 1409760 real_backward_count 111512   7.910%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.890625/  2.000070, val:  73.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3554%\n",
      "layer   2  Sparsity: 67.7741%\n",
      "layer   3  Sparsity: 68.7838%\n",
      "total_backward_count 1419550 real_backward_count 112016   7.891%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.891279/  1.999886, val:  65.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3565%\n",
      "layer   2  Sparsity: 68.1043%\n",
      "layer   3  Sparsity: 69.3416%\n",
      "total_backward_count 1429340 real_backward_count 112559   7.875%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.888465/  1.994933, val:  73.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3572%\n",
      "layer   2  Sparsity: 67.9712%\n",
      "layer   3  Sparsity: 69.0525%\n",
      "total_backward_count 1439130 real_backward_count 113101   7.859%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.890695/  1.999883, val:  63.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3496%\n",
      "layer   2  Sparsity: 67.6630%\n",
      "layer   3  Sparsity: 68.7411%\n",
      "total_backward_count 1448920 real_backward_count 113596   7.840%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.893308/  1.997664, val:  77.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3590%\n",
      "layer   2  Sparsity: 67.5575%\n",
      "layer   3  Sparsity: 68.0219%\n",
      "total_backward_count 1458710 real_backward_count 114135   7.824%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.891313/  1.995898, val:  72.50%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3576%\n",
      "layer   2  Sparsity: 67.6532%\n",
      "layer   3  Sparsity: 68.8488%\n",
      "total_backward_count 1468500 real_backward_count 114672   7.809%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.887450/  2.003148, val:  72.50%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3461%\n",
      "layer   2  Sparsity: 67.6849%\n",
      "layer   3  Sparsity: 68.8338%\n",
      "total_backward_count 1478290 real_backward_count 115194   7.792%\n",
      "fc layer 1 self.abs_max_out: 10453.0\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.896688/  2.004299, val:  72.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3608%\n",
      "layer   2  Sparsity: 67.6785%\n",
      "layer   3  Sparsity: 68.3187%\n",
      "total_backward_count 1488080 real_backward_count 115730   7.777%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.889614/  1.986793, val:  75.83%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.19 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3568%\n",
      "layer   2  Sparsity: 67.6783%\n",
      "layer   3  Sparsity: 68.2689%\n",
      "total_backward_count 1497870 real_backward_count 116260   7.762%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.891883/  1.994449, val:  80.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3519%\n",
      "layer   2  Sparsity: 68.0516%\n",
      "layer   3  Sparsity: 68.6975%\n",
      "total_backward_count 1507660 real_backward_count 116806   7.748%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.890663/  1.992764, val:  75.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3517%\n",
      "layer   2  Sparsity: 68.0872%\n",
      "layer   3  Sparsity: 68.6507%\n",
      "total_backward_count 1517450 real_backward_count 117275   7.728%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.887733/  2.000208, val:  72.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3509%\n",
      "layer   2  Sparsity: 67.6403%\n",
      "layer   3  Sparsity: 68.5332%\n",
      "total_backward_count 1527240 real_backward_count 117761   7.711%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.894049/  2.008631, val:  70.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3483%\n",
      "layer   2  Sparsity: 67.6339%\n",
      "layer   3  Sparsity: 68.5450%\n",
      "total_backward_count 1537030 real_backward_count 118308   7.697%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.896658/  2.004672, val:  69.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3592%\n",
      "layer   2  Sparsity: 67.4841%\n",
      "layer   3  Sparsity: 68.7685%\n",
      "total_backward_count 1546820 real_backward_count 118800   7.680%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.898198/  2.002817, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3629%\n",
      "layer   2  Sparsity: 67.4095%\n",
      "layer   3  Sparsity: 68.6164%\n",
      "total_backward_count 1556610 real_backward_count 119312   7.665%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.902031/  2.003349, val:  72.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3527%\n",
      "layer   2  Sparsity: 67.5513%\n",
      "layer   3  Sparsity: 68.7859%\n",
      "total_backward_count 1566400 real_backward_count 119799   7.648%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.898991/  1.995274, val:  71.67%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3562%\n",
      "layer   2  Sparsity: 67.6152%\n",
      "layer   3  Sparsity: 68.2797%\n",
      "total_backward_count 1576190 real_backward_count 120292   7.632%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.889978/  1.989928, val:  79.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3484%\n",
      "layer   2  Sparsity: 67.9416%\n",
      "layer   3  Sparsity: 68.1653%\n",
      "total_backward_count 1585980 real_backward_count 120841   7.619%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.888671/  2.002479, val:  70.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.94 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3585%\n",
      "layer   2  Sparsity: 67.7746%\n",
      "layer   3  Sparsity: 68.1028%\n",
      "total_backward_count 1595770 real_backward_count 121331   7.603%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.877177/  1.978745, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3492%\n",
      "layer   2  Sparsity: 67.6423%\n",
      "layer   3  Sparsity: 68.3920%\n",
      "total_backward_count 1605560 real_backward_count 121845   7.589%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.875597/  1.988422, val:  70.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3502%\n",
      "layer   2  Sparsity: 67.5826%\n",
      "layer   3  Sparsity: 68.0751%\n",
      "total_backward_count 1615350 real_backward_count 122382   7.576%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.877989/  1.994475, val:  67.08%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3567%\n",
      "layer   2  Sparsity: 67.7828%\n",
      "layer   3  Sparsity: 67.4110%\n",
      "total_backward_count 1625140 real_backward_count 122908   7.563%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.877645/  1.991342, val:  67.92%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.3606%\n",
      "layer   2  Sparsity: 67.7310%\n",
      "layer   3  Sparsity: 66.9939%\n",
      "total_backward_count 1634930 real_backward_count 123397   7.548%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.880259/  1.984868, val:  71.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3575%\n",
      "layer   2  Sparsity: 67.8278%\n",
      "layer   3  Sparsity: 68.0381%\n",
      "total_backward_count 1644720 real_backward_count 123924   7.535%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.881542/  1.995447, val:  72.50%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.97 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3504%\n",
      "layer   2  Sparsity: 67.7129%\n",
      "layer   3  Sparsity: 68.0797%\n",
      "total_backward_count 1654510 real_backward_count 124443   7.521%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.878070/  1.987150, val:  70.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3557%\n",
      "layer   2  Sparsity: 67.9550%\n",
      "layer   3  Sparsity: 68.0591%\n",
      "total_backward_count 1664300 real_backward_count 124982   7.510%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.879263/  1.986082, val:  75.42%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3548%\n",
      "layer   2  Sparsity: 67.8062%\n",
      "layer   3  Sparsity: 68.1889%\n",
      "total_backward_count 1674090 real_backward_count 125461   7.494%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.877817/  1.984490, val:  62.92%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3507%\n",
      "layer   2  Sparsity: 67.6067%\n",
      "layer   3  Sparsity: 67.5153%\n",
      "total_backward_count 1683880 real_backward_count 125973   7.481%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.874288/  1.983894, val:  75.42%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3448%\n",
      "layer   2  Sparsity: 67.6301%\n",
      "layer   3  Sparsity: 68.2699%\n",
      "total_backward_count 1693670 real_backward_count 126467   7.467%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.875520/  1.988564, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3563%\n",
      "layer   2  Sparsity: 68.0323%\n",
      "layer   3  Sparsity: 68.5329%\n",
      "total_backward_count 1703460 real_backward_count 127016   7.456%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.874477/  1.984290, val:  73.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3548%\n",
      "layer   2  Sparsity: 67.9415%\n",
      "layer   3  Sparsity: 68.2626%\n",
      "total_backward_count 1713250 real_backward_count 127501   7.442%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.868760/  1.975475, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.90 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3545%\n",
      "layer   2  Sparsity: 67.8147%\n",
      "layer   3  Sparsity: 68.6259%\n",
      "total_backward_count 1723040 real_backward_count 128016   7.430%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.867435/  1.986148, val:  74.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3598%\n",
      "layer   2  Sparsity: 67.8677%\n",
      "layer   3  Sparsity: 68.0675%\n",
      "total_backward_count 1732830 real_backward_count 128502   7.416%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.865450/  1.986469, val:  71.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3548%\n",
      "layer   2  Sparsity: 67.7108%\n",
      "layer   3  Sparsity: 68.1654%\n",
      "total_backward_count 1742620 real_backward_count 129003   7.403%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.867260/  1.979974, val:  72.92%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3589%\n",
      "layer   2  Sparsity: 67.6122%\n",
      "layer   3  Sparsity: 67.7620%\n",
      "total_backward_count 1752410 real_backward_count 129494   7.389%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.865769/  1.977705, val:  80.42%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3443%\n",
      "layer   2  Sparsity: 67.9046%\n",
      "layer   3  Sparsity: 66.9761%\n",
      "total_backward_count 1762200 real_backward_count 130010   7.378%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.871198/  1.982465, val:  74.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3570%\n",
      "layer   2  Sparsity: 67.9194%\n",
      "layer   3  Sparsity: 66.9659%\n",
      "total_backward_count 1771990 real_backward_count 130485   7.364%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.864004/  1.992319, val:  64.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3593%\n",
      "layer   2  Sparsity: 68.1609%\n",
      "layer   3  Sparsity: 67.0491%\n",
      "total_backward_count 1781780 real_backward_count 130949   7.349%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.869593/  1.985184, val:  76.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3462%\n",
      "layer   2  Sparsity: 67.9389%\n",
      "layer   3  Sparsity: 67.5645%\n",
      "total_backward_count 1791570 real_backward_count 131461   7.338%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.874767/  1.987252, val:  73.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.3567%\n",
      "layer   2  Sparsity: 67.5543%\n",
      "layer   3  Sparsity: 67.7194%\n",
      "total_backward_count 1801360 real_backward_count 131967   7.326%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.877165/  1.987233, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3521%\n",
      "layer   2  Sparsity: 67.4636%\n",
      "layer   3  Sparsity: 67.3846%\n",
      "total_backward_count 1811150 real_backward_count 132501   7.316%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.874520/  1.980249, val:  73.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3542%\n",
      "layer   2  Sparsity: 67.4822%\n",
      "layer   3  Sparsity: 68.3499%\n",
      "total_backward_count 1820940 real_backward_count 133002   7.304%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.872138/  1.981345, val:  79.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.23 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3566%\n",
      "layer   2  Sparsity: 67.7527%\n",
      "layer   3  Sparsity: 67.3413%\n",
      "total_backward_count 1830730 real_backward_count 133472   7.291%\n",
      "lif layer 2 self.abs_max_v: 7106.5\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.870255/  1.993575, val:  67.50%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3498%\n",
      "layer   2  Sparsity: 67.7377%\n",
      "layer   3  Sparsity: 67.5925%\n",
      "total_backward_count 1840520 real_backward_count 133960   7.278%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.875807/  1.987452, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3475%\n",
      "layer   2  Sparsity: 67.8256%\n",
      "layer   3  Sparsity: 67.6144%\n",
      "total_backward_count 1850310 real_backward_count 134455   7.267%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.864264/  1.976615, val:  78.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3499%\n",
      "layer   2  Sparsity: 67.7256%\n",
      "layer   3  Sparsity: 67.6837%\n",
      "total_backward_count 1860100 real_backward_count 134955   7.255%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.859134/  1.980413, val:  71.67%, val_best:  84.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3502%\n",
      "layer   2  Sparsity: 67.6037%\n",
      "layer   3  Sparsity: 67.4840%\n",
      "total_backward_count 1869890 real_backward_count 135435   7.243%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.869709/  1.993596, val:  63.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3545%\n",
      "layer   2  Sparsity: 67.6804%\n",
      "layer   3  Sparsity: 67.8374%\n",
      "total_backward_count 1879680 real_backward_count 135940   7.232%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.873439/  1.996421, val:  74.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3506%\n",
      "layer   2  Sparsity: 67.6096%\n",
      "layer   3  Sparsity: 68.5857%\n",
      "total_backward_count 1889470 real_backward_count 136445   7.221%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.874087/  1.998379, val:  69.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3527%\n",
      "layer   2  Sparsity: 67.8182%\n",
      "layer   3  Sparsity: 68.2101%\n",
      "total_backward_count 1899260 real_backward_count 136916   7.209%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.880653/  1.981986, val:  74.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3577%\n",
      "layer   2  Sparsity: 67.6713%\n",
      "layer   3  Sparsity: 68.2460%\n",
      "total_backward_count 1909050 real_backward_count 137375   7.196%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.868140/  1.982273, val:  76.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3531%\n",
      "layer   2  Sparsity: 67.5253%\n",
      "layer   3  Sparsity: 67.3102%\n",
      "total_backward_count 1918840 real_backward_count 137884   7.186%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.870736/  1.988999, val:  72.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3487%\n",
      "layer   2  Sparsity: 67.2366%\n",
      "layer   3  Sparsity: 67.4957%\n",
      "total_backward_count 1928630 real_backward_count 138380   7.175%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.875786/  1.987461, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.08 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.3550%\n",
      "layer   2  Sparsity: 67.5776%\n",
      "layer   3  Sparsity: 68.7004%\n",
      "total_backward_count 1938420 real_backward_count 138895   7.165%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.867679/  1.971085, val:  80.42%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.3424%\n",
      "layer   2  Sparsity: 67.3454%\n",
      "layer   3  Sparsity: 67.5443%\n",
      "total_backward_count 1948210 real_backward_count 139403   7.155%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.870983/  1.973059, val:  75.00%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.3551%\n",
      "layer   2  Sparsity: 67.3441%\n",
      "layer   3  Sparsity: 67.6416%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9f1191d3164a8bbc8a519a4df524c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÑ‚ñà‚ñÖ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÑ‚ñà‚ñÖ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.87098</td></tr><tr><td>val_acc_best</td><td>0.84583</td></tr><tr><td>val_acc_now</td><td>0.75</td></tr><tr><td>val_loss</td><td>1.97306</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">good-sweep-59</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rccfvxb2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rccfvxb2</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_014008-rccfvxb2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nckcemrn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_060616-nckcemrn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nckcemrn' target=\"_blank\">vocal-sweep-64</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nckcemrn' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nckcemrn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251115_060625_702', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 6, 'leaky_temporal_filter': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 511.0\n",
      "lif layer 1 self.abs_max_v: 511.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 958.0\n",
      "lif layer 2 self.abs_max_v: 958.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 388.0\n",
      "lif layer 1 self.abs_max_v: 586.5\n",
      "fc layer 2 self.abs_max_out: 969.0\n",
      "lif layer 2 self.abs_max_v: 1357.5\n",
      "fc layer 3 self.abs_max_out: 389.0\n",
      "fc layer 1 self.abs_max_out: 567.0\n",
      "lif layer 1 self.abs_max_v: 858.5\n",
      "fc layer 1 self.abs_max_out: 629.0\n",
      "lif layer 1 self.abs_max_v: 906.5\n",
      "fc layer 2 self.abs_max_out: 1042.0\n",
      "lif layer 2 self.abs_max_v: 1540.5\n",
      "lif layer 2 self.abs_max_v: 1695.5\n",
      "fc layer 2 self.abs_max_out: 1214.0\n",
      "lif layer 2 self.abs_max_v: 2062.0\n",
      "fc layer 1 self.abs_max_out: 903.0\n",
      "fc layer 2 self.abs_max_out: 1222.0\n",
      "fc layer 3 self.abs_max_out: 417.0\n",
      "lif layer 1 self.abs_max_v: 1079.0\n",
      "lif layer 1 self.abs_max_v: 1151.5\n",
      "fc layer 3 self.abs_max_out: 442.0\n",
      "fc layer 1 self.abs_max_out: 1034.0\n",
      "fc layer 3 self.abs_max_out: 460.0\n",
      "lif layer 1 self.abs_max_v: 1202.0\n",
      "lif layer 1 self.abs_max_v: 1233.0\n",
      "lif layer 1 self.abs_max_v: 1329.5\n",
      "fc layer 3 self.abs_max_out: 466.0\n",
      "fc layer 2 self.abs_max_out: 1267.0\n",
      "lif layer 1 self.abs_max_v: 1361.0\n",
      "fc layer 1 self.abs_max_out: 1042.0\n",
      "fc layer 1 self.abs_max_out: 1122.0\n",
      "lif layer 1 self.abs_max_v: 1475.5\n",
      "lif layer 1 self.abs_max_v: 1658.0\n",
      "fc layer 2 self.abs_max_out: 1271.0\n",
      "lif layer 2 self.abs_max_v: 2242.0\n",
      "lif layer 1 self.abs_max_v: 1810.0\n",
      "fc layer 2 self.abs_max_out: 1287.0\n",
      "fc layer 1 self.abs_max_out: 1123.0\n",
      "fc layer 2 self.abs_max_out: 1489.0\n",
      "lif layer 2 self.abs_max_v: 2416.5\n",
      "lif layer 2 self.abs_max_v: 2478.0\n",
      "fc layer 3 self.abs_max_out: 523.0\n",
      "fc layer 3 self.abs_max_out: 610.0\n",
      "fc layer 1 self.abs_max_out: 1141.0\n",
      "fc layer 1 self.abs_max_out: 1227.0\n",
      "lif layer 1 self.abs_max_v: 1875.0\n",
      "fc layer 1 self.abs_max_out: 1286.0\n",
      "lif layer 2 self.abs_max_v: 2506.0\n",
      "lif layer 1 self.abs_max_v: 1917.5\n",
      "fc layer 1 self.abs_max_out: 1437.0\n",
      "fc layer 2 self.abs_max_out: 1585.0\n",
      "lif layer 1 self.abs_max_v: 1957.5\n",
      "lif layer 1 self.abs_max_v: 2227.0\n",
      "fc layer 3 self.abs_max_out: 671.0\n",
      "fc layer 1 self.abs_max_out: 1511.0\n",
      "lif layer 2 self.abs_max_v: 2540.0\n",
      "lif layer 2 self.abs_max_v: 2540.5\n",
      "lif layer 1 self.abs_max_v: 2297.0\n",
      "lif layer 1 self.abs_max_v: 2443.5\n",
      "lif layer 1 self.abs_max_v: 2451.0\n",
      "lif layer 1 self.abs_max_v: 2460.5\n",
      "fc layer 1 self.abs_max_out: 1553.0\n",
      "fc layer 1 self.abs_max_out: 1584.0\n",
      "lif layer 1 self.abs_max_v: 2492.5\n",
      "lif layer 1 self.abs_max_v: 2550.5\n",
      "lif layer 1 self.abs_max_v: 2696.0\n",
      "fc layer 1 self.abs_max_out: 1833.0\n",
      "lif layer 1 self.abs_max_v: 2727.0\n",
      "lif layer 1 self.abs_max_v: 3007.5\n",
      "lif layer 2 self.abs_max_v: 2577.0\n",
      "lif layer 2 self.abs_max_v: 2609.0\n",
      "fc layer 1 self.abs_max_out: 1877.0\n",
      "lif layer 1 self.abs_max_v: 3019.5\n",
      "lif layer 1 self.abs_max_v: 3115.0\n",
      "lif layer 1 self.abs_max_v: 3156.5\n",
      "fc layer 1 self.abs_max_out: 2050.0\n",
      "lif layer 1 self.abs_max_v: 3557.5\n",
      "fc layer 3 self.abs_max_out: 676.0\n",
      "fc layer 3 self.abs_max_out: 707.0\n",
      "fc layer 3 self.abs_max_out: 731.0\n",
      "fc layer 3 self.abs_max_out: 737.0\n",
      "lif layer 2 self.abs_max_v: 2666.5\n",
      "fc layer 1 self.abs_max_out: 2103.0\n",
      "lif layer 1 self.abs_max_v: 3672.5\n",
      "fc layer 1 self.abs_max_out: 2135.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  1.825885/  1.965688, val:  35.42%, val_best:  35.42%, tr:  94.89%, tr_best:  94.89%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3897%\n",
      "layer   2  Sparsity: 70.9740%\n",
      "layer   3  Sparsity: 64.1211%\n",
      "total_backward_count 9790 real_backward_count 2274  23.228%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 745.0\n",
      "fc layer 1 self.abs_max_out: 2145.0\n",
      "lif layer 1 self.abs_max_v: 3706.0\n",
      "lif layer 1 self.abs_max_v: 3747.0\n",
      "fc layer 1 self.abs_max_out: 2155.0\n",
      "fc layer 3 self.abs_max_out: 847.0\n",
      "fc layer 2 self.abs_max_out: 1622.0\n",
      "fc layer 1 self.abs_max_out: 2227.0\n",
      "lif layer 1 self.abs_max_v: 3784.5\n",
      "lif layer 1 self.abs_max_v: 3813.5\n",
      "fc layer 1 self.abs_max_out: 2304.0\n",
      "lif layer 1 self.abs_max_v: 3954.0\n",
      "lif layer 1 self.abs_max_v: 3967.0\n",
      "lif layer 1 self.abs_max_v: 4050.5\n",
      "lif layer 1 self.abs_max_v: 4194.5\n",
      "fc layer 1 self.abs_max_out: 2446.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.744441/  1.962844, val:  43.75%, val_best:  43.75%, tr:  98.98%, tr_best:  98.98%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3859%\n",
      "layer   2  Sparsity: 73.4453%\n",
      "layer   3  Sparsity: 65.0478%\n",
      "total_backward_count 19580 real_backward_count 3803  19.423%\n",
      "lif layer 1 self.abs_max_v: 4227.5\n",
      "lif layer 2 self.abs_max_v: 2692.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.727005/  1.902317, val:  56.25%, val_best:  56.25%, tr:  99.39%, tr_best:  99.39%, epoch time: 80.00 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3831%\n",
      "layer   2  Sparsity: 73.4378%\n",
      "layer   3  Sparsity: 64.7406%\n",
      "total_backward_count 29370 real_backward_count 5127  17.457%\n",
      "lif layer 1 self.abs_max_v: 4232.0\n",
      "lif layer 1 self.abs_max_v: 4393.0\n",
      "lif layer 1 self.abs_max_v: 4465.5\n",
      "fc layer 1 self.abs_max_out: 2463.0\n",
      "fc layer 1 self.abs_max_out: 2466.0\n",
      "lif layer 1 self.abs_max_v: 4472.5\n",
      "fc layer 1 self.abs_max_out: 2565.0\n",
      "lif layer 1 self.abs_max_v: 4524.0\n",
      "fc layer 1 self.abs_max_out: 2626.0\n",
      "lif layer 1 self.abs_max_v: 4888.0\n",
      "lif layer 2 self.abs_max_v: 2740.5\n",
      "fc layer 1 self.abs_max_out: 2734.0\n",
      "lif layer 1 self.abs_max_v: 4914.5\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  1.716496/  1.907093, val:  50.83%, val_best:  56.25%, tr:  99.39%, tr_best:  99.39%, epoch time: 80.63 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.3782%\n",
      "layer   2  Sparsity: 74.2922%\n",
      "layer   3  Sparsity: 64.2596%\n",
      "total_backward_count 39160 real_backward_count 6323  16.147%\n",
      "fc layer 1 self.abs_max_out: 2854.0\n",
      "lif layer 1 self.abs_max_v: 5108.5\n",
      "lif layer 1 self.abs_max_v: 5126.5\n",
      "fc layer 1 self.abs_max_out: 2893.0\n",
      "lif layer 1 self.abs_max_v: 5456.5\n",
      "lif layer 2 self.abs_max_v: 2753.5\n",
      "lif layer 2 self.abs_max_v: 2904.5\n",
      "fc layer 2 self.abs_max_out: 1635.0\n",
      "lif layer 2 self.abs_max_v: 2906.0\n",
      "lif layer 2 self.abs_max_v: 3072.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  1.716798/  1.913659, val:  55.83%, val_best:  56.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 80.32 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.3850%\n",
      "layer   2  Sparsity: 75.2752%\n",
      "layer   3  Sparsity: 63.9505%\n",
      "total_backward_count 48950 real_backward_count 7519  15.361%\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  1.700579/  1.896501, val:  53.33%, val_best:  56.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3841%\n",
      "layer   2  Sparsity: 74.4924%\n",
      "layer   3  Sparsity: 63.8206%\n",
      "total_backward_count 58740 real_backward_count 8617  14.670%\n",
      "fc layer 1 self.abs_max_out: 2971.0\n",
      "fc layer 2 self.abs_max_out: 1649.0\n",
      "fc layer 2 self.abs_max_out: 1719.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  1.706877/  1.902203, val:  55.83%, val_best:  56.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3934%\n",
      "layer   2  Sparsity: 73.3183%\n",
      "layer   3  Sparsity: 62.8349%\n",
      "total_backward_count 68530 real_backward_count 9681  14.127%\n",
      "fc layer 2 self.abs_max_out: 1799.0\n",
      "fc layer 3 self.abs_max_out: 891.0\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  1.679139/  1.877062, val:  57.08%, val_best:  57.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.3895%\n",
      "layer   2  Sparsity: 74.3449%\n",
      "layer   3  Sparsity: 63.6717%\n",
      "total_backward_count 78320 real_backward_count 10688  13.647%\n",
      "fc layer 1 self.abs_max_out: 3049.0\n",
      "fc layer 1 self.abs_max_out: 3125.0\n",
      "lif layer 1 self.abs_max_v: 5647.0\n",
      "lif layer 2 self.abs_max_v: 3077.0\n",
      "fc layer 1 self.abs_max_out: 3358.0\n",
      "lif layer 1 self.abs_max_v: 5964.0\n",
      "lif layer 1 self.abs_max_v: 6222.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  1.685263/  1.866216, val:  65.00%, val_best:  65.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3692%\n",
      "layer   2  Sparsity: 74.6577%\n",
      "layer   3  Sparsity: 63.6263%\n",
      "total_backward_count 88110 real_backward_count 11762  13.349%\n",
      "fc layer 1 self.abs_max_out: 3368.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  1.683429/  1.885851, val:  51.67%, val_best:  65.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.95 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3915%\n",
      "layer   2  Sparsity: 75.1393%\n",
      "layer   3  Sparsity: 63.6430%\n",
      "total_backward_count 97900 real_backward_count 12707  12.980%\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  1.664346/  1.853737, val:  57.08%, val_best:  65.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 80.45 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.3825%\n",
      "layer   2  Sparsity: 74.9801%\n",
      "layer   3  Sparsity: 63.4531%\n",
      "total_backward_count 107690 real_backward_count 13671  12.695%\n",
      "lif layer 2 self.abs_max_v: 3103.5\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  1.658462/  1.844324, val:  61.25%, val_best:  65.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3882%\n",
      "layer   2  Sparsity: 75.0453%\n",
      "layer   3  Sparsity: 63.1506%\n",
      "total_backward_count 117480 real_backward_count 14696  12.509%\n",
      "lif layer 2 self.abs_max_v: 3113.5\n",
      "lif layer 2 self.abs_max_v: 3306.0\n",
      "lif layer 2 self.abs_max_v: 3375.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  1.657511/  1.867190, val:  62.08%, val_best:  65.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3988%\n",
      "layer   2  Sparsity: 74.5091%\n",
      "layer   3  Sparsity: 63.2375%\n",
      "total_backward_count 127270 real_backward_count 15684  12.323%\n",
      "fc layer 2 self.abs_max_out: 1810.0\n",
      "fc layer 1 self.abs_max_out: 3404.0\n",
      "lif layer 1 self.abs_max_v: 6315.5\n",
      "lif layer 1 self.abs_max_v: 6316.0\n",
      "fc layer 1 self.abs_max_out: 3431.0\n",
      "lif layer 1 self.abs_max_v: 6565.0\n",
      "fc layer 1 self.abs_max_out: 3641.0\n",
      "lif layer 1 self.abs_max_v: 6632.0\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  1.655615/  1.852137, val:  54.17%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3894%\n",
      "layer   2  Sparsity: 74.2395%\n",
      "layer   3  Sparsity: 62.8664%\n",
      "total_backward_count 137060 real_backward_count 16606  12.116%\n",
      "fc layer 2 self.abs_max_out: 1912.0\n",
      "lif layer 2 self.abs_max_v: 3562.0\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  1.646382/  1.853829, val:  58.33%, val_best:  65.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3736%\n",
      "layer   2  Sparsity: 74.1071%\n",
      "layer   3  Sparsity: 62.5953%\n",
      "total_backward_count 146850 real_backward_count 17469  11.896%\n",
      "lif layer 1 self.abs_max_v: 6655.5\n",
      "fc layer 1 self.abs_max_out: 3724.0\n",
      "lif layer 1 self.abs_max_v: 6752.5\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  1.640140/  1.844361, val:  52.50%, val_best:  65.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3847%\n",
      "layer   2  Sparsity: 74.3936%\n",
      "layer   3  Sparsity: 62.9291%\n",
      "total_backward_count 156640 real_backward_count 18367  11.726%\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  1.632456/  1.836549, val:  61.25%, val_best:  65.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3821%\n",
      "layer   2  Sparsity: 74.9418%\n",
      "layer   3  Sparsity: 63.5393%\n",
      "total_backward_count 166430 real_backward_count 19270  11.578%\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  1.629911/  1.817699, val:  64.58%, val_best:  65.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3907%\n",
      "layer   2  Sparsity: 74.6546%\n",
      "layer   3  Sparsity: 63.6260%\n",
      "total_backward_count 176220 real_backward_count 20193  11.459%\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  1.618143/  1.815729, val:  60.00%, val_best:  65.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.4032%\n",
      "layer   2  Sparsity: 74.6036%\n",
      "layer   3  Sparsity: 63.6475%\n",
      "total_backward_count 186010 real_backward_count 21100  11.343%\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  1.625501/  1.857844, val:  50.42%, val_best:  65.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3819%\n",
      "layer   2  Sparsity: 74.9113%\n",
      "layer   3  Sparsity: 64.2888%\n",
      "total_backward_count 195800 real_backward_count 21962  11.217%\n",
      "fc layer 1 self.abs_max_out: 3772.0\n",
      "lif layer 1 self.abs_max_v: 6818.0\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  1.636692/  1.837739, val:  60.00%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3798%\n",
      "layer   2  Sparsity: 74.9180%\n",
      "layer   3  Sparsity: 64.3833%\n",
      "total_backward_count 205590 real_backward_count 22832  11.106%\n",
      "fc layer 1 self.abs_max_out: 3778.0\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  1.621605/  1.846940, val:  58.75%, val_best:  65.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3678%\n",
      "layer   2  Sparsity: 74.5161%\n",
      "layer   3  Sparsity: 64.2732%\n",
      "total_backward_count 215380 real_backward_count 23726  11.016%\n",
      "fc layer 1 self.abs_max_out: 4023.0\n",
      "lif layer 1 self.abs_max_v: 6839.5\n",
      "lif layer 1 self.abs_max_v: 6974.0\n",
      "lif layer 1 self.abs_max_v: 7165.0\n",
      "lif layer 1 self.abs_max_v: 7292.5\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  1.631336/  1.833099, val:  57.92%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3678%\n",
      "layer   2  Sparsity: 73.9032%\n",
      "layer   3  Sparsity: 64.3047%\n",
      "total_backward_count 225170 real_backward_count 24621  10.934%\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  1.622548/  1.822361, val:  62.50%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3979%\n",
      "layer   2  Sparsity: 74.4994%\n",
      "layer   3  Sparsity: 64.3292%\n",
      "total_backward_count 234960 real_backward_count 25473  10.841%\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  1.623096/  1.825611, val:  63.75%, val_best:  65.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.4029%\n",
      "layer   2  Sparsity: 74.7954%\n",
      "layer   3  Sparsity: 64.1508%\n",
      "total_backward_count 244750 real_backward_count 26320  10.754%\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  1.640434/  1.836737, val:  60.42%, val_best:  65.00%, tr:  99.49%, tr_best:  99.90%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3940%\n",
      "layer   2  Sparsity: 75.1958%\n",
      "layer   3  Sparsity: 64.0809%\n",
      "total_backward_count 254540 real_backward_count 27169  10.674%\n",
      "fc layer 2 self.abs_max_out: 1942.0\n",
      "fc layer 1 self.abs_max_out: 4243.0\n",
      "lif layer 1 self.abs_max_v: 7319.0\n",
      "lif layer 1 self.abs_max_v: 7382.5\n",
      "lif layer 1 self.abs_max_v: 7565.5\n",
      "lif layer 1 self.abs_max_v: 7666.0\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  1.634052/  1.843674, val:  54.17%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3954%\n",
      "layer   2  Sparsity: 75.0979%\n",
      "layer   3  Sparsity: 64.5337%\n",
      "total_backward_count 264330 real_backward_count 27997  10.592%\n",
      "fc layer 1 self.abs_max_out: 4254.0\n",
      "lif layer 1 self.abs_max_v: 7700.5\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  1.629537/  1.815694, val:  63.75%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3959%\n",
      "layer   2  Sparsity: 75.1010%\n",
      "layer   3  Sparsity: 65.4932%\n",
      "total_backward_count 274120 real_backward_count 28841  10.521%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  1.631869/  1.842599, val:  64.17%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3855%\n",
      "layer   2  Sparsity: 74.9327%\n",
      "layer   3  Sparsity: 65.9463%\n",
      "total_backward_count 283910 real_backward_count 29683  10.455%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  1.634617/  1.831776, val:  55.83%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3750%\n",
      "layer   2  Sparsity: 74.7978%\n",
      "layer   3  Sparsity: 65.4881%\n",
      "total_backward_count 293700 real_backward_count 30528  10.394%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  1.628982/  1.824860, val:  69.17%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3880%\n",
      "layer   2  Sparsity: 74.7464%\n",
      "layer   3  Sparsity: 65.3127%\n",
      "total_backward_count 303490 real_backward_count 31363  10.334%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  1.628641/  1.828197, val:  59.58%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.4041%\n",
      "layer   2  Sparsity: 74.3242%\n",
      "layer   3  Sparsity: 65.6121%\n",
      "total_backward_count 313280 real_backward_count 32141  10.260%\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  1.637577/  1.814938, val:  59.58%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3920%\n",
      "layer   2  Sparsity: 74.9844%\n",
      "layer   3  Sparsity: 66.0364%\n",
      "total_backward_count 323070 real_backward_count 32899  10.183%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  1.612714/  1.811242, val:  65.00%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.45 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.3890%\n",
      "layer   2  Sparsity: 75.6320%\n",
      "layer   3  Sparsity: 66.2503%\n",
      "total_backward_count 332860 real_backward_count 33689  10.121%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  1.608937/  1.806121, val:  63.75%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3891%\n",
      "layer   2  Sparsity: 75.1032%\n",
      "layer   3  Sparsity: 65.9180%\n",
      "total_backward_count 342650 real_backward_count 34471  10.060%\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  1.609769/  1.823852, val:  67.50%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3994%\n",
      "layer   2  Sparsity: 75.4580%\n",
      "layer   3  Sparsity: 65.7573%\n",
      "total_backward_count 352440 real_backward_count 35273  10.008%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  1.625312/  1.813651, val:  66.25%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.3834%\n",
      "layer   2  Sparsity: 75.4704%\n",
      "layer   3  Sparsity: 67.0509%\n",
      "total_backward_count 362230 real_backward_count 36077   9.960%\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  1.612808/  1.824717, val:  70.83%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3916%\n",
      "layer   2  Sparsity: 75.4149%\n",
      "layer   3  Sparsity: 67.1979%\n",
      "total_backward_count 372020 real_backward_count 36776   9.885%\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  1.620968/  1.828030, val:  61.67%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.08 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3791%\n",
      "layer   2  Sparsity: 75.0248%\n",
      "layer   3  Sparsity: 67.0373%\n",
      "total_backward_count 381810 real_backward_count 37565   9.839%\n",
      "fc layer 1 self.abs_max_out: 4272.0\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  1.616164/  1.823272, val:  68.75%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3933%\n",
      "layer   2  Sparsity: 75.1390%\n",
      "layer   3  Sparsity: 67.0149%\n",
      "total_backward_count 391600 real_backward_count 38357   9.795%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  1.626795/  1.815230, val:  65.42%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3835%\n",
      "layer   2  Sparsity: 75.1615%\n",
      "layer   3  Sparsity: 67.0988%\n",
      "total_backward_count 401390 real_backward_count 39159   9.756%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  1.611264/  1.812935, val:  65.00%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3774%\n",
      "layer   2  Sparsity: 75.2961%\n",
      "layer   3  Sparsity: 66.3811%\n",
      "total_backward_count 411180 real_backward_count 39972   9.721%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  1.607693/  1.807403, val:  68.33%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3976%\n",
      "layer   2  Sparsity: 75.3769%\n",
      "layer   3  Sparsity: 66.1977%\n",
      "total_backward_count 420970 real_backward_count 40706   9.670%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  1.619569/  1.818707, val:  66.67%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3862%\n",
      "layer   2  Sparsity: 75.3707%\n",
      "layer   3  Sparsity: 66.6525%\n",
      "total_backward_count 430760 real_backward_count 41461   9.625%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  1.612540/  1.805890, val:  65.83%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.94 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3884%\n",
      "layer   2  Sparsity: 75.5800%\n",
      "layer   3  Sparsity: 65.5988%\n",
      "total_backward_count 440550 real_backward_count 42229   9.586%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  1.604401/  1.786552, val:  63.33%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.16 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.4032%\n",
      "layer   2  Sparsity: 75.5242%\n",
      "layer   3  Sparsity: 65.6154%\n",
      "total_backward_count 450340 real_backward_count 43015   9.552%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  1.605108/  1.792458, val:  68.33%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3805%\n",
      "layer   2  Sparsity: 75.3356%\n",
      "layer   3  Sparsity: 65.9356%\n",
      "total_backward_count 460130 real_backward_count 43788   9.516%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  1.601895/  1.812887, val:  61.67%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.4013%\n",
      "layer   2  Sparsity: 75.2618%\n",
      "layer   3  Sparsity: 66.5438%\n",
      "total_backward_count 469920 real_backward_count 44506   9.471%\n",
      "fc layer 1 self.abs_max_out: 4308.0\n",
      "fc layer 1 self.abs_max_out: 4323.0\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  1.609875/  1.794464, val:  67.92%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3919%\n",
      "layer   2  Sparsity: 75.2540%\n",
      "layer   3  Sparsity: 67.1359%\n",
      "total_backward_count 479710 real_backward_count 45242   9.431%\n",
      "fc layer 1 self.abs_max_out: 4330.0\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  1.600653/  1.793335, val:  66.67%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3840%\n",
      "layer   2  Sparsity: 75.1950%\n",
      "layer   3  Sparsity: 66.9295%\n",
      "total_backward_count 489500 real_backward_count 45944   9.386%\n",
      "fc layer 2 self.abs_max_out: 1952.0\n",
      "fc layer 2 self.abs_max_out: 1986.0\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  1.602832/  1.798643, val:  63.33%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3868%\n",
      "layer   2  Sparsity: 74.9240%\n",
      "layer   3  Sparsity: 66.6646%\n",
      "total_backward_count 499290 real_backward_count 46690   9.351%\n",
      "fc layer 1 self.abs_max_out: 4341.0\n",
      "fc layer 1 self.abs_max_out: 4437.0\n",
      "fc layer 1 self.abs_max_out: 4769.0\n",
      "lif layer 1 self.abs_max_v: 7923.5\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  1.584695/  1.784155, val:  66.25%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3815%\n",
      "layer   2  Sparsity: 74.9153%\n",
      "layer   3  Sparsity: 66.1815%\n",
      "total_backward_count 509080 real_backward_count 47436   9.318%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  1.596652/  1.778903, val:  69.17%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.4090%\n",
      "layer   2  Sparsity: 75.1665%\n",
      "layer   3  Sparsity: 65.2885%\n",
      "total_backward_count 518870 real_backward_count 48174   9.284%\n",
      "fc layer 2 self.abs_max_out: 2023.0\n",
      "lif layer 1 self.abs_max_v: 8112.5\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  1.579516/  1.766141, val:  67.08%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3895%\n",
      "layer   2  Sparsity: 75.3291%\n",
      "layer   3  Sparsity: 64.9278%\n",
      "total_backward_count 528660 real_backward_count 48868   9.244%\n",
      "fc layer 1 self.abs_max_out: 4986.0\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  1.568187/  1.783671, val:  67.92%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3714%\n",
      "layer   2  Sparsity: 74.8511%\n",
      "layer   3  Sparsity: 65.7512%\n",
      "total_backward_count 538450 real_backward_count 49581   9.208%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.582726/  1.785612, val:  62.08%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3992%\n",
      "layer   2  Sparsity: 74.4004%\n",
      "layer   3  Sparsity: 65.8781%\n",
      "total_backward_count 548240 real_backward_count 50307   9.176%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.576300/  1.783152, val:  66.25%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.4035%\n",
      "layer   2  Sparsity: 74.6724%\n",
      "layer   3  Sparsity: 65.4660%\n",
      "total_backward_count 558030 real_backward_count 51006   9.140%\n",
      "lif layer 1 self.abs_max_v: 8371.0\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.557623/  1.753299, val:  62.08%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3854%\n",
      "layer   2  Sparsity: 74.1839%\n",
      "layer   3  Sparsity: 65.2061%\n",
      "total_backward_count 567820 real_backward_count 51663   9.098%\n",
      "lif layer 1 self.abs_max_v: 8422.0\n",
      "lif layer 2 self.abs_max_v: 3562.5\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.562507/  1.778269, val:  67.50%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3819%\n",
      "layer   2  Sparsity: 74.5425%\n",
      "layer   3  Sparsity: 64.3698%\n",
      "total_backward_count 577610 real_backward_count 52340   9.061%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.563510/  1.796072, val:  63.33%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3929%\n",
      "layer   2  Sparsity: 74.8665%\n",
      "layer   3  Sparsity: 64.7325%\n",
      "total_backward_count 587400 real_backward_count 52994   9.022%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.567078/  1.751311, val:  65.00%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3836%\n",
      "layer   2  Sparsity: 74.5452%\n",
      "layer   3  Sparsity: 65.6949%\n",
      "total_backward_count 597190 real_backward_count 53711   8.994%\n",
      "lif layer 2 self.abs_max_v: 3579.5\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.556439/  1.765309, val:  70.42%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3762%\n",
      "layer   2  Sparsity: 74.4779%\n",
      "layer   3  Sparsity: 65.6547%\n",
      "total_backward_count 606980 real_backward_count 54401   8.963%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.568594/  1.769300, val:  73.33%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3855%\n",
      "layer   2  Sparsity: 74.7694%\n",
      "layer   3  Sparsity: 65.4185%\n",
      "total_backward_count 616770 real_backward_count 55112   8.936%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.565605/  1.766465, val:  62.92%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.4033%\n",
      "layer   2  Sparsity: 74.8840%\n",
      "layer   3  Sparsity: 65.0984%\n",
      "total_backward_count 626560 real_backward_count 55784   8.903%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.567831/  1.773939, val:  63.33%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.3886%\n",
      "layer   2  Sparsity: 74.9900%\n",
      "layer   3  Sparsity: 65.7279%\n",
      "total_backward_count 636350 real_backward_count 56450   8.871%\n",
      "lif layer 2 self.abs_max_v: 3646.5\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.564188/  1.756928, val:  72.50%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3753%\n",
      "layer   2  Sparsity: 74.7414%\n",
      "layer   3  Sparsity: 66.1286%\n",
      "total_backward_count 646140 real_backward_count 57098   8.837%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.559620/  1.763458, val:  70.42%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3852%\n",
      "layer   2  Sparsity: 75.0114%\n",
      "layer   3  Sparsity: 65.5727%\n",
      "total_backward_count 655930 real_backward_count 57786   8.810%\n",
      "fc layer 1 self.abs_max_out: 5049.0\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.546795/  1.753894, val:  66.67%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3965%\n",
      "layer   2  Sparsity: 74.9817%\n",
      "layer   3  Sparsity: 65.3171%\n",
      "total_backward_count 665720 real_backward_count 58462   8.782%\n",
      "fc layer 1 self.abs_max_out: 5318.0\n",
      "lif layer 1 self.abs_max_v: 8738.5\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.548718/  1.740443, val:  70.42%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3936%\n",
      "layer   2  Sparsity: 74.6994%\n",
      "layer   3  Sparsity: 65.5398%\n",
      "total_backward_count 675510 real_backward_count 59147   8.756%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.548988/  1.765418, val:  65.00%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.47 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.3819%\n",
      "layer   2  Sparsity: 74.5945%\n",
      "layer   3  Sparsity: 64.9787%\n",
      "total_backward_count 685300 real_backward_count 59819   8.729%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.555900/  1.754499, val:  60.42%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.97 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3796%\n",
      "layer   2  Sparsity: 74.7793%\n",
      "layer   3  Sparsity: 64.3057%\n",
      "total_backward_count 695090 real_backward_count 60498   8.704%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.545043/  1.751142, val:  64.17%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3730%\n",
      "layer   2  Sparsity: 75.0481%\n",
      "layer   3  Sparsity: 64.7040%\n",
      "total_backward_count 704880 real_backward_count 61151   8.675%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.541507/  1.736718, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3955%\n",
      "layer   2  Sparsity: 75.4797%\n",
      "layer   3  Sparsity: 65.1014%\n",
      "total_backward_count 714670 real_backward_count 61844   8.654%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.532987/  1.743654, val:  66.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.4019%\n",
      "layer   2  Sparsity: 75.0870%\n",
      "layer   3  Sparsity: 64.9555%\n",
      "total_backward_count 724460 real_backward_count 62464   8.622%\n",
      "fc layer 2 self.abs_max_out: 2033.0\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.542913/  1.731780, val:  77.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3647%\n",
      "layer   2  Sparsity: 74.8349%\n",
      "layer   3  Sparsity: 64.6860%\n",
      "total_backward_count 734250 real_backward_count 63153   8.601%\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.531069/  1.730891, val:  73.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.4008%\n",
      "layer   2  Sparsity: 74.4523%\n",
      "layer   3  Sparsity: 64.3210%\n",
      "total_backward_count 744040 real_backward_count 63787   8.573%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.535548/  1.741029, val:  71.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3739%\n",
      "layer   2  Sparsity: 74.3740%\n",
      "layer   3  Sparsity: 63.9554%\n",
      "total_backward_count 753830 real_backward_count 64427   8.547%\n",
      "fc layer 2 self.abs_max_out: 2092.0\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.539705/  1.774459, val:  55.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.4099%\n",
      "layer   2  Sparsity: 74.1397%\n",
      "layer   3  Sparsity: 63.9354%\n",
      "total_backward_count 763620 real_backward_count 65064   8.520%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.539657/  1.744560, val:  73.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3927%\n",
      "layer   2  Sparsity: 74.1239%\n",
      "layer   3  Sparsity: 64.7978%\n",
      "total_backward_count 773410 real_backward_count 65747   8.501%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.534794/  1.742592, val:  72.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3911%\n",
      "layer   2  Sparsity: 74.0932%\n",
      "layer   3  Sparsity: 65.0993%\n",
      "total_backward_count 783200 real_backward_count 66423   8.481%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.525445/  1.720248, val:  71.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3935%\n",
      "layer   2  Sparsity: 74.0915%\n",
      "layer   3  Sparsity: 65.2167%\n",
      "total_backward_count 792990 real_backward_count 67071   8.458%\n",
      "fc layer 3 self.abs_max_out: 940.0\n",
      "fc layer 3 self.abs_max_out: 947.0\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.537508/  1.743966, val:  66.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3885%\n",
      "layer   2  Sparsity: 74.0430%\n",
      "layer   3  Sparsity: 65.5676%\n",
      "total_backward_count 802780 real_backward_count 67695   8.433%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.528606/  1.736714, val:  70.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.3679%\n",
      "layer   2  Sparsity: 74.1401%\n",
      "layer   3  Sparsity: 65.2589%\n",
      "total_backward_count 812570 real_backward_count 68339   8.410%\n",
      "fc layer 3 self.abs_max_out: 986.0\n",
      "fc layer 3 self.abs_max_out: 1043.0\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.513284/  1.721253, val:  64.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.3896%\n",
      "layer   2  Sparsity: 74.0092%\n",
      "layer   3  Sparsity: 65.3608%\n",
      "total_backward_count 822360 real_backward_count 68995   8.390%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.510590/  1.703828, val:  71.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3949%\n",
      "layer   2  Sparsity: 74.0763%\n",
      "layer   3  Sparsity: 63.8506%\n",
      "total_backward_count 832150 real_backward_count 69617   8.366%\n",
      "lif layer 1 self.abs_max_v: 8858.5\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.516171/  1.733802, val:  75.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3876%\n",
      "layer   2  Sparsity: 74.0144%\n",
      "layer   3  Sparsity: 64.7571%\n",
      "total_backward_count 841940 real_backward_count 70262   8.345%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.516921/  1.719972, val:  72.92%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3851%\n",
      "layer   2  Sparsity: 74.1591%\n",
      "layer   3  Sparsity: 64.8660%\n",
      "total_backward_count 851730 real_backward_count 70922   8.327%\n",
      "fc layer 2 self.abs_max_out: 2122.0\n",
      "lif layer 2 self.abs_max_v: 3872.5\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.518006/  1.740992, val:  64.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3693%\n",
      "layer   2  Sparsity: 73.8416%\n",
      "layer   3  Sparsity: 64.7021%\n",
      "total_backward_count 861520 real_backward_count 71569   8.307%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.537322/  1.740019, val:  68.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.3777%\n",
      "layer   2  Sparsity: 73.5241%\n",
      "layer   3  Sparsity: 65.0199%\n",
      "total_backward_count 871310 real_backward_count 72270   8.294%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.518247/  1.715893, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.4131%\n",
      "layer   2  Sparsity: 73.6027%\n",
      "layer   3  Sparsity: 65.0282%\n",
      "total_backward_count 881100 real_backward_count 72920   8.276%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.522670/  1.734861, val:  76.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3753%\n",
      "layer   2  Sparsity: 73.4247%\n",
      "layer   3  Sparsity: 65.1980%\n",
      "total_backward_count 890890 real_backward_count 73569   8.258%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.517141/  1.731080, val:  68.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.90 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3817%\n",
      "layer   2  Sparsity: 73.9849%\n",
      "layer   3  Sparsity: 65.0425%\n",
      "total_backward_count 900680 real_backward_count 74183   8.236%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.505382/  1.719384, val:  73.33%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3873%\n",
      "layer   2  Sparsity: 73.7874%\n",
      "layer   3  Sparsity: 65.1322%\n",
      "total_backward_count 910470 real_backward_count 74810   8.217%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.502614/  1.723621, val:  73.75%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.3942%\n",
      "layer   2  Sparsity: 73.5125%\n",
      "layer   3  Sparsity: 64.7797%\n",
      "total_backward_count 920260 real_backward_count 75459   8.200%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.508623/  1.717384, val:  66.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.3913%\n",
      "layer   2  Sparsity: 73.6433%\n",
      "layer   3  Sparsity: 64.9132%\n",
      "total_backward_count 930050 real_backward_count 76114   8.184%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.503123/  1.720945, val:  67.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3876%\n",
      "layer   2  Sparsity: 74.1015%\n",
      "layer   3  Sparsity: 64.3274%\n",
      "total_backward_count 939840 real_backward_count 76742   8.165%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.496579/  1.721268, val:  64.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3832%\n",
      "layer   2  Sparsity: 74.4374%\n",
      "layer   3  Sparsity: 64.8161%\n",
      "total_backward_count 949630 real_backward_count 77346   8.145%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.493179/  1.713345, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3996%\n",
      "layer   2  Sparsity: 74.5588%\n",
      "layer   3  Sparsity: 64.3064%\n",
      "total_backward_count 959420 real_backward_count 77949   8.125%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.499059/  1.717265, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3948%\n",
      "layer   2  Sparsity: 73.9057%\n",
      "layer   3  Sparsity: 63.9560%\n",
      "total_backward_count 969210 real_backward_count 78553   8.105%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.497590/  1.704071, val:  70.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3872%\n",
      "layer   2  Sparsity: 73.9774%\n",
      "layer   3  Sparsity: 63.7296%\n",
      "total_backward_count 979000 real_backward_count 79168   8.087%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.498855/  1.710328, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3813%\n",
      "layer   2  Sparsity: 73.7755%\n",
      "layer   3  Sparsity: 63.5215%\n",
      "total_backward_count 988790 real_backward_count 79778   8.068%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.498478/  1.715939, val:  70.00%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3954%\n",
      "layer   2  Sparsity: 73.9875%\n",
      "layer   3  Sparsity: 63.7693%\n",
      "total_backward_count 998580 real_backward_count 80409   8.052%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.504323/  1.714011, val:  70.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3906%\n",
      "layer   2  Sparsity: 74.5686%\n",
      "layer   3  Sparsity: 64.4093%\n",
      "total_backward_count 1008370 real_backward_count 81027   8.035%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.496924/  1.722763, val:  78.33%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3872%\n",
      "layer   2  Sparsity: 74.1995%\n",
      "layer   3  Sparsity: 64.8774%\n",
      "total_backward_count 1018160 real_backward_count 81625   8.017%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.516368/  1.722322, val:  68.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3894%\n",
      "layer   2  Sparsity: 73.9443%\n",
      "layer   3  Sparsity: 64.4717%\n",
      "total_backward_count 1027950 real_backward_count 82206   7.997%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.494317/  1.697228, val:  73.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.4006%\n",
      "layer   2  Sparsity: 74.0763%\n",
      "layer   3  Sparsity: 64.2451%\n",
      "total_backward_count 1037740 real_backward_count 82784   7.977%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.490009/  1.706145, val:  70.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.4069%\n",
      "layer   2  Sparsity: 73.8839%\n",
      "layer   3  Sparsity: 64.2316%\n",
      "total_backward_count 1047530 real_backward_count 83372   7.959%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.491499/  1.707238, val:  63.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3901%\n",
      "layer   2  Sparsity: 73.8476%\n",
      "layer   3  Sparsity: 65.4077%\n",
      "total_backward_count 1057320 real_backward_count 83978   7.943%\n",
      "fc layer 1 self.abs_max_out: 5320.0\n",
      "lif layer 2 self.abs_max_v: 3893.5\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.481661/  1.687080, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3873%\n",
      "layer   2  Sparsity: 73.8997%\n",
      "layer   3  Sparsity: 65.3001%\n",
      "total_backward_count 1067110 real_backward_count 84542   7.923%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.486319/  1.687088, val:  71.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3690%\n",
      "layer   2  Sparsity: 73.4366%\n",
      "layer   3  Sparsity: 65.6325%\n",
      "total_backward_count 1076900 real_backward_count 85167   7.909%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.475616/  1.682645, val:  74.58%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3700%\n",
      "layer   2  Sparsity: 73.9248%\n",
      "layer   3  Sparsity: 64.6064%\n",
      "total_backward_count 1086690 real_backward_count 85781   7.894%\n",
      "fc layer 1 self.abs_max_out: 5350.0\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.471333/  1.704293, val:  64.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3918%\n",
      "layer   2  Sparsity: 73.9779%\n",
      "layer   3  Sparsity: 64.8512%\n",
      "total_backward_count 1096480 real_backward_count 86343   7.875%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.470219/  1.668399, val:  72.92%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3879%\n",
      "layer   2  Sparsity: 73.7068%\n",
      "layer   3  Sparsity: 64.7779%\n",
      "total_backward_count 1106270 real_backward_count 86935   7.858%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.460435/  1.687014, val:  62.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.4020%\n",
      "layer   2  Sparsity: 73.7878%\n",
      "layer   3  Sparsity: 64.1646%\n",
      "total_backward_count 1116060 real_backward_count 87516   7.842%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.468364/  1.686740, val:  72.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.4018%\n",
      "layer   2  Sparsity: 74.1277%\n",
      "layer   3  Sparsity: 63.9153%\n",
      "total_backward_count 1125850 real_backward_count 88094   7.825%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.470301/  1.691972, val:  67.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3774%\n",
      "layer   2  Sparsity: 74.4399%\n",
      "layer   3  Sparsity: 63.7478%\n",
      "total_backward_count 1135640 real_backward_count 88671   7.808%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.471859/  1.689276, val:  70.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3960%\n",
      "layer   2  Sparsity: 74.7339%\n",
      "layer   3  Sparsity: 63.8112%\n",
      "total_backward_count 1145430 real_backward_count 89251   7.792%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.477525/  1.683951, val:  71.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3954%\n",
      "layer   2  Sparsity: 73.6830%\n",
      "layer   3  Sparsity: 63.9130%\n",
      "total_backward_count 1155220 real_backward_count 89829   7.776%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.474475/  1.698786, val:  69.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3689%\n",
      "layer   2  Sparsity: 73.5442%\n",
      "layer   3  Sparsity: 64.3565%\n",
      "total_backward_count 1165010 real_backward_count 90406   7.760%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.471899/  1.706279, val:  73.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.4020%\n",
      "layer   2  Sparsity: 73.7670%\n",
      "layer   3  Sparsity: 64.1275%\n",
      "total_backward_count 1174800 real_backward_count 90990   7.745%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.477314/  1.707399, val:  70.00%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3917%\n",
      "layer   2  Sparsity: 74.0089%\n",
      "layer   3  Sparsity: 64.3101%\n",
      "total_backward_count 1184590 real_backward_count 91516   7.726%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.466402/  1.695282, val:  65.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3951%\n",
      "layer   2  Sparsity: 74.1983%\n",
      "layer   3  Sparsity: 64.5895%\n",
      "total_backward_count 1194380 real_backward_count 92101   7.711%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.465742/  1.682434, val:  75.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3934%\n",
      "layer   2  Sparsity: 74.2047%\n",
      "layer   3  Sparsity: 64.7317%\n",
      "total_backward_count 1204170 real_backward_count 92688   7.697%\n",
      "fc layer 1 self.abs_max_out: 5423.0\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.464071/  1.685363, val:  74.17%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3958%\n",
      "layer   2  Sparsity: 73.5277%\n",
      "layer   3  Sparsity: 64.5340%\n",
      "total_backward_count 1213960 real_backward_count 93294   7.685%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.474257/  1.677798, val:  74.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3878%\n",
      "layer   2  Sparsity: 73.3506%\n",
      "layer   3  Sparsity: 64.9299%\n",
      "total_backward_count 1223750 real_backward_count 93881   7.672%\n",
      "fc layer 1 self.abs_max_out: 5444.0\n",
      "fc layer 1 self.abs_max_out: 5686.0\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.467169/  1.681700, val:  68.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3880%\n",
      "layer   2  Sparsity: 73.0249%\n",
      "layer   3  Sparsity: 64.4698%\n",
      "total_backward_count 1233540 real_backward_count 94451   7.657%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.461926/  1.694975, val:  71.25%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.4007%\n",
      "layer   2  Sparsity: 73.2182%\n",
      "layer   3  Sparsity: 64.7855%\n",
      "total_backward_count 1243330 real_backward_count 95044   7.644%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.463422/  1.695496, val:  68.75%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3997%\n",
      "layer   2  Sparsity: 73.4105%\n",
      "layer   3  Sparsity: 64.5220%\n",
      "total_backward_count 1253120 real_backward_count 95639   7.632%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.453514/  1.672346, val:  72.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.3831%\n",
      "layer   2  Sparsity: 73.7882%\n",
      "layer   3  Sparsity: 63.8370%\n",
      "total_backward_count 1262910 real_backward_count 96208   7.618%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.458346/  1.689401, val:  75.83%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3878%\n",
      "layer   2  Sparsity: 73.2663%\n",
      "layer   3  Sparsity: 64.1546%\n",
      "total_backward_count 1272700 real_backward_count 96795   7.605%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.464897/  1.697543, val:  69.17%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3837%\n",
      "layer   2  Sparsity: 73.5069%\n",
      "layer   3  Sparsity: 64.1911%\n",
      "total_backward_count 1282490 real_backward_count 97419   7.596%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.464642/  1.686857, val:  69.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3860%\n",
      "layer   2  Sparsity: 73.4852%\n",
      "layer   3  Sparsity: 64.5230%\n",
      "total_backward_count 1292280 real_backward_count 98040   7.587%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.460645/  1.684581, val:  79.58%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3821%\n",
      "layer   2  Sparsity: 73.1660%\n",
      "layer   3  Sparsity: 64.7200%\n",
      "total_backward_count 1302070 real_backward_count 98619   7.574%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.465806/  1.688558, val:  64.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3919%\n",
      "layer   2  Sparsity: 73.3533%\n",
      "layer   3  Sparsity: 64.9489%\n",
      "total_backward_count 1311860 real_backward_count 99172   7.560%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.468564/  1.688110, val:  62.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.4084%\n",
      "layer   2  Sparsity: 73.6631%\n",
      "layer   3  Sparsity: 64.5725%\n",
      "total_backward_count 1321650 real_backward_count 99770   7.549%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.464623/  1.688534, val:  72.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.4042%\n",
      "layer   2  Sparsity: 73.2815%\n",
      "layer   3  Sparsity: 64.5409%\n",
      "total_backward_count 1331440 real_backward_count 100335   7.536%\n",
      "lif layer 1 self.abs_max_v: 9029.0\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.465057/  1.680560, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3788%\n",
      "layer   2  Sparsity: 72.8797%\n",
      "layer   3  Sparsity: 63.9391%\n",
      "total_backward_count 1341230 real_backward_count 100910   7.524%\n",
      "lif layer 1 self.abs_max_v: 9105.0\n",
      "lif layer 1 self.abs_max_v: 9470.0\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.481299/  1.686831, val:  74.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3972%\n",
      "layer   2  Sparsity: 72.8001%\n",
      "layer   3  Sparsity: 64.6436%\n",
      "total_backward_count 1351020 real_backward_count 101473   7.511%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.485892/  1.707634, val:  69.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3983%\n",
      "layer   2  Sparsity: 72.9134%\n",
      "layer   3  Sparsity: 64.8673%\n",
      "total_backward_count 1360810 real_backward_count 102018   7.497%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.493307/  1.708786, val:  74.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3711%\n",
      "layer   2  Sparsity: 73.3160%\n",
      "layer   3  Sparsity: 65.3253%\n",
      "total_backward_count 1370600 real_backward_count 102621   7.487%\n",
      "lif layer 1 self.abs_max_v: 9537.5\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.487608/  1.694526, val:  72.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3985%\n",
      "layer   2  Sparsity: 73.5868%\n",
      "layer   3  Sparsity: 64.7463%\n",
      "total_backward_count 1380390 real_backward_count 103181   7.475%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.470779/  1.682558, val:  69.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3799%\n",
      "layer   2  Sparsity: 73.5193%\n",
      "layer   3  Sparsity: 65.0521%\n",
      "total_backward_count 1390180 real_backward_count 103718   7.461%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.459825/  1.666283, val:  73.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3870%\n",
      "layer   2  Sparsity: 73.3576%\n",
      "layer   3  Sparsity: 64.7183%\n",
      "total_backward_count 1399970 real_backward_count 104305   7.451%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.466012/  1.693336, val:  65.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3864%\n",
      "layer   2  Sparsity: 73.0255%\n",
      "layer   3  Sparsity: 64.6003%\n",
      "total_backward_count 1409760 real_backward_count 104868   7.439%\n",
      "lif layer 1 self.abs_max_v: 9621.0\n",
      "lif layer 1 self.abs_max_v: 9750.0\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.477772/  1.702899, val:  64.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3844%\n",
      "layer   2  Sparsity: 73.0435%\n",
      "layer   3  Sparsity: 65.0961%\n",
      "total_backward_count 1419550 real_backward_count 105433   7.427%\n",
      "lif layer 1 self.abs_max_v: 10036.0\n",
      "lif layer 1 self.abs_max_v: 10092.0\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.479610/  1.703030, val:  62.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.10 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.3732%\n",
      "layer   2  Sparsity: 72.9856%\n",
      "layer   3  Sparsity: 64.5024%\n",
      "total_backward_count 1429340 real_backward_count 105938   7.412%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.484496/  1.685961, val:  75.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3872%\n",
      "layer   2  Sparsity: 73.0852%\n",
      "layer   3  Sparsity: 63.6540%\n",
      "total_backward_count 1439130 real_backward_count 106466   7.398%\n",
      "lif layer 1 self.abs_max_v: 10324.0\n",
      "fc layer 1 self.abs_max_out: 5710.0\n",
      "lif layer 1 self.abs_max_v: 10361.5\n",
      "fc layer 1 self.abs_max_out: 5801.0\n",
      "fc layer 1 self.abs_max_out: 6185.0\n",
      "lif layer 1 self.abs_max_v: 10442.5\n",
      "lif layer 1 self.abs_max_v: 10672.0\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.462341/  1.679959, val:  70.42%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.3812%\n",
      "layer   2  Sparsity: 73.1582%\n",
      "layer   3  Sparsity: 63.7683%\n",
      "total_backward_count 1448920 real_backward_count 106987   7.384%\n",
      "lif layer 1 self.abs_max_v: 10974.5\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.457029/  1.684096, val:  77.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3828%\n",
      "layer   2  Sparsity: 73.8160%\n",
      "layer   3  Sparsity: 63.5959%\n",
      "total_backward_count 1458710 real_backward_count 107547   7.373%\n",
      "fc layer 1 self.abs_max_out: 6312.0\n",
      "lif layer 1 self.abs_max_v: 11224.5\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.451383/  1.703292, val:  69.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3904%\n",
      "layer   2  Sparsity: 73.5646%\n",
      "layer   3  Sparsity: 64.2617%\n",
      "total_backward_count 1468500 real_backward_count 108089   7.361%\n",
      "lif layer 1 self.abs_max_v: 11258.0\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.467166/  1.681162, val:  73.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3909%\n",
      "layer   2  Sparsity: 72.6859%\n",
      "layer   3  Sparsity: 63.7850%\n",
      "total_backward_count 1478290 real_backward_count 108628   7.348%\n",
      "fc layer 1 self.abs_max_out: 6368.0\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.472967/  1.692162, val:  70.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.3946%\n",
      "layer   2  Sparsity: 72.9783%\n",
      "layer   3  Sparsity: 62.9415%\n",
      "total_backward_count 1488080 real_backward_count 109166   7.336%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.480046/  1.707393, val:  70.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.3942%\n",
      "layer   2  Sparsity: 72.9344%\n",
      "layer   3  Sparsity: 63.1813%\n",
      "total_backward_count 1497870 real_backward_count 109706   7.324%\n",
      "lif layer 2 self.abs_max_v: 3931.5\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.475028/  1.676787, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3980%\n",
      "layer   2  Sparsity: 73.0327%\n",
      "layer   3  Sparsity: 62.3555%\n",
      "total_backward_count 1507660 real_backward_count 110276   7.314%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.455187/  1.694890, val:  70.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3934%\n",
      "layer   2  Sparsity: 72.7652%\n",
      "layer   3  Sparsity: 63.2174%\n",
      "total_backward_count 1517450 real_backward_count 110823   7.303%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.455792/  1.684579, val:  73.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3828%\n",
      "layer   2  Sparsity: 72.7023%\n",
      "layer   3  Sparsity: 63.3552%\n",
      "total_backward_count 1527240 real_backward_count 111381   7.293%\n",
      "fc layer 1 self.abs_max_out: 6435.0\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.454032/  1.697225, val:  72.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3903%\n",
      "layer   2  Sparsity: 72.9742%\n",
      "layer   3  Sparsity: 63.2600%\n",
      "total_backward_count 1537030 real_backward_count 111876   7.279%\n",
      "fc layer 1 self.abs_max_out: 6557.0\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.459392/  1.686442, val:  72.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.3883%\n",
      "layer   2  Sparsity: 72.9769%\n",
      "layer   3  Sparsity: 63.6955%\n",
      "total_backward_count 1546820 real_backward_count 112419   7.268%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.462792/  1.688415, val:  71.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3958%\n",
      "layer   2  Sparsity: 72.8756%\n",
      "layer   3  Sparsity: 64.4811%\n",
      "total_backward_count 1556610 real_backward_count 112960   7.257%\n",
      "lif layer 1 self.abs_max_v: 11397.0\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.465976/  1.673993, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.18 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.3999%\n",
      "layer   2  Sparsity: 73.2137%\n",
      "layer   3  Sparsity: 64.0587%\n",
      "total_backward_count 1566400 real_backward_count 113487   7.245%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.455953/  1.680128, val:  67.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.4023%\n",
      "layer   2  Sparsity: 73.4960%\n",
      "layer   3  Sparsity: 63.6898%\n",
      "total_backward_count 1576190 real_backward_count 114046   7.236%\n",
      "lif layer 1 self.abs_max_v: 11458.0\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.460417/  1.681072, val:  73.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3940%\n",
      "layer   2  Sparsity: 73.0403%\n",
      "layer   3  Sparsity: 64.2397%\n",
      "total_backward_count 1585980 real_backward_count 114622   7.227%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.460049/  1.684852, val:  65.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3822%\n",
      "layer   2  Sparsity: 73.0408%\n",
      "layer   3  Sparsity: 64.3591%\n",
      "total_backward_count 1595770 real_backward_count 115200   7.219%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.450533/  1.672598, val:  70.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.4021%\n",
      "layer   2  Sparsity: 73.4699%\n",
      "layer   3  Sparsity: 64.5360%\n",
      "total_backward_count 1605560 real_backward_count 115727   7.208%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.445037/  1.658003, val:  67.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.4080%\n",
      "layer   2  Sparsity: 73.7847%\n",
      "layer   3  Sparsity: 64.8806%\n",
      "total_backward_count 1615350 real_backward_count 116250   7.197%\n",
      "lif layer 1 self.abs_max_v: 11601.5\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.457905/  1.688670, val:  69.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3749%\n",
      "layer   2  Sparsity: 73.7887%\n",
      "layer   3  Sparsity: 64.5065%\n",
      "total_backward_count 1625140 real_backward_count 116784   7.186%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.456695/  1.712047, val:  66.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3824%\n",
      "layer   2  Sparsity: 73.4670%\n",
      "layer   3  Sparsity: 64.2211%\n",
      "total_backward_count 1634930 real_backward_count 117351   7.178%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.463141/  1.693164, val:  62.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.08 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3911%\n",
      "layer   2  Sparsity: 73.0298%\n",
      "layer   3  Sparsity: 64.5430%\n",
      "total_backward_count 1644720 real_backward_count 117886   7.168%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.457886/  1.660999, val:  74.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3983%\n",
      "layer   2  Sparsity: 73.6230%\n",
      "layer   3  Sparsity: 64.7130%\n",
      "total_backward_count 1654510 real_backward_count 118456   7.160%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.453485/  1.651934, val:  75.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3894%\n",
      "layer   2  Sparsity: 73.9136%\n",
      "layer   3  Sparsity: 64.1211%\n",
      "total_backward_count 1664300 real_backward_count 118982   7.149%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.437797/  1.687309, val:  64.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 87.4115%\n",
      "layer   2  Sparsity: 73.8880%\n",
      "layer   3  Sparsity: 63.8057%\n",
      "total_backward_count 1674090 real_backward_count 119511   7.139%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.456566/  1.687803, val:  64.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3804%\n",
      "layer   2  Sparsity: 73.4995%\n",
      "layer   3  Sparsity: 64.6038%\n",
      "total_backward_count 1683880 real_backward_count 120017   7.127%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.456748/  1.671000, val:  71.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3683%\n",
      "layer   2  Sparsity: 73.5730%\n",
      "layer   3  Sparsity: 64.8500%\n",
      "total_backward_count 1693670 real_backward_count 120564   7.119%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.446637/  1.647508, val:  72.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3873%\n",
      "layer   2  Sparsity: 73.5082%\n",
      "layer   3  Sparsity: 64.2751%\n",
      "total_backward_count 1703460 real_backward_count 121108   7.110%\n",
      "lif layer 1 self.abs_max_v: 11769.0\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.434395/  1.648942, val:  78.75%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.3879%\n",
      "layer   2  Sparsity: 73.7664%\n",
      "layer   3  Sparsity: 63.9489%\n",
      "total_backward_count 1713250 real_backward_count 121653   7.101%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.434833/  1.647565, val:  80.83%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.4027%\n",
      "layer   2  Sparsity: 73.8087%\n",
      "layer   3  Sparsity: 64.2784%\n",
      "total_backward_count 1723040 real_backward_count 122149   7.089%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.456380/  1.660606, val:  73.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3920%\n",
      "layer   2  Sparsity: 73.5779%\n",
      "layer   3  Sparsity: 64.2678%\n",
      "total_backward_count 1732830 real_backward_count 122694   7.081%\n",
      "fc layer 2 self.abs_max_out: 2136.0\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.450173/  1.681875, val:  70.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3723%\n",
      "layer   2  Sparsity: 73.5639%\n",
      "layer   3  Sparsity: 63.4057%\n",
      "total_backward_count 1742620 real_backward_count 123201   7.070%\n",
      "lif layer 1 self.abs_max_v: 12021.0\n",
      "fc layer 2 self.abs_max_out: 2145.0\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.444624/  1.653405, val:  72.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3745%\n",
      "layer   2  Sparsity: 74.5340%\n",
      "layer   3  Sparsity: 63.6944%\n",
      "total_backward_count 1752410 real_backward_count 123717   7.060%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.442120/  1.664263, val:  75.00%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.4246%\n",
      "layer   2  Sparsity: 74.4965%\n",
      "layer   3  Sparsity: 63.6249%\n",
      "total_backward_count 1762200 real_backward_count 124251   7.051%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.442744/  1.652870, val:  70.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3861%\n",
      "layer   2  Sparsity: 73.9814%\n",
      "layer   3  Sparsity: 63.3085%\n",
      "total_backward_count 1771990 real_backward_count 124750   7.040%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.445300/  1.673342, val:  72.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3892%\n",
      "layer   2  Sparsity: 73.8535%\n",
      "layer   3  Sparsity: 63.7347%\n",
      "total_backward_count 1781780 real_backward_count 125233   7.029%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.457805/  1.670270, val:  77.92%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3924%\n",
      "layer   2  Sparsity: 73.5956%\n",
      "layer   3  Sparsity: 64.4216%\n",
      "total_backward_count 1791570 real_backward_count 125767   7.020%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.443414/  1.678749, val:  62.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3792%\n",
      "layer   2  Sparsity: 73.8323%\n",
      "layer   3  Sparsity: 64.0637%\n",
      "total_backward_count 1801360 real_backward_count 126261   7.009%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.434865/  1.670416, val:  72.08%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3802%\n",
      "layer   2  Sparsity: 73.9318%\n",
      "layer   3  Sparsity: 64.3662%\n",
      "total_backward_count 1811150 real_backward_count 126777   7.000%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.432116/  1.655567, val:  72.92%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3810%\n",
      "layer   2  Sparsity: 74.1834%\n",
      "layer   3  Sparsity: 63.7224%\n",
      "total_backward_count 1820940 real_backward_count 127321   6.992%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.432246/  1.657117, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3801%\n",
      "layer   2  Sparsity: 73.7842%\n",
      "layer   3  Sparsity: 63.8784%\n",
      "total_backward_count 1830730 real_backward_count 127821   6.982%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.431827/  1.645236, val:  72.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3679%\n",
      "layer   2  Sparsity: 73.6502%\n",
      "layer   3  Sparsity: 63.7715%\n",
      "total_backward_count 1840520 real_backward_count 128387   6.976%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.417964/  1.638554, val:  75.42%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3870%\n",
      "layer   2  Sparsity: 74.0461%\n",
      "layer   3  Sparsity: 64.3315%\n",
      "total_backward_count 1850310 real_backward_count 128978   6.971%\n",
      "fc layer 2 self.abs_max_out: 2151.0\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.436569/  1.668739, val:  65.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3881%\n",
      "layer   2  Sparsity: 74.0150%\n",
      "layer   3  Sparsity: 64.3310%\n",
      "total_backward_count 1860100 real_backward_count 129493   6.962%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.438191/  1.650525, val:  72.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3758%\n",
      "layer   2  Sparsity: 73.5883%\n",
      "layer   3  Sparsity: 64.3023%\n",
      "total_backward_count 1869890 real_backward_count 129958   6.950%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.446050/  1.668835, val:  59.58%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3922%\n",
      "layer   2  Sparsity: 73.5646%\n",
      "layer   3  Sparsity: 64.4719%\n",
      "total_backward_count 1879680 real_backward_count 130491   6.942%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.439495/  1.660329, val:  77.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3574%\n",
      "layer   2  Sparsity: 73.5948%\n",
      "layer   3  Sparsity: 64.8495%\n",
      "total_backward_count 1889470 real_backward_count 130991   6.933%\n",
      "fc layer 2 self.abs_max_out: 2278.0\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.441013/  1.677915, val:  77.08%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.80 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3956%\n",
      "layer   2  Sparsity: 73.6876%\n",
      "layer   3  Sparsity: 64.7484%\n",
      "total_backward_count 1899260 real_backward_count 131472   6.922%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.451564/  1.664560, val:  64.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.3826%\n",
      "layer   2  Sparsity: 73.7619%\n",
      "layer   3  Sparsity: 63.7685%\n",
      "total_backward_count 1909050 real_backward_count 131983   6.914%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.433241/  1.664667, val:  65.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3805%\n",
      "layer   2  Sparsity: 74.1878%\n",
      "layer   3  Sparsity: 63.2235%\n",
      "total_backward_count 1918840 real_backward_count 132474   6.904%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.440098/  1.657091, val:  72.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3999%\n",
      "layer   2  Sparsity: 74.0027%\n",
      "layer   3  Sparsity: 63.1702%\n",
      "total_backward_count 1928630 real_backward_count 132954   6.894%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.444406/  1.652375, val:  75.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.3863%\n",
      "layer   2  Sparsity: 73.8287%\n",
      "layer   3  Sparsity: 63.2742%\n",
      "total_backward_count 1938420 real_backward_count 133461   6.885%\n",
      "fc layer 1 self.abs_max_out: 6723.0\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.444977/  1.675537, val:  69.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.3934%\n",
      "layer   2  Sparsity: 73.7066%\n",
      "layer   3  Sparsity: 63.3144%\n",
      "total_backward_count 1948210 real_backward_count 133970   6.877%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.447900/  1.669567, val:  67.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.3814%\n",
      "layer   2  Sparsity: 73.3418%\n",
      "layer   3  Sparsity: 63.9115%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b3ad46195444afacefd6886dc5b91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñá‚ñà‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.4479</td></tr><tr><td>val_acc_best</td><td>0.825</td></tr><tr><td>val_acc_now</td><td>0.67083</td></tr><tr><td>val_loss</td><td>1.66957</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vocal-sweep-64</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nckcemrn' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nckcemrn</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_060616-nckcemrn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vp4sykfl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_103156-vp4sykfl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vp4sykfl' target=\"_blank\">ruby-sweep-68</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vp4sykfl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vp4sykfl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251115_103205_385', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 8, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 379.0\n",
      "lif layer 1 self.abs_max_v: 379.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 382.0\n",
      "lif layer 1 self.abs_max_v: 528.0\n",
      "fc layer 2 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "lif layer 1 self.abs_max_v: 547.0\n",
      "fc layer 1 self.abs_max_out: 392.0\n",
      "lif layer 1 self.abs_max_v: 665.5\n",
      "lif layer 2 self.abs_max_v: 74.5\n",
      "lif layer 2 self.abs_max_v: 102.0\n",
      "fc layer 1 self.abs_max_out: 522.0\n",
      "lif layer 2 self.abs_max_v: 118.0\n",
      "fc layer 1 self.abs_max_out: 639.0\n",
      "lif layer 1 self.abs_max_v: 872.5\n",
      "fc layer 2 self.abs_max_out: 247.0\n",
      "lif layer 2 self.abs_max_v: 269.0\n",
      "lif layer 1 self.abs_max_v: 894.5\n",
      "fc layer 2 self.abs_max_out: 257.0\n",
      "lif layer 2 self.abs_max_v: 313.0\n",
      "lif layer 2 self.abs_max_v: 324.0\n",
      "fc layer 2 self.abs_max_out: 258.0\n",
      "lif layer 2 self.abs_max_v: 350.5\n",
      "lif layer 2 self.abs_max_v: 381.0\n",
      "fc layer 1 self.abs_max_out: 741.0\n",
      "lif layer 1 self.abs_max_v: 917.0\n",
      "fc layer 2 self.abs_max_out: 380.0\n",
      "lif layer 2 self.abs_max_v: 460.0\n",
      "fc layer 1 self.abs_max_out: 774.0\n",
      "fc layer 1 self.abs_max_out: 797.0\n",
      "lif layer 1 self.abs_max_v: 920.5\n",
      "fc layer 2 self.abs_max_out: 406.0\n",
      "lif layer 2 self.abs_max_v: 489.5\n",
      "fc layer 1 self.abs_max_out: 984.0\n",
      "lif layer 1 self.abs_max_v: 984.0\n",
      "lif layer 2 self.abs_max_v: 510.5\n",
      "lif layer 2 self.abs_max_v: 593.0\n",
      "fc layer 3 self.abs_max_out: 69.0\n",
      "lif layer 2 self.abs_max_v: 596.5\n",
      "lif layer 1 self.abs_max_v: 1055.5\n",
      "fc layer 2 self.abs_max_out: 457.0\n",
      "lif layer 2 self.abs_max_v: 652.5\n",
      "fc layer 1 self.abs_max_out: 1037.0\n",
      "fc layer 1 self.abs_max_out: 1272.0\n",
      "lif layer 1 self.abs_max_v: 1321.0\n",
      "lif layer 2 self.abs_max_v: 673.0\n",
      "fc layer 2 self.abs_max_out: 475.0\n",
      "lif layer 2 self.abs_max_v: 724.0\n",
      "lif layer 1 self.abs_max_v: 1336.0\n",
      "fc layer 2 self.abs_max_out: 493.0\n",
      "lif layer 2 self.abs_max_v: 745.0\n",
      "fc layer 2 self.abs_max_out: 623.0\n",
      "lif layer 2 self.abs_max_v: 801.0\n",
      "lif layer 2 self.abs_max_v: 874.5\n",
      "lif layer 2 self.abs_max_v: 1004.5\n",
      "lif layer 2 self.abs_max_v: 1066.5\n",
      "fc layer 1 self.abs_max_out: 1325.0\n",
      "fc layer 3 self.abs_max_out: 130.0\n",
      "fc layer 1 self.abs_max_out: 1350.0\n",
      "lif layer 1 self.abs_max_v: 1350.0\n",
      "fc layer 2 self.abs_max_out: 805.0\n",
      "fc layer 3 self.abs_max_out: 151.0\n",
      "fc layer 1 self.abs_max_out: 1420.0\n",
      "lif layer 1 self.abs_max_v: 1420.0\n",
      "fc layer 1 self.abs_max_out: 1534.0\n",
      "lif layer 1 self.abs_max_v: 1534.0\n",
      "fc layer 1 self.abs_max_out: 1611.0\n",
      "lif layer 1 self.abs_max_v: 1611.0\n",
      "fc layer 1 self.abs_max_out: 1681.0\n",
      "lif layer 1 self.abs_max_v: 1681.0\n",
      "lif layer 2 self.abs_max_v: 1111.0\n",
      "lif layer 2 self.abs_max_v: 1193.5\n",
      "fc layer 2 self.abs_max_out: 912.0\n",
      "fc layer 3 self.abs_max_out: 159.0\n",
      "fc layer 2 self.abs_max_out: 929.0\n",
      "fc layer 3 self.abs_max_out: 185.0\n",
      "fc layer 2 self.abs_max_out: 945.0\n",
      "fc layer 1 self.abs_max_out: 1993.0\n",
      "lif layer 1 self.abs_max_v: 1993.0\n",
      "lif layer 2 self.abs_max_v: 1284.0\n",
      "fc layer 1 self.abs_max_out: 2158.0\n",
      "lif layer 1 self.abs_max_v: 2158.0\n",
      "lif layer 2 self.abs_max_v: 1522.0\n",
      "fc layer 3 self.abs_max_out: 196.0\n",
      "fc layer 2 self.abs_max_out: 962.0\n",
      "lif layer 2 self.abs_max_v: 1536.0\n",
      "lif layer 2 self.abs_max_v: 1561.0\n",
      "fc layer 3 self.abs_max_out: 221.0\n",
      "fc layer 1 self.abs_max_out: 2339.0\n",
      "lif layer 1 self.abs_max_v: 2339.0\n",
      "fc layer 2 self.abs_max_out: 992.0\n",
      "fc layer 2 self.abs_max_out: 1061.0\n",
      "fc layer 2 self.abs_max_out: 1121.0\n",
      "fc layer 2 self.abs_max_out: 1129.0\n",
      "fc layer 2 self.abs_max_out: 1145.0\n",
      "fc layer 2 self.abs_max_out: 1226.0\n",
      "fc layer 1 self.abs_max_out: 2505.0\n",
      "lif layer 1 self.abs_max_v: 2505.0\n",
      "fc layer 2 self.abs_max_out: 1289.0\n",
      "fc layer 2 self.abs_max_out: 1329.0\n",
      "fc layer 3 self.abs_max_out: 253.0\n",
      "fc layer 3 self.abs_max_out: 259.0\n",
      "fc layer 3 self.abs_max_out: 263.0\n",
      "fc layer 2 self.abs_max_out: 1341.0\n",
      "fc layer 3 self.abs_max_out: 269.0\n",
      "fc layer 2 self.abs_max_out: 1436.0\n",
      "fc layer 3 self.abs_max_out: 282.0\n",
      "fc layer 2 self.abs_max_out: 1446.0\n",
      "fc layer 2 self.abs_max_out: 1505.0\n",
      "fc layer 2 self.abs_max_out: 1687.0\n",
      "lif layer 2 self.abs_max_v: 1687.0\n",
      "lif layer 2 self.abs_max_v: 1840.0\n",
      "fc layer 2 self.abs_max_out: 1711.0\n",
      "fc layer 2 self.abs_max_out: 1757.0\n",
      "fc layer 2 self.abs_max_out: 1826.0\n",
      "fc layer 3 self.abs_max_out: 302.0\n",
      "lif layer 2 self.abs_max_v: 1942.5\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "fc layer 1 self.abs_max_out: 2594.0\n",
      "lif layer 1 self.abs_max_v: 2594.0\n",
      "fc layer 1 self.abs_max_out: 3112.0\n",
      "lif layer 1 self.abs_max_v: 3112.0\n",
      "fc layer 3 self.abs_max_out: 326.0\n",
      "fc layer 2 self.abs_max_out: 1933.0\n",
      "fc layer 2 self.abs_max_out: 2016.0\n",
      "lif layer 2 self.abs_max_v: 2016.0\n",
      "fc layer 3 self.abs_max_out: 329.0\n",
      "lif layer 2 self.abs_max_v: 2025.0\n",
      "fc layer 3 self.abs_max_out: 359.0\n",
      "lif layer 2 self.abs_max_v: 2026.0\n",
      "lif layer 2 self.abs_max_v: 2043.0\n",
      "fc layer 1 self.abs_max_out: 3157.0\n",
      "lif layer 1 self.abs_max_v: 3157.0\n",
      "fc layer 1 self.abs_max_out: 3208.0\n",
      "lif layer 1 self.abs_max_v: 3208.0\n",
      "lif layer 2 self.abs_max_v: 2072.0\n",
      "lif layer 2 self.abs_max_v: 2090.0\n",
      "fc layer 2 self.abs_max_out: 2021.0\n",
      "fc layer 2 self.abs_max_out: 2113.0\n",
      "lif layer 2 self.abs_max_v: 2113.0\n",
      "fc layer 1 self.abs_max_out: 3249.0\n",
      "lif layer 1 self.abs_max_v: 3249.0\n",
      "lif layer 2 self.abs_max_v: 2200.5\n",
      "fc layer 1 self.abs_max_out: 3270.0\n",
      "lif layer 1 self.abs_max_v: 3270.0\n",
      "fc layer 1 self.abs_max_out: 3343.0\n",
      "lif layer 1 self.abs_max_v: 3343.0\n",
      "fc layer 1 self.abs_max_out: 3582.0\n",
      "lif layer 1 self.abs_max_v: 3582.0\n",
      "fc layer 1 self.abs_max_out: 3959.0\n",
      "lif layer 1 self.abs_max_v: 3959.0\n",
      "fc layer 2 self.abs_max_out: 2218.0\n",
      "lif layer 2 self.abs_max_v: 2218.0\n",
      "fc layer 3 self.abs_max_out: 388.0\n",
      "fc layer 2 self.abs_max_out: 2228.0\n",
      "lif layer 2 self.abs_max_v: 2228.0\n",
      "fc layer 2 self.abs_max_out: 2231.0\n",
      "lif layer 2 self.abs_max_v: 2281.0\n",
      "fc layer 2 self.abs_max_out: 2284.0\n",
      "lif layer 2 self.abs_max_v: 2284.0\n",
      "fc layer 3 self.abs_max_out: 421.0\n",
      "lif layer 2 self.abs_max_v: 2294.0\n",
      "fc layer 1 self.abs_max_out: 3967.0\n",
      "lif layer 1 self.abs_max_v: 3967.0\n",
      "fc layer 1 self.abs_max_out: 4002.0\n",
      "lif layer 1 self.abs_max_v: 4002.0\n",
      "lif layer 1 self.abs_max_v: 4268.5\n",
      "lif layer 1 self.abs_max_v: 4313.5\n",
      "fc layer 1 self.abs_max_out: 4054.0\n",
      "lif layer 2 self.abs_max_v: 2301.5\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.092802/  2.120204, val:  48.75%, val_best:  48.75%, tr:  79.67%, tr_best:  79.67%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7899%\n",
      "layer   2  Sparsity: 85.4769%\n",
      "layer   3  Sparsity: 89.2486%\n",
      "total_backward_count 9790 real_backward_count 3838  39.203%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 2352.0\n",
      "fc layer 2 self.abs_max_out: 2352.0\n",
      "fc layer 2 self.abs_max_out: 2395.0\n",
      "lif layer 2 self.abs_max_v: 2395.0\n",
      "fc layer 3 self.abs_max_out: 444.0\n",
      "fc layer 2 self.abs_max_out: 2413.0\n",
      "lif layer 2 self.abs_max_v: 2413.0\n",
      "fc layer 2 self.abs_max_out: 2424.0\n",
      "lif layer 2 self.abs_max_v: 2424.0\n",
      "fc layer 2 self.abs_max_out: 2509.0\n",
      "lif layer 2 self.abs_max_v: 2509.0\n",
      "fc layer 1 self.abs_max_out: 4074.0\n",
      "fc layer 1 self.abs_max_out: 4319.0\n",
      "lif layer 1 self.abs_max_v: 4319.0\n",
      "lif layer 2 self.abs_max_v: 2544.0\n",
      "fc layer 2 self.abs_max_out: 2557.0\n",
      "lif layer 2 self.abs_max_v: 2557.0\n",
      "fc layer 1 self.abs_max_out: 4453.0\n",
      "lif layer 1 self.abs_max_v: 4453.0\n",
      "fc layer 1 self.abs_max_out: 4555.0\n",
      "lif layer 1 self.abs_max_v: 4555.0\n",
      "fc layer 2 self.abs_max_out: 2576.0\n",
      "lif layer 2 self.abs_max_v: 2576.0\n",
      "fc layer 2 self.abs_max_out: 2579.0\n",
      "lif layer 2 self.abs_max_v: 2579.0\n",
      "fc layer 2 self.abs_max_out: 2636.0\n",
      "lif layer 2 self.abs_max_v: 2636.0\n",
      "lif layer 2 self.abs_max_v: 2685.0\n",
      "fc layer 2 self.abs_max_out: 2748.0\n",
      "lif layer 2 self.abs_max_v: 2748.0\n",
      "fc layer 2 self.abs_max_out: 2786.0\n",
      "lif layer 2 self.abs_max_v: 2786.0\n",
      "fc layer 1 self.abs_max_out: 4665.0\n",
      "lif layer 1 self.abs_max_v: 4665.0\n",
      "fc layer 1 self.abs_max_out: 4872.0\n",
      "lif layer 1 self.abs_max_v: 4872.0\n",
      "lif layer 1 self.abs_max_v: 4888.0\n",
      "fc layer 2 self.abs_max_out: 2815.0\n",
      "lif layer 2 self.abs_max_v: 2815.0\n",
      "fc layer 2 self.abs_max_out: 2857.0\n",
      "lif layer 2 self.abs_max_v: 2857.0\n",
      "fc layer 2 self.abs_max_out: 2898.0\n",
      "lif layer 2 self.abs_max_v: 2898.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.031991/  2.123317, val:  45.83%, val_best:  48.75%, tr:  95.10%, tr_best:  95.10%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7889%\n",
      "layer   2  Sparsity: 83.3797%\n",
      "layer   3  Sparsity: 85.8238%\n",
      "total_backward_count 19580 real_backward_count 6115  31.231%\n",
      "fc layer 1 self.abs_max_out: 5206.0\n",
      "lif layer 1 self.abs_max_v: 5206.0\n",
      "fc layer 2 self.abs_max_out: 3185.0\n",
      "lif layer 2 self.abs_max_v: 3185.0\n",
      "fc layer 1 self.abs_max_out: 5230.0\n",
      "lif layer 1 self.abs_max_v: 5230.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.044357/  2.126007, val:  57.92%, val_best:  57.92%, tr:  97.24%, tr_best:  97.24%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7982%\n",
      "layer   2  Sparsity: 83.0619%\n",
      "layer   3  Sparsity: 85.2904%\n",
      "total_backward_count 29370 real_backward_count 7997  27.228%\n",
      "fc layer 2 self.abs_max_out: 3289.0\n",
      "lif layer 2 self.abs_max_v: 3289.0\n",
      "fc layer 2 self.abs_max_out: 3311.0\n",
      "lif layer 2 self.abs_max_v: 3311.0\n",
      "fc layer 1 self.abs_max_out: 5449.0\n",
      "lif layer 1 self.abs_max_v: 5449.0\n",
      "fc layer 1 self.abs_max_out: 5521.0\n",
      "lif layer 1 self.abs_max_v: 5521.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.044054/  2.133365, val:  42.08%, val_best:  57.92%, tr:  98.16%, tr_best:  98.16%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7811%\n",
      "layer   2  Sparsity: 82.7290%\n",
      "layer   3  Sparsity: 84.7729%\n",
      "total_backward_count 39160 real_backward_count 9748  24.893%\n",
      "fc layer 1 self.abs_max_out: 5697.0\n",
      "lif layer 1 self.abs_max_v: 5697.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.056083/  2.126880, val:  53.33%, val_best:  57.92%, tr:  98.57%, tr_best:  98.57%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.7820%\n",
      "layer   2  Sparsity: 82.5571%\n",
      "layer   3  Sparsity: 85.0450%\n",
      "total_backward_count 48950 real_backward_count 11365  23.218%\n",
      "fc layer 1 self.abs_max_out: 6173.0\n",
      "lif layer 1 self.abs_max_v: 6173.0\n",
      "fc layer 2 self.abs_max_out: 3325.0\n",
      "lif layer 2 self.abs_max_v: 3325.0\n",
      "fc layer 2 self.abs_max_out: 3344.0\n",
      "lif layer 2 self.abs_max_v: 3344.0\n",
      "fc layer 2 self.abs_max_out: 3402.0\n",
      "lif layer 2 self.abs_max_v: 3402.0\n",
      "fc layer 2 self.abs_max_out: 3507.0\n",
      "lif layer 2 self.abs_max_v: 3507.0\n",
      "fc layer 2 self.abs_max_out: 3559.0\n",
      "lif layer 2 self.abs_max_v: 3559.0\n",
      "lif layer 1 self.abs_max_v: 6345.0\n",
      "lif layer 1 self.abs_max_v: 6655.0\n",
      "lif layer 1 self.abs_max_v: 6720.5\n",
      "lif layer 1 self.abs_max_v: 6919.5\n",
      "lif layer 1 self.abs_max_v: 6966.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.041675/  2.122972, val:  58.75%, val_best:  58.75%, tr:  98.98%, tr_best:  98.98%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7862%\n",
      "layer   2  Sparsity: 82.2734%\n",
      "layer   3  Sparsity: 84.2022%\n",
      "total_backward_count 58740 real_backward_count 12932  22.016%\n",
      "fc layer 1 self.abs_max_out: 6251.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.040870/  2.117929, val:  53.33%, val_best:  58.75%, tr:  99.18%, tr_best:  99.18%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7804%\n",
      "layer   2  Sparsity: 82.4199%\n",
      "layer   3  Sparsity: 83.7020%\n",
      "total_backward_count 68530 real_backward_count 14430  21.056%\n",
      "fc layer 1 self.abs_max_out: 6392.0\n",
      "fc layer 1 self.abs_max_out: 6598.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  2.041603/  2.129140, val:  52.92%, val_best:  58.75%, tr:  99.18%, tr_best:  99.18%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7862%\n",
      "layer   2  Sparsity: 82.2937%\n",
      "layer   3  Sparsity: 84.2735%\n",
      "total_backward_count 78320 real_backward_count 15878  20.273%\n",
      "fc layer 2 self.abs_max_out: 3639.0\n",
      "lif layer 2 self.abs_max_v: 3639.0\n",
      "fc layer 2 self.abs_max_out: 3646.0\n",
      "lif layer 2 self.abs_max_v: 3646.0\n",
      "fc layer 1 self.abs_max_out: 6748.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  2.042797/  2.114022, val:  62.92%, val_best:  62.92%, tr:  98.88%, tr_best:  99.18%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7807%\n",
      "layer   2  Sparsity: 81.9910%\n",
      "layer   3  Sparsity: 84.3018%\n",
      "total_backward_count 88110 real_backward_count 17425  19.776%\n",
      "fc layer 1 self.abs_max_out: 6936.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  2.038707/  2.134366, val:  49.17%, val_best:  62.92%, tr:  99.49%, tr_best:  99.49%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.7962%\n",
      "layer   2  Sparsity: 81.4057%\n",
      "layer   3  Sparsity: 84.1879%\n",
      "total_backward_count 97900 real_backward_count 18876  19.281%\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  2.045082/  2.135624, val:  45.83%, val_best:  62.92%, tr:  99.18%, tr_best:  99.49%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7807%\n",
      "layer   2  Sparsity: 81.3592%\n",
      "layer   3  Sparsity: 84.2543%\n",
      "total_backward_count 107690 real_backward_count 20291  18.842%\n",
      "fc layer 2 self.abs_max_out: 3684.0\n",
      "lif layer 2 self.abs_max_v: 3684.0\n",
      "fc layer 2 self.abs_max_out: 3723.0\n",
      "lif layer 2 self.abs_max_v: 3723.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  2.036697/  2.116001, val:  60.83%, val_best:  62.92%, tr:  99.49%, tr_best:  99.49%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7741%\n",
      "layer   2  Sparsity: 81.3998%\n",
      "layer   3  Sparsity: 83.8008%\n",
      "total_backward_count 117480 real_backward_count 21684  18.458%\n",
      "lif layer 2 self.abs_max_v: 3801.5\n",
      "lif layer 2 self.abs_max_v: 3907.5\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  2.041670/  2.121215, val:  52.08%, val_best:  62.92%, tr:  99.39%, tr_best:  99.49%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7830%\n",
      "layer   2  Sparsity: 81.6447%\n",
      "layer   3  Sparsity: 84.5201%\n",
      "total_backward_count 127270 real_backward_count 23072  18.128%\n",
      "fc layer 2 self.abs_max_out: 3733.0\n",
      "fc layer 2 self.abs_max_out: 3744.0\n",
      "fc layer 1 self.abs_max_out: 7078.0\n",
      "lif layer 1 self.abs_max_v: 7078.0\n",
      "fc layer 2 self.abs_max_out: 3787.0\n",
      "lif layer 2 self.abs_max_v: 4010.5\n",
      "lif layer 1 self.abs_max_v: 7230.0\n",
      "lif layer 1 self.abs_max_v: 7449.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  2.038293/  2.121871, val:  52.92%, val_best:  62.92%, tr:  99.39%, tr_best:  99.49%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7950%\n",
      "layer   2  Sparsity: 81.3533%\n",
      "layer   3  Sparsity: 84.1314%\n",
      "total_backward_count 137060 real_backward_count 24405  17.806%\n",
      "fc layer 2 self.abs_max_out: 3837.0\n",
      "fc layer 1 self.abs_max_out: 7111.0\n",
      "fc layer 2 self.abs_max_out: 3913.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  2.036843/  2.126370, val:  51.25%, val_best:  62.92%, tr:  99.39%, tr_best:  99.49%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7908%\n",
      "layer   2  Sparsity: 81.0735%\n",
      "layer   3  Sparsity: 84.1461%\n",
      "total_backward_count 146850 real_backward_count 25700  17.501%\n",
      "fc layer 1 self.abs_max_out: 7395.0\n",
      "lif layer 1 self.abs_max_v: 7553.5\n",
      "lif layer 1 self.abs_max_v: 7679.5\n",
      "lif layer 1 self.abs_max_v: 8076.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  2.039743/  2.128541, val:  55.42%, val_best:  62.92%, tr:  99.49%, tr_best:  99.49%, epoch time: 79.94 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7856%\n",
      "layer   2  Sparsity: 81.2184%\n",
      "layer   3  Sparsity: 83.9530%\n",
      "total_backward_count 156640 real_backward_count 27002  17.238%\n",
      "lif layer 2 self.abs_max_v: 4019.0\n",
      "fc layer 1 self.abs_max_out: 7803.0\n",
      "lif layer 1 self.abs_max_v: 8305.0\n",
      "lif layer 1 self.abs_max_v: 8750.5\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  2.032758/  2.113721, val:  60.83%, val_best:  62.92%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7960%\n",
      "layer   2  Sparsity: 81.3032%\n",
      "layer   3  Sparsity: 84.0754%\n",
      "total_backward_count 166430 real_backward_count 28317  17.014%\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  2.027524/  2.110373, val:  60.00%, val_best:  62.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7888%\n",
      "layer   2  Sparsity: 80.9792%\n",
      "layer   3  Sparsity: 84.0752%\n",
      "total_backward_count 176220 real_backward_count 29640  16.820%\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  2.030966/  2.115287, val:  46.25%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7775%\n",
      "layer   2  Sparsity: 80.5112%\n",
      "layer   3  Sparsity: 84.0628%\n",
      "total_backward_count 186010 real_backward_count 30892  16.608%\n",
      "lif layer 2 self.abs_max_v: 4020.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  2.028279/  2.120502, val:  55.42%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7923%\n",
      "layer   2  Sparsity: 80.4558%\n",
      "layer   3  Sparsity: 83.9533%\n",
      "total_backward_count 195800 real_backward_count 32131  16.410%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  2.026953/  2.111031, val:  53.75%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.7892%\n",
      "layer   2  Sparsity: 80.5440%\n",
      "layer   3  Sparsity: 83.9138%\n",
      "total_backward_count 205590 real_backward_count 33343  16.218%\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  2.031041/  2.114723, val:  58.75%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7918%\n",
      "layer   2  Sparsity: 80.7191%\n",
      "layer   3  Sparsity: 83.6782%\n",
      "total_backward_count 215380 real_backward_count 34585  16.058%\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  2.021105/  2.090110, val:  61.25%, val_best:  62.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7865%\n",
      "layer   2  Sparsity: 80.5002%\n",
      "layer   3  Sparsity: 83.6158%\n",
      "total_backward_count 225170 real_backward_count 35858  15.925%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  2.013594/  2.106935, val:  58.75%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7847%\n",
      "layer   2  Sparsity: 80.5253%\n",
      "layer   3  Sparsity: 83.4998%\n",
      "total_backward_count 234960 real_backward_count 37043  15.766%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  2.019760/  2.097859, val:  67.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7889%\n",
      "layer   2  Sparsity: 80.3629%\n",
      "layer   3  Sparsity: 83.4161%\n",
      "total_backward_count 244750 real_backward_count 38216  15.614%\n",
      "fc layer 1 self.abs_max_out: 7920.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  2.016346/  2.092530, val:  61.67%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7798%\n",
      "layer   2  Sparsity: 80.4642%\n",
      "layer   3  Sparsity: 83.1098%\n",
      "total_backward_count 254540 real_backward_count 39435  15.493%\n",
      "lif layer 2 self.abs_max_v: 4121.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  2.008148/  2.088851, val:  60.83%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7857%\n",
      "layer   2  Sparsity: 80.7669%\n",
      "layer   3  Sparsity: 83.2915%\n",
      "total_backward_count 264330 real_backward_count 40589  15.355%\n",
      "fc layer 1 self.abs_max_out: 7945.0\n",
      "fc layer 1 self.abs_max_out: 8104.0\n",
      "lif layer 1 self.abs_max_v: 8751.5\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  2.006020/  2.093928, val:  65.00%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7977%\n",
      "layer   2  Sparsity: 80.5461%\n",
      "layer   3  Sparsity: 82.9818%\n",
      "total_backward_count 274120 real_backward_count 41730  15.223%\n",
      "lif layer 2 self.abs_max_v: 4366.0\n",
      "lif layer 2 self.abs_max_v: 4385.0\n",
      "fc layer 2 self.abs_max_out: 3993.0\n",
      "lif layer 1 self.abs_max_v: 8945.5\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.999702/  2.104105, val:  60.00%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7889%\n",
      "layer   2  Sparsity: 80.5202%\n",
      "layer   3  Sparsity: 83.2503%\n",
      "total_backward_count 283910 real_backward_count 42887  15.106%\n",
      "fc layer 1 self.abs_max_out: 8163.0\n",
      "fc layer 1 self.abs_max_out: 8586.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  2.017992/  2.100224, val:  52.92%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7834%\n",
      "layer   2  Sparsity: 80.4795%\n",
      "layer   3  Sparsity: 83.4369%\n",
      "total_backward_count 293700 real_backward_count 44020  14.988%\n",
      "lif layer 1 self.abs_max_v: 9071.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  2.012378/  2.104143, val:  69.17%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7872%\n",
      "layer   2  Sparsity: 80.0318%\n",
      "layer   3  Sparsity: 83.0743%\n",
      "total_backward_count 303490 real_backward_count 45139  14.873%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  2.011739/  2.091051, val:  63.33%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7920%\n",
      "layer   2  Sparsity: 80.1532%\n",
      "layer   3  Sparsity: 83.3275%\n",
      "total_backward_count 313280 real_backward_count 46274  14.771%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  2.009945/  2.083203, val:  67.08%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7831%\n",
      "layer   2  Sparsity: 80.2212%\n",
      "layer   3  Sparsity: 83.4895%\n",
      "total_backward_count 323070 real_backward_count 47381  14.666%\n",
      "fc layer 1 self.abs_max_out: 8599.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  2.001191/  2.093617, val:  65.42%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7878%\n",
      "layer   2  Sparsity: 80.0306%\n",
      "layer   3  Sparsity: 82.8499%\n",
      "total_backward_count 332860 real_backward_count 48455  14.557%\n",
      "lif layer 1 self.abs_max_v: 9275.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  2.012577/  2.092365, val:  61.25%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7665%\n",
      "layer   2  Sparsity: 80.0325%\n",
      "layer   3  Sparsity: 82.9979%\n",
      "total_backward_count 342650 real_backward_count 49574  14.468%\n",
      "lif layer 2 self.abs_max_v: 4472.0\n",
      "lif layer 1 self.abs_max_v: 9437.5\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  2.018433/  2.100886, val:  78.33%, val_best:  78.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7895%\n",
      "layer   2  Sparsity: 80.1143%\n",
      "layer   3  Sparsity: 83.4771%\n",
      "total_backward_count 352440 real_backward_count 50693  14.383%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  2.011410/  2.095458, val:  65.00%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7937%\n",
      "layer   2  Sparsity: 80.1330%\n",
      "layer   3  Sparsity: 83.4138%\n",
      "total_backward_count 362230 real_backward_count 51757  14.288%\n",
      "fc layer 2 self.abs_max_out: 4028.0\n",
      "fc layer 1 self.abs_max_out: 8725.0\n",
      "fc layer 2 self.abs_max_out: 4034.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  2.005249/  2.087157, val:  62.08%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7758%\n",
      "layer   2  Sparsity: 80.1934%\n",
      "layer   3  Sparsity: 82.7564%\n",
      "total_backward_count 372020 real_backward_count 52844  14.205%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  2.007486/  2.094116, val:  68.33%, val_best:  78.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7908%\n",
      "layer   2  Sparsity: 80.1262%\n",
      "layer   3  Sparsity: 82.9447%\n",
      "total_backward_count 381810 real_backward_count 53926  14.124%\n",
      "fc layer 2 self.abs_max_out: 4058.0\n",
      "lif layer 1 self.abs_max_v: 9483.5\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  2.008274/  2.100490, val:  68.75%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7682%\n",
      "layer   2  Sparsity: 79.9101%\n",
      "layer   3  Sparsity: 82.6355%\n",
      "total_backward_count 391600 real_backward_count 54999  14.045%\n",
      "lif layer 1 self.abs_max_v: 9547.5\n",
      "fc layer 2 self.abs_max_out: 4252.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  2.008218/  2.093802, val:  57.08%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.95 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7881%\n",
      "layer   2  Sparsity: 79.9780%\n",
      "layer   3  Sparsity: 82.6720%\n",
      "total_backward_count 401390 real_backward_count 56098  13.976%\n",
      "lif layer 1 self.abs_max_v: 9563.5\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.997137/  2.089911, val:  57.92%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.7832%\n",
      "layer   2  Sparsity: 79.9518%\n",
      "layer   3  Sparsity: 82.2688%\n",
      "total_backward_count 411180 real_backward_count 57146  13.898%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.995247/  2.077832, val:  66.67%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7864%\n",
      "layer   2  Sparsity: 80.0943%\n",
      "layer   3  Sparsity: 82.2483%\n",
      "total_backward_count 420970 real_backward_count 58209  13.827%\n",
      "fc layer 3 self.abs_max_out: 450.0\n",
      "lif layer 1 self.abs_max_v: 9814.5\n",
      "fc layer 2 self.abs_max_out: 4269.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.993852/  2.092271, val:  67.08%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7799%\n",
      "layer   2  Sparsity: 80.0657%\n",
      "layer   3  Sparsity: 82.3882%\n",
      "total_backward_count 430760 real_backward_count 59279  13.761%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  2.002559/  2.082417, val:  64.17%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7865%\n",
      "layer   2  Sparsity: 79.9602%\n",
      "layer   3  Sparsity: 82.3186%\n",
      "total_backward_count 440550 real_backward_count 60333  13.695%\n",
      "lif layer 1 self.abs_max_v: 9846.5\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.985544/  2.069960, val:  71.25%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7939%\n",
      "layer   2  Sparsity: 80.0472%\n",
      "layer   3  Sparsity: 82.3882%\n",
      "total_backward_count 450340 real_backward_count 61355  13.624%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.989702/  2.082160, val:  60.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.88 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7844%\n",
      "layer   2  Sparsity: 80.2303%\n",
      "layer   3  Sparsity: 82.7789%\n",
      "total_backward_count 460130 real_backward_count 62350  13.551%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.991207/  2.080005, val:  63.33%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.8009%\n",
      "layer   2  Sparsity: 80.4094%\n",
      "layer   3  Sparsity: 83.0052%\n",
      "total_backward_count 469920 real_backward_count 63359  13.483%\n",
      "fc layer 3 self.abs_max_out: 457.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.988721/  2.080338, val:  69.58%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7932%\n",
      "layer   2  Sparsity: 80.1090%\n",
      "layer   3  Sparsity: 82.5181%\n",
      "total_backward_count 479710 real_backward_count 64351  13.415%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.991751/  2.073467, val:  70.42%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7784%\n",
      "layer   2  Sparsity: 79.8119%\n",
      "layer   3  Sparsity: 82.6990%\n",
      "total_backward_count 489500 real_backward_count 65353  13.351%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.986799/  2.081811, val:  66.67%, val_best:  78.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.8015%\n",
      "layer   2  Sparsity: 79.7634%\n",
      "layer   3  Sparsity: 82.6688%\n",
      "total_backward_count 499290 real_backward_count 66302  13.279%\n",
      "fc layer 2 self.abs_max_out: 4300.0\n",
      "lif layer 2 self.abs_max_v: 4663.5\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  2.000591/  2.093679, val:  66.67%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7831%\n",
      "layer   2  Sparsity: 79.3865%\n",
      "layer   3  Sparsity: 82.7272%\n",
      "total_backward_count 509080 real_backward_count 67294  13.219%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.995697/  2.075856, val:  63.33%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7845%\n",
      "layer   2  Sparsity: 79.6589%\n",
      "layer   3  Sparsity: 82.5648%\n",
      "total_backward_count 518870 real_backward_count 68328  13.169%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.986948/  2.075929, val:  60.00%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7792%\n",
      "layer   2  Sparsity: 79.7736%\n",
      "layer   3  Sparsity: 82.4730%\n",
      "total_backward_count 528660 real_backward_count 69301  13.109%\n",
      "lif layer 2 self.abs_max_v: 4874.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.983697/  2.066611, val:  71.67%, val_best:  78.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7920%\n",
      "layer   2  Sparsity: 79.6965%\n",
      "layer   3  Sparsity: 82.3364%\n",
      "total_backward_count 538450 real_backward_count 70298  13.056%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.983622/  2.069703, val:  71.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.39 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.7845%\n",
      "layer   2  Sparsity: 79.9085%\n",
      "layer   3  Sparsity: 82.3694%\n",
      "total_backward_count 548240 real_backward_count 71330  13.011%\n",
      "fc layer 1 self.abs_max_out: 8835.0\n",
      "fc layer 2 self.abs_max_out: 4329.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.987922/  2.067725, val:  68.75%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7870%\n",
      "layer   2  Sparsity: 79.8442%\n",
      "layer   3  Sparsity: 82.3748%\n",
      "total_backward_count 558030 real_backward_count 72281  12.953%\n",
      "lif layer 1 self.abs_max_v: 9952.5\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.984499/  2.071763, val:  72.92%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.13 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.7724%\n",
      "layer   2  Sparsity: 79.4492%\n",
      "layer   3  Sparsity: 82.0245%\n",
      "total_backward_count 567820 real_backward_count 73201  12.892%\n",
      "fc layer 1 self.abs_max_out: 8985.0\n",
      "lif layer 1 self.abs_max_v: 10432.5\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.986872/  2.072198, val:  63.75%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7756%\n",
      "layer   2  Sparsity: 79.2891%\n",
      "layer   3  Sparsity: 81.8822%\n",
      "total_backward_count 577610 real_backward_count 74168  12.840%\n",
      "lif layer 1 self.abs_max_v: 10763.0\n",
      "fc layer 2 self.abs_max_out: 4339.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.988054/  2.065502, val:  61.25%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7938%\n",
      "layer   2  Sparsity: 79.3619%\n",
      "layer   3  Sparsity: 82.1256%\n",
      "total_backward_count 587400 real_backward_count 75148  12.793%\n",
      "lif layer 1 self.abs_max_v: 10900.5\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.981233/  2.065121, val:  67.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7873%\n",
      "layer   2  Sparsity: 79.2251%\n",
      "layer   3  Sparsity: 82.1966%\n",
      "total_backward_count 597190 real_backward_count 76125  12.747%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.979910/  2.055499, val:  74.17%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7832%\n",
      "layer   2  Sparsity: 79.1542%\n",
      "layer   3  Sparsity: 82.2295%\n",
      "total_backward_count 606980 real_backward_count 77065  12.696%\n",
      "fc layer 1 self.abs_max_out: 9003.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.975950/  2.068609, val:  67.92%, val_best:  78.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7923%\n",
      "layer   2  Sparsity: 79.2713%\n",
      "layer   3  Sparsity: 82.0624%\n",
      "total_backward_count 616770 real_backward_count 78027  12.651%\n",
      "lif layer 1 self.abs_max_v: 11009.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.979317/  2.062356, val:  69.58%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7932%\n",
      "layer   2  Sparsity: 79.0856%\n",
      "layer   3  Sparsity: 81.8618%\n",
      "total_backward_count 626560 real_backward_count 78964  12.603%\n",
      "fc layer 1 self.abs_max_out: 9127.0\n",
      "lif layer 1 self.abs_max_v: 11069.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.982301/  2.066207, val:  67.50%, val_best:  78.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 80.28 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.7935%\n",
      "layer   2  Sparsity: 79.1545%\n",
      "layer   3  Sparsity: 81.8646%\n",
      "total_backward_count 636350 real_backward_count 79903  12.556%\n",
      "lif layer 1 self.abs_max_v: 11262.5\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.967855/  2.063322, val:  76.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7865%\n",
      "layer   2  Sparsity: 79.2111%\n",
      "layer   3  Sparsity: 81.8050%\n",
      "total_backward_count 646140 real_backward_count 80828  12.509%\n",
      "fc layer 1 self.abs_max_out: 9469.0\n",
      "lif layer 1 self.abs_max_v: 11469.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.968338/  2.060833, val:  69.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.7831%\n",
      "layer   2  Sparsity: 79.4440%\n",
      "layer   3  Sparsity: 81.9888%\n",
      "total_backward_count 655930 real_backward_count 81787  12.469%\n",
      "fc layer 3 self.abs_max_out: 462.0\n",
      "lif layer 1 self.abs_max_v: 12052.5\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.968794/  2.063035, val:  68.75%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.7959%\n",
      "layer   2  Sparsity: 79.2781%\n",
      "layer   3  Sparsity: 82.4151%\n",
      "total_backward_count 665720 real_backward_count 82730  12.427%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.978470/  2.055106, val:  65.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7794%\n",
      "layer   2  Sparsity: 79.4168%\n",
      "layer   3  Sparsity: 82.4566%\n",
      "total_backward_count 675510 real_backward_count 83647  12.383%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.960740/  2.056947, val:  56.67%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7914%\n",
      "layer   2  Sparsity: 79.4153%\n",
      "layer   3  Sparsity: 82.0481%\n",
      "total_backward_count 685300 real_backward_count 84575  12.341%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.955310/  2.033727, val:  72.08%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7861%\n",
      "layer   2  Sparsity: 79.4505%\n",
      "layer   3  Sparsity: 82.0152%\n",
      "total_backward_count 695090 real_backward_count 85534  12.305%\n",
      "fc layer 2 self.abs_max_out: 4357.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.961983/  2.057477, val:  66.25%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.38 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.7958%\n",
      "layer   2  Sparsity: 79.1598%\n",
      "layer   3  Sparsity: 82.0107%\n",
      "total_backward_count 704880 real_backward_count 86449  12.264%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.964285/  2.054843, val:  81.67%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.55 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.7907%\n",
      "layer   2  Sparsity: 79.2760%\n",
      "layer   3  Sparsity: 82.0732%\n",
      "total_backward_count 714670 real_backward_count 87409  12.231%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.970173/  2.054978, val:  61.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7840%\n",
      "layer   2  Sparsity: 79.3321%\n",
      "layer   3  Sparsity: 82.1340%\n",
      "total_backward_count 724460 real_backward_count 88306  12.189%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.967059/  2.062996, val:  67.92%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.10 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.7798%\n",
      "layer   2  Sparsity: 79.3237%\n",
      "layer   3  Sparsity: 81.9591%\n",
      "total_backward_count 734250 real_backward_count 89233  12.153%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.961664/  2.054859, val:  66.25%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7779%\n",
      "layer   2  Sparsity: 79.3021%\n",
      "layer   3  Sparsity: 81.8724%\n",
      "total_backward_count 744040 real_backward_count 90136  12.114%\n",
      "fc layer 2 self.abs_max_out: 4582.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.960542/  2.058561, val:  70.42%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.8028%\n",
      "layer   2  Sparsity: 79.1566%\n",
      "layer   3  Sparsity: 81.9067%\n",
      "total_backward_count 753830 real_backward_count 91051  12.078%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.967934/  2.067921, val:  51.25%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7895%\n",
      "layer   2  Sparsity: 79.1564%\n",
      "layer   3  Sparsity: 82.3539%\n",
      "total_backward_count 763620 real_backward_count 91983  12.046%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.966063/  2.058207, val:  73.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7881%\n",
      "layer   2  Sparsity: 79.1193%\n",
      "layer   3  Sparsity: 81.9158%\n",
      "total_backward_count 773410 real_backward_count 92907  12.013%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.960117/  2.052269, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.07 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7862%\n",
      "layer   2  Sparsity: 79.1948%\n",
      "layer   3  Sparsity: 81.4068%\n",
      "total_backward_count 783200 real_backward_count 93789  11.975%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.963384/  2.058798, val:  69.58%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7865%\n",
      "layer   2  Sparsity: 79.1174%\n",
      "layer   3  Sparsity: 81.4947%\n",
      "total_backward_count 792990 real_backward_count 94731  11.946%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.970360/  2.060469, val:  67.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.16 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.7867%\n",
      "layer   2  Sparsity: 79.2244%\n",
      "layer   3  Sparsity: 81.9877%\n",
      "total_backward_count 802780 real_backward_count 95681  11.919%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.963761/  2.050719, val:  60.42%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7867%\n",
      "layer   2  Sparsity: 79.3261%\n",
      "layer   3  Sparsity: 81.6582%\n",
      "total_backward_count 812570 real_backward_count 96626  11.891%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.957399/  2.071917, val:  65.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7926%\n",
      "layer   2  Sparsity: 79.1029%\n",
      "layer   3  Sparsity: 81.8745%\n",
      "total_backward_count 822360 real_backward_count 97580  11.866%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.982482/  2.069530, val:  67.08%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.15 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.7906%\n",
      "layer   2  Sparsity: 79.1064%\n",
      "layer   3  Sparsity: 81.8546%\n",
      "total_backward_count 832150 real_backward_count 98532  11.841%\n",
      "fc layer 2 self.abs_max_out: 4602.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.972704/  2.052702, val:  71.25%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.07 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7874%\n",
      "layer   2  Sparsity: 79.2113%\n",
      "layer   3  Sparsity: 81.5793%\n",
      "total_backward_count 841940 real_backward_count 99421  11.809%\n",
      "fc layer 2 self.abs_max_out: 4621.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.966643/  2.059319, val:  69.17%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7783%\n",
      "layer   2  Sparsity: 79.1855%\n",
      "layer   3  Sparsity: 81.7262%\n",
      "total_backward_count 851730 real_backward_count 100310  11.777%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.962887/  2.047448, val:  61.67%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7909%\n",
      "layer   2  Sparsity: 79.1345%\n",
      "layer   3  Sparsity: 81.7581%\n",
      "total_backward_count 861520 real_backward_count 101207  11.747%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.971869/  2.055097, val:  66.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.19 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.7917%\n",
      "layer   2  Sparsity: 79.0189%\n",
      "layer   3  Sparsity: 82.2163%\n",
      "total_backward_count 871310 real_backward_count 102082  11.716%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.973592/  2.055876, val:  78.75%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.21 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.7819%\n",
      "layer   2  Sparsity: 79.1434%\n",
      "layer   3  Sparsity: 82.3295%\n",
      "total_backward_count 881100 real_backward_count 102986  11.688%\n",
      "fc layer 1 self.abs_max_out: 9581.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.972376/  2.054504, val:  70.00%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.7879%\n",
      "layer   2  Sparsity: 79.2118%\n",
      "layer   3  Sparsity: 82.0857%\n",
      "total_backward_count 890890 real_backward_count 103854  11.657%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.972785/  2.053306, val:  75.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.7853%\n",
      "layer   2  Sparsity: 79.0909%\n",
      "layer   3  Sparsity: 81.9452%\n",
      "total_backward_count 900680 real_backward_count 104757  11.631%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.978692/  2.048638, val:  69.58%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7767%\n",
      "layer   2  Sparsity: 78.9775%\n",
      "layer   3  Sparsity: 81.9872%\n",
      "total_backward_count 910470 real_backward_count 105637  11.602%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.981069/  2.063683, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7904%\n",
      "layer   2  Sparsity: 79.0535%\n",
      "layer   3  Sparsity: 82.0429%\n",
      "total_backward_count 920260 real_backward_count 106517  11.575%\n",
      "fc layer 3 self.abs_max_out: 481.0\n",
      "fc layer 3 self.abs_max_out: 497.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.980091/  2.058254, val:  69.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.27 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.7887%\n",
      "layer   2  Sparsity: 79.1865%\n",
      "layer   3  Sparsity: 82.0824%\n",
      "total_backward_count 930050 real_backward_count 107401  11.548%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.963415/  2.050469, val:  72.08%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7805%\n",
      "layer   2  Sparsity: 79.1966%\n",
      "layer   3  Sparsity: 81.6956%\n",
      "total_backward_count 939840 real_backward_count 108276  11.521%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.964795/  2.058454, val:  66.67%, val_best:  82.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7940%\n",
      "layer   2  Sparsity: 79.1741%\n",
      "layer   3  Sparsity: 82.2261%\n",
      "total_backward_count 949630 real_backward_count 109138  11.493%\n",
      "lif layer 2 self.abs_max_v: 4883.0\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.962182/  2.048091, val:  76.25%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7827%\n",
      "layer   2  Sparsity: 78.8766%\n",
      "layer   3  Sparsity: 82.1041%\n",
      "total_backward_count 959420 real_backward_count 110062  11.472%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.964605/  2.054955, val:  66.67%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7726%\n",
      "layer   2  Sparsity: 78.7410%\n",
      "layer   3  Sparsity: 82.1609%\n",
      "total_backward_count 969210 real_backward_count 110944  11.447%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.961356/  2.052097, val:  66.25%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7972%\n",
      "layer   2  Sparsity: 78.8815%\n",
      "layer   3  Sparsity: 82.2014%\n",
      "total_backward_count 979000 real_backward_count 111829  11.423%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.959407/  2.047448, val:  76.67%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7933%\n",
      "layer   2  Sparsity: 78.6430%\n",
      "layer   3  Sparsity: 82.2901%\n",
      "total_backward_count 988790 real_backward_count 112686  11.396%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.966018/  2.051695, val:  77.92%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7881%\n",
      "layer   2  Sparsity: 78.5844%\n",
      "layer   3  Sparsity: 82.1498%\n",
      "total_backward_count 998580 real_backward_count 113563  11.372%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.970945/  2.050665, val:  75.42%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7839%\n",
      "layer   2  Sparsity: 78.4409%\n",
      "layer   3  Sparsity: 82.1279%\n",
      "total_backward_count 1008370 real_backward_count 114446  11.350%\n",
      "fc layer 1 self.abs_max_out: 9631.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.961752/  2.051766, val:  70.00%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7839%\n",
      "layer   2  Sparsity: 78.3719%\n",
      "layer   3  Sparsity: 81.9306%\n",
      "total_backward_count 1018160 real_backward_count 115327  11.327%\n",
      "fc layer 1 self.abs_max_out: 9674.0\n",
      "fc layer 2 self.abs_max_out: 4629.0\n",
      "fc layer 2 self.abs_max_out: 4682.0\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.968651/  2.051254, val:  81.67%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.68 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.8032%\n",
      "layer   2  Sparsity: 78.4811%\n",
      "layer   3  Sparsity: 81.6463%\n",
      "total_backward_count 1027950 real_backward_count 116196  11.304%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.966529/  2.058265, val:  69.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.7839%\n",
      "layer   2  Sparsity: 78.6636%\n",
      "layer   3  Sparsity: 82.0822%\n",
      "total_backward_count 1037740 real_backward_count 117031  11.277%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.967552/  2.049304, val:  67.50%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.79 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 93.7901%\n",
      "layer   2  Sparsity: 78.7078%\n",
      "layer   3  Sparsity: 81.8640%\n",
      "total_backward_count 1047530 real_backward_count 117897  11.255%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.963069/  2.048908, val:  73.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7740%\n",
      "layer   2  Sparsity: 78.7748%\n",
      "layer   3  Sparsity: 81.4061%\n",
      "total_backward_count 1057320 real_backward_count 118762  11.232%\n",
      "fc layer 1 self.abs_max_out: 9678.0\n",
      "fc layer 1 self.abs_max_out: 9718.0\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.961713/  2.050758, val:  68.33%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7990%\n",
      "layer   2  Sparsity: 78.6667%\n",
      "layer   3  Sparsity: 81.7475%\n",
      "total_backward_count 1067110 real_backward_count 119590  11.207%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.959557/  2.037778, val:  79.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7845%\n",
      "layer   2  Sparsity: 78.6601%\n",
      "layer   3  Sparsity: 81.8343%\n",
      "total_backward_count 1076900 real_backward_count 120452  11.185%\n",
      "fc layer 1 self.abs_max_out: 9816.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.955097/  2.042101, val:  70.00%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7821%\n",
      "layer   2  Sparsity: 78.6749%\n",
      "layer   3  Sparsity: 81.7025%\n",
      "total_backward_count 1086690 real_backward_count 121296  11.162%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.962130/  2.045681, val:  71.25%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7845%\n",
      "layer   2  Sparsity: 78.6195%\n",
      "layer   3  Sparsity: 81.8502%\n",
      "total_backward_count 1096480 real_backward_count 122133  11.139%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.959555/  2.045988, val:  73.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7769%\n",
      "layer   2  Sparsity: 78.7135%\n",
      "layer   3  Sparsity: 82.4233%\n",
      "total_backward_count 1106270 real_backward_count 122984  11.117%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.958858/  2.050057, val:  70.00%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7892%\n",
      "layer   2  Sparsity: 78.6509%\n",
      "layer   3  Sparsity: 81.7770%\n",
      "total_backward_count 1116060 real_backward_count 123844  11.097%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.960289/  2.054212, val:  65.83%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.7937%\n",
      "layer   2  Sparsity: 78.7370%\n",
      "layer   3  Sparsity: 81.9700%\n",
      "total_backward_count 1125850 real_backward_count 124699  11.076%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.971041/  2.055726, val:  65.83%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7879%\n",
      "layer   2  Sparsity: 78.5764%\n",
      "layer   3  Sparsity: 81.5717%\n",
      "total_backward_count 1135640 real_backward_count 125545  11.055%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.967532/  2.049654, val:  64.58%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7800%\n",
      "layer   2  Sparsity: 78.3935%\n",
      "layer   3  Sparsity: 81.1149%\n",
      "total_backward_count 1145430 real_backward_count 126411  11.036%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.957546/  2.047189, val:  75.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.95 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7908%\n",
      "layer   2  Sparsity: 78.4841%\n",
      "layer   3  Sparsity: 81.5699%\n",
      "total_backward_count 1155220 real_backward_count 127239  11.014%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.962409/  2.049436, val:  69.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7830%\n",
      "layer   2  Sparsity: 78.5739%\n",
      "layer   3  Sparsity: 82.3651%\n",
      "total_backward_count 1165010 real_backward_count 128080  10.994%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.959020/  2.050953, val:  75.42%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.8045%\n",
      "layer   2  Sparsity: 78.5703%\n",
      "layer   3  Sparsity: 82.2557%\n",
      "total_backward_count 1174800 real_backward_count 128973  10.978%\n",
      "lif layer 2 self.abs_max_v: 4954.0\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.960897/  2.053317, val:  69.17%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7931%\n",
      "layer   2  Sparsity: 78.3900%\n",
      "layer   3  Sparsity: 81.5934%\n",
      "total_backward_count 1184590 real_backward_count 129774  10.955%\n",
      "lif layer 2 self.abs_max_v: 4994.5\n",
      "fc layer 2 self.abs_max_out: 4704.0\n",
      "fc layer 1 self.abs_max_out: 9958.0\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.960254/  2.058093, val:  78.33%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7765%\n",
      "layer   2  Sparsity: 78.5365%\n",
      "layer   3  Sparsity: 81.8046%\n",
      "total_backward_count 1194380 real_backward_count 130623  10.936%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.972843/  2.055348, val:  66.25%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7950%\n",
      "layer   2  Sparsity: 78.5034%\n",
      "layer   3  Sparsity: 82.0057%\n",
      "total_backward_count 1204170 real_backward_count 131511  10.921%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.962775/  2.049525, val:  76.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7822%\n",
      "layer   2  Sparsity: 78.5724%\n",
      "layer   3  Sparsity: 82.1450%\n",
      "total_backward_count 1213960 real_backward_count 132381  10.905%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.964364/  2.038923, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7932%\n",
      "layer   2  Sparsity: 78.5758%\n",
      "layer   3  Sparsity: 82.2033%\n",
      "total_backward_count 1223750 real_backward_count 133222  10.886%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.959086/  2.040446, val:  72.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7904%\n",
      "layer   2  Sparsity: 78.6285%\n",
      "layer   3  Sparsity: 82.1041%\n",
      "total_backward_count 1233540 real_backward_count 134058  10.868%\n",
      "fc layer 3 self.abs_max_out: 501.0\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.966226/  2.051776, val:  74.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7929%\n",
      "layer   2  Sparsity: 78.6931%\n",
      "layer   3  Sparsity: 82.0918%\n",
      "total_backward_count 1243330 real_backward_count 134860  10.847%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.963200/  2.059530, val:  72.50%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7881%\n",
      "layer   2  Sparsity: 78.5306%\n",
      "layer   3  Sparsity: 82.0530%\n",
      "total_backward_count 1253120 real_backward_count 135754  10.833%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.965285/  2.050644, val:  70.00%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7816%\n",
      "layer   2  Sparsity: 78.4298%\n",
      "layer   3  Sparsity: 82.3729%\n",
      "total_backward_count 1262910 real_backward_count 136654  10.821%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.961086/  2.043547, val:  74.58%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7900%\n",
      "layer   2  Sparsity: 78.5085%\n",
      "layer   3  Sparsity: 82.0980%\n",
      "total_backward_count 1272700 real_backward_count 137540  10.807%\n",
      "fc layer 1 self.abs_max_out: 10168.0\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.951198/  2.045937, val:  65.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7885%\n",
      "layer   2  Sparsity: 78.6267%\n",
      "layer   3  Sparsity: 81.4213%\n",
      "total_backward_count 1282490 real_backward_count 138389  10.791%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.951985/  2.048830, val:  67.92%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7899%\n",
      "layer   2  Sparsity: 78.6193%\n",
      "layer   3  Sparsity: 81.6908%\n",
      "total_backward_count 1292280 real_backward_count 139260  10.776%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.957978/  2.046731, val:  70.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7907%\n",
      "layer   2  Sparsity: 78.5808%\n",
      "layer   3  Sparsity: 81.7888%\n",
      "total_backward_count 1302070 real_backward_count 140067  10.757%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.950603/  2.030759, val:  68.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7874%\n",
      "layer   2  Sparsity: 78.2774%\n",
      "layer   3  Sparsity: 81.7653%\n",
      "total_backward_count 1311860 real_backward_count 140928  10.743%\n",
      "lif layer 2 self.abs_max_v: 5093.5\n",
      "lif layer 2 self.abs_max_v: 5135.5\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.946968/  2.038659, val:  78.75%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7810%\n",
      "layer   2  Sparsity: 78.4222%\n",
      "layer   3  Sparsity: 81.6858%\n",
      "total_backward_count 1321650 real_backward_count 141768  10.727%\n",
      "lif layer 2 self.abs_max_v: 5223.5\n",
      "lif layer 2 self.abs_max_v: 5310.0\n",
      "lif layer 2 self.abs_max_v: 5492.0\n",
      "lif layer 2 self.abs_max_v: 5566.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.961096/  2.043336, val:  76.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7880%\n",
      "layer   2  Sparsity: 78.4768%\n",
      "layer   3  Sparsity: 82.0372%\n",
      "total_backward_count 1331440 real_backward_count 142608  10.711%\n",
      "fc layer 2 self.abs_max_out: 4742.0\n",
      "fc layer 2 self.abs_max_out: 4778.0\n",
      "lif layer 1 self.abs_max_v: 12107.5\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.955791/  2.043445, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7879%\n",
      "layer   2  Sparsity: 78.3155%\n",
      "layer   3  Sparsity: 81.8142%\n",
      "total_backward_count 1341230 real_backward_count 143387  10.691%\n",
      "fc layer 2 self.abs_max_out: 4842.0\n",
      "lif layer 1 self.abs_max_v: 12246.0\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.952536/  2.035871, val:  68.75%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7950%\n",
      "layer   2  Sparsity: 78.2392%\n",
      "layer   3  Sparsity: 82.0446%\n",
      "total_backward_count 1351020 real_backward_count 144211  10.674%\n",
      "fc layer 1 self.abs_max_out: 10248.0\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.951893/  2.034569, val:  74.58%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7899%\n",
      "layer   2  Sparsity: 78.1986%\n",
      "layer   3  Sparsity: 82.0917%\n",
      "total_backward_count 1360810 real_backward_count 145051  10.659%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.944000/  2.026745, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7890%\n",
      "layer   2  Sparsity: 78.1307%\n",
      "layer   3  Sparsity: 81.2722%\n",
      "total_backward_count 1370600 real_backward_count 145878  10.643%\n",
      "lif layer 1 self.abs_max_v: 12324.0\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.945042/  2.036270, val:  67.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7853%\n",
      "layer   2  Sparsity: 78.4674%\n",
      "layer   3  Sparsity: 81.3692%\n",
      "total_backward_count 1380390 real_backward_count 146697  10.627%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.948379/  2.035289, val:  72.50%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7771%\n",
      "layer   2  Sparsity: 78.6381%\n",
      "layer   3  Sparsity: 81.3435%\n",
      "total_backward_count 1390180 real_backward_count 147507  10.611%\n",
      "fc layer 3 self.abs_max_out: 514.0\n",
      "lif layer 1 self.abs_max_v: 12564.0\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.951595/  2.030234, val:  65.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7922%\n",
      "layer   2  Sparsity: 78.5076%\n",
      "layer   3  Sparsity: 81.3826%\n",
      "total_backward_count 1399970 real_backward_count 148366  10.598%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.944377/  2.030049, val:  66.67%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.8053%\n",
      "layer   2  Sparsity: 78.3096%\n",
      "layer   3  Sparsity: 81.5569%\n",
      "total_backward_count 1409760 real_backward_count 149199  10.583%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.942443/  2.024677, val:  74.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.7831%\n",
      "layer   2  Sparsity: 78.2325%\n",
      "layer   3  Sparsity: 81.7379%\n",
      "total_backward_count 1419550 real_backward_count 149998  10.567%\n",
      "lif layer 1 self.abs_max_v: 12590.0\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.938924/  2.030517, val:  68.75%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7890%\n",
      "layer   2  Sparsity: 78.4978%\n",
      "layer   3  Sparsity: 81.8879%\n",
      "total_backward_count 1429340 real_backward_count 150810  10.551%\n",
      "lif layer 1 self.abs_max_v: 12836.0\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.941889/  2.036187, val:  79.58%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7976%\n",
      "layer   2  Sparsity: 78.4029%\n",
      "layer   3  Sparsity: 81.5219%\n",
      "total_backward_count 1439130 real_backward_count 151628  10.536%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.934452/  2.026500, val:  62.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.00 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7934%\n",
      "layer   2  Sparsity: 78.2625%\n",
      "layer   3  Sparsity: 81.5484%\n",
      "total_backward_count 1448920 real_backward_count 152439  10.521%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.935957/  2.021655, val:  70.42%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7849%\n",
      "layer   2  Sparsity: 78.2593%\n",
      "layer   3  Sparsity: 81.9085%\n",
      "total_backward_count 1458710 real_backward_count 153298  10.509%\n",
      "lif layer 1 self.abs_max_v: 12855.5\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.943274/  2.039116, val:  73.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7797%\n",
      "layer   2  Sparsity: 78.3778%\n",
      "layer   3  Sparsity: 82.1596%\n",
      "total_backward_count 1468500 real_backward_count 154088  10.493%\n",
      "lif layer 1 self.abs_max_v: 13008.5\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.943565/  2.044812, val:  65.00%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7827%\n",
      "layer   2  Sparsity: 78.2973%\n",
      "layer   3  Sparsity: 82.0360%\n",
      "total_backward_count 1478290 real_backward_count 154889  10.478%\n",
      "fc layer 1 self.abs_max_out: 10269.0\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.945235/  2.031689, val:  65.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7869%\n",
      "layer   2  Sparsity: 78.2833%\n",
      "layer   3  Sparsity: 82.1368%\n",
      "total_backward_count 1488080 real_backward_count 155666  10.461%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.936691/  2.023431, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7853%\n",
      "layer   2  Sparsity: 78.0935%\n",
      "layer   3  Sparsity: 82.0854%\n",
      "total_backward_count 1497870 real_backward_count 156465  10.446%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.943478/  2.029979, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7867%\n",
      "layer   2  Sparsity: 78.0791%\n",
      "layer   3  Sparsity: 82.0890%\n",
      "total_backward_count 1507660 real_backward_count 157313  10.434%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.945543/  2.035436, val:  71.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7853%\n",
      "layer   2  Sparsity: 78.2053%\n",
      "layer   3  Sparsity: 82.0671%\n",
      "total_backward_count 1517450 real_backward_count 158097  10.419%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.945593/  2.029381, val:  65.00%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.7956%\n",
      "layer   2  Sparsity: 78.3635%\n",
      "layer   3  Sparsity: 81.7745%\n",
      "total_backward_count 1527240 real_backward_count 158866  10.402%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.945380/  2.034670, val:  77.08%, val_best:  82.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.7790%\n",
      "layer   2  Sparsity: 78.4018%\n",
      "layer   3  Sparsity: 82.2960%\n",
      "total_backward_count 1537030 real_backward_count 159652  10.387%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.945429/  2.035115, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7916%\n",
      "layer   2  Sparsity: 78.3386%\n",
      "layer   3  Sparsity: 82.2651%\n",
      "total_backward_count 1546820 real_backward_count 160424  10.371%\n",
      "lif layer 1 self.abs_max_v: 13064.0\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.950703/  2.046734, val:  82.08%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7882%\n",
      "layer   2  Sparsity: 78.3886%\n",
      "layer   3  Sparsity: 82.0367%\n",
      "total_backward_count 1556610 real_backward_count 161204  10.356%\n",
      "lif layer 1 self.abs_max_v: 13198.5\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.951526/  2.031134, val:  75.00%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7865%\n",
      "layer   2  Sparsity: 78.1534%\n",
      "layer   3  Sparsity: 81.4380%\n",
      "total_backward_count 1566400 real_backward_count 161997  10.342%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.941338/  2.026186, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7921%\n",
      "layer   2  Sparsity: 78.1932%\n",
      "layer   3  Sparsity: 81.6267%\n",
      "total_backward_count 1576190 real_backward_count 162783  10.328%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.948078/  2.034924, val:  76.67%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7859%\n",
      "layer   2  Sparsity: 78.2089%\n",
      "layer   3  Sparsity: 81.9322%\n",
      "total_backward_count 1585980 real_backward_count 163603  10.316%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.944803/  2.026773, val:  70.42%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.8020%\n",
      "layer   2  Sparsity: 78.3231%\n",
      "layer   3  Sparsity: 82.1246%\n",
      "total_backward_count 1595770 real_backward_count 164387  10.301%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.940089/  2.040527, val:  71.67%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7871%\n",
      "layer   2  Sparsity: 78.2114%\n",
      "layer   3  Sparsity: 81.9156%\n",
      "total_backward_count 1605560 real_backward_count 165185  10.288%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.952808/  2.037167, val:  74.58%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7913%\n",
      "layer   2  Sparsity: 78.4781%\n",
      "layer   3  Sparsity: 82.1498%\n",
      "total_backward_count 1615350 real_backward_count 165992  10.276%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.945110/  2.032343, val:  62.50%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7855%\n",
      "layer   2  Sparsity: 78.2390%\n",
      "layer   3  Sparsity: 81.8626%\n",
      "total_backward_count 1625140 real_backward_count 166817  10.265%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.941626/  2.034249, val:  73.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7970%\n",
      "layer   2  Sparsity: 78.1079%\n",
      "layer   3  Sparsity: 81.7952%\n",
      "total_backward_count 1634930 real_backward_count 167588  10.250%\n",
      "fc layer 2 self.abs_max_out: 4881.0\n",
      "lif layer 1 self.abs_max_v: 13376.5\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.944870/  2.032542, val:  70.00%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7899%\n",
      "layer   2  Sparsity: 78.2016%\n",
      "layer   3  Sparsity: 81.8187%\n",
      "total_backward_count 1644720 real_backward_count 168354  10.236%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.947963/  2.032829, val:  76.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7810%\n",
      "layer   2  Sparsity: 78.1370%\n",
      "layer   3  Sparsity: 81.7401%\n",
      "total_backward_count 1654510 real_backward_count 169139  10.223%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.948794/  2.024343, val:  76.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7909%\n",
      "layer   2  Sparsity: 78.2805%\n",
      "layer   3  Sparsity: 81.7116%\n",
      "total_backward_count 1664300 real_backward_count 169942  10.211%\n",
      "lif layer 1 self.abs_max_v: 13459.0\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.939983/  2.032693, val:  69.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7872%\n",
      "layer   2  Sparsity: 78.2264%\n",
      "layer   3  Sparsity: 81.5819%\n",
      "total_backward_count 1674090 real_backward_count 170715  10.197%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.939192/  2.031823, val:  71.25%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7874%\n",
      "layer   2  Sparsity: 78.3680%\n",
      "layer   3  Sparsity: 81.5611%\n",
      "total_backward_count 1683880 real_backward_count 171477  10.183%\n",
      "lif layer 1 self.abs_max_v: 13660.0\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.939719/  2.035841, val:  69.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7839%\n",
      "layer   2  Sparsity: 78.1039%\n",
      "layer   3  Sparsity: 81.5001%\n",
      "total_backward_count 1693670 real_backward_count 172270  10.171%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.946906/  2.038135, val:  71.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7846%\n",
      "layer   2  Sparsity: 78.0478%\n",
      "layer   3  Sparsity: 81.3432%\n",
      "total_backward_count 1703460 real_backward_count 173045  10.158%\n",
      "lif layer 1 self.abs_max_v: 13668.5\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.951918/  2.036996, val:  73.75%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7908%\n",
      "layer   2  Sparsity: 78.0544%\n",
      "layer   3  Sparsity: 81.8299%\n",
      "total_backward_count 1713250 real_backward_count 173783  10.143%\n",
      "lif layer 1 self.abs_max_v: 13738.5\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.942217/  2.024770, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 93.7727%\n",
      "layer   2  Sparsity: 77.6446%\n",
      "layer   3  Sparsity: 81.2481%\n",
      "total_backward_count 1723040 real_backward_count 174574  10.132%\n",
      "lif layer 1 self.abs_max_v: 14023.0\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.941626/  2.027271, val:  75.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.8051%\n",
      "layer   2  Sparsity: 77.7355%\n",
      "layer   3  Sparsity: 81.3105%\n",
      "total_backward_count 1732830 real_backward_count 175331  10.118%\n",
      "fc layer 1 self.abs_max_out: 10456.0\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.940956/  2.040562, val:  67.08%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7863%\n",
      "layer   2  Sparsity: 78.0550%\n",
      "layer   3  Sparsity: 81.3799%\n",
      "total_backward_count 1742620 real_backward_count 176099  10.105%\n",
      "fc layer 2 self.abs_max_out: 4930.0\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.946732/  2.032447, val:  60.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7808%\n",
      "layer   2  Sparsity: 78.0166%\n",
      "layer   3  Sparsity: 81.5226%\n",
      "total_backward_count 1752410 real_backward_count 176880  10.094%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.941070/  2.033456, val:  78.75%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7898%\n",
      "layer   2  Sparsity: 78.0539%\n",
      "layer   3  Sparsity: 81.6837%\n",
      "total_backward_count 1762200 real_backward_count 177661  10.082%\n",
      "fc layer 2 self.abs_max_out: 4943.0\n",
      "fc layer 2 self.abs_max_out: 4964.0\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.945009/  2.025355, val:  70.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7921%\n",
      "layer   2  Sparsity: 78.1171%\n",
      "layer   3  Sparsity: 81.6923%\n",
      "total_backward_count 1771990 real_backward_count 178429  10.069%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.939779/  2.045071, val:  64.17%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.10 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7808%\n",
      "layer   2  Sparsity: 77.9405%\n",
      "layer   3  Sparsity: 81.2482%\n",
      "total_backward_count 1781780 real_backward_count 179236  10.059%\n",
      "fc layer 2 self.abs_max_out: 5078.0\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.942270/  2.032630, val:  70.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7873%\n",
      "layer   2  Sparsity: 77.9215%\n",
      "layer   3  Sparsity: 81.6676%\n",
      "total_backward_count 1791570 real_backward_count 180002  10.047%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.937085/  2.024514, val:  66.25%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7857%\n",
      "layer   2  Sparsity: 77.9041%\n",
      "layer   3  Sparsity: 81.7241%\n",
      "total_backward_count 1801360 real_backward_count 180772  10.035%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.937886/  2.030792, val:  68.33%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.7872%\n",
      "layer   2  Sparsity: 77.9193%\n",
      "layer   3  Sparsity: 81.8442%\n",
      "total_backward_count 1811150 real_backward_count 181544  10.024%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.946060/  2.027729, val:  66.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7892%\n",
      "layer   2  Sparsity: 77.9105%\n",
      "layer   3  Sparsity: 81.8752%\n",
      "total_backward_count 1820940 real_backward_count 182368  10.015%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.950700/  2.036745, val:  78.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7890%\n",
      "layer   2  Sparsity: 78.0620%\n",
      "layer   3  Sparsity: 82.0092%\n",
      "total_backward_count 1830730 real_backward_count 183134  10.003%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.945650/  2.037146, val:  75.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.7850%\n",
      "layer   2  Sparsity: 78.0411%\n",
      "layer   3  Sparsity: 81.8522%\n",
      "total_backward_count 1840520 real_backward_count 183892   9.991%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.939759/  2.033247, val:  77.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7807%\n",
      "layer   2  Sparsity: 78.0246%\n",
      "layer   3  Sparsity: 81.3603%\n",
      "total_backward_count 1850310 real_backward_count 184688   9.981%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.934489/  2.022565, val:  82.08%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 93.7802%\n",
      "layer   2  Sparsity: 77.9475%\n",
      "layer   3  Sparsity: 81.8293%\n",
      "total_backward_count 1860100 real_backward_count 185480   9.972%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.938321/  2.029336, val:  80.00%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7924%\n",
      "layer   2  Sparsity: 77.9073%\n",
      "layer   3  Sparsity: 81.7402%\n",
      "total_backward_count 1869890 real_backward_count 186257   9.961%\n",
      "fc layer 2 self.abs_max_out: 5114.0\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.929847/  2.018613, val:  70.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7881%\n",
      "layer   2  Sparsity: 77.7695%\n",
      "layer   3  Sparsity: 81.0223%\n",
      "total_backward_count 1879680 real_backward_count 187064   9.952%\n",
      "fc layer 2 self.abs_max_out: 5137.0\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.921985/  2.025105, val:  70.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7746%\n",
      "layer   2  Sparsity: 77.8022%\n",
      "layer   3  Sparsity: 80.9598%\n",
      "total_backward_count 1889470 real_backward_count 187819   9.940%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.925056/  2.027015, val:  68.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7972%\n",
      "layer   2  Sparsity: 77.8488%\n",
      "layer   3  Sparsity: 81.6588%\n",
      "total_backward_count 1899260 real_backward_count 188573   9.929%\n",
      "lif layer 1 self.abs_max_v: 14117.5\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.924491/  2.017188, val:  59.17%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7918%\n",
      "layer   2  Sparsity: 78.0601%\n",
      "layer   3  Sparsity: 81.2732%\n",
      "total_backward_count 1909050 real_backward_count 189296   9.916%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.923488/  2.014136, val:  72.50%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7911%\n",
      "layer   2  Sparsity: 78.0343%\n",
      "layer   3  Sparsity: 81.1833%\n",
      "total_backward_count 1918840 real_backward_count 190088   9.906%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.917769/  2.019309, val:  69.58%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7815%\n",
      "layer   2  Sparsity: 78.0980%\n",
      "layer   3  Sparsity: 81.3600%\n",
      "total_backward_count 1928630 real_backward_count 190889   9.898%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.933476/  2.018974, val:  73.75%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7857%\n",
      "layer   2  Sparsity: 78.0397%\n",
      "layer   3  Sparsity: 81.5474%\n",
      "total_backward_count 1938420 real_backward_count 191721   9.891%\n",
      "fc layer 2 self.abs_max_out: 5176.0\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.926347/  2.023308, val:  70.00%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.90 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7816%\n",
      "layer   2  Sparsity: 77.9912%\n",
      "layer   3  Sparsity: 81.5555%\n",
      "total_backward_count 1948210 real_backward_count 192472   9.879%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.927651/  2.021590, val:  77.92%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 93.7876%\n",
      "layer   2  Sparsity: 77.7594%\n",
      "layer   3  Sparsity: 81.3784%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f59bcab848745509bcf7d5a2ffb6888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>1.92765</td></tr><tr><td>val_acc_best</td><td>0.84167</td></tr><tr><td>val_acc_now</td><td>0.77917</td></tr><tr><td>val_loss</td><td>2.02159</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-sweep-68</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vp4sykfl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/vp4sykfl</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_103156-vp4sykfl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1awl74mu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_145738-1awl74mu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1awl74mu' target=\"_blank\">spring-sweep-72</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1awl74mu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1awl74mu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251115_145748_118', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 6, 'leaky_temporal_filter': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 555.0\n",
      "lif layer 1 self.abs_max_v: 555.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1039.0\n",
      "lif layer 2 self.abs_max_v: 1039.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 502.0\n",
      "fc layer 1 self.abs_max_out: 584.0\n",
      "lif layer 1 self.abs_max_v: 829.5\n",
      "lif layer 2 self.abs_max_v: 1379.5\n",
      "fc layer 1 self.abs_max_out: 672.0\n",
      "lif layer 1 self.abs_max_v: 894.0\n",
      "fc layer 1 self.abs_max_out: 701.0\n",
      "lif layer 1 self.abs_max_v: 955.0\n",
      "lif layer 2 self.abs_max_v: 1443.5\n",
      "fc layer 1 self.abs_max_out: 766.0\n",
      "lif layer 2 self.abs_max_v: 1737.0\n",
      "fc layer 1 self.abs_max_out: 930.0\n",
      "lif layer 1 self.abs_max_v: 1158.5\n",
      "fc layer 2 self.abs_max_out: 1065.0\n",
      "lif layer 2 self.abs_max_v: 1774.0\n",
      "lif layer 1 self.abs_max_v: 1195.5\n",
      "lif layer 2 self.abs_max_v: 1816.0\n",
      "lif layer 1 self.abs_max_v: 1401.0\n",
      "fc layer 2 self.abs_max_out: 1281.0\n",
      "lif layer 2 self.abs_max_v: 1972.0\n",
      "fc layer 1 self.abs_max_out: 1159.0\n",
      "fc layer 1 self.abs_max_out: 1172.0\n",
      "lif layer 1 self.abs_max_v: 1567.0\n",
      "lif layer 1 self.abs_max_v: 1882.5\n",
      "fc layer 1 self.abs_max_out: 1261.0\n",
      "lif layer 1 self.abs_max_v: 1983.5\n",
      "fc layer 1 self.abs_max_out: 1524.0\n",
      "lif layer 1 self.abs_max_v: 2348.5\n",
      "fc layer 1 self.abs_max_out: 1614.0\n",
      "lif layer 1 self.abs_max_v: 2413.0\n",
      "lif layer 1 self.abs_max_v: 2561.5\n",
      "fc layer 1 self.abs_max_out: 1665.0\n",
      "fc layer 1 self.abs_max_out: 2134.0\n",
      "fc layer 1 self.abs_max_out: 2210.0\n",
      "lif layer 1 self.abs_max_v: 2605.0\n",
      "lif layer 1 self.abs_max_v: 3412.5\n",
      "fc layer 1 self.abs_max_out: 2297.0\n",
      "lif layer 1 self.abs_max_v: 3885.5\n",
      "fc layer 1 self.abs_max_out: 2626.0\n",
      "lif layer 1 self.abs_max_v: 3887.5\n",
      "lif layer 1 self.abs_max_v: 4210.0\n",
      "fc layer 3 self.abs_max_out: 602.0\n",
      "fc layer 1 self.abs_max_out: 2863.0\n",
      "fc layer 1 self.abs_max_out: 2968.0\n",
      "fc layer 2 self.abs_max_out: 1343.0\n",
      "fc layer 2 self.abs_max_out: 1363.0\n",
      "lif layer 2 self.abs_max_v: 2006.5\n",
      "fc layer 2 self.abs_max_out: 1675.0\n",
      "lif layer 2 self.abs_max_v: 2048.5\n",
      "lif layer 2 self.abs_max_v: 2255.0\n",
      "lif layer 2 self.abs_max_v: 2271.5\n",
      "fc layer 3 self.abs_max_out: 635.0\n",
      "lif layer 2 self.abs_max_v: 2511.5\n",
      "fc layer 2 self.abs_max_out: 1734.0\n",
      "fc layer 2 self.abs_max_out: 1789.0\n",
      "lif layer 2 self.abs_max_v: 2513.0\n",
      "lif layer 2 self.abs_max_v: 2555.0\n",
      "fc layer 2 self.abs_max_out: 1845.0\n",
      "fc layer 2 self.abs_max_out: 1846.0\n",
      "lif layer 2 self.abs_max_v: 2577.5\n",
      "lif layer 1 self.abs_max_v: 4374.5\n",
      "lif layer 2 self.abs_max_v: 2626.5\n",
      "lif layer 2 self.abs_max_v: 2639.5\n",
      "lif layer 2 self.abs_max_v: 2728.0\n",
      "lif layer 2 self.abs_max_v: 2953.0\n",
      "fc layer 2 self.abs_max_out: 1911.0\n",
      "lif layer 2 self.abs_max_v: 2979.0\n",
      "lif layer 2 self.abs_max_v: 3031.0\n",
      "lif layer 2 self.abs_max_v: 3142.5\n",
      "lif layer 2 self.abs_max_v: 3151.5\n",
      "lif layer 1 self.abs_max_v: 4728.0\n",
      "lif layer 2 self.abs_max_v: 3229.0\n",
      "fc layer 1 self.abs_max_out: 2971.0\n",
      "lif layer 1 self.abs_max_v: 4921.0\n",
      "lif layer 1 self.abs_max_v: 5034.5\n",
      "lif layer 1 self.abs_max_v: 5318.5\n",
      "lif layer 1 self.abs_max_v: 5383.5\n",
      "lif layer 1 self.abs_max_v: 5514.0\n",
      "fc layer 1 self.abs_max_out: 3267.0\n",
      "fc layer 1 self.abs_max_out: 3306.0\n",
      "lif layer 1 self.abs_max_v: 5612.5\n",
      "fc layer 1 self.abs_max_out: 3590.0\n",
      "lif layer 1 self.abs_max_v: 5754.5\n",
      "lif layer 1 self.abs_max_v: 6067.0\n",
      "lif layer 1 self.abs_max_v: 6137.5\n",
      "lif layer 1 self.abs_max_v: 6211.0\n",
      "lif layer 1 self.abs_max_v: 6268.0\n",
      "lif layer 1 self.abs_max_v: 6437.0\n",
      "fc layer 1 self.abs_max_out: 3762.0\n",
      "fc layer 1 self.abs_max_out: 3779.0\n",
      "lif layer 1 self.abs_max_v: 6438.0\n",
      "lif layer 1 self.abs_max_v: 6649.0\n",
      "lif layer 1 self.abs_max_v: 6849.5\n",
      "fc layer 3 self.abs_max_out: 668.0\n",
      "fc layer 3 self.abs_max_out: 687.0\n",
      "fc layer 3 self.abs_max_out: 725.0\n",
      "fc layer 3 self.abs_max_out: 812.0\n",
      "lif layer 1 self.abs_max_v: 7027.5\n",
      "lif layer 1 self.abs_max_v: 7038.5\n",
      "lif layer 1 self.abs_max_v: 7053.0\n",
      "fc layer 1 self.abs_max_out: 3864.0\n",
      "fc layer 1 self.abs_max_out: 3940.0\n",
      "lif layer 1 self.abs_max_v: 7099.0\n",
      "fc layer 3 self.abs_max_out: 852.0\n",
      "lif layer 1 self.abs_max_v: 7149.0\n",
      "lif layer 1 self.abs_max_v: 7289.0\n",
      "fc layer 1 self.abs_max_out: 3981.0\n",
      "fc layer 1 self.abs_max_out: 4032.0\n",
      "fc layer 1 self.abs_max_out: 4345.0\n",
      "lif layer 1 self.abs_max_v: 7568.0\n",
      "lif layer 1 self.abs_max_v: 7609.5\n",
      "fc layer 1 self.abs_max_out: 4403.0\n",
      "lif layer 1 self.abs_max_v: 8208.0\n",
      "lif layer 1 self.abs_max_v: 8299.0\n",
      "lif layer 1 self.abs_max_v: 8399.5\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.723642/  1.905908, val:  41.67%, val_best:  41.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6830%\n",
      "layer   2  Sparsity: 73.3906%\n",
      "layer   3  Sparsity: 64.3709%\n",
      "total_backward_count 9790 real_backward_count 1481  15.128%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 4405.0\n",
      "fc layer 1 self.abs_max_out: 4709.0\n",
      "lif layer 1 self.abs_max_v: 8527.0\n",
      "fc layer 3 self.abs_max_out: 919.0\n",
      "fc layer 3 self.abs_max_out: 924.0\n",
      "fc layer 3 self.abs_max_out: 930.0\n",
      "fc layer 2 self.abs_max_out: 1968.0\n",
      "fc layer 3 self.abs_max_out: 952.0\n",
      "fc layer 3 self.abs_max_out: 1063.0\n",
      "fc layer 3 self.abs_max_out: 1108.0\n",
      "fc layer 3 self.abs_max_out: 1118.0\n",
      "fc layer 1 self.abs_max_out: 4804.0\n",
      "lif layer 1 self.abs_max_v: 8888.5\n",
      "fc layer 1 self.abs_max_out: 5047.0\n",
      "lif layer 1 self.abs_max_v: 8950.0\n",
      "lif layer 1 self.abs_max_v: 9132.5\n",
      "lif layer 1 self.abs_max_v: 9260.0\n",
      "fc layer 1 self.abs_max_out: 5113.0\n",
      "fc layer 1 self.abs_max_out: 5269.0\n",
      "lif layer 1 self.abs_max_v: 9824.5\n",
      "lif layer 1 self.abs_max_v: 9924.5\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.579159/  1.852209, val:  39.58%, val_best:  41.67%, tr:  99.49%, tr_best:  99.69%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6807%\n",
      "layer   2  Sparsity: 77.6252%\n",
      "layer   3  Sparsity: 65.7158%\n",
      "total_backward_count 19580 real_backward_count 2676  13.667%\n",
      "fc layer 1 self.abs_max_out: 5426.0\n",
      "fc layer 1 self.abs_max_out: 5878.0\n",
      "lif layer 1 self.abs_max_v: 10771.0\n",
      "lif layer 1 self.abs_max_v: 10825.5\n",
      "lif layer 1 self.abs_max_v: 11070.0\n",
      "lif layer 1 self.abs_max_v: 11345.5\n",
      "fc layer 1 self.abs_max_out: 5920.0\n",
      "lif layer 2 self.abs_max_v: 3253.5\n",
      "lif layer 2 self.abs_max_v: 3257.0\n",
      "lif layer 2 self.abs_max_v: 3514.5\n",
      "lif layer 2 self.abs_max_v: 3601.0\n",
      "fc layer 2 self.abs_max_out: 2004.0\n",
      "lif layer 2 self.abs_max_v: 3601.5\n",
      "lif layer 2 self.abs_max_v: 3622.0\n",
      "lif layer 2 self.abs_max_v: 3705.5\n",
      "fc layer 2 self.abs_max_out: 2007.0\n",
      "lif layer 2 self.abs_max_v: 3708.0\n",
      "fc layer 1 self.abs_max_out: 6096.0\n",
      "fc layer 2 self.abs_max_out: 2012.0\n",
      "fc layer 2 self.abs_max_out: 2026.0\n",
      "fc layer 2 self.abs_max_out: 2053.0\n",
      "lif layer 1 self.abs_max_v: 11347.0\n",
      "fc layer 1 self.abs_max_out: 6193.0\n",
      "lif layer 1 self.abs_max_v: 11866.5\n",
      "fc layer 2 self.abs_max_out: 2158.0\n",
      "lif layer 2 self.abs_max_v: 3791.0\n",
      "lif layer 2 self.abs_max_v: 4037.5\n",
      "fc layer 2 self.abs_max_out: 2227.0\n",
      "lif layer 2 self.abs_max_v: 4246.0\n",
      "fc layer 1 self.abs_max_out: 6492.0\n",
      "fc layer 1 self.abs_max_out: 6734.0\n",
      "lif layer 1 self.abs_max_v: 12513.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.560582/  1.836881, val:  51.25%, val_best:  51.25%, tr:  99.18%, tr_best:  99.69%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6884%\n",
      "layer   2  Sparsity: 79.6060%\n",
      "layer   3  Sparsity: 67.1276%\n",
      "total_backward_count 29370 real_backward_count 3859  13.139%\n",
      "fc layer 1 self.abs_max_out: 6807.0\n",
      "lif layer 1 self.abs_max_v: 12539.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.565352/  1.819973, val:  49.17%, val_best:  51.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6845%\n",
      "layer   2  Sparsity: 81.1462%\n",
      "layer   3  Sparsity: 67.8390%\n",
      "total_backward_count 39160 real_backward_count 5006  12.783%\n",
      "fc layer 3 self.abs_max_out: 1154.0\n",
      "lif layer 1 self.abs_max_v: 12566.5\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.522527/  1.761559, val:  50.42%, val_best:  51.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6853%\n",
      "layer   2  Sparsity: 81.9795%\n",
      "layer   3  Sparsity: 67.9498%\n",
      "total_backward_count 48950 real_backward_count 6130  12.523%\n",
      "lif layer 1 self.abs_max_v: 12886.0\n",
      "fc layer 1 self.abs_max_out: 6916.0\n",
      "lif layer 1 self.abs_max_v: 12932.0\n",
      "fc layer 1 self.abs_max_out: 7136.0\n",
      "fc layer 1 self.abs_max_out: 7150.0\n",
      "lif layer 1 self.abs_max_v: 13095.5\n",
      "fc layer 1 self.abs_max_out: 7320.0\n",
      "lif layer 1 self.abs_max_v: 13868.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.515793/  1.819274, val:  47.08%, val_best:  51.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6895%\n",
      "layer   2  Sparsity: 81.9024%\n",
      "layer   3  Sparsity: 67.7202%\n",
      "total_backward_count 58740 real_backward_count 7243  12.331%\n",
      "fc layer 2 self.abs_max_out: 2230.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.507132/  1.792776, val:  53.75%, val_best:  53.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6975%\n",
      "layer   2  Sparsity: 81.9211%\n",
      "layer   3  Sparsity: 68.2609%\n",
      "total_backward_count 68530 real_backward_count 8349  12.183%\n",
      "fc layer 1 self.abs_max_out: 7334.0\n",
      "fc layer 3 self.abs_max_out: 1187.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.501604/  1.765563, val:  54.58%, val_best:  54.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7022%\n",
      "layer   2  Sparsity: 81.9388%\n",
      "layer   3  Sparsity: 69.6021%\n",
      "total_backward_count 78320 real_backward_count 9352  11.941%\n",
      "fc layer 2 self.abs_max_out: 2250.0\n",
      "lif layer 2 self.abs_max_v: 4287.0\n",
      "fc layer 2 self.abs_max_out: 2312.0\n",
      "fc layer 2 self.abs_max_out: 2315.0\n",
      "lif layer 2 self.abs_max_v: 4372.0\n",
      "fc layer 2 self.abs_max_out: 2348.0\n",
      "fc layer 2 self.abs_max_out: 2415.0\n",
      "lif layer 2 self.abs_max_v: 4565.5\n",
      "lif layer 2 self.abs_max_v: 4624.0\n",
      "fc layer 2 self.abs_max_out: 2456.0\n",
      "lif layer 2 self.abs_max_v: 4661.5\n",
      "fc layer 2 self.abs_max_out: 2469.0\n",
      "fc layer 2 self.abs_max_out: 2481.0\n",
      "lif layer 2 self.abs_max_v: 4704.5\n",
      "fc layer 2 self.abs_max_out: 2598.0\n",
      "lif layer 2 self.abs_max_v: 4727.0\n",
      "lif layer 2 self.abs_max_v: 4746.5\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.499536/  1.704676, val:  58.75%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6680%\n",
      "layer   2  Sparsity: 80.0627%\n",
      "layer   3  Sparsity: 69.8078%\n",
      "total_backward_count 88110 real_backward_count 10434  11.842%\n",
      "fc layer 1 self.abs_max_out: 7379.0\n",
      "fc layer 1 self.abs_max_out: 7609.0\n",
      "lif layer 2 self.abs_max_v: 4748.5\n",
      "fc layer 1 self.abs_max_out: 7869.0\n",
      "lif layer 1 self.abs_max_v: 14386.5\n",
      "fc layer 1 self.abs_max_out: 8119.0\n",
      "lif layer 1 self.abs_max_v: 14744.5\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.483949/  1.797013, val:  50.83%, val_best:  58.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6989%\n",
      "layer   2  Sparsity: 79.1291%\n",
      "layer   3  Sparsity: 69.5908%\n",
      "total_backward_count 97900 real_backward_count 11431  11.676%\n",
      "fc layer 3 self.abs_max_out: 1196.0\n",
      "fc layer 3 self.abs_max_out: 1254.0\n",
      "fc layer 3 self.abs_max_out: 1259.0\n",
      "fc layer 1 self.abs_max_out: 8355.0\n",
      "lif layer 1 self.abs_max_v: 15088.0\n",
      "fc layer 1 self.abs_max_out: 8534.0\n",
      "lif layer 1 self.abs_max_v: 15488.5\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.481591/  1.794519, val:  48.33%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6858%\n",
      "layer   2  Sparsity: 79.7248%\n",
      "layer   3  Sparsity: 69.3587%\n",
      "total_backward_count 107690 real_backward_count 12419  11.532%\n",
      "fc layer 1 self.abs_max_out: 8683.0\n",
      "lif layer 1 self.abs_max_v: 15576.0\n",
      "lif layer 1 self.abs_max_v: 15596.5\n",
      "fc layer 3 self.abs_max_out: 1300.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.476876/  1.743048, val:  56.67%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.7037%\n",
      "layer   2  Sparsity: 79.4907%\n",
      "layer   3  Sparsity: 69.0803%\n",
      "total_backward_count 117480 real_backward_count 13420  11.423%\n",
      "lif layer 1 self.abs_max_v: 15971.5\n",
      "fc layer 1 self.abs_max_out: 8753.0\n",
      "fc layer 1 self.abs_max_out: 9131.0\n",
      "lif layer 2 self.abs_max_v: 4784.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.452816/  1.711156, val:  57.08%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6972%\n",
      "layer   2  Sparsity: 79.6628%\n",
      "layer   3  Sparsity: 69.0819%\n",
      "total_backward_count 127270 real_backward_count 14363  11.285%\n",
      "fc layer 3 self.abs_max_out: 1308.0\n",
      "fc layer 2 self.abs_max_out: 2607.0\n",
      "lif layer 1 self.abs_max_v: 16248.5\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.437176/  1.728548, val:  52.50%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6936%\n",
      "layer   2  Sparsity: 79.0035%\n",
      "layer   3  Sparsity: 67.9532%\n",
      "total_backward_count 137060 real_backward_count 15333  11.187%\n",
      "fc layer 2 self.abs_max_out: 2694.0\n",
      "lif layer 2 self.abs_max_v: 4897.5\n",
      "fc layer 2 self.abs_max_out: 2762.0\n",
      "fc layer 2 self.abs_max_out: 2872.0\n",
      "lif layer 2 self.abs_max_v: 4909.0\n",
      "lif layer 2 self.abs_max_v: 5059.5\n",
      "lif layer 2 self.abs_max_v: 5072.0\n",
      "lif layer 2 self.abs_max_v: 5149.0\n",
      "lif layer 2 self.abs_max_v: 5154.0\n",
      "lif layer 2 self.abs_max_v: 5174.0\n",
      "lif layer 2 self.abs_max_v: 5241.0\n",
      "lif layer 2 self.abs_max_v: 5386.0\n",
      "fc layer 2 self.abs_max_out: 2990.0\n",
      "lif layer 2 self.abs_max_v: 5434.5\n",
      "lif layer 2 self.abs_max_v: 5637.5\n",
      "fc layer 2 self.abs_max_out: 2999.0\n",
      "lif layer 2 self.abs_max_v: 5818.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.423190/  1.725204, val:  51.67%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6793%\n",
      "layer   2  Sparsity: 77.6180%\n",
      "layer   3  Sparsity: 67.6427%\n",
      "total_backward_count 146850 real_backward_count 16301  11.100%\n",
      "fc layer 1 self.abs_max_out: 9256.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.406259/  1.708385, val:  53.75%, val_best:  58.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6850%\n",
      "layer   2  Sparsity: 77.5712%\n",
      "layer   3  Sparsity: 66.9867%\n",
      "total_backward_count 156640 real_backward_count 17243  11.008%\n",
      "fc layer 3 self.abs_max_out: 1312.0\n",
      "lif layer 1 self.abs_max_v: 16266.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.405148/  1.697270, val:  59.58%, val_best:  59.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6862%\n",
      "layer   2  Sparsity: 76.9127%\n",
      "layer   3  Sparsity: 67.3290%\n",
      "total_backward_count 166430 real_backward_count 18181  10.924%\n",
      "fc layer 3 self.abs_max_out: 1369.0\n",
      "fc layer 3 self.abs_max_out: 1440.0\n",
      "lif layer 1 self.abs_max_v: 16609.0\n",
      "lif layer 1 self.abs_max_v: 16760.5\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.396551/  1.699594, val:  61.25%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6979%\n",
      "layer   2  Sparsity: 76.3789%\n",
      "layer   3  Sparsity: 68.0558%\n",
      "total_backward_count 176220 real_backward_count 19100  10.839%\n",
      "fc layer 2 self.abs_max_out: 3029.0\n",
      "fc layer 2 self.abs_max_out: 3131.0\n",
      "lif layer 1 self.abs_max_v: 16812.0\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.364425/  1.689701, val:  53.75%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7169%\n",
      "layer   2  Sparsity: 76.6826%\n",
      "layer   3  Sparsity: 66.9718%\n",
      "total_backward_count 186010 real_backward_count 20027  10.767%\n",
      "fc layer 1 self.abs_max_out: 9284.0\n",
      "lif layer 1 self.abs_max_v: 17014.0\n",
      "lif layer 1 self.abs_max_v: 17103.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.361916/  1.727641, val:  49.17%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6813%\n",
      "layer   2  Sparsity: 77.1214%\n",
      "layer   3  Sparsity: 66.5881%\n",
      "total_backward_count 195800 real_backward_count 20913  10.681%\n",
      "fc layer 1 self.abs_max_out: 9376.0\n",
      "fc layer 1 self.abs_max_out: 9508.0\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.374066/  1.701401, val:  55.83%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6756%\n",
      "layer   2  Sparsity: 75.8151%\n",
      "layer   3  Sparsity: 67.4461%\n",
      "total_backward_count 205590 real_backward_count 21825  10.616%\n",
      "fc layer 1 self.abs_max_out: 9598.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.364510/  1.717858, val:  55.42%, val_best:  61.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6707%\n",
      "layer   2  Sparsity: 75.2008%\n",
      "layer   3  Sparsity: 66.9379%\n",
      "total_backward_count 215380 real_backward_count 22754  10.565%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.364156/  1.666118, val:  56.67%, val_best:  61.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6750%\n",
      "layer   2  Sparsity: 74.5705%\n",
      "layer   3  Sparsity: 66.8364%\n",
      "total_backward_count 225170 real_backward_count 23642  10.500%\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.355917/  1.662495, val:  60.00%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7012%\n",
      "layer   2  Sparsity: 74.9862%\n",
      "layer   3  Sparsity: 66.4769%\n",
      "total_backward_count 234960 real_backward_count 24570  10.457%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.326856/  1.637131, val:  62.50%, val_best:  62.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7065%\n",
      "layer   2  Sparsity: 75.0643%\n",
      "layer   3  Sparsity: 65.3961%\n",
      "total_backward_count 244750 real_backward_count 25477  10.409%\n",
      "fc layer 2 self.abs_max_out: 3152.0\n",
      "lif layer 2 self.abs_max_v: 5958.0\n",
      "lif layer 2 self.abs_max_v: 6034.5\n",
      "lif layer 2 self.abs_max_v: 6049.5\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.366688/  1.669265, val:  54.58%, val_best:  62.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7052%\n",
      "layer   2  Sparsity: 74.8160%\n",
      "layer   3  Sparsity: 66.6932%\n",
      "total_backward_count 254540 real_backward_count 26419  10.379%\n",
      "fc layer 2 self.abs_max_out: 3173.0\n",
      "fc layer 2 self.abs_max_out: 3222.0\n",
      "fc layer 2 self.abs_max_out: 3268.0\n",
      "lif layer 2 self.abs_max_v: 6098.0\n",
      "fc layer 1 self.abs_max_out: 9689.0\n",
      "fc layer 2 self.abs_max_out: 3276.0\n",
      "lif layer 2 self.abs_max_v: 6253.0\n",
      "fc layer 2 self.abs_max_out: 3365.0\n",
      "lif layer 2 self.abs_max_v: 6491.5\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.343350/  1.643947, val:  55.42%, val_best:  62.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 80.08 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6943%\n",
      "layer   2  Sparsity: 76.0438%\n",
      "layer   3  Sparsity: 66.6662%\n",
      "total_backward_count 264330 real_backward_count 27273  10.318%\n",
      "fc layer 1 self.abs_max_out: 10241.0\n",
      "lif layer 1 self.abs_max_v: 17255.0\n",
      "fc layer 1 self.abs_max_out: 10329.0\n",
      "lif layer 1 self.abs_max_v: 17720.0\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.345376/  1.633452, val:  61.67%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7033%\n",
      "layer   2  Sparsity: 75.8750%\n",
      "layer   3  Sparsity: 65.2119%\n",
      "total_backward_count 274120 real_backward_count 28138  10.265%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.315569/  1.637591, val:  59.17%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.61 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6947%\n",
      "layer   2  Sparsity: 74.6373%\n",
      "layer   3  Sparsity: 65.7744%\n",
      "total_backward_count 283910 real_backward_count 29000  10.215%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.314328/  1.624486, val:  57.08%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6784%\n",
      "layer   2  Sparsity: 74.0167%\n",
      "layer   3  Sparsity: 65.0630%\n",
      "total_backward_count 293700 real_backward_count 29846  10.162%\n",
      "lif layer 1 self.abs_max_v: 18146.0\n",
      "lif layer 1 self.abs_max_v: 18186.0\n",
      "fc layer 1 self.abs_max_out: 10519.0\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.292007/  1.580304, val:  63.75%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6980%\n",
      "layer   2  Sparsity: 75.3411%\n",
      "layer   3  Sparsity: 64.8273%\n",
      "total_backward_count 303490 real_backward_count 30730  10.126%\n",
      "fc layer 1 self.abs_max_out: 10883.0\n",
      "lif layer 1 self.abs_max_v: 18955.5\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.282367/  1.618827, val:  49.17%, val_best:  63.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7041%\n",
      "layer   2  Sparsity: 75.9100%\n",
      "layer   3  Sparsity: 64.5654%\n",
      "total_backward_count 313280 real_backward_count 31543  10.069%\n",
      "fc layer 3 self.abs_max_out: 1460.0\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.274254/  1.637508, val:  59.58%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6988%\n",
      "layer   2  Sparsity: 75.5165%\n",
      "layer   3  Sparsity: 64.7808%\n",
      "total_backward_count 323070 real_backward_count 32365  10.018%\n",
      "fc layer 3 self.abs_max_out: 1481.0\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.300614/  1.605880, val:  58.33%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6857%\n",
      "layer   2  Sparsity: 74.5893%\n",
      "layer   3  Sparsity: 64.7898%\n",
      "total_backward_count 332860 real_backward_count 33247   9.988%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.271867/  1.588171, val:  60.00%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6950%\n",
      "layer   2  Sparsity: 74.6575%\n",
      "layer   3  Sparsity: 65.4276%\n",
      "total_backward_count 342650 real_backward_count 34090   9.949%\n",
      "fc layer 2 self.abs_max_out: 3429.0\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.267938/  1.612905, val:  55.83%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7063%\n",
      "layer   2  Sparsity: 74.5377%\n",
      "layer   3  Sparsity: 65.2153%\n",
      "total_backward_count 352440 real_backward_count 34940   9.914%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.268616/  1.576796, val:  54.17%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6872%\n",
      "layer   2  Sparsity: 74.5031%\n",
      "layer   3  Sparsity: 66.0685%\n",
      "total_backward_count 362230 real_backward_count 35780   9.878%\n",
      "lif layer 1 self.abs_max_v: 19114.5\n",
      "fc layer 3 self.abs_max_out: 1530.0\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.256443/  1.575068, val:  58.33%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.7019%\n",
      "layer   2  Sparsity: 74.4999%\n",
      "layer   3  Sparsity: 65.9845%\n",
      "total_backward_count 372020 real_backward_count 36578   9.832%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.256939/  1.580110, val:  63.75%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6802%\n",
      "layer   2  Sparsity: 74.6077%\n",
      "layer   3  Sparsity: 65.3721%\n",
      "total_backward_count 381810 real_backward_count 37441   9.806%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.273709/  1.615419, val:  60.00%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7010%\n",
      "layer   2  Sparsity: 74.5921%\n",
      "layer   3  Sparsity: 66.6014%\n",
      "total_backward_count 391600 real_backward_count 38268   9.772%\n",
      "fc layer 1 self.abs_max_out: 11169.0\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.276155/  1.589652, val:  58.33%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6841%\n",
      "layer   2  Sparsity: 74.8145%\n",
      "layer   3  Sparsity: 66.2936%\n",
      "total_backward_count 401390 real_backward_count 39116   9.745%\n",
      "fc layer 3 self.abs_max_out: 1545.0\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.238659/  1.579585, val:  58.33%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6824%\n",
      "layer   2  Sparsity: 74.5401%\n",
      "layer   3  Sparsity: 65.6751%\n",
      "total_backward_count 411180 real_backward_count 39971   9.721%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.249665/  1.626223, val:  57.92%, val_best:  63.75%, tr:  99.39%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6989%\n",
      "layer   2  Sparsity: 74.2134%\n",
      "layer   3  Sparsity: 65.9851%\n",
      "total_backward_count 420970 real_backward_count 40785   9.688%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.236871/  1.575775, val:  61.67%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6858%\n",
      "layer   2  Sparsity: 74.2114%\n",
      "layer   3  Sparsity: 64.9230%\n",
      "total_backward_count 430760 real_backward_count 41589   9.655%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.223822/  1.550728, val:  61.25%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6961%\n",
      "layer   2  Sparsity: 74.3767%\n",
      "layer   3  Sparsity: 65.1207%\n",
      "total_backward_count 440550 real_backward_count 42374   9.618%\n",
      "lif layer 1 self.abs_max_v: 19655.0\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.220394/  1.552100, val:  55.42%, val_best:  63.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6967%\n",
      "layer   2  Sparsity: 75.2523%\n",
      "layer   3  Sparsity: 64.0299%\n",
      "total_backward_count 450340 real_backward_count 43235   9.601%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.237031/  1.541301, val:  62.50%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6816%\n",
      "layer   2  Sparsity: 75.2108%\n",
      "layer   3  Sparsity: 63.6404%\n",
      "total_backward_count 460130 real_backward_count 44065   9.577%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.213955/  1.596204, val:  51.25%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7039%\n",
      "layer   2  Sparsity: 75.4729%\n",
      "layer   3  Sparsity: 62.9566%\n",
      "total_backward_count 469920 real_backward_count 44848   9.544%\n",
      "lif layer 1 self.abs_max_v: 19822.5\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.220769/  1.602660, val:  58.75%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7001%\n",
      "layer   2  Sparsity: 75.6345%\n",
      "layer   3  Sparsity: 64.3112%\n",
      "total_backward_count 479710 real_backward_count 45621   9.510%\n",
      "fc layer 2 self.abs_max_out: 3569.0\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.224012/  1.596137, val:  53.33%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6967%\n",
      "layer   2  Sparsity: 75.6055%\n",
      "layer   3  Sparsity: 63.9878%\n",
      "total_backward_count 489500 real_backward_count 46435   9.486%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.207386/  1.548295, val:  60.83%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6857%\n",
      "layer   2  Sparsity: 75.0998%\n",
      "layer   3  Sparsity: 63.3971%\n",
      "total_backward_count 499290 real_backward_count 47277   9.469%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.200640/  1.560790, val:  61.25%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6861%\n",
      "layer   2  Sparsity: 74.7585%\n",
      "layer   3  Sparsity: 63.0011%\n",
      "total_backward_count 509080 real_backward_count 48058   9.440%\n",
      "fc layer 2 self.abs_max_out: 3639.0\n",
      "lif layer 2 self.abs_max_v: 6597.0\n",
      "lif layer 2 self.abs_max_v: 6615.5\n",
      "fc layer 1 self.abs_max_out: 11763.0\n",
      "lif layer 2 self.abs_max_v: 6639.5\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.206756/  1.548863, val:  61.25%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7284%\n",
      "layer   2  Sparsity: 74.6601%\n",
      "layer   3  Sparsity: 63.4233%\n",
      "total_backward_count 518870 real_backward_count 48841   9.413%\n",
      "lif layer 2 self.abs_max_v: 6798.5\n",
      "fc layer 2 self.abs_max_out: 3779.0\n",
      "lif layer 2 self.abs_max_v: 6799.0\n",
      "lif layer 2 self.abs_max_v: 7022.5\n",
      "lif layer 2 self.abs_max_v: 7090.5\n",
      "fc layer 2 self.abs_max_out: 3789.0\n",
      "fc layer 2 self.abs_max_out: 3926.0\n",
      "lif layer 2 self.abs_max_v: 7180.0\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.203561/  1.540809, val:  61.25%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6962%\n",
      "layer   2  Sparsity: 74.7767%\n",
      "layer   3  Sparsity: 63.9660%\n",
      "total_backward_count 528660 real_backward_count 49634   9.389%\n",
      "fc layer 3 self.abs_max_out: 1557.0\n",
      "fc layer 3 self.abs_max_out: 1566.0\n",
      "lif layer 1 self.abs_max_v: 20081.0\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.179061/  1.557614, val:  57.50%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6817%\n",
      "layer   2  Sparsity: 75.0107%\n",
      "layer   3  Sparsity: 64.5324%\n",
      "total_backward_count 538450 real_backward_count 50427   9.365%\n",
      "lif layer 1 self.abs_max_v: 20292.5\n",
      "fc layer 2 self.abs_max_out: 3934.0\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.190518/  1.548920, val:  58.75%, val_best:  63.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6981%\n",
      "layer   2  Sparsity: 75.0639%\n",
      "layer   3  Sparsity: 64.0557%\n",
      "total_backward_count 548240 real_backward_count 51220   9.343%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.173179/  1.564675, val:  52.08%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7058%\n",
      "layer   2  Sparsity: 75.1690%\n",
      "layer   3  Sparsity: 64.3438%\n",
      "total_backward_count 558030 real_backward_count 52010   9.320%\n",
      "lif layer 1 self.abs_max_v: 20304.5\n",
      "fc layer 3 self.abs_max_out: 1607.0\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.175329/  1.526148, val:  62.50%, val_best:  63.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.29 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.6892%\n",
      "layer   2  Sparsity: 75.3640%\n",
      "layer   3  Sparsity: 64.0318%\n",
      "total_backward_count 567820 real_backward_count 52762   9.292%\n",
      "lif layer 1 self.abs_max_v: 20353.5\n",
      "lif layer 1 self.abs_max_v: 20819.0\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.166719/  1.541518, val:  60.83%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6826%\n",
      "layer   2  Sparsity: 75.4421%\n",
      "layer   3  Sparsity: 63.1064%\n",
      "total_backward_count 577610 real_backward_count 53541   9.269%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.181934/  1.511148, val:  63.33%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7024%\n",
      "layer   2  Sparsity: 75.1984%\n",
      "layer   3  Sparsity: 63.4917%\n",
      "total_backward_count 587400 real_backward_count 54320   9.248%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.160790/  1.516788, val:  58.33%, val_best:  63.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6856%\n",
      "layer   2  Sparsity: 75.4574%\n",
      "layer   3  Sparsity: 62.9979%\n",
      "total_backward_count 597190 real_backward_count 55144   9.234%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.180417/  1.567766, val:  60.83%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6823%\n",
      "layer   2  Sparsity: 75.2112%\n",
      "layer   3  Sparsity: 62.3390%\n",
      "total_backward_count 606980 real_backward_count 55964   9.220%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.177671/  1.563990, val:  55.83%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6927%\n",
      "layer   2  Sparsity: 75.1943%\n",
      "layer   3  Sparsity: 61.0821%\n",
      "total_backward_count 616770 real_backward_count 56770   9.204%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.151101/  1.544717, val:  59.58%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7206%\n",
      "layer   2  Sparsity: 75.4772%\n",
      "layer   3  Sparsity: 61.9853%\n",
      "total_backward_count 626560 real_backward_count 57518   9.180%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.129771/  1.499984, val:  65.83%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6999%\n",
      "layer   2  Sparsity: 75.8322%\n",
      "layer   3  Sparsity: 61.7625%\n",
      "total_backward_count 636350 real_backward_count 58262   9.156%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.114084/  1.530669, val:  60.83%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6839%\n",
      "layer   2  Sparsity: 75.6859%\n",
      "layer   3  Sparsity: 61.3443%\n",
      "total_backward_count 646140 real_backward_count 58980   9.128%\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.139101/  1.546989, val:  60.42%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6966%\n",
      "layer   2  Sparsity: 75.4186%\n",
      "layer   3  Sparsity: 63.0721%\n",
      "total_backward_count 655930 real_backward_count 59794   9.116%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.143605/  1.500637, val:  59.17%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6976%\n",
      "layer   2  Sparsity: 75.0998%\n",
      "layer   3  Sparsity: 62.9982%\n",
      "total_backward_count 665720 real_backward_count 60591   9.102%\n",
      "lif layer 2 self.abs_max_v: 7290.0\n",
      "fc layer 3 self.abs_max_out: 1666.0\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.163088/  1.515254, val:  64.17%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6965%\n",
      "layer   2  Sparsity: 74.4950%\n",
      "layer   3  Sparsity: 63.7231%\n",
      "total_backward_count 675510 real_backward_count 61351   9.082%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.160029/  1.601675, val:  53.33%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6778%\n",
      "layer   2  Sparsity: 74.1734%\n",
      "layer   3  Sparsity: 63.8669%\n",
      "total_backward_count 685300 real_backward_count 62128   9.066%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.180594/  1.544148, val:  55.83%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6736%\n",
      "layer   2  Sparsity: 74.5421%\n",
      "layer   3  Sparsity: 64.0418%\n",
      "total_backward_count 695090 real_backward_count 62943   9.055%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.163385/  1.524988, val:  59.17%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6762%\n",
      "layer   2  Sparsity: 75.7308%\n",
      "layer   3  Sparsity: 64.5014%\n",
      "total_backward_count 704880 real_backward_count 63743   9.043%\n",
      "lif layer 1 self.abs_max_v: 21001.5\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.172819/  1.515031, val:  62.92%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6924%\n",
      "layer   2  Sparsity: 75.1937%\n",
      "layer   3  Sparsity: 64.9527%\n",
      "total_backward_count 714670 real_backward_count 64567   9.035%\n",
      "fc layer 2 self.abs_max_out: 3979.0\n",
      "fc layer 1 self.abs_max_out: 12538.0\n",
      "lif layer 1 self.abs_max_v: 21787.0\n",
      "lif layer 1 self.abs_max_v: 22360.5\n",
      "fc layer 1 self.abs_max_out: 12616.0\n",
      "lif layer 1 self.abs_max_v: 22385.0\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.150919/  1.541631, val:  60.42%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7005%\n",
      "layer   2  Sparsity: 75.1277%\n",
      "layer   3  Sparsity: 65.1954%\n",
      "total_backward_count 724460 real_backward_count 65344   9.020%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.162573/  1.528787, val:  62.50%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6719%\n",
      "layer   2  Sparsity: 75.0016%\n",
      "layer   3  Sparsity: 65.4145%\n",
      "total_backward_count 734250 real_backward_count 66154   9.010%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.151126/  1.531864, val:  61.67%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7112%\n",
      "layer   2  Sparsity: 75.6101%\n",
      "layer   3  Sparsity: 65.7578%\n",
      "total_backward_count 744040 real_backward_count 66901   8.992%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.169370/  1.528092, val:  55.42%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6783%\n",
      "layer   2  Sparsity: 75.7148%\n",
      "layer   3  Sparsity: 64.8685%\n",
      "total_backward_count 753830 real_backward_count 67733   8.985%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.159872/  1.517421, val:  62.50%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7143%\n",
      "layer   2  Sparsity: 75.9128%\n",
      "layer   3  Sparsity: 63.8243%\n",
      "total_backward_count 763620 real_backward_count 68510   8.972%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.167753/  1.536042, val:  60.83%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7036%\n",
      "layer   2  Sparsity: 76.5570%\n",
      "layer   3  Sparsity: 66.1610%\n",
      "total_backward_count 773410 real_backward_count 69279   8.958%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.187591/  1.536787, val:  63.75%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6894%\n",
      "layer   2  Sparsity: 76.3095%\n",
      "layer   3  Sparsity: 66.1082%\n",
      "total_backward_count 783200 real_backward_count 70036   8.942%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.179798/  1.506041, val:  65.42%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7020%\n",
      "layer   2  Sparsity: 75.8863%\n",
      "layer   3  Sparsity: 66.2795%\n",
      "total_backward_count 792990 real_backward_count 70783   8.926%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.190030/  1.525209, val:  60.00%, val_best:  65.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.7003%\n",
      "layer   2  Sparsity: 75.3361%\n",
      "layer   3  Sparsity: 67.0710%\n",
      "total_backward_count 802780 real_backward_count 71548   8.913%\n",
      "lif layer 2 self.abs_max_v: 7411.5\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.186849/  1.541681, val:  61.25%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6712%\n",
      "layer   2  Sparsity: 75.1909%\n",
      "layer   3  Sparsity: 67.1795%\n",
      "total_backward_count 812570 real_backward_count 72280   8.895%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.176502/  1.537251, val:  60.83%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6886%\n",
      "layer   2  Sparsity: 74.7699%\n",
      "layer   3  Sparsity: 67.3967%\n",
      "total_backward_count 822360 real_backward_count 73040   8.882%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.192431/  1.550915, val:  63.33%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6982%\n",
      "layer   2  Sparsity: 74.8285%\n",
      "layer   3  Sparsity: 67.6562%\n",
      "total_backward_count 832150 real_backward_count 73793   8.868%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.198114/  1.532108, val:  62.08%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6980%\n",
      "layer   2  Sparsity: 75.2455%\n",
      "layer   3  Sparsity: 67.3138%\n",
      "total_backward_count 841940 real_backward_count 74570   8.857%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.163868/  1.524214, val:  62.08%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6990%\n",
      "layer   2  Sparsity: 75.0453%\n",
      "layer   3  Sparsity: 66.0811%\n",
      "total_backward_count 851730 real_backward_count 75302   8.841%\n",
      "lif layer 2 self.abs_max_v: 7496.5\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.174981/  1.547386, val:  64.17%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6787%\n",
      "layer   2  Sparsity: 74.6519%\n",
      "layer   3  Sparsity: 65.8307%\n",
      "total_backward_count 861520 real_backward_count 76074   8.830%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.157533/  1.506217, val:  57.92%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6874%\n",
      "layer   2  Sparsity: 74.9233%\n",
      "layer   3  Sparsity: 64.9217%\n",
      "total_backward_count 871310 real_backward_count 76827   8.817%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.154418/  1.543266, val:  65.42%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7223%\n",
      "layer   2  Sparsity: 74.8718%\n",
      "layer   3  Sparsity: 64.6751%\n",
      "total_backward_count 881100 real_backward_count 77574   8.804%\n",
      "lif layer 2 self.abs_max_v: 7515.5\n",
      "lif layer 2 self.abs_max_v: 7604.0\n",
      "fc layer 1 self.abs_max_out: 12819.0\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.139866/  1.462192, val:  71.25%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6772%\n",
      "layer   2  Sparsity: 74.8596%\n",
      "layer   3  Sparsity: 64.0696%\n",
      "total_backward_count 890890 real_backward_count 78375   8.797%\n",
      "fc layer 2 self.abs_max_out: 4020.0\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.127529/  1.481067, val:  66.67%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6889%\n",
      "layer   2  Sparsity: 75.4308%\n",
      "layer   3  Sparsity: 64.3074%\n",
      "total_backward_count 900680 real_backward_count 79077   8.780%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.120982/  1.487457, val:  62.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7088%\n",
      "layer   2  Sparsity: 75.2903%\n",
      "layer   3  Sparsity: 64.7147%\n",
      "total_backward_count 910470 real_backward_count 79807   8.765%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.114783/  1.458225, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6979%\n",
      "layer   2  Sparsity: 75.2982%\n",
      "layer   3  Sparsity: 64.0283%\n",
      "total_backward_count 920260 real_backward_count 80588   8.757%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.106918/  1.491342, val:  64.17%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6948%\n",
      "layer   2  Sparsity: 76.2736%\n",
      "layer   3  Sparsity: 65.0668%\n",
      "total_backward_count 930050 real_backward_count 81379   8.750%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.136875/  1.510048, val:  62.08%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6916%\n",
      "layer   2  Sparsity: 75.5275%\n",
      "layer   3  Sparsity: 64.0268%\n",
      "total_backward_count 939840 real_backward_count 82168   8.743%\n",
      "lif layer 2 self.abs_max_v: 7605.5\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.117246/  1.475961, val:  64.17%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6841%\n",
      "layer   2  Sparsity: 75.2408%\n",
      "layer   3  Sparsity: 64.6687%\n",
      "total_backward_count 949630 real_backward_count 82931   8.733%\n",
      "lif layer 1 self.abs_max_v: 22860.5\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.097314/  1.462946, val:  63.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7065%\n",
      "layer   2  Sparsity: 74.5536%\n",
      "layer   3  Sparsity: 64.7379%\n",
      "total_backward_count 959420 real_backward_count 83663   8.720%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.114361/  1.509443, val:  57.08%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6888%\n",
      "layer   2  Sparsity: 74.6617%\n",
      "layer   3  Sparsity: 64.8680%\n",
      "total_backward_count 969210 real_backward_count 84420   8.710%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.115266/  1.468181, val:  62.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6994%\n",
      "layer   2  Sparsity: 74.9858%\n",
      "layer   3  Sparsity: 64.6188%\n",
      "total_backward_count 979000 real_backward_count 85168   8.699%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.115984/  1.509642, val:  65.00%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.6822%\n",
      "layer   2  Sparsity: 75.4619%\n",
      "layer   3  Sparsity: 65.5184%\n",
      "total_backward_count 988790 real_backward_count 85931   8.691%\n",
      "fc layer 1 self.abs_max_out: 12833.0\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.135711/  1.531621, val:  58.33%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.7054%\n",
      "layer   2  Sparsity: 75.5160%\n",
      "layer   3  Sparsity: 64.2893%\n",
      "total_backward_count 998580 real_backward_count 86708   8.683%\n",
      "fc layer 1 self.abs_max_out: 13280.0\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.114197/  1.493606, val:  60.42%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6903%\n",
      "layer   2  Sparsity: 75.7584%\n",
      "layer   3  Sparsity: 64.1754%\n",
      "total_backward_count 1008370 real_backward_count 87450   8.672%\n",
      "fc layer 1 self.abs_max_out: 13784.0\n",
      "lif layer 1 self.abs_max_v: 23036.0\n",
      "lif layer 1 self.abs_max_v: 23455.0\n",
      "lif layer 1 self.abs_max_v: 23789.5\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.122223/  1.555066, val:  61.25%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6905%\n",
      "layer   2  Sparsity: 75.9255%\n",
      "layer   3  Sparsity: 63.8844%\n",
      "total_backward_count 1018160 real_backward_count 88211   8.664%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.143240/  1.494289, val:  58.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6962%\n",
      "layer   2  Sparsity: 76.4920%\n",
      "layer   3  Sparsity: 64.4902%\n",
      "total_backward_count 1027950 real_backward_count 89016   8.660%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.102034/  1.449898, val:  64.58%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.7080%\n",
      "layer   2  Sparsity: 76.3490%\n",
      "layer   3  Sparsity: 63.6727%\n",
      "total_backward_count 1037740 real_backward_count 89781   8.652%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.101889/  1.473208, val:  63.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.7106%\n",
      "layer   2  Sparsity: 76.7384%\n",
      "layer   3  Sparsity: 64.5849%\n",
      "total_backward_count 1047530 real_backward_count 90504   8.640%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.133368/  1.511382, val:  57.92%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6873%\n",
      "layer   2  Sparsity: 76.5579%\n",
      "layer   3  Sparsity: 66.2573%\n",
      "total_backward_count 1057320 real_backward_count 91282   8.633%\n",
      "lif layer 1 self.abs_max_v: 23822.0\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.134729/  1.484212, val:  62.08%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6862%\n",
      "layer   2  Sparsity: 76.1799%\n",
      "layer   3  Sparsity: 66.0377%\n",
      "total_backward_count 1067110 real_backward_count 92046   8.626%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.109549/  1.454941, val:  65.00%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6717%\n",
      "layer   2  Sparsity: 75.8751%\n",
      "layer   3  Sparsity: 66.3719%\n",
      "total_backward_count 1076900 real_backward_count 92821   8.619%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.103219/  1.513638, val:  62.50%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6653%\n",
      "layer   2  Sparsity: 75.9368%\n",
      "layer   3  Sparsity: 66.6722%\n",
      "total_backward_count 1086690 real_backward_count 93554   8.609%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.133882/  1.489427, val:  64.17%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7004%\n",
      "layer   2  Sparsity: 75.6436%\n",
      "layer   3  Sparsity: 66.6585%\n",
      "total_backward_count 1096480 real_backward_count 94244   8.595%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.128991/  1.462536, val:  60.83%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6847%\n",
      "layer   2  Sparsity: 75.7711%\n",
      "layer   3  Sparsity: 65.8027%\n",
      "total_backward_count 1106270 real_backward_count 94968   8.585%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.116055/  1.531483, val:  59.58%, val_best:  71.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7072%\n",
      "layer   2  Sparsity: 75.3300%\n",
      "layer   3  Sparsity: 66.1199%\n",
      "total_backward_count 1116060 real_backward_count 95735   8.578%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.175357/  1.520721, val:  63.75%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6996%\n",
      "layer   2  Sparsity: 74.5907%\n",
      "layer   3  Sparsity: 66.0845%\n",
      "total_backward_count 1125850 real_backward_count 96491   8.571%\n",
      "lif layer 1 self.abs_max_v: 24958.0\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.158345/  1.505380, val:  59.58%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6777%\n",
      "layer   2  Sparsity: 74.4620%\n",
      "layer   3  Sparsity: 66.8758%\n",
      "total_backward_count 1135640 real_backward_count 97261   8.564%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.112186/  1.455899, val:  67.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6985%\n",
      "layer   2  Sparsity: 74.5741%\n",
      "layer   3  Sparsity: 66.3292%\n",
      "total_backward_count 1145430 real_backward_count 98007   8.556%\n",
      "fc layer 1 self.abs_max_out: 14929.0\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.126313/  1.484732, val:  65.83%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.7012%\n",
      "layer   2  Sparsity: 75.0173%\n",
      "layer   3  Sparsity: 66.7157%\n",
      "total_backward_count 1155220 real_backward_count 98759   8.549%\n",
      "fc layer 2 self.abs_max_out: 4107.0\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.133261/  1.525733, val:  60.83%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6738%\n",
      "layer   2  Sparsity: 74.9390%\n",
      "layer   3  Sparsity: 66.4692%\n",
      "total_backward_count 1165010 real_backward_count 99537   8.544%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.126637/  1.475661, val:  61.25%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7109%\n",
      "layer   2  Sparsity: 75.5995%\n",
      "layer   3  Sparsity: 65.9358%\n",
      "total_backward_count 1174800 real_backward_count 100276   8.536%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.087114/  1.440628, val:  64.17%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6976%\n",
      "layer   2  Sparsity: 75.2457%\n",
      "layer   3  Sparsity: 66.2967%\n",
      "total_backward_count 1184590 real_backward_count 101007   8.527%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.123466/  1.493264, val:  64.58%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7020%\n",
      "layer   2  Sparsity: 74.6934%\n",
      "layer   3  Sparsity: 66.3342%\n",
      "total_backward_count 1194380 real_backward_count 101794   8.523%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.120733/  1.527207, val:  59.17%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6963%\n",
      "layer   2  Sparsity: 75.0270%\n",
      "layer   3  Sparsity: 66.9785%\n",
      "total_backward_count 1204170 real_backward_count 102538   8.515%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.134117/  1.480257, val:  65.83%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7058%\n",
      "layer   2  Sparsity: 74.7460%\n",
      "layer   3  Sparsity: 66.8334%\n",
      "total_backward_count 1213960 real_backward_count 103288   8.508%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.156618/  1.465367, val:  65.00%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6968%\n",
      "layer   2  Sparsity: 74.6205%\n",
      "layer   3  Sparsity: 66.1881%\n",
      "total_backward_count 1223750 real_backward_count 104035   8.501%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.115510/  1.452111, val:  66.25%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6973%\n",
      "layer   2  Sparsity: 73.6014%\n",
      "layer   3  Sparsity: 65.1299%\n",
      "total_backward_count 1233540 real_backward_count 104754   8.492%\n",
      "lif layer 2 self.abs_max_v: 7642.0\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.117232/  1.493510, val:  67.92%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7046%\n",
      "layer   2  Sparsity: 73.1521%\n",
      "layer   3  Sparsity: 65.6894%\n",
      "total_backward_count 1243330 real_backward_count 105495   8.485%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.138210/  1.505896, val:  64.58%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6927%\n",
      "layer   2  Sparsity: 73.0824%\n",
      "layer   3  Sparsity: 65.4692%\n",
      "total_backward_count 1253120 real_backward_count 106225   8.477%\n",
      "lif layer 1 self.abs_max_v: 25005.5\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.147692/  1.497763, val:  65.00%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6867%\n",
      "layer   2  Sparsity: 73.2236%\n",
      "layer   3  Sparsity: 66.1568%\n",
      "total_backward_count 1262910 real_backward_count 106974   8.470%\n",
      "lif layer 1 self.abs_max_v: 25364.0\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.149577/  1.465137, val:  70.00%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6993%\n",
      "layer   2  Sparsity: 73.0708%\n",
      "layer   3  Sparsity: 66.6152%\n",
      "total_backward_count 1272700 real_backward_count 107669   8.460%\n",
      "lif layer 1 self.abs_max_v: 25491.0\n",
      "fc layer 1 self.abs_max_out: 15252.0\n",
      "lif layer 1 self.abs_max_v: 25648.0\n",
      "fc layer 1 self.abs_max_out: 15412.0\n",
      "lif layer 1 self.abs_max_v: 25894.0\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.135599/  1.493977, val:  60.42%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6812%\n",
      "layer   2  Sparsity: 73.6107%\n",
      "layer   3  Sparsity: 64.9281%\n",
      "total_backward_count 1282490 real_backward_count 108422   8.454%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.133698/  1.490834, val:  67.50%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6902%\n",
      "layer   2  Sparsity: 73.6728%\n",
      "layer   3  Sparsity: 65.2253%\n",
      "total_backward_count 1292280 real_backward_count 109242   8.453%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.152114/  1.515860, val:  59.17%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6860%\n",
      "layer   2  Sparsity: 73.6432%\n",
      "layer   3  Sparsity: 65.9771%\n",
      "total_backward_count 1302070 real_backward_count 109974   8.446%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.135098/  1.456566, val:  63.33%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6962%\n",
      "layer   2  Sparsity: 73.2883%\n",
      "layer   3  Sparsity: 65.6854%\n",
      "total_backward_count 1311860 real_backward_count 110696   8.438%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.127712/  1.497922, val:  55.00%, val_best:  71.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7150%\n",
      "layer   2  Sparsity: 72.8816%\n",
      "layer   3  Sparsity: 65.5583%\n",
      "total_backward_count 1321650 real_backward_count 111458   8.433%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.135867/  1.465426, val:  62.50%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7105%\n",
      "layer   2  Sparsity: 72.4734%\n",
      "layer   3  Sparsity: 66.3354%\n",
      "total_backward_count 1331440 real_backward_count 112203   8.427%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.124980/  1.460029, val:  66.67%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6859%\n",
      "layer   2  Sparsity: 72.4505%\n",
      "layer   3  Sparsity: 67.2498%\n",
      "total_backward_count 1341230 real_backward_count 112938   8.420%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.103101/  1.470682, val:  60.00%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7089%\n",
      "layer   2  Sparsity: 72.1023%\n",
      "layer   3  Sparsity: 66.1347%\n",
      "total_backward_count 1351020 real_backward_count 113644   8.412%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.109603/  1.465259, val:  69.17%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.7051%\n",
      "layer   2  Sparsity: 72.6301%\n",
      "layer   3  Sparsity: 66.6388%\n",
      "total_backward_count 1360810 real_backward_count 114398   8.407%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.113515/  1.478991, val:  69.58%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6725%\n",
      "layer   2  Sparsity: 72.4780%\n",
      "layer   3  Sparsity: 65.2418%\n",
      "total_backward_count 1370600 real_backward_count 115151   8.402%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.129884/  1.496683, val:  58.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6929%\n",
      "layer   2  Sparsity: 72.4550%\n",
      "layer   3  Sparsity: 65.8847%\n",
      "total_backward_count 1380390 real_backward_count 115881   8.395%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.112098/  1.450879, val:  59.58%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6815%\n",
      "layer   2  Sparsity: 72.3421%\n",
      "layer   3  Sparsity: 65.0375%\n",
      "total_backward_count 1390180 real_backward_count 116614   8.388%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.090497/  1.437296, val:  61.25%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6865%\n",
      "layer   2  Sparsity: 72.4885%\n",
      "layer   3  Sparsity: 65.6356%\n",
      "total_backward_count 1399970 real_backward_count 117355   8.383%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.076130/  1.431772, val:  60.00%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6893%\n",
      "layer   2  Sparsity: 72.4183%\n",
      "layer   3  Sparsity: 66.0334%\n",
      "total_backward_count 1409760 real_backward_count 118064   8.375%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.102037/  1.476212, val:  62.50%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6961%\n",
      "layer   2  Sparsity: 72.2481%\n",
      "layer   3  Sparsity: 66.6064%\n",
      "total_backward_count 1419550 real_backward_count 118814   8.370%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.086071/  1.469072, val:  60.00%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6688%\n",
      "layer   2  Sparsity: 72.4548%\n",
      "layer   3  Sparsity: 64.4165%\n",
      "total_backward_count 1429340 real_backward_count 119542   8.363%\n",
      "lif layer 2 self.abs_max_v: 7659.5\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.112546/  1.456746, val:  64.17%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6842%\n",
      "layer   2  Sparsity: 72.3787%\n",
      "layer   3  Sparsity: 64.3436%\n",
      "total_backward_count 1439130 real_backward_count 120256   8.356%\n",
      "lif layer 1 self.abs_max_v: 26375.5\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.086724/  1.447338, val:  68.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6909%\n",
      "layer   2  Sparsity: 72.0864%\n",
      "layer   3  Sparsity: 65.4658%\n",
      "total_backward_count 1448920 real_backward_count 120945   8.347%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.096304/  1.451818, val:  61.67%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6937%\n",
      "layer   2  Sparsity: 71.9820%\n",
      "layer   3  Sparsity: 65.6490%\n",
      "total_backward_count 1458710 real_backward_count 121718   8.344%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.083408/  1.481251, val:  57.92%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.7030%\n",
      "layer   2  Sparsity: 72.4032%\n",
      "layer   3  Sparsity: 65.9532%\n",
      "total_backward_count 1468500 real_backward_count 122462   8.339%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.107060/  1.461349, val:  64.58%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6882%\n",
      "layer   2  Sparsity: 72.2707%\n",
      "layer   3  Sparsity: 66.0781%\n",
      "total_backward_count 1478290 real_backward_count 123154   8.331%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.104427/  1.458166, val:  58.33%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6954%\n",
      "layer   2  Sparsity: 72.6622%\n",
      "layer   3  Sparsity: 65.6217%\n",
      "total_backward_count 1488080 real_backward_count 123864   8.324%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.108869/  1.538424, val:  55.00%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7022%\n",
      "layer   2  Sparsity: 72.9073%\n",
      "layer   3  Sparsity: 65.1209%\n",
      "total_backward_count 1497870 real_backward_count 124557   8.316%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.129361/  1.439549, val:  67.08%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7067%\n",
      "layer   2  Sparsity: 72.8377%\n",
      "layer   3  Sparsity: 64.7931%\n",
      "total_backward_count 1507660 real_backward_count 125334   8.313%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.093448/  1.421340, val:  64.58%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.6981%\n",
      "layer   2  Sparsity: 73.0854%\n",
      "layer   3  Sparsity: 65.6890%\n",
      "total_backward_count 1517450 real_backward_count 126056   8.307%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.077615/  1.419470, val:  69.58%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6864%\n",
      "layer   2  Sparsity: 73.1364%\n",
      "layer   3  Sparsity: 66.0207%\n",
      "total_backward_count 1527240 real_backward_count 126785   8.302%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.077568/  1.465452, val:  63.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6917%\n",
      "layer   2  Sparsity: 73.1845%\n",
      "layer   3  Sparsity: 65.1715%\n",
      "total_backward_count 1537030 real_backward_count 127518   8.296%\n",
      "fc layer 1 self.abs_max_out: 15857.0\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.100272/  1.474458, val:  66.67%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6938%\n",
      "layer   2  Sparsity: 73.3217%\n",
      "layer   3  Sparsity: 65.0429%\n",
      "total_backward_count 1546820 real_backward_count 128230   8.290%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.122965/  1.456807, val:  59.58%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.7031%\n",
      "layer   2  Sparsity: 73.5724%\n",
      "layer   3  Sparsity: 66.0119%\n",
      "total_backward_count 1556610 real_backward_count 128970   8.285%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.113445/  1.501216, val:  64.58%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.7034%\n",
      "layer   2  Sparsity: 73.3087%\n",
      "layer   3  Sparsity: 66.7477%\n",
      "total_backward_count 1566400 real_backward_count 129741   8.283%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.125440/  1.458697, val:  65.83%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6960%\n",
      "layer   2  Sparsity: 72.7879%\n",
      "layer   3  Sparsity: 66.1513%\n",
      "total_backward_count 1576190 real_backward_count 130481   8.278%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.114227/  1.411767, val:  65.83%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6950%\n",
      "layer   2  Sparsity: 73.0897%\n",
      "layer   3  Sparsity: 66.2974%\n",
      "total_backward_count 1585980 real_backward_count 131230   8.274%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.075621/  1.429673, val:  65.83%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6892%\n",
      "layer   2  Sparsity: 72.9138%\n",
      "layer   3  Sparsity: 67.6221%\n",
      "total_backward_count 1595770 real_backward_count 131974   8.270%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.074901/  1.445708, val:  63.33%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.7038%\n",
      "layer   2  Sparsity: 72.9175%\n",
      "layer   3  Sparsity: 66.6348%\n",
      "total_backward_count 1605560 real_backward_count 132667   8.263%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.083144/  1.458092, val:  59.58%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7190%\n",
      "layer   2  Sparsity: 72.8419%\n",
      "layer   3  Sparsity: 66.1658%\n",
      "total_backward_count 1615350 real_backward_count 133342   8.255%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.074083/  1.424945, val:  70.00%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6800%\n",
      "layer   2  Sparsity: 73.0509%\n",
      "layer   3  Sparsity: 65.4258%\n",
      "total_backward_count 1625140 real_backward_count 134057   8.249%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.084879/  1.492066, val:  57.08%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6905%\n",
      "layer   2  Sparsity: 73.2889%\n",
      "layer   3  Sparsity: 66.3180%\n",
      "total_backward_count 1634930 real_backward_count 134755   8.242%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.086565/  1.531409, val:  52.50%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6864%\n",
      "layer   2  Sparsity: 73.3496%\n",
      "layer   3  Sparsity: 66.4073%\n",
      "total_backward_count 1644720 real_backward_count 135489   8.238%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.125391/  1.513633, val:  64.58%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7138%\n",
      "layer   2  Sparsity: 72.9477%\n",
      "layer   3  Sparsity: 66.6727%\n",
      "total_backward_count 1654510 real_backward_count 136245   8.235%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.098064/  1.477571, val:  61.67%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.7005%\n",
      "layer   2  Sparsity: 73.3792%\n",
      "layer   3  Sparsity: 66.4772%\n",
      "total_backward_count 1664300 real_backward_count 136957   8.229%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.089931/  1.478586, val:  64.17%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7164%\n",
      "layer   2  Sparsity: 73.5089%\n",
      "layer   3  Sparsity: 65.8137%\n",
      "total_backward_count 1674090 real_backward_count 137680   8.224%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.098558/  1.505330, val:  56.25%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6747%\n",
      "layer   2  Sparsity: 73.3781%\n",
      "layer   3  Sparsity: 64.9799%\n",
      "total_backward_count 1683880 real_backward_count 138395   8.219%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.100724/  1.453851, val:  66.25%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6723%\n",
      "layer   2  Sparsity: 73.0854%\n",
      "layer   3  Sparsity: 65.2756%\n",
      "total_backward_count 1693670 real_backward_count 139120   8.214%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.094078/  1.474561, val:  62.92%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6887%\n",
      "layer   2  Sparsity: 72.9037%\n",
      "layer   3  Sparsity: 64.8280%\n",
      "total_backward_count 1703460 real_backward_count 139818   8.208%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.101562/  1.478938, val:  60.83%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6981%\n",
      "layer   2  Sparsity: 72.6031%\n",
      "layer   3  Sparsity: 65.7038%\n",
      "total_backward_count 1713250 real_backward_count 140546   8.203%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.092776/  1.447312, val:  65.83%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7205%\n",
      "layer   2  Sparsity: 73.0292%\n",
      "layer   3  Sparsity: 65.4339%\n",
      "total_backward_count 1723040 real_backward_count 141246   8.197%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.085370/  1.431654, val:  66.25%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6972%\n",
      "layer   2  Sparsity: 73.4900%\n",
      "layer   3  Sparsity: 64.8553%\n",
      "total_backward_count 1732830 real_backward_count 141981   8.194%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.086092/  1.451958, val:  67.50%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6692%\n",
      "layer   2  Sparsity: 72.9239%\n",
      "layer   3  Sparsity: 67.4772%\n",
      "total_backward_count 1742620 real_backward_count 142648   8.186%\n",
      "lif layer 1 self.abs_max_v: 26385.5\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.079318/  1.468839, val:  65.83%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6834%\n",
      "layer   2  Sparsity: 72.9836%\n",
      "layer   3  Sparsity: 67.3503%\n",
      "total_backward_count 1752410 real_backward_count 143330   8.179%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.100325/  1.462136, val:  65.83%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.7211%\n",
      "layer   2  Sparsity: 72.6583%\n",
      "layer   3  Sparsity: 66.9038%\n",
      "total_backward_count 1762200 real_backward_count 144025   8.173%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.101669/  1.456688, val:  63.75%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6946%\n",
      "layer   2  Sparsity: 72.6303%\n",
      "layer   3  Sparsity: 67.1535%\n",
      "total_backward_count 1771990 real_backward_count 144691   8.165%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.092853/  1.490274, val:  64.58%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 83.6829%\n",
      "layer   2  Sparsity: 72.4105%\n",
      "layer   3  Sparsity: 67.4162%\n",
      "total_backward_count 1781780 real_backward_count 145376   8.159%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.100093/  1.453999, val:  59.58%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6957%\n",
      "layer   2  Sparsity: 73.1487%\n",
      "layer   3  Sparsity: 66.4315%\n",
      "total_backward_count 1791570 real_backward_count 146107   8.155%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.083963/  1.454204, val:  64.58%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6814%\n",
      "layer   2  Sparsity: 73.2268%\n",
      "layer   3  Sparsity: 66.6571%\n",
      "total_backward_count 1801360 real_backward_count 146791   8.149%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.083030/  1.445753, val:  67.92%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6974%\n",
      "layer   2  Sparsity: 72.9328%\n",
      "layer   3  Sparsity: 66.1514%\n",
      "total_backward_count 1811150 real_backward_count 147502   8.144%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.092897/  1.453199, val:  60.00%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6935%\n",
      "layer   2  Sparsity: 73.1740%\n",
      "layer   3  Sparsity: 66.2155%\n",
      "total_backward_count 1820940 real_backward_count 148210   8.139%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.091858/  1.433436, val:  68.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6811%\n",
      "layer   2  Sparsity: 73.5121%\n",
      "layer   3  Sparsity: 65.4048%\n",
      "total_backward_count 1830730 real_backward_count 148888   8.133%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.076038/  1.466802, val:  60.83%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6700%\n",
      "layer   2  Sparsity: 73.5231%\n",
      "layer   3  Sparsity: 65.9934%\n",
      "total_backward_count 1840520 real_backward_count 149601   8.128%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.072996/  1.452433, val:  64.17%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6922%\n",
      "layer   2  Sparsity: 73.3801%\n",
      "layer   3  Sparsity: 67.4705%\n",
      "total_backward_count 1850310 real_backward_count 150315   8.124%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.095974/  1.440892, val:  61.67%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6915%\n",
      "layer   2  Sparsity: 72.7068%\n",
      "layer   3  Sparsity: 68.2158%\n",
      "total_backward_count 1860100 real_backward_count 151002   8.118%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.066017/  1.438510, val:  62.92%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6827%\n",
      "layer   2  Sparsity: 73.2740%\n",
      "layer   3  Sparsity: 66.5274%\n",
      "total_backward_count 1869890 real_backward_count 151662   8.111%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.057787/  1.453858, val:  58.75%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6924%\n",
      "layer   2  Sparsity: 73.3140%\n",
      "layer   3  Sparsity: 66.5838%\n",
      "total_backward_count 1879680 real_backward_count 152378   8.107%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.086798/  1.452916, val:  67.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6611%\n",
      "layer   2  Sparsity: 73.3305%\n",
      "layer   3  Sparsity: 67.5763%\n",
      "total_backward_count 1889470 real_backward_count 153067   8.101%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.099251/  1.473699, val:  57.08%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7012%\n",
      "layer   2  Sparsity: 73.7179%\n",
      "layer   3  Sparsity: 67.1432%\n",
      "total_backward_count 1899260 real_backward_count 153722   8.094%\n",
      "fc layer 1 self.abs_max_out: 16072.0\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.109067/  1.472733, val:  57.08%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6867%\n",
      "layer   2  Sparsity: 73.4408%\n",
      "layer   3  Sparsity: 66.9152%\n",
      "total_backward_count 1909050 real_backward_count 154422   8.089%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.094768/  1.463903, val:  61.67%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6878%\n",
      "layer   2  Sparsity: 73.8511%\n",
      "layer   3  Sparsity: 66.2263%\n",
      "total_backward_count 1918840 real_backward_count 155163   8.086%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.101928/  1.451998, val:  65.00%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7100%\n",
      "layer   2  Sparsity: 74.0831%\n",
      "layer   3  Sparsity: 66.4482%\n",
      "total_backward_count 1928630 real_backward_count 155853   8.081%\n",
      "lif layer 1 self.abs_max_v: 26464.5\n",
      "lif layer 1 self.abs_max_v: 26625.5\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.093660/  1.422035, val:  65.42%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6879%\n",
      "layer   2  Sparsity: 74.5894%\n",
      "layer   3  Sparsity: 66.6509%\n",
      "total_backward_count 1938420 real_backward_count 156535   8.075%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.066960/  1.471552, val:  64.17%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7071%\n",
      "layer   2  Sparsity: 74.9683%\n",
      "layer   3  Sparsity: 66.8784%\n",
      "total_backward_count 1948210 real_backward_count 157193   8.069%\n",
      "lif layer 1 self.abs_max_v: 27239.0\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.083545/  1.445568, val:  63.75%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6786%\n",
      "layer   2  Sparsity: 74.6700%\n",
      "layer   3  Sparsity: 66.6527%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0390509acd48e0a375a0e44720ee22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñÑ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÇ‚ñÜ‚ñÜ‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñà‚ñá‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÉ‚ñá‚ñà‚ñà‚ñÖ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÖ‚ñà‚ñÖ‚ñà‚ñá‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñÑ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>1.08355</td></tr><tr><td>val_acc_best</td><td>0.7125</td></tr><tr><td>val_acc_now</td><td>0.6375</td></tr><tr><td>val_loss</td><td>1.44557</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">spring-sweep-72</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1awl74mu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1awl74mu</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_145738-1awl74mu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 26uu6kfa with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_192208-26uu6kfa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/26uu6kfa' target=\"_blank\">robust-sweep-76</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/26uu6kfa' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/26uu6kfa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251115_192217_315', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 6, 'leaky_temporal_filter': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 278.0\n",
      "lif layer 1 self.abs_max_v: 278.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 532.0\n",
      "lif layer 2 self.abs_max_v: 532.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 196.0\n",
      "fc layer 1 self.abs_max_out: 293.0\n",
      "lif layer 1 self.abs_max_v: 417.0\n",
      "lif layer 2 self.abs_max_v: 767.0\n",
      "fc layer 3 self.abs_max_out: 214.0\n",
      "fc layer 1 self.abs_max_out: 339.0\n",
      "lif layer 1 self.abs_max_v: 450.5\n",
      "fc layer 2 self.abs_max_out: 537.0\n",
      "fc layer 1 self.abs_max_out: 353.0\n",
      "lif layer 1 self.abs_max_v: 480.5\n",
      "lif layer 2 self.abs_max_v: 798.5\n",
      "fc layer 2 self.abs_max_out: 544.0\n",
      "lif layer 2 self.abs_max_v: 943.5\n",
      "fc layer 3 self.abs_max_out: 241.0\n",
      "lif layer 2 self.abs_max_v: 952.5\n",
      "fc layer 3 self.abs_max_out: 266.0\n",
      "fc layer 1 self.abs_max_out: 476.0\n",
      "fc layer 2 self.abs_max_out: 645.0\n",
      "lif layer 1 self.abs_max_v: 584.5\n",
      "lif layer 1 self.abs_max_v: 649.0\n",
      "fc layer 1 self.abs_max_out: 506.0\n",
      "lif layer 1 self.abs_max_v: 668.0\n",
      "lif layer 1 self.abs_max_v: 677.0\n",
      "lif layer 1 self.abs_max_v: 704.5\n",
      "lif layer 1 self.abs_max_v: 753.5\n",
      "fc layer 3 self.abs_max_out: 286.0\n",
      "fc layer 2 self.abs_max_out: 693.0\n",
      "fc layer 1 self.abs_max_out: 550.0\n",
      "lif layer 2 self.abs_max_v: 1030.0\n",
      "lif layer 1 self.abs_max_v: 797.5\n",
      "lif layer 2 self.abs_max_v: 1107.0\n",
      "lif layer 1 self.abs_max_v: 808.0\n",
      "lif layer 1 self.abs_max_v: 841.5\n",
      "lif layer 1 self.abs_max_v: 948.0\n",
      "fc layer 1 self.abs_max_out: 637.0\n",
      "lif layer 1 self.abs_max_v: 950.0\n",
      "fc layer 2 self.abs_max_out: 712.0\n",
      "lif layer 1 self.abs_max_v: 1076.0\n",
      "lif layer 2 self.abs_max_v: 1128.5\n",
      "lif layer 2 self.abs_max_v: 1137.0\n",
      "lif layer 2 self.abs_max_v: 1142.5\n",
      "fc layer 2 self.abs_max_out: 730.0\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "lif layer 2 self.abs_max_v: 1226.0\n",
      "fc layer 3 self.abs_max_out: 329.0\n",
      "fc layer 2 self.abs_max_out: 779.0\n",
      "fc layer 1 self.abs_max_out: 723.0\n",
      "lif layer 2 self.abs_max_v: 1285.5\n",
      "fc layer 3 self.abs_max_out: 373.0\n",
      "lif layer 1 self.abs_max_v: 1130.0\n",
      "fc layer 2 self.abs_max_out: 785.0\n",
      "fc layer 3 self.abs_max_out: 394.0\n",
      "fc layer 2 self.abs_max_out: 795.0\n",
      "fc layer 2 self.abs_max_out: 805.0\n",
      "fc layer 2 self.abs_max_out: 819.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.369386/  2.367069, val:  11.67%, val_best:  11.67%, tr:  11.03%, tr_best:  11.03%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6830%\n",
      "layer   2  Sparsity: 62.2425%\n",
      "layer   3  Sparsity: 65.7455%\n",
      "total_backward_count 9790 real_backward_count 8684  88.703%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 1181.5\n",
      "lif layer 1 self.abs_max_v: 1188.5\n",
      "lif layer 2 self.abs_max_v: 1357.0\n",
      "fc layer 1 self.abs_max_out: 730.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.375514/  2.367069, val:  11.67%, val_best:  11.67%, tr:  11.24%, tr_best:  11.24%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 83.6807%\n",
      "layer   2  Sparsity: 62.2474%\n",
      "layer   3  Sparsity: 65.7108%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976ab1a3fd674efeb63646c5767ffb24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÅ‚ñà</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.11236</td></tr><tr><td>tr_epoch_loss</td><td>2.37551</td></tr><tr><td>val_acc_best</td><td>0.11667</td></tr><tr><td>val_acc_now</td><td>0.11667</td></tr><tr><td>val_loss</td><td>2.36707</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">robust-sweep-76</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/26uu6kfa' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/26uu6kfa</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_192208-26uu6kfa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 26uu6kfa errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_15806/2189894352.py\", line 114, in hyper_iter\n",
      "    my_snn_system(\n",
      "  File \"/tmp/ipykernel_15806/256537533.py\", line 952, in my_snn_system\n",
      "    assert val_acc_best > 0.2\n",
      "AssertionError\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 26uu6kfa errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_15806/2189894352.py\", line 114, in hyper_iter\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     my_snn_system(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_15806/256537533.py\", line 952, in my_snn_system\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     assert val_acc_best > 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AssertionError\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7dg4so5e with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_192522-7dg4so5e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7dg4so5e' target=\"_blank\">lunar-sweep-77</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7dg4so5e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7dg4so5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251115_192531_492', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 6, 'leaky_temporal_filter': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 470.0\n",
      "lif layer 1 self.abs_max_v: 470.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 383.0\n",
      "lif layer 2 self.abs_max_v: 383.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "lif layer 1 self.abs_max_v: 485.0\n",
      "fc layer 2 self.abs_max_out: 458.0\n",
      "lif layer 2 self.abs_max_v: 557.0\n",
      "fc layer 3 self.abs_max_out: 139.0\n",
      "lif layer 1 self.abs_max_v: 693.5\n",
      "fc layer 1 self.abs_max_out: 562.0\n",
      "lif layer 1 self.abs_max_v: 771.0\n",
      "fc layer 2 self.abs_max_out: 483.0\n",
      "lif layer 2 self.abs_max_v: 650.0\n",
      "fc layer 3 self.abs_max_out: 158.0\n",
      "fc layer 1 self.abs_max_out: 685.0\n",
      "lif layer 1 self.abs_max_v: 830.5\n",
      "fc layer 2 self.abs_max_out: 602.0\n",
      "lif layer 2 self.abs_max_v: 769.5\n",
      "fc layer 3 self.abs_max_out: 229.0\n",
      "fc layer 1 self.abs_max_out: 1079.0\n",
      "lif layer 1 self.abs_max_v: 1079.0\n",
      "fc layer 2 self.abs_max_out: 639.0\n",
      "lif layer 2 self.abs_max_v: 883.0\n",
      "fc layer 2 self.abs_max_out: 681.0\n",
      "lif layer 2 self.abs_max_v: 1122.5\n",
      "fc layer 3 self.abs_max_out: 261.0\n",
      "lif layer 1 self.abs_max_v: 1167.5\n",
      "lif layer 2 self.abs_max_v: 1237.5\n",
      "fc layer 1 self.abs_max_out: 1294.0\n",
      "lif layer 1 self.abs_max_v: 1294.0\n",
      "fc layer 3 self.abs_max_out: 267.0\n",
      "fc layer 2 self.abs_max_out: 884.0\n",
      "fc layer 3 self.abs_max_out: 343.0\n",
      "fc layer 1 self.abs_max_out: 1320.0\n",
      "lif layer 1 self.abs_max_v: 1330.0\n",
      "fc layer 1 self.abs_max_out: 1375.0\n",
      "lif layer 1 self.abs_max_v: 1453.5\n",
      "lif layer 2 self.abs_max_v: 1305.5\n",
      "fc layer 2 self.abs_max_out: 909.0\n",
      "lif layer 2 self.abs_max_v: 1478.0\n",
      "fc layer 1 self.abs_max_out: 1789.0\n",
      "lif layer 1 self.abs_max_v: 1807.0\n",
      "fc layer 2 self.abs_max_out: 998.0\n",
      "fc layer 3 self.abs_max_out: 363.0\n",
      "fc layer 1 self.abs_max_out: 1933.0\n",
      "lif layer 1 self.abs_max_v: 1933.0\n",
      "fc layer 3 self.abs_max_out: 402.0\n",
      "fc layer 3 self.abs_max_out: 501.0\n",
      "fc layer 3 self.abs_max_out: 556.0\n",
      "fc layer 1 self.abs_max_out: 2035.0\n",
      "lif layer 1 self.abs_max_v: 2035.0\n",
      "lif layer 2 self.abs_max_v: 1508.0\n",
      "fc layer 2 self.abs_max_out: 1176.0\n",
      "lif layer 2 self.abs_max_v: 1596.0\n",
      "fc layer 1 self.abs_max_out: 2057.0\n",
      "lif layer 1 self.abs_max_v: 2057.0\n",
      "fc layer 2 self.abs_max_out: 1203.0\n",
      "fc layer 2 self.abs_max_out: 1324.0\n",
      "fc layer 2 self.abs_max_out: 1401.0\n",
      "lif layer 2 self.abs_max_v: 1861.5\n",
      "lif layer 2 self.abs_max_v: 1922.0\n",
      "fc layer 1 self.abs_max_out: 2190.0\n",
      "lif layer 1 self.abs_max_v: 2190.0\n",
      "fc layer 2 self.abs_max_out: 1564.0\n",
      "fc layer 1 self.abs_max_out: 2215.0\n",
      "lif layer 1 self.abs_max_v: 2215.0\n",
      "lif layer 1 self.abs_max_v: 2312.5\n",
      "fc layer 1 self.abs_max_out: 2380.0\n",
      "lif layer 1 self.abs_max_v: 2380.0\n",
      "fc layer 1 self.abs_max_out: 2910.0\n",
      "lif layer 1 self.abs_max_v: 2910.0\n",
      "lif layer 2 self.abs_max_v: 2060.5\n",
      "fc layer 1 self.abs_max_out: 3010.0\n",
      "lif layer 1 self.abs_max_v: 3010.0\n",
      "fc layer 2 self.abs_max_out: 1630.0\n",
      "lif layer 2 self.abs_max_v: 2169.5\n",
      "lif layer 2 self.abs_max_v: 2239.5\n",
      "fc layer 2 self.abs_max_out: 1662.0\n",
      "lif layer 2 self.abs_max_v: 2302.0\n",
      "fc layer 2 self.abs_max_out: 1744.0\n",
      "lif layer 2 self.abs_max_v: 2404.5\n",
      "fc layer 3 self.abs_max_out: 565.0\n",
      "lif layer 2 self.abs_max_v: 2487.5\n",
      "fc layer 3 self.abs_max_out: 640.0\n",
      "lif layer 1 self.abs_max_v: 3224.0\n",
      "fc layer 1 self.abs_max_out: 3134.0\n",
      "lif layer 1 self.abs_max_v: 3310.0\n",
      "lif layer 2 self.abs_max_v: 2560.0\n",
      "fc layer 1 self.abs_max_out: 3755.0\n",
      "lif layer 1 self.abs_max_v: 3755.0\n",
      "fc layer 1 self.abs_max_out: 3831.0\n",
      "lif layer 1 self.abs_max_v: 3831.0\n",
      "fc layer 1 self.abs_max_out: 4026.0\n",
      "lif layer 1 self.abs_max_v: 4026.0\n",
      "fc layer 2 self.abs_max_out: 1750.0\n",
      "lif layer 2 self.abs_max_v: 2567.0\n",
      "fc layer 2 self.abs_max_out: 1784.0\n",
      "fc layer 2 self.abs_max_out: 1786.0\n",
      "lif layer 2 self.abs_max_v: 2592.5\n",
      "lif layer 2 self.abs_max_v: 2765.5\n",
      "fc layer 2 self.abs_max_out: 1825.0\n",
      "fc layer 2 self.abs_max_out: 1904.0\n",
      "lif layer 2 self.abs_max_v: 2787.5\n",
      "lif layer 2 self.abs_max_v: 3053.0\n",
      "lif layer 2 self.abs_max_v: 3199.5\n",
      "fc layer 3 self.abs_max_out: 705.0\n",
      "fc layer 2 self.abs_max_out: 2033.0\n",
      "lif layer 1 self.abs_max_v: 4053.5\n",
      "lif layer 1 self.abs_max_v: 4149.0\n",
      "fc layer 2 self.abs_max_out: 2042.0\n",
      "lif layer 2 self.abs_max_v: 3452.5\n",
      "lif layer 2 self.abs_max_v: 3614.5\n",
      "fc layer 2 self.abs_max_out: 2068.0\n",
      "lif layer 2 self.abs_max_v: 3815.5\n",
      "fc layer 2 self.abs_max_out: 2151.0\n",
      "lif layer 2 self.abs_max_v: 3960.0\n",
      "fc layer 2 self.abs_max_out: 2213.0\n",
      "lif layer 2 self.abs_max_v: 4098.5\n",
      "fc layer 2 self.abs_max_out: 2229.0\n",
      "fc layer 2 self.abs_max_out: 2401.0\n",
      "lif layer 2 self.abs_max_v: 4279.5\n",
      "lif layer 2 self.abs_max_v: 4475.0\n",
      "fc layer 3 self.abs_max_out: 750.0\n",
      "fc layer 2 self.abs_max_out: 2549.0\n",
      "fc layer 2 self.abs_max_out: 2680.0\n",
      "fc layer 3 self.abs_max_out: 767.0\n",
      "fc layer 3 self.abs_max_out: 776.0\n",
      "lif layer 2 self.abs_max_v: 4524.5\n",
      "lif layer 2 self.abs_max_v: 4712.5\n",
      "lif layer 1 self.abs_max_v: 4169.5\n",
      "fc layer 2 self.abs_max_out: 2690.0\n",
      "fc layer 2 self.abs_max_out: 2693.0\n",
      "lif layer 2 self.abs_max_v: 4897.5\n",
      "fc layer 2 self.abs_max_out: 2802.0\n",
      "lif layer 1 self.abs_max_v: 4314.0\n",
      "fc layer 3 self.abs_max_out: 836.0\n",
      "fc layer 3 self.abs_max_out: 863.0\n",
      "fc layer 3 self.abs_max_out: 941.0\n",
      "fc layer 2 self.abs_max_out: 2812.0\n",
      "lif layer 2 self.abs_max_v: 4929.5\n",
      "lif layer 2 self.abs_max_v: 5134.0\n",
      "fc layer 3 self.abs_max_out: 966.0\n",
      "fc layer 2 self.abs_max_out: 2817.0\n",
      "fc layer 2 self.abs_max_out: 2958.0\n",
      "lif layer 2 self.abs_max_v: 5149.5\n",
      "lif layer 2 self.abs_max_v: 5281.5\n",
      "lif layer 2 self.abs_max_v: 5515.0\n",
      "fc layer 1 self.abs_max_out: 4187.0\n",
      "lif layer 1 self.abs_max_v: 5036.5\n",
      "lif layer 1 self.abs_max_v: 5195.5\n",
      "lif layer 1 self.abs_max_v: 5433.0\n",
      "lif layer 1 self.abs_max_v: 5494.5\n",
      "fc layer 3 self.abs_max_out: 989.0\n",
      "fc layer 3 self.abs_max_out: 991.0\n",
      "lif layer 1 self.abs_max_v: 5545.5\n",
      "lif layer 1 self.abs_max_v: 6001.5\n",
      "fc layer 3 self.abs_max_out: 1101.0\n",
      "fc layer 3 self.abs_max_out: 1136.0\n",
      "fc layer 3 self.abs_max_out: 1153.0\n",
      "fc layer 2 self.abs_max_out: 3062.0\n",
      "fc layer 2 self.abs_max_out: 3337.0\n",
      "fc layer 2 self.abs_max_out: 3360.0\n",
      "fc layer 2 self.abs_max_out: 3453.0\n",
      "fc layer 2 self.abs_max_out: 3542.0\n",
      "lif layer 2 self.abs_max_v: 5709.0\n",
      "lif layer 2 self.abs_max_v: 5911.5\n",
      "fc layer 2 self.abs_max_out: 3618.0\n",
      "lif layer 2 self.abs_max_v: 6188.5\n",
      "fc layer 1 self.abs_max_out: 4295.0\n",
      "fc layer 1 self.abs_max_out: 4924.0\n",
      "lif layer 1 self.abs_max_v: 6272.5\n",
      "lif layer 1 self.abs_max_v: 6325.5\n",
      "fc layer 2 self.abs_max_out: 3699.0\n",
      "lif layer 2 self.abs_max_v: 6227.0\n",
      "fc layer 2 self.abs_max_out: 3736.0\n",
      "fc layer 2 self.abs_max_out: 3764.0\n",
      "lif layer 2 self.abs_max_v: 6235.0\n",
      "lif layer 2 self.abs_max_v: 6288.5\n",
      "lif layer 2 self.abs_max_v: 6316.5\n",
      "lif layer 1 self.abs_max_v: 6404.5\n",
      "lif layer 1 self.abs_max_v: 6739.0\n",
      "lif layer 1 self.abs_max_v: 6949.0\n",
      "fc layer 2 self.abs_max_out: 4056.0\n",
      "fc layer 2 self.abs_max_out: 4083.0\n",
      "fc layer 2 self.abs_max_out: 4115.0\n",
      "fc layer 3 self.abs_max_out: 1184.0\n",
      "lif layer 1 self.abs_max_v: 7211.0\n",
      "lif layer 1 self.abs_max_v: 7223.0\n",
      "lif layer 1 self.abs_max_v: 7589.5\n",
      "lif layer 1 self.abs_max_v: 7625.0\n",
      "fc layer 3 self.abs_max_out: 1199.0\n",
      "lif layer 1 self.abs_max_v: 7776.5\n",
      "lif layer 2 self.abs_max_v: 6457.0\n",
      "lif layer 2 self.abs_max_v: 6534.5\n",
      "lif layer 2 self.abs_max_v: 6766.0\n",
      "fc layer 3 self.abs_max_out: 1239.0\n",
      "fc layer 1 self.abs_max_out: 5117.0\n",
      "lif layer 1 self.abs_max_v: 7872.0\n",
      "fc layer 1 self.abs_max_out: 5332.0\n",
      "lif layer 1 self.abs_max_v: 8217.5\n",
      "fc layer 1 self.abs_max_out: 5334.0\n",
      "lif layer 2 self.abs_max_v: 6785.5\n",
      "lif layer 2 self.abs_max_v: 6895.5\n",
      "fc layer 3 self.abs_max_out: 1253.0\n",
      "lif layer 1 self.abs_max_v: 8647.0\n",
      "lif layer 1 self.abs_max_v: 8841.0\n",
      "lif layer 1 self.abs_max_v: 9036.0\n",
      "lif layer 1 self.abs_max_v: 9354.5\n",
      "fc layer 1 self.abs_max_out: 5864.0\n",
      "lif layer 1 self.abs_max_v: 9467.0\n",
      "lif layer 1 self.abs_max_v: 9821.5\n",
      "lif layer 1 self.abs_max_v: 9924.5\n",
      "lif layer 1 self.abs_max_v: 10408.5\n",
      "lif layer 1 self.abs_max_v: 10598.5\n",
      "fc layer 1 self.abs_max_out: 6128.0\n",
      "fc layer 1 self.abs_max_out: 6335.0\n",
      "lif layer 1 self.abs_max_v: 10785.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.428113/  1.843969, val:  35.42%, val_best:  35.42%, tr:  98.88%, tr_best:  98.88%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0121%\n",
      "layer   2  Sparsity: 71.2789%\n",
      "layer   3  Sparsity: 69.1087%\n",
      "total_backward_count 9790 real_backward_count 1372  14.014%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 1267.0\n",
      "lif layer 1 self.abs_max_v: 10987.0\n",
      "fc layer 3 self.abs_max_out: 1323.0\n",
      "fc layer 3 self.abs_max_out: 1357.0\n",
      "fc layer 3 self.abs_max_out: 1389.0\n",
      "fc layer 3 self.abs_max_out: 1521.0\n",
      "lif layer 2 self.abs_max_v: 7143.0\n",
      "lif layer 2 self.abs_max_v: 7470.5\n",
      "lif layer 2 self.abs_max_v: 7759.5\n",
      "fc layer 2 self.abs_max_out: 4233.0\n",
      "lif layer 2 self.abs_max_v: 7932.5\n",
      "fc layer 3 self.abs_max_out: 1547.0\n",
      "fc layer 2 self.abs_max_out: 4548.0\n",
      "lif layer 2 self.abs_max_v: 7988.0\n",
      "lif layer 1 self.abs_max_v: 11109.5\n",
      "fc layer 1 self.abs_max_out: 6351.0\n",
      "fc layer 1 self.abs_max_out: 6437.0\n",
      "lif layer 1 self.abs_max_v: 11703.0\n",
      "fc layer 1 self.abs_max_out: 6524.0\n",
      "lif layer 1 self.abs_max_v: 12143.5\n",
      "fc layer 2 self.abs_max_out: 4593.0\n",
      "lif layer 2 self.abs_max_v: 8561.0\n",
      "fc layer 2 self.abs_max_out: 4621.0\n",
      "lif layer 2 self.abs_max_v: 8901.5\n",
      "lif layer 2 self.abs_max_v: 9023.0\n",
      "fc layer 1 self.abs_max_out: 6869.0\n",
      "fc layer 1 self.abs_max_out: 7208.0\n",
      "fc layer 1 self.abs_max_out: 7920.0\n",
      "fc layer 1 self.abs_max_out: 8207.0\n",
      "lif layer 1 self.abs_max_v: 13858.0\n",
      "fc layer 3 self.abs_max_out: 1682.0\n",
      "fc layer 2 self.abs_max_out: 4848.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.345858/  1.774279, val:  45.00%, val_best:  45.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0023%\n",
      "layer   2  Sparsity: 75.7659%\n",
      "layer   3  Sparsity: 72.5870%\n",
      "total_backward_count 19580 real_backward_count 2595  13.253%\n",
      "fc layer 1 self.abs_max_out: 8822.0\n",
      "lif layer 2 self.abs_max_v: 9258.5\n",
      "fc layer 1 self.abs_max_out: 9177.0\n",
      "lif layer 1 self.abs_max_v: 15489.5\n",
      "lif layer 1 self.abs_max_v: 16335.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.310728/  1.714239, val:  45.42%, val_best:  45.42%, tr:  99.39%, tr_best:  99.59%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0057%\n",
      "layer   2  Sparsity: 76.4228%\n",
      "layer   3  Sparsity: 73.3877%\n",
      "total_backward_count 29370 real_backward_count 3747  12.758%\n",
      "fc layer 2 self.abs_max_out: 4871.0\n",
      "lif layer 2 self.abs_max_v: 9261.5\n",
      "lif layer 2 self.abs_max_v: 9486.0\n",
      "fc layer 2 self.abs_max_out: 5005.0\n",
      "fc layer 2 self.abs_max_out: 5108.0\n",
      "fc layer 2 self.abs_max_out: 5112.0\n",
      "lif layer 2 self.abs_max_v: 9505.0\n",
      "lif layer 2 self.abs_max_v: 9557.5\n",
      "lif layer 2 self.abs_max_v: 9574.0\n",
      "lif layer 2 self.abs_max_v: 9757.0\n",
      "fc layer 1 self.abs_max_out: 9247.0\n",
      "lif layer 1 self.abs_max_v: 16730.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.259049/  1.704795, val:  40.83%, val_best:  45.42%, tr:  99.69%, tr_best:  99.69%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0067%\n",
      "layer   2  Sparsity: 75.8783%\n",
      "layer   3  Sparsity: 73.5138%\n",
      "total_backward_count 39160 real_backward_count 4860  12.411%\n",
      "fc layer 1 self.abs_max_out: 9526.0\n",
      "fc layer 1 self.abs_max_out: 9558.0\n",
      "fc layer 2 self.abs_max_out: 5146.0\n",
      "fc layer 2 self.abs_max_out: 5177.0\n",
      "lif layer 2 self.abs_max_v: 9831.0\n",
      "fc layer 2 self.abs_max_out: 5250.0\n",
      "lif layer 2 self.abs_max_v: 10165.5\n",
      "fc layer 1 self.abs_max_out: 9564.0\n",
      "fc layer 1 self.abs_max_out: 10354.0\n",
      "fc layer 1 self.abs_max_out: 10805.0\n",
      "lif layer 1 self.abs_max_v: 17573.0\n",
      "lif layer 1 self.abs_max_v: 18920.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.247166/  1.648542, val:  49.58%, val_best:  49.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0071%\n",
      "layer   2  Sparsity: 76.0319%\n",
      "layer   3  Sparsity: 73.8589%\n",
      "total_backward_count 48950 real_backward_count 5976  12.208%\n",
      "fc layer 1 self.abs_max_out: 10905.0\n",
      "fc layer 1 self.abs_max_out: 10954.0\n",
      "fc layer 2 self.abs_max_out: 5264.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.222599/  1.672004, val:  42.92%, val_best:  49.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0018%\n",
      "layer   2  Sparsity: 74.9735%\n",
      "layer   3  Sparsity: 72.9333%\n",
      "total_backward_count 58740 real_backward_count 7032  11.971%\n",
      "fc layer 2 self.abs_max_out: 5288.0\n",
      "fc layer 2 self.abs_max_out: 5420.0\n",
      "fc layer 2 self.abs_max_out: 5596.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.232699/  1.655411, val:  57.08%, val_best:  57.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0128%\n",
      "layer   2  Sparsity: 75.9384%\n",
      "layer   3  Sparsity: 73.5930%\n",
      "total_backward_count 68530 real_backward_count 8153  11.897%\n",
      "lif layer 2 self.abs_max_v: 10198.5\n",
      "lif layer 2 self.abs_max_v: 10473.0\n",
      "lif layer 2 self.abs_max_v: 10543.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.192514/  1.658734, val:  50.42%, val_best:  57.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0077%\n",
      "layer   2  Sparsity: 76.3291%\n",
      "layer   3  Sparsity: 74.4061%\n",
      "total_backward_count 78320 real_backward_count 9131  11.659%\n",
      "lif layer 2 self.abs_max_v: 10660.0\n",
      "fc layer 2 self.abs_max_out: 5906.0\n",
      "lif layer 2 self.abs_max_v: 11014.5\n",
      "fc layer 2 self.abs_max_out: 5950.0\n",
      "lif layer 2 self.abs_max_v: 11440.0\n",
      "lif layer 2 self.abs_max_v: 11542.0\n",
      "lif layer 2 self.abs_max_v: 11601.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.206526/  1.603788, val:  55.83%, val_best:  57.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9880%\n",
      "layer   2  Sparsity: 76.0425%\n",
      "layer   3  Sparsity: 73.6381%\n",
      "total_backward_count 88110 real_backward_count 10195  11.571%\n",
      "fc layer 1 self.abs_max_out: 11387.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.167974/  1.686195, val:  49.58%, val_best:  57.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0169%\n",
      "layer   2  Sparsity: 76.3926%\n",
      "layer   3  Sparsity: 74.3165%\n",
      "total_backward_count 97900 real_backward_count 11215  11.456%\n",
      "fc layer 1 self.abs_max_out: 11591.0\n",
      "lif layer 1 self.abs_max_v: 20965.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.181847/  1.690035, val:  48.75%, val_best:  57.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0035%\n",
      "layer   2  Sparsity: 75.5400%\n",
      "layer   3  Sparsity: 73.8950%\n",
      "total_backward_count 107690 real_backward_count 12209  11.337%\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.199208/  1.586127, val:  60.83%, val_best:  60.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0034%\n",
      "layer   2  Sparsity: 74.5266%\n",
      "layer   3  Sparsity: 73.6780%\n",
      "total_backward_count 117480 real_backward_count 13220  11.253%\n",
      "fc layer 1 self.abs_max_out: 12363.0\n",
      "lif layer 1 self.abs_max_v: 22077.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.230965/  1.730703, val:  46.25%, val_best:  60.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0089%\n",
      "layer   2  Sparsity: 75.1189%\n",
      "layer   3  Sparsity: 75.6998%\n",
      "total_backward_count 127270 real_backward_count 14226  11.178%\n",
      "fc layer 2 self.abs_max_out: 6062.0\n",
      "fc layer 2 self.abs_max_out: 6324.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.254913/  1.686323, val:  51.25%, val_best:  60.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.9981%\n",
      "layer   2  Sparsity: 74.4349%\n",
      "layer   3  Sparsity: 74.8426%\n",
      "total_backward_count 137060 real_backward_count 15249  11.126%\n",
      "lif layer 1 self.abs_max_v: 22826.5\n",
      "lif layer 1 self.abs_max_v: 22860.5\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.185297/  1.666969, val:  51.25%, val_best:  60.83%, tr:  99.39%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0046%\n",
      "layer   2  Sparsity: 75.5606%\n",
      "layer   3  Sparsity: 73.9150%\n",
      "total_backward_count 146850 real_backward_count 16198  11.030%\n",
      "lif layer 2 self.abs_max_v: 11885.5\n",
      "lif layer 2 self.abs_max_v: 12041.0\n",
      "fc layer 2 self.abs_max_out: 6471.0\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.224822/  1.698469, val:  55.83%, val_best:  60.83%, tr:  99.39%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0029%\n",
      "layer   2  Sparsity: 74.9428%\n",
      "layer   3  Sparsity: 76.0296%\n",
      "total_backward_count 156640 real_backward_count 17212  10.988%\n",
      "fc layer 2 self.abs_max_out: 6557.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.206197/  1.615983, val:  56.67%, val_best:  60.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.9949%\n",
      "layer   2  Sparsity: 74.9651%\n",
      "layer   3  Sparsity: 75.8430%\n",
      "total_backward_count 166430 real_backward_count 18190  10.930%\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.233043/  1.690128, val:  55.83%, val_best:  60.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0144%\n",
      "layer   2  Sparsity: 74.9287%\n",
      "layer   3  Sparsity: 76.4375%\n",
      "total_backward_count 176220 real_backward_count 19131  10.856%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.235992/  1.664315, val:  52.92%, val_best:  60.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0124%\n",
      "layer   2  Sparsity: 73.9983%\n",
      "layer   3  Sparsity: 76.2814%\n",
      "total_backward_count 186010 real_backward_count 20107  10.810%\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.178065/  1.640853, val:  50.00%, val_best:  60.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0025%\n",
      "layer   2  Sparsity: 73.1450%\n",
      "layer   3  Sparsity: 75.2952%\n",
      "total_backward_count 195800 real_backward_count 21013  10.732%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.158129/  1.611705, val:  55.00%, val_best:  60.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9934%\n",
      "layer   2  Sparsity: 73.0153%\n",
      "layer   3  Sparsity: 74.8301%\n",
      "total_backward_count 205590 real_backward_count 21910  10.657%\n",
      "fc layer 1 self.abs_max_out: 12390.0\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.192755/  1.703975, val:  52.92%, val_best:  60.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9959%\n",
      "layer   2  Sparsity: 72.8911%\n",
      "layer   3  Sparsity: 74.8295%\n",
      "total_backward_count 215380 real_backward_count 22927  10.645%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.214343/  1.610567, val:  58.75%, val_best:  60.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.9971%\n",
      "layer   2  Sparsity: 72.9124%\n",
      "layer   3  Sparsity: 77.3714%\n",
      "total_backward_count 225170 real_backward_count 23910  10.619%\n",
      "fc layer 1 self.abs_max_out: 12833.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.193865/  1.540702, val:  59.58%, val_best:  60.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0192%\n",
      "layer   2  Sparsity: 72.7519%\n",
      "layer   3  Sparsity: 75.4860%\n",
      "total_backward_count 234960 real_backward_count 24842  10.573%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.146660/  1.554974, val:  57.92%, val_best:  60.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0047%\n",
      "layer   2  Sparsity: 73.2209%\n",
      "layer   3  Sparsity: 73.7072%\n",
      "total_backward_count 244750 real_backward_count 25780  10.533%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.141937/  1.563972, val:  62.08%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0099%\n",
      "layer   2  Sparsity: 73.0654%\n",
      "layer   3  Sparsity: 74.3208%\n",
      "total_backward_count 254540 real_backward_count 26742  10.506%\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.154712/  1.536796, val:  62.50%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0046%\n",
      "layer   2  Sparsity: 72.9304%\n",
      "layer   3  Sparsity: 75.5492%\n",
      "total_backward_count 264330 real_backward_count 27669  10.468%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.128936/  1.586026, val:  63.75%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0068%\n",
      "layer   2  Sparsity: 72.4146%\n",
      "layer   3  Sparsity: 75.1098%\n",
      "total_backward_count 274120 real_backward_count 28614  10.438%\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.123128/  1.571893, val:  61.25%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0023%\n",
      "layer   2  Sparsity: 72.3158%\n",
      "layer   3  Sparsity: 74.5714%\n",
      "total_backward_count 283910 real_backward_count 29542  10.405%\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.174474/  1.598560, val:  52.92%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9955%\n",
      "layer   2  Sparsity: 72.7827%\n",
      "layer   3  Sparsity: 75.1684%\n",
      "total_backward_count 293700 real_backward_count 30498  10.384%\n",
      "fc layer 1 self.abs_max_out: 12867.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.157658/  1.595971, val:  62.50%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0071%\n",
      "layer   2  Sparsity: 72.2651%\n",
      "layer   3  Sparsity: 76.4099%\n",
      "total_backward_count 303490 real_backward_count 31407  10.349%\n",
      "fc layer 1 self.abs_max_out: 13005.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.148725/  1.608755, val:  55.00%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0218%\n",
      "layer   2  Sparsity: 72.7792%\n",
      "layer   3  Sparsity: 77.3545%\n",
      "total_backward_count 313280 real_backward_count 32357  10.328%\n",
      "lif layer 1 self.abs_max_v: 23297.5\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.159415/  1.632639, val:  52.50%, val_best:  63.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0110%\n",
      "layer   2  Sparsity: 72.5398%\n",
      "layer   3  Sparsity: 77.4749%\n",
      "total_backward_count 323070 real_backward_count 33291  10.305%\n",
      "fc layer 1 self.abs_max_out: 13073.0\n",
      "fc layer 3 self.abs_max_out: 1776.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.206755/  1.552684, val:  59.17%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0045%\n",
      "layer   2  Sparsity: 72.4393%\n",
      "layer   3  Sparsity: 77.7587%\n",
      "total_backward_count 332860 real_backward_count 34231  10.284%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.159381/  1.603679, val:  60.00%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0054%\n",
      "layer   2  Sparsity: 71.4993%\n",
      "layer   3  Sparsity: 77.2909%\n",
      "total_backward_count 342650 real_backward_count 35153  10.259%\n",
      "fc layer 2 self.abs_max_out: 6755.0\n",
      "lif layer 2 self.abs_max_v: 12186.0\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.192148/  1.561094, val:  67.92%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0102%\n",
      "layer   2  Sparsity: 71.4534%\n",
      "layer   3  Sparsity: 76.8511%\n",
      "total_backward_count 352440 real_backward_count 36124  10.250%\n",
      "fc layer 1 self.abs_max_out: 13200.0\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.158365/  1.519804, val:  66.25%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0011%\n",
      "layer   2  Sparsity: 72.2158%\n",
      "layer   3  Sparsity: 75.6852%\n",
      "total_backward_count 362230 real_backward_count 37013  10.218%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.115149/  1.589365, val:  58.33%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0085%\n",
      "layer   2  Sparsity: 71.4734%\n",
      "layer   3  Sparsity: 76.0489%\n",
      "total_backward_count 372020 real_backward_count 37856  10.176%\n",
      "fc layer 3 self.abs_max_out: 1799.0\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.128934/  1.577201, val:  63.33%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9936%\n",
      "layer   2  Sparsity: 71.4999%\n",
      "layer   3  Sparsity: 77.0268%\n",
      "total_backward_count 381810 real_backward_count 38780  10.157%\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.172346/  1.615577, val:  58.75%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0113%\n",
      "layer   2  Sparsity: 71.2460%\n",
      "layer   3  Sparsity: 77.7326%\n",
      "total_backward_count 391600 real_backward_count 39689  10.135%\n",
      "fc layer 1 self.abs_max_out: 13209.0\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.165592/  1.550821, val:  55.00%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0006%\n",
      "layer   2  Sparsity: 71.6246%\n",
      "layer   3  Sparsity: 77.3218%\n",
      "total_backward_count 401390 real_backward_count 40606  10.116%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.200294/  1.586163, val:  64.17%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.9943%\n",
      "layer   2  Sparsity: 72.0324%\n",
      "layer   3  Sparsity: 77.4617%\n",
      "total_backward_count 411180 real_backward_count 41550  10.105%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.158508/  1.591813, val:  55.00%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0197%\n",
      "layer   2  Sparsity: 71.7654%\n",
      "layer   3  Sparsity: 76.3470%\n",
      "total_backward_count 420970 real_backward_count 42424  10.078%\n",
      "fc layer 1 self.abs_max_out: 13320.0\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.185360/  1.610437, val:  58.75%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0071%\n",
      "layer   2  Sparsity: 71.7020%\n",
      "layer   3  Sparsity: 77.0359%\n",
      "total_backward_count 430760 real_backward_count 43299  10.052%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.189421/  1.569635, val:  62.50%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0137%\n",
      "layer   2  Sparsity: 71.6600%\n",
      "layer   3  Sparsity: 76.5980%\n",
      "total_backward_count 440550 real_backward_count 44185  10.030%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.141323/  1.531310, val:  54.58%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0127%\n",
      "layer   2  Sparsity: 71.6320%\n",
      "layer   3  Sparsity: 75.0203%\n",
      "total_backward_count 450340 real_backward_count 45108  10.016%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.131127/  1.520820, val:  62.92%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.9939%\n",
      "layer   2  Sparsity: 71.8917%\n",
      "layer   3  Sparsity: 77.1007%\n",
      "total_backward_count 460130 real_backward_count 45962   9.989%\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.122548/  1.550515, val:  57.50%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0182%\n",
      "layer   2  Sparsity: 71.8284%\n",
      "layer   3  Sparsity: 77.0313%\n",
      "total_backward_count 469920 real_backward_count 46779   9.955%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.122777/  1.617156, val:  51.67%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0098%\n",
      "layer   2  Sparsity: 71.5700%\n",
      "layer   3  Sparsity: 76.6369%\n",
      "total_backward_count 479710 real_backward_count 47595   9.922%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.153342/  1.583329, val:  58.33%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9996%\n",
      "layer   2  Sparsity: 70.9458%\n",
      "layer   3  Sparsity: 77.1589%\n",
      "total_backward_count 489500 real_backward_count 48486   9.905%\n",
      "fc layer 1 self.abs_max_out: 13329.0\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.147342/  1.588193, val:  52.50%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0110%\n",
      "layer   2  Sparsity: 70.7614%\n",
      "layer   3  Sparsity: 75.1301%\n",
      "total_backward_count 499290 real_backward_count 49385   9.891%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.124872/  1.524281, val:  76.25%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9881%\n",
      "layer   2  Sparsity: 71.4783%\n",
      "layer   3  Sparsity: 77.1502%\n",
      "total_backward_count 509080 real_backward_count 50222   9.865%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.153081/  1.503338, val:  62.08%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0117%\n",
      "layer   2  Sparsity: 71.4482%\n",
      "layer   3  Sparsity: 76.5807%\n",
      "total_backward_count 518870 real_backward_count 51123   9.853%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.142393/  1.584269, val:  57.08%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0027%\n",
      "layer   2  Sparsity: 71.4129%\n",
      "layer   3  Sparsity: 76.7696%\n",
      "total_backward_count 528660 real_backward_count 51981   9.833%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.165051/  1.575197, val:  53.75%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9840%\n",
      "layer   2  Sparsity: 71.5018%\n",
      "layer   3  Sparsity: 77.8596%\n",
      "total_backward_count 538450 real_backward_count 52926   9.829%\n",
      "fc layer 1 self.abs_max_out: 13539.0\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.180542/  1.591693, val:  69.58%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0106%\n",
      "layer   2  Sparsity: 71.5025%\n",
      "layer   3  Sparsity: 78.6755%\n",
      "total_backward_count 548240 real_backward_count 53885   9.829%\n",
      "fc layer 1 self.abs_max_out: 14088.0\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.183722/  1.600237, val:  55.83%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0177%\n",
      "layer   2  Sparsity: 71.7098%\n",
      "layer   3  Sparsity: 78.8839%\n",
      "total_backward_count 558030 real_backward_count 54763   9.814%\n",
      "fc layer 1 self.abs_max_out: 14827.0\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.176875/  1.519479, val:  60.42%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0034%\n",
      "layer   2  Sparsity: 71.7581%\n",
      "layer   3  Sparsity: 77.1884%\n",
      "total_backward_count 567820 real_backward_count 55651   9.801%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.158288/  1.576051, val:  55.83%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9973%\n",
      "layer   2  Sparsity: 71.5185%\n",
      "layer   3  Sparsity: 78.1407%\n",
      "total_backward_count 577610 real_backward_count 56478   9.778%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.185513/  1.630191, val:  56.67%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0013%\n",
      "layer   2  Sparsity: 71.3674%\n",
      "layer   3  Sparsity: 78.2702%\n",
      "total_backward_count 587400 real_backward_count 57344   9.762%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.192596/  1.586519, val:  55.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0060%\n",
      "layer   2  Sparsity: 71.4453%\n",
      "layer   3  Sparsity: 77.7093%\n",
      "total_backward_count 597190 real_backward_count 58232   9.751%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.164814/  1.536460, val:  69.58%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0021%\n",
      "layer   2  Sparsity: 71.6132%\n",
      "layer   3  Sparsity: 77.2669%\n",
      "total_backward_count 606980 real_backward_count 59099   9.737%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.133735/  1.602882, val:  57.50%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0025%\n",
      "layer   2  Sparsity: 71.4488%\n",
      "layer   3  Sparsity: 75.7739%\n",
      "total_backward_count 616770 real_backward_count 60007   9.729%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.143155/  1.534715, val:  63.33%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0098%\n",
      "layer   2  Sparsity: 71.4047%\n",
      "layer   3  Sparsity: 76.0927%\n",
      "total_backward_count 626560 real_backward_count 60841   9.710%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.174540/  1.561853, val:  53.33%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0082%\n",
      "layer   2  Sparsity: 71.2276%\n",
      "layer   3  Sparsity: 78.1514%\n",
      "total_backward_count 636350 real_backward_count 61709   9.697%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.148887/  1.585641, val:  67.08%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0011%\n",
      "layer   2  Sparsity: 71.6448%\n",
      "layer   3  Sparsity: 77.6846%\n",
      "total_backward_count 646140 real_backward_count 62564   9.683%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.117632/  1.508486, val:  64.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.9976%\n",
      "layer   2  Sparsity: 71.2857%\n",
      "layer   3  Sparsity: 77.0062%\n",
      "total_backward_count 655930 real_backward_count 63433   9.671%\n",
      "fc layer 3 self.abs_max_out: 1809.0\n",
      "fc layer 3 self.abs_max_out: 1890.0\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.098377/  1.491830, val:  59.17%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0116%\n",
      "layer   2  Sparsity: 70.9388%\n",
      "layer   3  Sparsity: 77.1895%\n",
      "total_backward_count 665720 real_backward_count 64323   9.662%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.088378/  1.453596, val:  66.67%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0076%\n",
      "layer   2  Sparsity: 71.1991%\n",
      "layer   3  Sparsity: 78.5338%\n",
      "total_backward_count 675510 real_backward_count 65199   9.652%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.131394/  1.580997, val:  54.58%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9921%\n",
      "layer   2  Sparsity: 70.3890%\n",
      "layer   3  Sparsity: 77.8989%\n",
      "total_backward_count 685300 real_backward_count 66051   9.638%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.157605/  1.548345, val:  57.08%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0001%\n",
      "layer   2  Sparsity: 71.0614%\n",
      "layer   3  Sparsity: 77.4059%\n",
      "total_backward_count 695090 real_backward_count 66900   9.625%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.123435/  1.572378, val:  56.67%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.9974%\n",
      "layer   2  Sparsity: 70.7422%\n",
      "layer   3  Sparsity: 75.9192%\n",
      "total_backward_count 704880 real_backward_count 67739   9.610%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.148421/  1.534081, val:  66.25%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0120%\n",
      "layer   2  Sparsity: 71.2791%\n",
      "layer   3  Sparsity: 77.4317%\n",
      "total_backward_count 714670 real_backward_count 68620   9.602%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.119578/  1.540452, val:  66.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0115%\n",
      "layer   2  Sparsity: 71.7040%\n",
      "layer   3  Sparsity: 79.1772%\n",
      "total_backward_count 724460 real_backward_count 69454   9.587%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.146386/  1.513214, val:  60.83%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9876%\n",
      "layer   2  Sparsity: 71.8506%\n",
      "layer   3  Sparsity: 78.3361%\n",
      "total_backward_count 734250 real_backward_count 70341   9.580%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.115293/  1.458904, val:  65.42%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0148%\n",
      "layer   2  Sparsity: 71.4659%\n",
      "layer   3  Sparsity: 76.9338%\n",
      "total_backward_count 744040 real_backward_count 71144   9.562%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.128214/  1.520516, val:  56.67%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9928%\n",
      "layer   2  Sparsity: 71.6515%\n",
      "layer   3  Sparsity: 77.3120%\n",
      "total_backward_count 753830 real_backward_count 72015   9.553%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.104100/  1.652304, val:  52.50%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0161%\n",
      "layer   2  Sparsity: 71.4198%\n",
      "layer   3  Sparsity: 78.4826%\n",
      "total_backward_count 763620 real_backward_count 72833   9.538%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.152497/  1.564325, val:  57.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0054%\n",
      "layer   2  Sparsity: 71.0790%\n",
      "layer   3  Sparsity: 77.9929%\n",
      "total_backward_count 773410 real_backward_count 73689   9.528%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.108661/  1.515765, val:  63.33%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0018%\n",
      "layer   2  Sparsity: 71.2967%\n",
      "layer   3  Sparsity: 77.1083%\n",
      "total_backward_count 783200 real_backward_count 74536   9.517%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.160828/  1.560661, val:  59.58%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0048%\n",
      "layer   2  Sparsity: 71.2802%\n",
      "layer   3  Sparsity: 77.8747%\n",
      "total_backward_count 792990 real_backward_count 75392   9.507%\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.152446/  1.552393, val:  61.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0083%\n",
      "layer   2  Sparsity: 70.8461%\n",
      "layer   3  Sparsity: 77.1027%\n",
      "total_backward_count 802780 real_backward_count 76240   9.497%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.133802/  1.524169, val:  62.50%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9903%\n",
      "layer   2  Sparsity: 70.8311%\n",
      "layer   3  Sparsity: 78.2827%\n",
      "total_backward_count 812570 real_backward_count 77092   9.487%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.132015/  1.593252, val:  51.25%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0084%\n",
      "layer   2  Sparsity: 70.4038%\n",
      "layer   3  Sparsity: 78.2783%\n",
      "total_backward_count 822360 real_backward_count 77968   9.481%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.097734/  1.549677, val:  61.67%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0118%\n",
      "layer   2  Sparsity: 70.8832%\n",
      "layer   3  Sparsity: 78.2853%\n",
      "total_backward_count 832150 real_backward_count 78816   9.471%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.137443/  1.564092, val:  61.67%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0027%\n",
      "layer   2  Sparsity: 70.9745%\n",
      "layer   3  Sparsity: 78.7761%\n",
      "total_backward_count 841940 real_backward_count 79689   9.465%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.176333/  1.502007, val:  72.08%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0021%\n",
      "layer   2  Sparsity: 71.3101%\n",
      "layer   3  Sparsity: 79.4694%\n",
      "total_backward_count 851730 real_backward_count 80517   9.453%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.133000/  1.536306, val:  58.33%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.9850%\n",
      "layer   2  Sparsity: 71.3062%\n",
      "layer   3  Sparsity: 77.4640%\n",
      "total_backward_count 861520 real_backward_count 81408   9.449%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.138122/  1.445950, val:  62.50%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9985%\n",
      "layer   2  Sparsity: 71.0215%\n",
      "layer   3  Sparsity: 76.5500%\n",
      "total_backward_count 871310 real_backward_count 82246   9.439%\n",
      "lif layer 1 self.abs_max_v: 23804.5\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.118702/  1.519188, val:  67.08%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0173%\n",
      "layer   2  Sparsity: 71.2388%\n",
      "layer   3  Sparsity: 77.7132%\n",
      "total_backward_count 881100 real_backward_count 83101   9.432%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.112054/  1.532240, val:  70.00%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9927%\n",
      "layer   2  Sparsity: 71.4632%\n",
      "layer   3  Sparsity: 77.8067%\n",
      "total_backward_count 890890 real_backward_count 83948   9.423%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.142351/  1.509526, val:  70.00%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0007%\n",
      "layer   2  Sparsity: 71.2818%\n",
      "layer   3  Sparsity: 78.1984%\n",
      "total_backward_count 900680 real_backward_count 84788   9.414%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.148493/  1.522486, val:  73.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0004%\n",
      "layer   2  Sparsity: 71.0888%\n",
      "layer   3  Sparsity: 78.0676%\n",
      "total_backward_count 910470 real_backward_count 85653   9.408%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.177890/  1.589486, val:  65.00%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0027%\n",
      "layer   2  Sparsity: 71.5032%\n",
      "layer   3  Sparsity: 78.7484%\n",
      "total_backward_count 920260 real_backward_count 86529   9.403%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.189136/  1.607865, val:  60.00%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0056%\n",
      "layer   2  Sparsity: 71.5523%\n",
      "layer   3  Sparsity: 79.3235%\n",
      "total_backward_count 930050 real_backward_count 87415   9.399%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.204478/  1.509504, val:  64.58%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0042%\n",
      "layer   2  Sparsity: 71.5586%\n",
      "layer   3  Sparsity: 78.5909%\n",
      "total_backward_count 939840 real_backward_count 88302   9.395%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.122059/  1.527610, val:  58.33%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0037%\n",
      "layer   2  Sparsity: 71.2683%\n",
      "layer   3  Sparsity: 78.2125%\n",
      "total_backward_count 949630 real_backward_count 89147   9.388%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.195654/  1.555725, val:  59.17%, val_best:  76.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0185%\n",
      "layer   2  Sparsity: 70.9796%\n",
      "layer   3  Sparsity: 80.3319%\n",
      "total_backward_count 959420 real_backward_count 90016   9.382%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.198122/  1.569441, val:  61.25%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0112%\n",
      "layer   2  Sparsity: 70.8738%\n",
      "layer   3  Sparsity: 77.4751%\n",
      "total_backward_count 969210 real_backward_count 90846   9.373%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.133019/  1.530778, val:  56.67%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0031%\n",
      "layer   2  Sparsity: 71.9760%\n",
      "layer   3  Sparsity: 77.9442%\n",
      "total_backward_count 979000 real_backward_count 91698   9.366%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.126952/  1.536578, val:  67.08%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0037%\n",
      "layer   2  Sparsity: 71.7743%\n",
      "layer   3  Sparsity: 79.8758%\n",
      "total_backward_count 988790 real_backward_count 92515   9.356%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.195573/  1.577679, val:  65.00%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0039%\n",
      "layer   2  Sparsity: 71.2468%\n",
      "layer   3  Sparsity: 79.7989%\n",
      "total_backward_count 998580 real_backward_count 93422   9.355%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.205883/  1.527904, val:  65.42%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0065%\n",
      "layer   2  Sparsity: 71.8660%\n",
      "layer   3  Sparsity: 79.4326%\n",
      "total_backward_count 1008370 real_backward_count 94266   9.348%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.153474/  1.549493, val:  58.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.9957%\n",
      "layer   2  Sparsity: 71.6420%\n",
      "layer   3  Sparsity: 78.0398%\n",
      "total_backward_count 1018160 real_backward_count 95102   9.341%\n",
      "lif layer 1 self.abs_max_v: 24935.5\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.166953/  1.521117, val:  62.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0070%\n",
      "layer   2  Sparsity: 70.2361%\n",
      "layer   3  Sparsity: 77.4072%\n",
      "total_backward_count 1027950 real_backward_count 95917   9.331%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.156696/  1.548912, val:  63.75%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0140%\n",
      "layer   2  Sparsity: 71.0792%\n",
      "layer   3  Sparsity: 77.4162%\n",
      "total_backward_count 1037740 real_backward_count 96781   9.326%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.171959/  1.602863, val:  62.92%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0159%\n",
      "layer   2  Sparsity: 71.9026%\n",
      "layer   3  Sparsity: 79.3139%\n",
      "total_backward_count 1047530 real_backward_count 97636   9.321%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.201077/  1.578971, val:  53.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0063%\n",
      "layer   2  Sparsity: 71.2904%\n",
      "layer   3  Sparsity: 79.2851%\n",
      "total_backward_count 1057320 real_backward_count 98478   9.314%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.213654/  1.531587, val:  67.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0152%\n",
      "layer   2  Sparsity: 71.9477%\n",
      "layer   3  Sparsity: 80.1961%\n",
      "total_backward_count 1067110 real_backward_count 99317   9.307%\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.194156/  1.564204, val:  65.83%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.9932%\n",
      "layer   2  Sparsity: 71.6241%\n",
      "layer   3  Sparsity: 80.0431%\n",
      "total_backward_count 1076900 real_backward_count 100168   9.302%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.194003/  1.541585, val:  65.00%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.97 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.9869%\n",
      "layer   2  Sparsity: 71.3042%\n",
      "layer   3  Sparsity: 79.0579%\n",
      "total_backward_count 1086690 real_backward_count 101012   9.295%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.157749/  1.592880, val:  67.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0055%\n",
      "layer   2  Sparsity: 71.2323%\n",
      "layer   3  Sparsity: 79.4548%\n",
      "total_backward_count 1096480 real_backward_count 101858   9.290%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.234522/  1.550032, val:  70.42%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0041%\n",
      "layer   2  Sparsity: 71.4935%\n",
      "layer   3  Sparsity: 81.0152%\n",
      "total_backward_count 1106270 real_backward_count 102728   9.286%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.180270/  1.587657, val:  52.92%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.07 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0108%\n",
      "layer   2  Sparsity: 71.0751%\n",
      "layer   3  Sparsity: 78.8854%\n",
      "total_backward_count 1116060 real_backward_count 103587   9.281%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.163089/  1.528468, val:  67.08%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0059%\n",
      "layer   2  Sparsity: 71.3181%\n",
      "layer   3  Sparsity: 78.7066%\n",
      "total_backward_count 1125850 real_backward_count 104441   9.277%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.167124/  1.639591, val:  56.25%, val_best:  76.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9991%\n",
      "layer   2  Sparsity: 71.2868%\n",
      "layer   3  Sparsity: 78.9438%\n",
      "total_backward_count 1135640 real_backward_count 105317   9.274%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.162980/  1.570438, val:  64.17%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0055%\n",
      "layer   2  Sparsity: 71.1977%\n",
      "layer   3  Sparsity: 78.4046%\n",
      "total_backward_count 1145430 real_backward_count 106198   9.271%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.166313/  1.528486, val:  63.33%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0096%\n",
      "layer   2  Sparsity: 71.4596%\n",
      "layer   3  Sparsity: 78.4972%\n",
      "total_backward_count 1155220 real_backward_count 107060   9.267%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.179183/  1.579055, val:  57.08%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9861%\n",
      "layer   2  Sparsity: 70.8912%\n",
      "layer   3  Sparsity: 79.2968%\n",
      "total_backward_count 1165010 real_backward_count 107911   9.263%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.147047/  1.581953, val:  56.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0159%\n",
      "layer   2  Sparsity: 71.0554%\n",
      "layer   3  Sparsity: 78.0406%\n",
      "total_backward_count 1174800 real_backward_count 108739   9.256%\n",
      "fc layer 1 self.abs_max_out: 16131.0\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.181311/  1.571398, val:  60.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0101%\n",
      "layer   2  Sparsity: 71.2194%\n",
      "layer   3  Sparsity: 79.0996%\n",
      "total_backward_count 1184590 real_backward_count 109509   9.244%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.183362/  1.618579, val:  58.75%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.88 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0114%\n",
      "layer   2  Sparsity: 71.1761%\n",
      "layer   3  Sparsity: 79.6130%\n",
      "total_backward_count 1194380 real_backward_count 110372   9.241%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.234570/  1.591342, val:  60.83%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0142%\n",
      "layer   2  Sparsity: 71.5659%\n",
      "layer   3  Sparsity: 80.3187%\n",
      "total_backward_count 1204170 real_backward_count 111251   9.239%\n",
      "lif layer 1 self.abs_max_v: 25058.0\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.184694/  1.590062, val:  64.58%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.22 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0085%\n",
      "layer   2  Sparsity: 71.3974%\n",
      "layer   3  Sparsity: 79.4753%\n",
      "total_backward_count 1213960 real_backward_count 112103   9.234%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.216170/  1.531195, val:  67.08%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0054%\n",
      "layer   2  Sparsity: 71.4326%\n",
      "layer   3  Sparsity: 80.4447%\n",
      "total_backward_count 1223750 real_backward_count 112931   9.228%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.210418/  1.576157, val:  58.33%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0041%\n",
      "layer   2  Sparsity: 71.2847%\n",
      "layer   3  Sparsity: 79.8776%\n",
      "total_backward_count 1233540 real_backward_count 113818   9.227%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.149900/  1.558385, val:  58.33%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.90 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0232%\n",
      "layer   2  Sparsity: 71.3009%\n",
      "layer   3  Sparsity: 78.4766%\n",
      "total_backward_count 1243330 real_backward_count 114628   9.219%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.140333/  1.565220, val:  57.08%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0147%\n",
      "layer   2  Sparsity: 71.6765%\n",
      "layer   3  Sparsity: 79.1931%\n",
      "total_backward_count 1253120 real_backward_count 115443   9.212%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.136851/  1.539775, val:  65.42%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.9999%\n",
      "layer   2  Sparsity: 71.3771%\n",
      "layer   3  Sparsity: 77.8835%\n",
      "total_backward_count 1262910 real_backward_count 116283   9.208%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.161063/  1.541886, val:  65.00%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.9945%\n",
      "layer   2  Sparsity: 71.3694%\n",
      "layer   3  Sparsity: 79.7445%\n",
      "total_backward_count 1272700 real_backward_count 117120   9.202%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.196800/  1.550605, val:  54.58%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9960%\n",
      "layer   2  Sparsity: 71.3427%\n",
      "layer   3  Sparsity: 79.3372%\n",
      "total_backward_count 1282490 real_backward_count 117956   9.197%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.159038/  1.596793, val:  55.42%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.16 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0059%\n",
      "layer   2  Sparsity: 71.6924%\n",
      "layer   3  Sparsity: 79.2213%\n",
      "total_backward_count 1292280 real_backward_count 118854   9.197%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.185391/  1.508592, val:  65.42%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.9992%\n",
      "layer   2  Sparsity: 71.6367%\n",
      "layer   3  Sparsity: 79.8334%\n",
      "total_backward_count 1302070 real_backward_count 119693   9.193%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.093619/  1.530110, val:  60.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0066%\n",
      "layer   2  Sparsity: 71.3368%\n",
      "layer   3  Sparsity: 77.6009%\n",
      "total_backward_count 1311860 real_backward_count 120522   9.187%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.160081/  1.516544, val:  64.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0178%\n",
      "layer   2  Sparsity: 70.9591%\n",
      "layer   3  Sparsity: 78.0474%\n",
      "total_backward_count 1321650 real_backward_count 121354   9.182%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.161713/  1.539334, val:  61.25%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0110%\n",
      "layer   2  Sparsity: 71.5313%\n",
      "layer   3  Sparsity: 79.2121%\n",
      "total_backward_count 1331440 real_backward_count 122250   9.182%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.142574/  1.540080, val:  64.58%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0005%\n",
      "layer   2  Sparsity: 71.8781%\n",
      "layer   3  Sparsity: 78.4899%\n",
      "total_backward_count 1341230 real_backward_count 123043   9.174%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.166461/  1.606023, val:  58.33%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0108%\n",
      "layer   2  Sparsity: 71.8004%\n",
      "layer   3  Sparsity: 80.2274%\n",
      "total_backward_count 1351020 real_backward_count 123878   9.169%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.146699/  1.552429, val:  67.08%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0067%\n",
      "layer   2  Sparsity: 72.1404%\n",
      "layer   3  Sparsity: 79.7613%\n",
      "total_backward_count 1360810 real_backward_count 124708   9.164%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.143308/  1.472136, val:  62.92%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9884%\n",
      "layer   2  Sparsity: 71.6734%\n",
      "layer   3  Sparsity: 78.7939%\n",
      "total_backward_count 1370600 real_backward_count 125540   9.159%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.122353/  1.534102, val:  57.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.41 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0059%\n",
      "layer   2  Sparsity: 72.0403%\n",
      "layer   3  Sparsity: 78.8253%\n",
      "total_backward_count 1380390 real_backward_count 126398   9.157%\n",
      "lif layer 1 self.abs_max_v: 25259.5\n",
      "lif layer 1 self.abs_max_v: 25599.5\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.117559/  1.576593, val:  49.17%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.9989%\n",
      "layer   2  Sparsity: 72.0748%\n",
      "layer   3  Sparsity: 78.8480%\n",
      "total_backward_count 1390180 real_backward_count 127213   9.151%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.117642/  1.495460, val:  58.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0000%\n",
      "layer   2  Sparsity: 72.4964%\n",
      "layer   3  Sparsity: 78.6543%\n",
      "total_backward_count 1399970 real_backward_count 128020   9.144%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.077876/  1.522797, val:  55.00%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0014%\n",
      "layer   2  Sparsity: 71.4043%\n",
      "layer   3  Sparsity: 77.1002%\n",
      "total_backward_count 1409760 real_backward_count 128849   9.140%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.084979/  1.427575, val:  65.42%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0028%\n",
      "layer   2  Sparsity: 71.8226%\n",
      "layer   3  Sparsity: 77.4400%\n",
      "total_backward_count 1419550 real_backward_count 129702   9.137%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.092028/  1.582034, val:  54.58%, val_best:  76.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0006%\n",
      "layer   2  Sparsity: 71.8763%\n",
      "layer   3  Sparsity: 77.8608%\n",
      "total_backward_count 1429340 real_backward_count 130538   9.133%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.091281/  1.492677, val:  62.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0097%\n",
      "layer   2  Sparsity: 71.8659%\n",
      "layer   3  Sparsity: 76.8543%\n",
      "total_backward_count 1439130 real_backward_count 131366   9.128%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.077634/  1.537080, val:  57.08%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.9943%\n",
      "layer   2  Sparsity: 72.1040%\n",
      "layer   3  Sparsity: 77.2450%\n",
      "total_backward_count 1448920 real_backward_count 132152   9.121%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.090384/  1.537996, val:  52.92%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9932%\n",
      "layer   2  Sparsity: 71.7935%\n",
      "layer   3  Sparsity: 78.0979%\n",
      "total_backward_count 1458710 real_backward_count 132993   9.117%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.126511/  1.482165, val:  65.42%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.37 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0053%\n",
      "layer   2  Sparsity: 71.4348%\n",
      "layer   3  Sparsity: 79.7891%\n",
      "total_backward_count 1468500 real_backward_count 133808   9.112%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.104314/  1.472304, val:  61.67%, val_best:  76.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0078%\n",
      "layer   2  Sparsity: 71.1673%\n",
      "layer   3  Sparsity: 78.4228%\n",
      "total_backward_count 1478290 real_backward_count 134604   9.105%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.077156/  1.481936, val:  59.17%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0054%\n",
      "layer   2  Sparsity: 72.1308%\n",
      "layer   3  Sparsity: 78.1325%\n",
      "total_backward_count 1488080 real_backward_count 135411   9.100%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.106377/  1.507753, val:  63.75%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0098%\n",
      "layer   2  Sparsity: 71.8131%\n",
      "layer   3  Sparsity: 79.4756%\n",
      "total_backward_count 1497870 real_backward_count 136204   9.093%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.095629/  1.460884, val:  70.42%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0046%\n",
      "layer   2  Sparsity: 71.6348%\n",
      "layer   3  Sparsity: 78.8218%\n",
      "total_backward_count 1507660 real_backward_count 137032   9.089%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.082755/  1.460949, val:  65.42%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0006%\n",
      "layer   2  Sparsity: 71.6001%\n",
      "layer   3  Sparsity: 77.1368%\n",
      "total_backward_count 1517450 real_backward_count 137820   9.082%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.061746/  1.475497, val:  64.17%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0005%\n",
      "layer   2  Sparsity: 71.8971%\n",
      "layer   3  Sparsity: 77.0581%\n",
      "total_backward_count 1527240 real_backward_count 138635   9.077%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.085226/  1.505015, val:  55.42%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0148%\n",
      "layer   2  Sparsity: 72.2530%\n",
      "layer   3  Sparsity: 78.0051%\n",
      "total_backward_count 1537030 real_backward_count 139476   9.074%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.084102/  1.480930, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0092%\n",
      "layer   2  Sparsity: 72.0068%\n",
      "layer   3  Sparsity: 78.7859%\n",
      "total_backward_count 1546820 real_backward_count 140266   9.068%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.119281/  1.569646, val:  53.75%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0147%\n",
      "layer   2  Sparsity: 71.4584%\n",
      "layer   3  Sparsity: 78.6976%\n",
      "total_backward_count 1556610 real_backward_count 141131   9.067%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.108699/  1.471138, val:  66.25%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0106%\n",
      "layer   2  Sparsity: 71.9848%\n",
      "layer   3  Sparsity: 78.1252%\n",
      "total_backward_count 1566400 real_backward_count 141907   9.059%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.101349/  1.468148, val:  70.00%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0157%\n",
      "layer   2  Sparsity: 71.8689%\n",
      "layer   3  Sparsity: 78.3733%\n",
      "total_backward_count 1576190 real_backward_count 142695   9.053%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.131190/  1.476544, val:  64.58%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0094%\n",
      "layer   2  Sparsity: 71.5699%\n",
      "layer   3  Sparsity: 78.6487%\n",
      "total_backward_count 1585980 real_backward_count 143547   9.051%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.108828/  1.486462, val:  65.83%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0074%\n",
      "layer   2  Sparsity: 71.7184%\n",
      "layer   3  Sparsity: 78.9920%\n",
      "total_backward_count 1595770 real_backward_count 144443   9.052%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.090054/  1.517748, val:  64.17%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.95 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0202%\n",
      "layer   2  Sparsity: 72.1299%\n",
      "layer   3  Sparsity: 77.8534%\n",
      "total_backward_count 1605560 real_backward_count 145183   9.043%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.095997/  1.546607, val:  54.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0125%\n",
      "layer   2  Sparsity: 71.7739%\n",
      "layer   3  Sparsity: 77.8415%\n",
      "total_backward_count 1615350 real_backward_count 146035   9.040%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.098256/  1.526677, val:  60.42%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.9929%\n",
      "layer   2  Sparsity: 72.0259%\n",
      "layer   3  Sparsity: 77.5905%\n",
      "total_backward_count 1625140 real_backward_count 146880   9.038%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.053397/  1.489767, val:  63.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0087%\n",
      "layer   2  Sparsity: 71.6796%\n",
      "layer   3  Sparsity: 76.6640%\n",
      "total_backward_count 1634930 real_backward_count 147654   9.031%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.098428/  1.557267, val:  53.33%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.94 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0116%\n",
      "layer   2  Sparsity: 71.5847%\n",
      "layer   3  Sparsity: 78.3677%\n",
      "total_backward_count 1644720 real_backward_count 148483   9.028%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.072340/  1.457830, val:  70.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0115%\n",
      "layer   2  Sparsity: 71.9331%\n",
      "layer   3  Sparsity: 77.8258%\n",
      "total_backward_count 1654510 real_backward_count 149276   9.022%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.107902/  1.563707, val:  60.42%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0017%\n",
      "layer   2  Sparsity: 71.5469%\n",
      "layer   3  Sparsity: 78.5763%\n",
      "total_backward_count 1664300 real_backward_count 150035   9.015%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.211467/  1.551603, val:  57.50%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0224%\n",
      "layer   2  Sparsity: 71.3900%\n",
      "layer   3  Sparsity: 79.1477%\n",
      "total_backward_count 1674090 real_backward_count 150880   9.013%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.138489/  1.554262, val:  60.42%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0076%\n",
      "layer   2  Sparsity: 71.8129%\n",
      "layer   3  Sparsity: 79.2463%\n",
      "total_backward_count 1683880 real_backward_count 151696   9.009%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.104229/  1.512526, val:  60.83%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.9890%\n",
      "layer   2  Sparsity: 71.2687%\n",
      "layer   3  Sparsity: 78.7788%\n",
      "total_backward_count 1693670 real_backward_count 152560   9.008%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.115178/  1.480344, val:  62.92%, val_best:  78.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0023%\n",
      "layer   2  Sparsity: 71.4888%\n",
      "layer   3  Sparsity: 79.7981%\n",
      "total_backward_count 1703460 real_backward_count 153347   9.002%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.121000/  1.530827, val:  63.33%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0145%\n",
      "layer   2  Sparsity: 71.8578%\n",
      "layer   3  Sparsity: 79.5534%\n",
      "total_backward_count 1713250 real_backward_count 154135   8.997%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.091968/  1.520996, val:  78.75%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0183%\n",
      "layer   2  Sparsity: 72.0457%\n",
      "layer   3  Sparsity: 79.2595%\n",
      "total_backward_count 1723040 real_backward_count 154931   8.992%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.129399/  1.473634, val:  61.25%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0131%\n",
      "layer   2  Sparsity: 71.5670%\n",
      "layer   3  Sparsity: 78.4694%\n",
      "total_backward_count 1732830 real_backward_count 155760   8.989%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.098117/  1.475499, val:  66.25%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.9974%\n",
      "layer   2  Sparsity: 72.0797%\n",
      "layer   3  Sparsity: 78.2747%\n",
      "total_backward_count 1742620 real_backward_count 156593   8.986%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.087909/  1.591271, val:  55.00%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9977%\n",
      "layer   2  Sparsity: 71.9244%\n",
      "layer   3  Sparsity: 79.4485%\n",
      "total_backward_count 1752410 real_backward_count 157406   8.982%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.168774/  1.485636, val:  62.92%, val_best:  78.75%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0247%\n",
      "layer   2  Sparsity: 71.7878%\n",
      "layer   3  Sparsity: 80.3422%\n",
      "total_backward_count 1762200 real_backward_count 158279   8.982%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.111890/  1.507870, val:  63.33%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.9947%\n",
      "layer   2  Sparsity: 71.5488%\n",
      "layer   3  Sparsity: 78.1864%\n",
      "total_backward_count 1771990 real_backward_count 159048   8.976%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.101419/  1.509882, val:  62.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0106%\n",
      "layer   2  Sparsity: 71.6057%\n",
      "layer   3  Sparsity: 77.5487%\n",
      "total_backward_count 1781780 real_backward_count 159823   8.970%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.072746/  1.442515, val:  73.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0050%\n",
      "layer   2  Sparsity: 71.7219%\n",
      "layer   3  Sparsity: 78.2259%\n",
      "total_backward_count 1791570 real_backward_count 160610   8.965%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.119464/  1.521865, val:  58.75%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.9993%\n",
      "layer   2  Sparsity: 71.4475%\n",
      "layer   3  Sparsity: 79.2900%\n",
      "total_backward_count 1801360 real_backward_count 161379   8.959%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.089150/  1.522345, val:  57.92%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9928%\n",
      "layer   2  Sparsity: 72.1906%\n",
      "layer   3  Sparsity: 78.8558%\n",
      "total_backward_count 1811150 real_backward_count 162208   8.956%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.139713/  1.506819, val:  63.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0024%\n",
      "layer   2  Sparsity: 71.9100%\n",
      "layer   3  Sparsity: 79.3069%\n",
      "total_backward_count 1820940 real_backward_count 163072   8.955%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.186638/  1.513924, val:  64.17%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 90.9910%\n",
      "layer   2  Sparsity: 71.1632%\n",
      "layer   3  Sparsity: 79.5775%\n",
      "total_backward_count 1830730 real_backward_count 163977   8.957%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.152652/  1.492447, val:  68.75%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9945%\n",
      "layer   2  Sparsity: 71.1557%\n",
      "layer   3  Sparsity: 79.4195%\n",
      "total_backward_count 1840520 real_backward_count 164822   8.955%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.174468/  1.587506, val:  62.92%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0043%\n",
      "layer   2  Sparsity: 71.1961%\n",
      "layer   3  Sparsity: 80.8466%\n",
      "total_backward_count 1850310 real_backward_count 165623   8.951%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.166117/  1.474053, val:  65.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 90.9973%\n",
      "layer   2  Sparsity: 71.6987%\n",
      "layer   3  Sparsity: 79.7856%\n",
      "total_backward_count 1860100 real_backward_count 166497   8.951%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.083990/  1.469658, val:  70.42%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9949%\n",
      "layer   2  Sparsity: 71.3322%\n",
      "layer   3  Sparsity: 77.9791%\n",
      "total_backward_count 1869890 real_backward_count 167262   8.945%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.080382/  1.544089, val:  55.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0114%\n",
      "layer   2  Sparsity: 71.8217%\n",
      "layer   3  Sparsity: 78.2604%\n",
      "total_backward_count 1879680 real_backward_count 168047   8.940%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.131251/  1.548760, val:  67.50%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 90.9845%\n",
      "layer   2  Sparsity: 72.2360%\n",
      "layer   3  Sparsity: 78.8526%\n",
      "total_backward_count 1889470 real_backward_count 168882   8.938%\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.140411/  1.606076, val:  54.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0090%\n",
      "layer   2  Sparsity: 72.0212%\n",
      "layer   3  Sparsity: 79.8852%\n",
      "total_backward_count 1899260 real_backward_count 169695   8.935%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.157981/  1.542759, val:  60.42%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0008%\n",
      "layer   2  Sparsity: 71.0051%\n",
      "layer   3  Sparsity: 78.5374%\n",
      "total_backward_count 1909050 real_backward_count 170555   8.934%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.141625/  1.592139, val:  60.42%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.21 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0046%\n",
      "layer   2  Sparsity: 71.4226%\n",
      "layer   3  Sparsity: 79.8116%\n",
      "total_backward_count 1918840 real_backward_count 171408   8.933%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.172019/  1.541375, val:  65.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0075%\n",
      "layer   2  Sparsity: 71.5747%\n",
      "layer   3  Sparsity: 80.2312%\n",
      "total_backward_count 1928630 real_backward_count 172222   8.930%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.159856/  1.468112, val:  66.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0099%\n",
      "layer   2  Sparsity: 71.9154%\n",
      "layer   3  Sparsity: 78.9707%\n",
      "total_backward_count 1938420 real_backward_count 173042   8.927%\n",
      "lif layer 1 self.abs_max_v: 25658.0\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.116580/  1.579518, val:  60.42%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0107%\n",
      "layer   2  Sparsity: 71.9484%\n",
      "layer   3  Sparsity: 79.0396%\n",
      "total_backward_count 1948210 real_backward_count 173876   8.925%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.174500/  1.504410, val:  64.17%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0020%\n",
      "layer   2  Sparsity: 72.7664%\n",
      "layer   3  Sparsity: 80.3114%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cfd1a95e96b472b9ee35c7f627aa840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÉ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñÅ‚ñÜ‚ñÅ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñÅ‚ñÜ‚ñà‚ñà‚ñÖ‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñÖ‚ñà‚ñá‚ñá‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÉ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>1.1745</td></tr><tr><td>val_acc_best</td><td>0.7875</td></tr><tr><td>val_acc_now</td><td>0.64167</td></tr><tr><td>val_loss</td><td>1.50441</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lunar-sweep-77</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7dg4so5e' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7dg4so5e</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_192522-7dg4so5e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3udg0bjz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251115_235036-3udg0bjz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3udg0bjz' target=\"_blank\">rare-sweep-82</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3udg0bjz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3udg0bjz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251115_235045_987', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 6, 'leaky_temporal_filter': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 555.0\n",
      "lif layer 1 self.abs_max_v: 555.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1185.0\n",
      "lif layer 2 self.abs_max_v: 1185.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 311.0\n",
      "fc layer 1 self.abs_max_out: 584.0\n",
      "lif layer 1 self.abs_max_v: 829.5\n",
      "lif layer 2 self.abs_max_v: 1638.5\n",
      "fc layer 3 self.abs_max_out: 423.0\n",
      "fc layer 1 self.abs_max_out: 672.0\n",
      "lif layer 1 self.abs_max_v: 894.0\n",
      "fc layer 1 self.abs_max_out: 701.0\n",
      "lif layer 1 self.abs_max_v: 955.0\n",
      "fc layer 1 self.abs_max_out: 802.0\n",
      "lif layer 2 self.abs_max_v: 1761.5\n",
      "lif layer 2 self.abs_max_v: 1989.5\n",
      "fc layer 1 self.abs_max_out: 991.0\n",
      "lif layer 1 self.abs_max_v: 991.0\n",
      "fc layer 2 self.abs_max_out: 1316.0\n",
      "lif layer 1 self.abs_max_v: 1189.0\n",
      "fc layer 3 self.abs_max_out: 470.0\n",
      "fc layer 3 self.abs_max_out: 492.0\n",
      "fc layer 1 self.abs_max_out: 1057.0\n",
      "lif layer 1 self.abs_max_v: 1312.0\n",
      "fc layer 1 self.abs_max_out: 1308.0\n",
      "lif layer 1 self.abs_max_v: 1503.5\n",
      "lif layer 1 self.abs_max_v: 1574.5\n",
      "lif layer 1 self.abs_max_v: 1663.5\n",
      "lif layer 2 self.abs_max_v: 2126.5\n",
      "lif layer 1 self.abs_max_v: 1931.0\n",
      "fc layer 1 self.abs_max_out: 1422.0\n",
      "fc layer 1 self.abs_max_out: 1603.0\n",
      "fc layer 2 self.abs_max_out: 1319.0\n",
      "fc layer 1 self.abs_max_out: 1751.0\n",
      "lif layer 2 self.abs_max_v: 2221.5\n",
      "lif layer 2 self.abs_max_v: 2228.5\n",
      "fc layer 2 self.abs_max_out: 1362.0\n",
      "fc layer 2 self.abs_max_out: 1410.0\n",
      "lif layer 2 self.abs_max_v: 2368.5\n",
      "fc layer 2 self.abs_max_out: 1509.0\n",
      "fc layer 1 self.abs_max_out: 1816.0\n",
      "lif layer 1 self.abs_max_v: 2103.5\n",
      "fc layer 1 self.abs_max_out: 1838.0\n",
      "fc layer 1 self.abs_max_out: 1907.0\n",
      "lif layer 2 self.abs_max_v: 2461.5\n",
      "fc layer 2 self.abs_max_out: 1640.0\n",
      "lif layer 2 self.abs_max_v: 2506.0\n",
      "fc layer 3 self.abs_max_out: 501.0\n",
      "fc layer 3 self.abs_max_out: 504.0\n",
      "fc layer 3 self.abs_max_out: 570.0\n",
      "lif layer 2 self.abs_max_v: 2553.5\n",
      "fc layer 2 self.abs_max_out: 1672.0\n",
      "lif layer 2 self.abs_max_v: 2608.5\n",
      "lif layer 1 self.abs_max_v: 2390.5\n",
      "lif layer 1 self.abs_max_v: 2653.0\n",
      "lif layer 1 self.abs_max_v: 2769.5\n",
      "fc layer 3 self.abs_max_out: 632.0\n",
      "fc layer 1 self.abs_max_out: 1952.0\n",
      "lif layer 1 self.abs_max_v: 2991.5\n",
      "fc layer 1 self.abs_max_out: 2029.0\n",
      "lif layer 1 self.abs_max_v: 3030.5\n",
      "lif layer 1 self.abs_max_v: 3329.5\n",
      "lif layer 1 self.abs_max_v: 3335.0\n",
      "lif layer 1 self.abs_max_v: 3507.5\n",
      "lif layer 1 self.abs_max_v: 3663.0\n",
      "lif layer 2 self.abs_max_v: 2667.0\n",
      "lif layer 2 self.abs_max_v: 2824.5\n",
      "fc layer 1 self.abs_max_out: 2234.0\n",
      "fc layer 1 self.abs_max_out: 2489.0\n",
      "lif layer 1 self.abs_max_v: 3782.5\n",
      "lif layer 1 self.abs_max_v: 3817.0\n",
      "lif layer 1 self.abs_max_v: 4041.5\n",
      "lif layer 1 self.abs_max_v: 4179.0\n",
      "lif layer 2 self.abs_max_v: 2849.0\n",
      "fc layer 1 self.abs_max_out: 2555.0\n",
      "lif layer 1 self.abs_max_v: 4199.0\n",
      "fc layer 1 self.abs_max_out: 2613.0\n",
      "lif layer 1 self.abs_max_v: 4493.0\n",
      "lif layer 1 self.abs_max_v: 4592.0\n",
      "fc layer 1 self.abs_max_out: 2640.0\n",
      "lif layer 1 self.abs_max_v: 4677.0\n",
      "fc layer 1 self.abs_max_out: 2703.0\n",
      "lif layer 1 self.abs_max_v: 4846.5\n",
      "lif layer 1 self.abs_max_v: 4850.5\n",
      "lif layer 1 self.abs_max_v: 4858.5\n",
      "lif layer 1 self.abs_max_v: 4891.0\n",
      "fc layer 2 self.abs_max_out: 1779.0\n",
      "lif layer 2 self.abs_max_v: 2875.5\n",
      "lif layer 2 self.abs_max_v: 2965.5\n",
      "lif layer 2 self.abs_max_v: 3057.0\n",
      "fc layer 1 self.abs_max_out: 2704.0\n",
      "fc layer 1 self.abs_max_out: 3066.0\n",
      "lif layer 1 self.abs_max_v: 5144.0\n",
      "lif layer 1 self.abs_max_v: 5292.5\n",
      "lif layer 1 self.abs_max_v: 5387.5\n",
      "fc layer 1 self.abs_max_out: 3186.0\n",
      "lif layer 1 self.abs_max_v: 5474.0\n",
      "lif layer 1 self.abs_max_v: 5486.0\n",
      "lif layer 1 self.abs_max_v: 5503.0\n",
      "lif layer 1 self.abs_max_v: 5546.5\n",
      "fc layer 1 self.abs_max_out: 3348.0\n",
      "lif layer 1 self.abs_max_v: 5716.0\n",
      "lif layer 1 self.abs_max_v: 5735.0\n",
      "fc layer 1 self.abs_max_out: 3427.0\n",
      "lif layer 1 self.abs_max_v: 6101.0\n",
      "fc layer 1 self.abs_max_out: 3503.0\n",
      "lif layer 1 self.abs_max_v: 6483.5\n",
      "lif layer 2 self.abs_max_v: 3148.5\n",
      "fc layer 1 self.abs_max_out: 3550.0\n",
      "lif layer 1 self.abs_max_v: 6595.0\n",
      "fc layer 3 self.abs_max_out: 645.0\n",
      "fc layer 3 self.abs_max_out: 732.0\n",
      "lif layer 1 self.abs_max_v: 6733.5\n",
      "fc layer 1 self.abs_max_out: 3636.0\n",
      "lif layer 1 self.abs_max_v: 6880.5\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.861823/  1.965547, val:  40.42%, val_best:  40.42%, tr:  95.61%, tr_best:  95.61%, epoch time: 80.21 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6830%\n",
      "layer   2  Sparsity: 70.3916%\n",
      "layer   3  Sparsity: 64.0703%\n",
      "total_backward_count 9790 real_backward_count 2113  21.583%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 3692.0\n",
      "fc layer 1 self.abs_max_out: 3708.0\n",
      "lif layer 1 self.abs_max_v: 7085.0\n",
      "fc layer 1 self.abs_max_out: 3878.0\n",
      "fc layer 1 self.abs_max_out: 4015.0\n",
      "fc layer 1 self.abs_max_out: 4101.0\n",
      "fc layer 2 self.abs_max_out: 1808.0\n",
      "fc layer 3 self.abs_max_out: 736.0\n",
      "fc layer 1 self.abs_max_out: 4315.0\n",
      "fc layer 2 self.abs_max_out: 2001.0\n",
      "lif layer 1 self.abs_max_v: 7131.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.747703/  1.931096, val:  42.50%, val_best:  42.50%, tr:  99.08%, tr_best:  99.08%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6807%\n",
      "layer   2  Sparsity: 76.8842%\n",
      "layer   3  Sparsity: 64.6900%\n",
      "total_backward_count 19580 real_backward_count 3624  18.509%\n",
      "lif layer 1 self.abs_max_v: 7171.0\n",
      "lif layer 1 self.abs_max_v: 7461.5\n",
      "fc layer 3 self.abs_max_out: 737.0\n",
      "fc layer 3 self.abs_max_out: 738.0\n",
      "fc layer 3 self.abs_max_out: 745.0\n",
      "fc layer 3 self.abs_max_out: 785.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.720548/  1.917273, val:  50.83%, val_best:  50.83%, tr:  99.18%, tr_best:  99.18%, epoch time: 80.79 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 83.6884%\n",
      "layer   2  Sparsity: 77.4474%\n",
      "layer   3  Sparsity: 64.3957%\n",
      "total_backward_count 29370 real_backward_count 4929  16.782%\n",
      "lif layer 2 self.abs_max_v: 3160.0\n",
      "lif layer 1 self.abs_max_v: 7634.5\n",
      "lif layer 1 self.abs_max_v: 7791.5\n",
      "lif layer 2 self.abs_max_v: 3171.5\n",
      "lif layer 2 self.abs_max_v: 3272.0\n",
      "fc layer 3 self.abs_max_out: 798.0\n",
      "fc layer 1 self.abs_max_out: 4695.0\n",
      "lif layer 1 self.abs_max_v: 8028.5\n",
      "lif layer 1 self.abs_max_v: 8131.0\n",
      "lif layer 1 self.abs_max_v: 8292.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.704270/  1.877965, val:  53.75%, val_best:  53.75%, tr:  99.39%, tr_best:  99.39%, epoch time: 80.44 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6845%\n",
      "layer   2  Sparsity: 76.9591%\n",
      "layer   3  Sparsity: 62.8750%\n",
      "total_backward_count 39160 real_backward_count 6144  15.689%\n",
      "lif layer 1 self.abs_max_v: 8417.0\n",
      "fc layer 1 self.abs_max_out: 4717.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.690661/  1.886572, val:  53.75%, val_best:  53.75%, tr:  99.08%, tr_best:  99.39%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6853%\n",
      "layer   2  Sparsity: 78.0914%\n",
      "layer   3  Sparsity: 62.8208%\n",
      "total_backward_count 48950 real_backward_count 7245  14.801%\n",
      "lif layer 2 self.abs_max_v: 3295.5\n",
      "lif layer 2 self.abs_max_v: 3300.0\n",
      "lif layer 2 self.abs_max_v: 3382.0\n",
      "fc layer 3 self.abs_max_out: 843.0\n",
      "fc layer 1 self.abs_max_out: 4938.0\n",
      "lif layer 1 self.abs_max_v: 8640.0\n",
      "lif layer 1 self.abs_max_v: 8825.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.683309/  1.890269, val:  53.33%, val_best:  53.75%, tr:  99.39%, tr_best:  99.39%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6895%\n",
      "layer   2  Sparsity: 78.6534%\n",
      "layer   3  Sparsity: 64.7220%\n",
      "total_backward_count 58740 real_backward_count 8305  14.139%\n",
      "fc layer 2 self.abs_max_out: 2004.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.699995/  1.893815, val:  50.00%, val_best:  53.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 79.88 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6975%\n",
      "layer   2  Sparsity: 78.5849%\n",
      "layer   3  Sparsity: 63.2126%\n",
      "total_backward_count 68530 real_backward_count 9412  13.734%\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.698229/  1.852374, val:  54.58%, val_best:  54.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7022%\n",
      "layer   2  Sparsity: 79.4323%\n",
      "layer   3  Sparsity: 63.5532%\n",
      "total_backward_count 78320 real_backward_count 10414  13.297%\n",
      "fc layer 2 self.abs_max_out: 2094.0\n",
      "fc layer 2 self.abs_max_out: 2308.0\n",
      "lif layer 1 self.abs_max_v: 9012.0\n",
      "lif layer 1 self.abs_max_v: 9170.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.673415/  1.827613, val:  57.92%, val_best:  57.92%, tr:  99.69%, tr_best:  99.69%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6680%\n",
      "layer   2  Sparsity: 78.7283%\n",
      "layer   3  Sparsity: 64.0858%\n",
      "total_backward_count 88110 real_backward_count 11434  12.977%\n",
      "lif layer 2 self.abs_max_v: 3391.0\n",
      "lif layer 2 self.abs_max_v: 3414.5\n",
      "lif layer 2 self.abs_max_v: 3538.5\n",
      "lif layer 2 self.abs_max_v: 3678.5\n",
      "fc layer 1 self.abs_max_out: 5005.0\n",
      "lif layer 1 self.abs_max_v: 9175.5\n",
      "lif layer 1 self.abs_max_v: 9490.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.662075/  1.869050, val:  57.08%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6989%\n",
      "layer   2  Sparsity: 77.3881%\n",
      "layer   3  Sparsity: 64.7007%\n",
      "total_backward_count 97900 real_backward_count 12435  12.702%\n",
      "fc layer 1 self.abs_max_out: 5075.0\n",
      "lif layer 2 self.abs_max_v: 3695.5\n",
      "lif layer 2 self.abs_max_v: 3696.0\n",
      "lif layer 2 self.abs_max_v: 3829.5\n",
      "lif layer 2 self.abs_max_v: 3874.0\n",
      "lif layer 2 self.abs_max_v: 3902.0\n",
      "lif layer 2 self.abs_max_v: 3913.5\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.661370/  1.853899, val:  49.58%, val_best:  57.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6858%\n",
      "layer   2  Sparsity: 77.5076%\n",
      "layer   3  Sparsity: 62.7975%\n",
      "total_backward_count 107690 real_backward_count 13384  12.428%\n",
      "fc layer 1 self.abs_max_out: 5301.0\n",
      "fc layer 1 self.abs_max_out: 5580.0\n",
      "lif layer 1 self.abs_max_v: 10231.5\n",
      "fc layer 1 self.abs_max_out: 5682.0\n",
      "lif layer 1 self.abs_max_v: 10320.0\n",
      "lif layer 1 self.abs_max_v: 10460.0\n",
      "lif layer 1 self.abs_max_v: 10679.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.672413/  1.843990, val:  55.00%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7037%\n",
      "layer   2  Sparsity: 77.8992%\n",
      "layer   3  Sparsity: 63.4098%\n",
      "total_backward_count 117480 real_backward_count 14372  12.234%\n",
      "fc layer 1 self.abs_max_out: 5776.0\n",
      "fc layer 1 self.abs_max_out: 6285.0\n",
      "lif layer 1 self.abs_max_v: 10818.0\n",
      "lif layer 1 self.abs_max_v: 11072.0\n",
      "lif layer 1 self.abs_max_v: 11256.0\n",
      "lif layer 1 self.abs_max_v: 11527.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.663931/  1.868479, val:  56.25%, val_best:  57.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6972%\n",
      "layer   2  Sparsity: 77.5738%\n",
      "layer   3  Sparsity: 62.7597%\n",
      "total_backward_count 127270 real_backward_count 15306  12.026%\n",
      "fc layer 1 self.abs_max_out: 6312.0\n",
      "fc layer 3 self.abs_max_out: 846.0\n",
      "lif layer 2 self.abs_max_v: 3923.0\n",
      "fc layer 3 self.abs_max_out: 882.0\n",
      "lif layer 2 self.abs_max_v: 3925.5\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.655578/  1.861188, val:  52.08%, val_best:  57.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6936%\n",
      "layer   2  Sparsity: 76.0287%\n",
      "layer   3  Sparsity: 60.2724%\n",
      "total_backward_count 137060 real_backward_count 16229  11.841%\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.656753/  1.850761, val:  56.67%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6793%\n",
      "layer   2  Sparsity: 75.3769%\n",
      "layer   3  Sparsity: 61.3745%\n",
      "total_backward_count 146850 real_backward_count 17111  11.652%\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.655686/  1.840340, val:  57.08%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6850%\n",
      "layer   2  Sparsity: 75.6310%\n",
      "layer   3  Sparsity: 61.0591%\n",
      "total_backward_count 156640 real_backward_count 18031  11.511%\n",
      "lif layer 2 self.abs_max_v: 4039.5\n",
      "lif layer 2 self.abs_max_v: 4173.0\n",
      "lif layer 2 self.abs_max_v: 4222.5\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.653147/  1.823743, val:  62.08%, val_best:  62.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6862%\n",
      "layer   2  Sparsity: 75.9151%\n",
      "layer   3  Sparsity: 60.7530%\n",
      "total_backward_count 166430 real_backward_count 18911  11.363%\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.638481/  1.815097, val:  60.00%, val_best:  62.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6979%\n",
      "layer   2  Sparsity: 75.9039%\n",
      "layer   3  Sparsity: 62.6310%\n",
      "total_backward_count 176220 real_backward_count 19765  11.216%\n",
      "lif layer 2 self.abs_max_v: 4225.5\n",
      "lif layer 2 self.abs_max_v: 4242.0\n",
      "fc layer 2 self.abs_max_out: 2413.0\n",
      "lif layer 2 self.abs_max_v: 4534.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.632099/  1.821155, val:  59.17%, val_best:  62.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7169%\n",
      "layer   2  Sparsity: 76.0398%\n",
      "layer   3  Sparsity: 62.7825%\n",
      "total_backward_count 186010 real_backward_count 20704  11.131%\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.630472/  1.849643, val:  45.83%, val_best:  62.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6813%\n",
      "layer   2  Sparsity: 75.9357%\n",
      "layer   3  Sparsity: 62.0201%\n",
      "total_backward_count 195800 real_backward_count 21539  11.001%\n",
      "fc layer 3 self.abs_max_out: 885.0\n",
      "fc layer 3 self.abs_max_out: 899.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.615769/  1.807292, val:  61.25%, val_best:  62.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6756%\n",
      "layer   2  Sparsity: 75.3035%\n",
      "layer   3  Sparsity: 61.9152%\n",
      "total_backward_count 205590 real_backward_count 22371  10.881%\n",
      "lif layer 1 self.abs_max_v: 11725.0\n",
      "fc layer 1 self.abs_max_out: 6801.0\n",
      "lif layer 1 self.abs_max_v: 12140.5\n",
      "lif layer 1 self.abs_max_v: 12363.0\n",
      "lif layer 1 self.abs_max_v: 12656.5\n",
      "fc layer 1 self.abs_max_out: 6870.0\n",
      "lif layer 1 self.abs_max_v: 12966.0\n",
      "fc layer 1 self.abs_max_out: 7053.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.628212/  1.864980, val:  58.33%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6707%\n",
      "layer   2  Sparsity: 75.1183%\n",
      "layer   3  Sparsity: 61.8297%\n",
      "total_backward_count 215380 real_backward_count 23265  10.802%\n",
      "lif layer 1 self.abs_max_v: 13124.0\n",
      "lif layer 1 self.abs_max_v: 13223.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.637205/  1.800126, val:  55.00%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 80.21 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6750%\n",
      "layer   2  Sparsity: 76.1598%\n",
      "layer   3  Sparsity: 60.7205%\n",
      "total_backward_count 225170 real_backward_count 24159  10.729%\n",
      "fc layer 1 self.abs_max_out: 7412.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.633381/  1.837528, val:  57.50%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 80.17 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.7012%\n",
      "layer   2  Sparsity: 75.4309%\n",
      "layer   3  Sparsity: 60.6877%\n",
      "total_backward_count 234960 real_backward_count 25054  10.663%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.640919/  1.830745, val:  60.83%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7065%\n",
      "layer   2  Sparsity: 74.2692%\n",
      "layer   3  Sparsity: 61.0025%\n",
      "total_backward_count 244750 real_backward_count 25893  10.579%\n",
      "lif layer 1 self.abs_max_v: 13298.5\n",
      "lif layer 1 self.abs_max_v: 13351.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.634727/  1.822237, val:  60.42%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.88 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7052%\n",
      "layer   2  Sparsity: 74.3491%\n",
      "layer   3  Sparsity: 61.7318%\n",
      "total_backward_count 254540 real_backward_count 26753  10.510%\n",
      "fc layer 2 self.abs_max_out: 2435.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.606302/  1.798294, val:  61.25%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.36 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6943%\n",
      "layer   2  Sparsity: 73.8199%\n",
      "layer   3  Sparsity: 61.7497%\n",
      "total_backward_count 264330 real_backward_count 27607  10.444%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.597468/  1.798788, val:  59.58%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.95 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7033%\n",
      "layer   2  Sparsity: 74.0073%\n",
      "layer   3  Sparsity: 61.7620%\n",
      "total_backward_count 274120 real_backward_count 28436  10.374%\n",
      "lif layer 1 self.abs_max_v: 13925.0\n",
      "fc layer 1 self.abs_max_out: 7627.0\n",
      "lif layer 1 self.abs_max_v: 14317.5\n",
      "fc layer 2 self.abs_max_out: 2457.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.590429/  1.820317, val:  59.58%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.26 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6947%\n",
      "layer   2  Sparsity: 74.4720%\n",
      "layer   3  Sparsity: 61.7794%\n",
      "total_backward_count 283910 real_backward_count 29246  10.301%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.602055/  1.790207, val:  61.25%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6784%\n",
      "layer   2  Sparsity: 74.7256%\n",
      "layer   3  Sparsity: 61.7125%\n",
      "total_backward_count 293700 real_backward_count 30065  10.237%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.592480/  1.803478, val:  61.67%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.94 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6980%\n",
      "layer   2  Sparsity: 74.3961%\n",
      "layer   3  Sparsity: 60.8148%\n",
      "total_backward_count 303490 real_backward_count 30905  10.183%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.585578/  1.776998, val:  63.75%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7041%\n",
      "layer   2  Sparsity: 74.4801%\n",
      "layer   3  Sparsity: 61.3760%\n",
      "total_backward_count 313280 real_backward_count 31742  10.132%\n",
      "fc layer 3 self.abs_max_out: 909.0\n",
      "fc layer 3 self.abs_max_out: 911.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.568695/  1.771157, val:  60.83%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6988%\n",
      "layer   2  Sparsity: 74.7085%\n",
      "layer   3  Sparsity: 61.1692%\n",
      "total_backward_count 323070 real_backward_count 32525  10.067%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.576814/  1.800316, val:  65.00%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6857%\n",
      "layer   2  Sparsity: 74.3776%\n",
      "layer   3  Sparsity: 61.5328%\n",
      "total_backward_count 332860 real_backward_count 33316  10.009%\n",
      "fc layer 3 self.abs_max_out: 925.0\n",
      "fc layer 2 self.abs_max_out: 2471.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.586375/  1.785704, val:  62.92%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6950%\n",
      "layer   2  Sparsity: 74.3666%\n",
      "layer   3  Sparsity: 61.8196%\n",
      "total_backward_count 342650 real_backward_count 34118   9.957%\n",
      "fc layer 2 self.abs_max_out: 2482.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.601431/  1.813077, val:  62.50%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7063%\n",
      "layer   2  Sparsity: 73.9678%\n",
      "layer   3  Sparsity: 62.0428%\n",
      "total_backward_count 352440 real_backward_count 34891   9.900%\n",
      "fc layer 2 self.abs_max_out: 2544.0\n",
      "lif layer 2 self.abs_max_v: 4579.0\n",
      "lif layer 2 self.abs_max_v: 4712.5\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.603922/  1.796383, val:  61.25%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6872%\n",
      "layer   2  Sparsity: 73.1660%\n",
      "layer   3  Sparsity: 61.3356%\n",
      "total_backward_count 362230 real_backward_count 35680   9.850%\n",
      "fc layer 1 self.abs_max_out: 7765.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.590579/  1.803728, val:  60.83%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7019%\n",
      "layer   2  Sparsity: 72.7964%\n",
      "layer   3  Sparsity: 60.9470%\n",
      "total_backward_count 372020 real_backward_count 36432   9.793%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.583881/  1.797469, val:  59.17%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6802%\n",
      "layer   2  Sparsity: 74.0342%\n",
      "layer   3  Sparsity: 61.0325%\n",
      "total_backward_count 381810 real_backward_count 37223   9.749%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.575950/  1.794781, val:  63.33%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7010%\n",
      "layer   2  Sparsity: 74.3515%\n",
      "layer   3  Sparsity: 61.4276%\n",
      "total_backward_count 391600 real_backward_count 37983   9.699%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.586268/  1.788687, val:  60.00%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6841%\n",
      "layer   2  Sparsity: 74.6148%\n",
      "layer   3  Sparsity: 61.5250%\n",
      "total_backward_count 401390 real_backward_count 38792   9.664%\n",
      "lif layer 1 self.abs_max_v: 14331.0\n",
      "fc layer 2 self.abs_max_out: 2553.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.584937/  1.805906, val:  62.92%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6824%\n",
      "layer   2  Sparsity: 74.4701%\n",
      "layer   3  Sparsity: 61.7504%\n",
      "total_backward_count 411180 real_backward_count 39645   9.642%\n",
      "fc layer 1 self.abs_max_out: 7802.0\n",
      "lif layer 1 self.abs_max_v: 14460.0\n",
      "lif layer 1 self.abs_max_v: 14751.0\n",
      "lif layer 1 self.abs_max_v: 14807.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.588860/  1.792779, val:  58.33%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6989%\n",
      "layer   2  Sparsity: 74.7994%\n",
      "layer   3  Sparsity: 62.1205%\n",
      "total_backward_count 420970 real_backward_count 40405   9.598%\n",
      "lif layer 1 self.abs_max_v: 14842.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.575126/  1.772247, val:  62.92%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6858%\n",
      "layer   2  Sparsity: 75.1206%\n",
      "layer   3  Sparsity: 61.6732%\n",
      "total_backward_count 430760 real_backward_count 41168   9.557%\n",
      "fc layer 1 self.abs_max_out: 7951.0\n",
      "lif layer 1 self.abs_max_v: 14985.5\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.563376/  1.788101, val:  66.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6961%\n",
      "layer   2  Sparsity: 75.7956%\n",
      "layer   3  Sparsity: 61.3818%\n",
      "total_backward_count 440550 real_backward_count 41943   9.521%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.579871/  1.793368, val:  58.75%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6967%\n",
      "layer   2  Sparsity: 76.0682%\n",
      "layer   3  Sparsity: 61.3324%\n",
      "total_backward_count 450340 real_backward_count 42731   9.489%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.586292/  1.789029, val:  59.17%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6816%\n",
      "layer   2  Sparsity: 76.2501%\n",
      "layer   3  Sparsity: 61.0470%\n",
      "total_backward_count 460130 real_backward_count 43514   9.457%\n",
      "fc layer 1 self.abs_max_out: 8118.0\n",
      "lif layer 1 self.abs_max_v: 15283.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.573083/  1.806175, val:  57.92%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7039%\n",
      "layer   2  Sparsity: 76.1551%\n",
      "layer   3  Sparsity: 60.4000%\n",
      "total_backward_count 469920 real_backward_count 44282   9.423%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.581372/  1.790666, val:  60.00%, val_best:  66.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7001%\n",
      "layer   2  Sparsity: 75.7766%\n",
      "layer   3  Sparsity: 60.8452%\n",
      "total_backward_count 479710 real_backward_count 45006   9.382%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.563114/  1.776807, val:  62.08%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6967%\n",
      "layer   2  Sparsity: 76.2000%\n",
      "layer   3  Sparsity: 61.5192%\n",
      "total_backward_count 489500 real_backward_count 45769   9.350%\n",
      "fc layer 1 self.abs_max_out: 8196.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.569779/  1.776225, val:  63.75%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6857%\n",
      "layer   2  Sparsity: 76.1322%\n",
      "layer   3  Sparsity: 61.5685%\n",
      "total_backward_count 499290 real_backward_count 46496   9.312%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.560960/  1.774406, val:  67.50%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6861%\n",
      "layer   2  Sparsity: 75.6290%\n",
      "layer   3  Sparsity: 60.5525%\n",
      "total_backward_count 509080 real_backward_count 47232   9.278%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.560945/  1.765884, val:  66.25%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.09 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7284%\n",
      "layer   2  Sparsity: 75.0977%\n",
      "layer   3  Sparsity: 61.5612%\n",
      "total_backward_count 518870 real_backward_count 47989   9.249%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.544004/  1.760585, val:  61.25%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6962%\n",
      "layer   2  Sparsity: 75.0492%\n",
      "layer   3  Sparsity: 61.2940%\n",
      "total_backward_count 528660 real_backward_count 48729   9.217%\n",
      "fc layer 1 self.abs_max_out: 8214.0\n",
      "fc layer 3 self.abs_max_out: 927.0\n",
      "fc layer 3 self.abs_max_out: 930.0\n",
      "fc layer 3 self.abs_max_out: 974.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.547842/  1.787528, val:  64.17%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6817%\n",
      "layer   2  Sparsity: 74.5877%\n",
      "layer   3  Sparsity: 60.4634%\n",
      "total_backward_count 538450 real_backward_count 49451   9.184%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.554696/  1.763731, val:  62.50%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6981%\n",
      "layer   2  Sparsity: 75.0244%\n",
      "layer   3  Sparsity: 60.8515%\n",
      "total_backward_count 548240 real_backward_count 50218   9.160%\n",
      "lif layer 1 self.abs_max_v: 15579.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.545326/  1.777409, val:  57.08%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7058%\n",
      "layer   2  Sparsity: 74.4650%\n",
      "layer   3  Sparsity: 61.4556%\n",
      "total_backward_count 558030 real_backward_count 50965   9.133%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.538136/  1.752254, val:  62.50%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6892%\n",
      "layer   2  Sparsity: 74.3855%\n",
      "layer   3  Sparsity: 61.5409%\n",
      "total_backward_count 567820 real_backward_count 51641   9.095%\n",
      "fc layer 2 self.abs_max_out: 2555.0\n",
      "fc layer 2 self.abs_max_out: 2559.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.535997/  1.773413, val:  54.58%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.00 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6826%\n",
      "layer   2  Sparsity: 74.1474%\n",
      "layer   3  Sparsity: 61.0659%\n",
      "total_backward_count 577610 real_backward_count 52349   9.063%\n",
      "fc layer 1 self.abs_max_out: 8314.0\n",
      "lif layer 1 self.abs_max_v: 15613.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.545090/  1.769635, val:  62.08%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7024%\n",
      "layer   2  Sparsity: 74.4329%\n",
      "layer   3  Sparsity: 60.6210%\n",
      "total_backward_count 587400 real_backward_count 53039   9.029%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.546194/  1.736737, val:  61.67%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6856%\n",
      "layer   2  Sparsity: 74.3254%\n",
      "layer   3  Sparsity: 60.6891%\n",
      "total_backward_count 597190 real_backward_count 53777   9.005%\n",
      "fc layer 3 self.abs_max_out: 982.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.523207/  1.723800, val:  63.75%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6823%\n",
      "layer   2  Sparsity: 74.5174%\n",
      "layer   3  Sparsity: 61.2177%\n",
      "total_backward_count 606980 real_backward_count 54517   8.982%\n",
      "fc layer 1 self.abs_max_out: 8362.0\n",
      "fc layer 3 self.abs_max_out: 984.0\n",
      "fc layer 3 self.abs_max_out: 996.0\n",
      "fc layer 3 self.abs_max_out: 1009.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.516426/  1.743416, val:  60.00%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6927%\n",
      "layer   2  Sparsity: 74.1695%\n",
      "layer   3  Sparsity: 60.7750%\n",
      "total_backward_count 616770 real_backward_count 55313   8.968%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.519884/  1.732342, val:  58.33%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7206%\n",
      "layer   2  Sparsity: 73.8982%\n",
      "layer   3  Sparsity: 61.5550%\n",
      "total_backward_count 626560 real_backward_count 56012   8.940%\n",
      "fc layer 2 self.abs_max_out: 2601.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.532661/  1.734742, val:  67.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.16 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6999%\n",
      "layer   2  Sparsity: 73.5828%\n",
      "layer   3  Sparsity: 62.2474%\n",
      "total_backward_count 636350 real_backward_count 56687   8.908%\n",
      "fc layer 2 self.abs_max_out: 2741.0\n",
      "fc layer 3 self.abs_max_out: 1013.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.525977/  1.728295, val:  64.17%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6839%\n",
      "layer   2  Sparsity: 73.6956%\n",
      "layer   3  Sparsity: 62.1659%\n",
      "total_backward_count 646140 real_backward_count 57381   8.881%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.521540/  1.733145, val:  70.83%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.90 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6966%\n",
      "layer   2  Sparsity: 74.1031%\n",
      "layer   3  Sparsity: 62.3287%\n",
      "total_backward_count 655930 real_backward_count 58092   8.856%\n",
      "fc layer 1 self.abs_max_out: 8449.0\n",
      "lif layer 1 self.abs_max_v: 16111.0\n",
      "fc layer 1 self.abs_max_out: 8612.0\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.520297/  1.741140, val:  61.67%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.07 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6976%\n",
      "layer   2  Sparsity: 73.9592%\n",
      "layer   3  Sparsity: 61.0369%\n",
      "total_backward_count 665720 real_backward_count 58789   8.831%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.511521/  1.707403, val:  70.83%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6965%\n",
      "layer   2  Sparsity: 73.9533%\n",
      "layer   3  Sparsity: 61.2575%\n",
      "total_backward_count 675510 real_backward_count 59516   8.811%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.506412/  1.742437, val:  63.33%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6778%\n",
      "layer   2  Sparsity: 73.7873%\n",
      "layer   3  Sparsity: 62.0527%\n",
      "total_backward_count 685300 real_backward_count 60198   8.784%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.509604/  1.709012, val:  66.25%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.28 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6736%\n",
      "layer   2  Sparsity: 73.8387%\n",
      "layer   3  Sparsity: 60.7799%\n",
      "total_backward_count 695090 real_backward_count 60933   8.766%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.514131/  1.748276, val:  57.92%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.35 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6762%\n",
      "layer   2  Sparsity: 74.1223%\n",
      "layer   3  Sparsity: 60.5267%\n",
      "total_backward_count 704880 real_backward_count 61626   8.743%\n",
      "fc layer 3 self.abs_max_out: 1015.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.524416/  1.719802, val:  65.42%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6924%\n",
      "layer   2  Sparsity: 73.9026%\n",
      "layer   3  Sparsity: 60.8131%\n",
      "total_backward_count 714670 real_backward_count 62360   8.726%\n",
      "fc layer 1 self.abs_max_out: 8970.0\n",
      "fc layer 3 self.abs_max_out: 1024.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.508898/  1.745166, val:  60.42%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7005%\n",
      "layer   2  Sparsity: 73.8930%\n",
      "layer   3  Sparsity: 60.9116%\n",
      "total_backward_count 724460 real_backward_count 63027   8.700%\n",
      "fc layer 1 self.abs_max_out: 9417.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.526171/  1.746277, val:  65.42%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6719%\n",
      "layer   2  Sparsity: 74.1610%\n",
      "layer   3  Sparsity: 61.3546%\n",
      "total_backward_count 734250 real_backward_count 63794   8.688%\n",
      "lif layer 1 self.abs_max_v: 16473.5\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.526478/  1.732071, val:  63.75%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7112%\n",
      "layer   2  Sparsity: 73.9763%\n",
      "layer   3  Sparsity: 61.8114%\n",
      "total_backward_count 744040 real_backward_count 64504   8.669%\n",
      "lif layer 1 self.abs_max_v: 16593.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.504972/  1.727874, val:  68.33%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6783%\n",
      "layer   2  Sparsity: 73.6498%\n",
      "layer   3  Sparsity: 61.3671%\n",
      "total_backward_count 753830 real_backward_count 65227   8.653%\n",
      "lif layer 1 self.abs_max_v: 16601.5\n",
      "lif layer 1 self.abs_max_v: 16683.0\n",
      "lif layer 1 self.abs_max_v: 16934.5\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.500224/  1.752185, val:  59.17%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7143%\n",
      "layer   2  Sparsity: 73.7881%\n",
      "layer   3  Sparsity: 60.5979%\n",
      "total_backward_count 763620 real_backward_count 65908   8.631%\n",
      "fc layer 1 self.abs_max_out: 9538.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.509069/  1.743149, val:  64.58%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7036%\n",
      "layer   2  Sparsity: 73.8739%\n",
      "layer   3  Sparsity: 59.9773%\n",
      "total_backward_count 773410 real_backward_count 66618   8.614%\n",
      "lif layer 1 self.abs_max_v: 16984.0\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.506091/  1.748852, val:  65.00%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.08 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6894%\n",
      "layer   2  Sparsity: 73.9352%\n",
      "layer   3  Sparsity: 60.4782%\n",
      "total_backward_count 783200 real_backward_count 67296   8.592%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.500620/  1.722886, val:  63.75%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7020%\n",
      "layer   2  Sparsity: 74.0018%\n",
      "layer   3  Sparsity: 59.7996%\n",
      "total_backward_count 792990 real_backward_count 68001   8.575%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.504433/  1.749363, val:  61.25%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.27 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.7003%\n",
      "layer   2  Sparsity: 73.7430%\n",
      "layer   3  Sparsity: 60.4902%\n",
      "total_backward_count 802780 real_backward_count 68695   8.557%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.498739/  1.735527, val:  58.75%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.25 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6712%\n",
      "layer   2  Sparsity: 74.1065%\n",
      "layer   3  Sparsity: 60.1119%\n",
      "total_backward_count 812570 real_backward_count 69396   8.540%\n",
      "fc layer 1 self.abs_max_out: 9549.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.480514/  1.752104, val:  55.00%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6886%\n",
      "layer   2  Sparsity: 74.3094%\n",
      "layer   3  Sparsity: 60.5361%\n",
      "total_backward_count 822360 real_backward_count 70110   8.525%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.501494/  1.728318, val:  65.00%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6982%\n",
      "layer   2  Sparsity: 74.7920%\n",
      "layer   3  Sparsity: 59.3731%\n",
      "total_backward_count 832150 real_backward_count 70813   8.510%\n",
      "fc layer 1 self.abs_max_out: 9551.0\n",
      "fc layer 1 self.abs_max_out: 10082.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.491215/  1.725316, val:  64.17%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6980%\n",
      "layer   2  Sparsity: 74.8200%\n",
      "layer   3  Sparsity: 60.2792%\n",
      "total_backward_count 841940 real_backward_count 71497   8.492%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.499245/  1.753523, val:  59.58%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6990%\n",
      "layer   2  Sparsity: 74.6554%\n",
      "layer   3  Sparsity: 60.6478%\n",
      "total_backward_count 851730 real_backward_count 72164   8.473%\n",
      "fc layer 2 self.abs_max_out: 2791.0\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.495283/  1.740131, val:  64.17%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.34 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6787%\n",
      "layer   2  Sparsity: 74.4310%\n",
      "layer   3  Sparsity: 60.4621%\n",
      "total_backward_count 861520 real_backward_count 72845   8.455%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.509485/  1.744366, val:  60.42%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 80.33 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6874%\n",
      "layer   2  Sparsity: 74.4734%\n",
      "layer   3  Sparsity: 60.3446%\n",
      "total_backward_count 871310 real_backward_count 73534   8.439%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.494860/  1.723077, val:  67.92%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7223%\n",
      "layer   2  Sparsity: 74.4749%\n",
      "layer   3  Sparsity: 61.6861%\n",
      "total_backward_count 881100 real_backward_count 74216   8.423%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.489142/  1.732613, val:  67.50%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.80 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6772%\n",
      "layer   2  Sparsity: 74.4116%\n",
      "layer   3  Sparsity: 60.1196%\n",
      "total_backward_count 890890 real_backward_count 74943   8.412%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.496540/  1.728329, val:  65.00%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.34 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6889%\n",
      "layer   2  Sparsity: 74.2225%\n",
      "layer   3  Sparsity: 59.7982%\n",
      "total_backward_count 900680 real_backward_count 75592   8.393%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.488085/  1.706185, val:  68.75%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7088%\n",
      "layer   2  Sparsity: 75.0381%\n",
      "layer   3  Sparsity: 60.3160%\n",
      "total_backward_count 910470 real_backward_count 76252   8.375%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.488769/  1.728945, val:  65.83%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6979%\n",
      "layer   2  Sparsity: 74.8768%\n",
      "layer   3  Sparsity: 60.7508%\n",
      "total_backward_count 920260 real_backward_count 76925   8.359%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.481376/  1.726732, val:  61.25%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6948%\n",
      "layer   2  Sparsity: 74.1739%\n",
      "layer   3  Sparsity: 59.8506%\n",
      "total_backward_count 930050 real_backward_count 77626   8.346%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.497774/  1.721297, val:  66.67%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.11 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6916%\n",
      "layer   2  Sparsity: 73.9103%\n",
      "layer   3  Sparsity: 59.0710%\n",
      "total_backward_count 939840 real_backward_count 78341   8.336%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.485078/  1.729222, val:  60.83%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6841%\n",
      "layer   2  Sparsity: 73.9934%\n",
      "layer   3  Sparsity: 58.4800%\n",
      "total_backward_count 949630 real_backward_count 79003   8.319%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.475376/  1.699131, val:  68.75%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7065%\n",
      "layer   2  Sparsity: 73.8595%\n",
      "layer   3  Sparsity: 57.5572%\n",
      "total_backward_count 959420 real_backward_count 79659   8.303%\n",
      "fc layer 2 self.abs_max_out: 2795.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.475479/  1.735931, val:  68.75%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6888%\n",
      "layer   2  Sparsity: 73.3513%\n",
      "layer   3  Sparsity: 57.4197%\n",
      "total_backward_count 969210 real_backward_count 80319   8.287%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.467972/  1.717192, val:  65.00%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.25 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6994%\n",
      "layer   2  Sparsity: 73.6128%\n",
      "layer   3  Sparsity: 58.7385%\n",
      "total_backward_count 979000 real_backward_count 81009   8.275%\n",
      "lif layer 1 self.abs_max_v: 17009.5\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.467369/  1.722504, val:  61.25%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.19 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6822%\n",
      "layer   2  Sparsity: 73.5134%\n",
      "layer   3  Sparsity: 58.9461%\n",
      "total_backward_count 988790 real_backward_count 81649   8.257%\n",
      "lif layer 1 self.abs_max_v: 17214.0\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.473433/  1.714803, val:  63.75%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7054%\n",
      "layer   2  Sparsity: 73.1390%\n",
      "layer   3  Sparsity: 59.8554%\n",
      "total_backward_count 998580 real_backward_count 82320   8.244%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.465104/  1.699856, val:  64.58%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6903%\n",
      "layer   2  Sparsity: 73.5219%\n",
      "layer   3  Sparsity: 58.7155%\n",
      "total_backward_count 1008370 real_backward_count 82975   8.229%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.460754/  1.702137, val:  60.42%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6905%\n",
      "layer   2  Sparsity: 73.9863%\n",
      "layer   3  Sparsity: 58.1259%\n",
      "total_backward_count 1018160 real_backward_count 83647   8.216%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.461626/  1.707135, val:  59.17%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6962%\n",
      "layer   2  Sparsity: 74.2538%\n",
      "layer   3  Sparsity: 59.2192%\n",
      "total_backward_count 1027950 real_backward_count 84322   8.203%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.469175/  1.694177, val:  71.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7080%\n",
      "layer   2  Sparsity: 73.9536%\n",
      "layer   3  Sparsity: 58.7704%\n",
      "total_backward_count 1037740 real_backward_count 85005   8.191%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.458395/  1.685119, val:  62.92%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7106%\n",
      "layer   2  Sparsity: 74.0296%\n",
      "layer   3  Sparsity: 59.1811%\n",
      "total_backward_count 1047530 real_backward_count 85631   8.175%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.454995/  1.699140, val:  58.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6873%\n",
      "layer   2  Sparsity: 74.3027%\n",
      "layer   3  Sparsity: 58.8245%\n",
      "total_backward_count 1057320 real_backward_count 86301   8.162%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.449059/  1.680838, val:  68.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6862%\n",
      "layer   2  Sparsity: 74.3524%\n",
      "layer   3  Sparsity: 58.4535%\n",
      "total_backward_count 1067110 real_backward_count 86939   8.147%\n",
      "lif layer 1 self.abs_max_v: 17224.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.440990/  1.665173, val:  66.67%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.00 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6717%\n",
      "layer   2  Sparsity: 74.3344%\n",
      "layer   3  Sparsity: 59.4780%\n",
      "total_backward_count 1076900 real_backward_count 87584   8.133%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.432248/  1.690033, val:  62.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6653%\n",
      "layer   2  Sparsity: 74.1110%\n",
      "layer   3  Sparsity: 59.7993%\n",
      "total_backward_count 1086690 real_backward_count 88224   8.119%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.452610/  1.685415, val:  68.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7004%\n",
      "layer   2  Sparsity: 74.3422%\n",
      "layer   3  Sparsity: 60.4172%\n",
      "total_backward_count 1096480 real_backward_count 88852   8.103%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.447097/  1.665483, val:  65.83%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.56 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6847%\n",
      "layer   2  Sparsity: 73.9174%\n",
      "layer   3  Sparsity: 59.7980%\n",
      "total_backward_count 1106270 real_backward_count 89526   8.093%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.439778/  1.669536, val:  63.33%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7072%\n",
      "layer   2  Sparsity: 73.6479%\n",
      "layer   3  Sparsity: 58.5100%\n",
      "total_backward_count 1116060 real_backward_count 90168   8.079%\n",
      "fc layer 3 self.abs_max_out: 1052.0\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.435822/  1.680103, val:  68.75%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6996%\n",
      "layer   2  Sparsity: 73.3743%\n",
      "layer   3  Sparsity: 58.7671%\n",
      "total_backward_count 1125850 real_backward_count 90821   8.067%\n",
      "lif layer 1 self.abs_max_v: 17355.0\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.441021/  1.681211, val:  66.25%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6777%\n",
      "layer   2  Sparsity: 73.3213%\n",
      "layer   3  Sparsity: 58.9155%\n",
      "total_backward_count 1135640 real_backward_count 91470   8.054%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.444360/  1.676206, val:  67.08%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6985%\n",
      "layer   2  Sparsity: 73.3835%\n",
      "layer   3  Sparsity: 58.6861%\n",
      "total_backward_count 1145430 real_backward_count 92104   8.041%\n",
      "lif layer 2 self.abs_max_v: 4727.0\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.451806/  1.687230, val:  66.67%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7012%\n",
      "layer   2  Sparsity: 73.0794%\n",
      "layer   3  Sparsity: 58.8553%\n",
      "total_backward_count 1155220 real_backward_count 92761   8.030%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.447155/  1.691368, val:  65.00%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6738%\n",
      "layer   2  Sparsity: 73.4155%\n",
      "layer   3  Sparsity: 58.8573%\n",
      "total_backward_count 1165010 real_backward_count 93422   8.019%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.442984/  1.671235, val:  64.17%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7109%\n",
      "layer   2  Sparsity: 74.0048%\n",
      "layer   3  Sparsity: 58.8781%\n",
      "total_backward_count 1174800 real_backward_count 94027   8.004%\n",
      "lif layer 1 self.abs_max_v: 17422.5\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.424764/  1.678662, val:  63.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.13 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6976%\n",
      "layer   2  Sparsity: 73.8048%\n",
      "layer   3  Sparsity: 58.8342%\n",
      "total_backward_count 1184590 real_backward_count 94646   7.990%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.443005/  1.689169, val:  63.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.17 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.7020%\n",
      "layer   2  Sparsity: 73.4954%\n",
      "layer   3  Sparsity: 59.2458%\n",
      "total_backward_count 1194380 real_backward_count 95289   7.978%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.442229/  1.696827, val:  63.75%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6963%\n",
      "layer   2  Sparsity: 73.6965%\n",
      "layer   3  Sparsity: 58.6191%\n",
      "total_backward_count 1204170 real_backward_count 95992   7.972%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.436398/  1.673340, val:  67.50%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7058%\n",
      "layer   2  Sparsity: 73.1538%\n",
      "layer   3  Sparsity: 59.2210%\n",
      "total_backward_count 1213960 real_backward_count 96619   7.959%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.441784/  1.675088, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.24 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6968%\n",
      "layer   2  Sparsity: 73.2953%\n",
      "layer   3  Sparsity: 60.3916%\n",
      "total_backward_count 1223750 real_backward_count 97284   7.950%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.455903/  1.698256, val:  60.00%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6973%\n",
      "layer   2  Sparsity: 73.7719%\n",
      "layer   3  Sparsity: 60.6676%\n",
      "total_backward_count 1233540 real_backward_count 97934   7.939%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.459348/  1.712965, val:  67.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7046%\n",
      "layer   2  Sparsity: 73.9416%\n",
      "layer   3  Sparsity: 61.7260%\n",
      "total_backward_count 1243330 real_backward_count 98562   7.927%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.466250/  1.686095, val:  65.42%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.18 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6927%\n",
      "layer   2  Sparsity: 73.9061%\n",
      "layer   3  Sparsity: 61.1273%\n",
      "total_backward_count 1253120 real_backward_count 99199   7.916%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.459752/  1.700268, val:  65.83%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6867%\n",
      "layer   2  Sparsity: 73.7933%\n",
      "layer   3  Sparsity: 61.4608%\n",
      "total_backward_count 1262910 real_backward_count 99866   7.908%\n",
      "lif layer 2 self.abs_max_v: 4771.5\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.452565/  1.668053, val:  61.67%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6993%\n",
      "layer   2  Sparsity: 73.3462%\n",
      "layer   3  Sparsity: 60.6643%\n",
      "total_backward_count 1272700 real_backward_count 100483   7.895%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.434731/  1.671747, val:  70.42%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6812%\n",
      "layer   2  Sparsity: 72.9365%\n",
      "layer   3  Sparsity: 59.6095%\n",
      "total_backward_count 1282490 real_backward_count 101084   7.882%\n",
      "lif layer 2 self.abs_max_v: 4906.5\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.455562/  1.703876, val:  61.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6902%\n",
      "layer   2  Sparsity: 73.4353%\n",
      "layer   3  Sparsity: 60.1047%\n",
      "total_backward_count 1292280 real_backward_count 101743   7.873%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.456050/  1.687621, val:  68.33%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6860%\n",
      "layer   2  Sparsity: 73.8349%\n",
      "layer   3  Sparsity: 59.6444%\n",
      "total_backward_count 1302070 real_backward_count 102339   7.860%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.464978/  1.695645, val:  63.75%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6962%\n",
      "layer   2  Sparsity: 73.4440%\n",
      "layer   3  Sparsity: 60.0249%\n",
      "total_backward_count 1311860 real_backward_count 103011   7.852%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.463318/  1.685311, val:  70.42%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7150%\n",
      "layer   2  Sparsity: 73.5050%\n",
      "layer   3  Sparsity: 59.6246%\n",
      "total_backward_count 1321650 real_backward_count 103666   7.844%\n",
      "fc layer 2 self.abs_max_out: 2885.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.459239/  1.668455, val:  64.58%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7105%\n",
      "layer   2  Sparsity: 73.8018%\n",
      "layer   3  Sparsity: 58.2099%\n",
      "total_backward_count 1331440 real_backward_count 104342   7.837%\n",
      "fc layer 2 self.abs_max_out: 2895.0\n",
      "fc layer 2 self.abs_max_out: 2918.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.445410/  1.676837, val:  70.00%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6859%\n",
      "layer   2  Sparsity: 73.7708%\n",
      "layer   3  Sparsity: 57.9065%\n",
      "total_backward_count 1341230 real_backward_count 104988   7.828%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.434152/  1.659121, val:  65.42%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7089%\n",
      "layer   2  Sparsity: 73.8088%\n",
      "layer   3  Sparsity: 58.7280%\n",
      "total_backward_count 1351020 real_backward_count 105625   7.818%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.429030/  1.690147, val:  66.67%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.22 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.7051%\n",
      "layer   2  Sparsity: 74.2866%\n",
      "layer   3  Sparsity: 59.6255%\n",
      "total_backward_count 1360810 real_backward_count 106309   7.812%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.437538/  1.683105, val:  66.25%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6725%\n",
      "layer   2  Sparsity: 74.7714%\n",
      "layer   3  Sparsity: 60.1673%\n",
      "total_backward_count 1370600 real_backward_count 106992   7.806%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.441850/  1.678390, val:  57.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.97 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6929%\n",
      "layer   2  Sparsity: 75.3353%\n",
      "layer   3  Sparsity: 61.0176%\n",
      "total_backward_count 1380390 real_backward_count 107650   7.799%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.443394/  1.693135, val:  64.17%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6815%\n",
      "layer   2  Sparsity: 74.9428%\n",
      "layer   3  Sparsity: 60.8114%\n",
      "total_backward_count 1390180 real_backward_count 108256   7.787%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.438398/  1.672243, val:  65.83%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.21 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6865%\n",
      "layer   2  Sparsity: 75.2002%\n",
      "layer   3  Sparsity: 59.9315%\n",
      "total_backward_count 1399970 real_backward_count 108907   7.779%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.432061/  1.672314, val:  68.75%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.26 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6893%\n",
      "layer   2  Sparsity: 75.2737%\n",
      "layer   3  Sparsity: 59.8688%\n",
      "total_backward_count 1409760 real_backward_count 109549   7.771%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.439665/  1.684981, val:  63.33%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6961%\n",
      "layer   2  Sparsity: 75.1349%\n",
      "layer   3  Sparsity: 59.8836%\n",
      "total_backward_count 1419550 real_backward_count 110229   7.765%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.426162/  1.669099, val:  62.08%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.75 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 83.6688%\n",
      "layer   2  Sparsity: 74.7388%\n",
      "layer   3  Sparsity: 59.6049%\n",
      "total_backward_count 1429340 real_backward_count 110879   7.757%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.418698/  1.651715, val:  70.42%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.31 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6842%\n",
      "layer   2  Sparsity: 74.9537%\n",
      "layer   3  Sparsity: 59.8369%\n",
      "total_backward_count 1439130 real_backward_count 111540   7.751%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.413028/  1.657820, val:  71.25%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6909%\n",
      "layer   2  Sparsity: 75.0173%\n",
      "layer   3  Sparsity: 60.2096%\n",
      "total_backward_count 1448920 real_backward_count 112145   7.740%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.421779/  1.649261, val:  68.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.28 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6937%\n",
      "layer   2  Sparsity: 75.1298%\n",
      "layer   3  Sparsity: 60.9530%\n",
      "total_backward_count 1458710 real_backward_count 112814   7.734%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.414043/  1.692673, val:  62.92%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7030%\n",
      "layer   2  Sparsity: 75.5516%\n",
      "layer   3  Sparsity: 61.2400%\n",
      "total_backward_count 1468500 real_backward_count 113452   7.726%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.441617/  1.673212, val:  67.92%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6882%\n",
      "layer   2  Sparsity: 75.6971%\n",
      "layer   3  Sparsity: 61.4145%\n",
      "total_backward_count 1478290 real_backward_count 114057   7.715%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.445630/  1.686779, val:  70.00%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6954%\n",
      "layer   2  Sparsity: 75.4345%\n",
      "layer   3  Sparsity: 61.6705%\n",
      "total_backward_count 1488080 real_backward_count 114712   7.709%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.448608/  1.680213, val:  59.58%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7022%\n",
      "layer   2  Sparsity: 75.4036%\n",
      "layer   3  Sparsity: 61.3438%\n",
      "total_backward_count 1497870 real_backward_count 115298   7.697%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.439097/  1.684557, val:  69.58%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 80.39 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.7067%\n",
      "layer   2  Sparsity: 75.3100%\n",
      "layer   3  Sparsity: 60.8737%\n",
      "total_backward_count 1507660 real_backward_count 115954   7.691%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.441325/  1.681920, val:  66.67%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6981%\n",
      "layer   2  Sparsity: 75.2370%\n",
      "layer   3  Sparsity: 60.5699%\n",
      "total_backward_count 1517450 real_backward_count 116606   7.684%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.440841/  1.672012, val:  67.92%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6864%\n",
      "layer   2  Sparsity: 75.3336%\n",
      "layer   3  Sparsity: 61.0226%\n",
      "total_backward_count 1527240 real_backward_count 117259   7.678%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.439359/  1.683159, val:  65.42%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6917%\n",
      "layer   2  Sparsity: 75.3599%\n",
      "layer   3  Sparsity: 61.3708%\n",
      "total_backward_count 1537030 real_backward_count 117849   7.667%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.432005/  1.672707, val:  71.67%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6938%\n",
      "layer   2  Sparsity: 75.6488%\n",
      "layer   3  Sparsity: 61.0603%\n",
      "total_backward_count 1546820 real_backward_count 118454   7.658%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.442209/  1.667591, val:  65.83%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7031%\n",
      "layer   2  Sparsity: 75.6821%\n",
      "layer   3  Sparsity: 60.6432%\n",
      "total_backward_count 1556610 real_backward_count 119080   7.650%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.442389/  1.689907, val:  64.58%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7034%\n",
      "layer   2  Sparsity: 75.3363%\n",
      "layer   3  Sparsity: 60.7872%\n",
      "total_backward_count 1566400 real_backward_count 119673   7.640%\n",
      "lif layer 2 self.abs_max_v: 4944.5\n",
      "lif layer 2 self.abs_max_v: 5114.5\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.448968/  1.687060, val:  63.75%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6960%\n",
      "layer   2  Sparsity: 74.7894%\n",
      "layer   3  Sparsity: 60.5476%\n",
      "total_backward_count 1576190 real_backward_count 120319   7.634%\n",
      "lif layer 2 self.abs_max_v: 5163.0\n",
      "lif layer 2 self.abs_max_v: 5251.0\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.452135/  1.664272, val:  70.00%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6950%\n",
      "layer   2  Sparsity: 74.9414%\n",
      "layer   3  Sparsity: 59.8678%\n",
      "total_backward_count 1585980 real_backward_count 120959   7.627%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.429226/  1.670237, val:  66.25%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6892%\n",
      "layer   2  Sparsity: 75.2436%\n",
      "layer   3  Sparsity: 60.3207%\n",
      "total_backward_count 1595770 real_backward_count 121598   7.620%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.439284/  1.673481, val:  65.42%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7038%\n",
      "layer   2  Sparsity: 75.2642%\n",
      "layer   3  Sparsity: 61.1565%\n",
      "total_backward_count 1605560 real_backward_count 122210   7.612%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.426804/  1.701169, val:  61.25%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.07 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7190%\n",
      "layer   2  Sparsity: 75.1689%\n",
      "layer   3  Sparsity: 61.1389%\n",
      "total_backward_count 1615350 real_backward_count 122815   7.603%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.437421/  1.693255, val:  65.00%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6800%\n",
      "layer   2  Sparsity: 75.3111%\n",
      "layer   3  Sparsity: 60.6758%\n",
      "total_backward_count 1625140 real_backward_count 123398   7.593%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.425129/  1.682609, val:  59.17%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.00 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6905%\n",
      "layer   2  Sparsity: 75.4717%\n",
      "layer   3  Sparsity: 59.5502%\n",
      "total_backward_count 1634930 real_backward_count 123984   7.583%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.432983/  1.664293, val:  60.00%, val_best:  71.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 80.16 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6864%\n",
      "layer   2  Sparsity: 75.3682%\n",
      "layer   3  Sparsity: 59.8871%\n",
      "total_backward_count 1644720 real_backward_count 124616   7.577%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.420465/  1.648134, val:  73.33%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7138%\n",
      "layer   2  Sparsity: 75.4743%\n",
      "layer   3  Sparsity: 60.2630%\n",
      "total_backward_count 1654510 real_backward_count 125286   7.572%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.430246/  1.681819, val:  60.42%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7005%\n",
      "layer   2  Sparsity: 75.3262%\n",
      "layer   3  Sparsity: 59.4695%\n",
      "total_backward_count 1664300 real_backward_count 125892   7.564%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.412530/  1.656380, val:  63.75%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.09 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7164%\n",
      "layer   2  Sparsity: 75.8749%\n",
      "layer   3  Sparsity: 59.4103%\n",
      "total_backward_count 1674090 real_backward_count 126515   7.557%\n",
      "lif layer 2 self.abs_max_v: 5275.0\n",
      "lif layer 2 self.abs_max_v: 5310.5\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.424437/  1.680446, val:  55.83%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6747%\n",
      "layer   2  Sparsity: 75.5536%\n",
      "layer   3  Sparsity: 60.3322%\n",
      "total_backward_count 1683880 real_backward_count 127154   7.551%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.428826/  1.664825, val:  63.33%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.95 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6723%\n",
      "layer   2  Sparsity: 75.5707%\n",
      "layer   3  Sparsity: 61.0111%\n",
      "total_backward_count 1693670 real_backward_count 127755   7.543%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.428441/  1.684205, val:  60.00%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6887%\n",
      "layer   2  Sparsity: 75.8715%\n",
      "layer   3  Sparsity: 59.8794%\n",
      "total_backward_count 1703460 real_backward_count 128358   7.535%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.439417/  1.675105, val:  67.08%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6981%\n",
      "layer   2  Sparsity: 75.3888%\n",
      "layer   3  Sparsity: 60.3082%\n",
      "total_backward_count 1713250 real_backward_count 128999   7.529%\n",
      "lif layer 2 self.abs_max_v: 5365.0\n",
      "fc layer 2 self.abs_max_out: 2943.0\n",
      "lif layer 2 self.abs_max_v: 5422.0\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.433749/  1.653938, val:  70.00%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.33 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.7205%\n",
      "layer   2  Sparsity: 75.2517%\n",
      "layer   3  Sparsity: 60.9586%\n",
      "total_backward_count 1723040 real_backward_count 129611   7.522%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.419236/  1.648533, val:  69.58%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6972%\n",
      "layer   2  Sparsity: 75.2867%\n",
      "layer   3  Sparsity: 60.7596%\n",
      "total_backward_count 1732830 real_backward_count 130212   7.514%\n",
      "lif layer 2 self.abs_max_v: 5447.0\n",
      "fc layer 1 self.abs_max_out: 10101.0\n",
      "lif layer 2 self.abs_max_v: 5547.0\n",
      "fc layer 2 self.abs_max_out: 2953.0\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.414835/  1.664887, val:  69.58%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6692%\n",
      "layer   2  Sparsity: 75.0072%\n",
      "layer   3  Sparsity: 60.0940%\n",
      "total_backward_count 1742620 real_backward_count 130808   7.506%\n",
      "fc layer 2 self.abs_max_out: 2996.0\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.419942/  1.653855, val:  70.83%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6834%\n",
      "layer   2  Sparsity: 75.0256%\n",
      "layer   3  Sparsity: 60.0532%\n",
      "total_backward_count 1752410 real_backward_count 131389   7.498%\n",
      "lif layer 1 self.abs_max_v: 17534.5\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.408592/  1.657565, val:  61.67%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7211%\n",
      "layer   2  Sparsity: 75.1776%\n",
      "layer   3  Sparsity: 60.2457%\n",
      "total_backward_count 1762200 real_backward_count 132001   7.491%\n",
      "fc layer 1 self.abs_max_out: 10403.0\n",
      "lif layer 1 self.abs_max_v: 17850.5\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.406441/  1.633760, val:  68.75%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6946%\n",
      "layer   2  Sparsity: 75.3376%\n",
      "layer   3  Sparsity: 58.9661%\n",
      "total_backward_count 1771990 real_backward_count 132581   7.482%\n",
      "lif layer 1 self.abs_max_v: 17944.5\n",
      "fc layer 1 self.abs_max_out: 11425.0\n",
      "lif layer 1 self.abs_max_v: 18682.5\n",
      "lif layer 1 self.abs_max_v: 18753.5\n",
      "lif layer 1 self.abs_max_v: 18888.0\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.398780/  1.654769, val:  68.33%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.02 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6829%\n",
      "layer   2  Sparsity: 75.1707%\n",
      "layer   3  Sparsity: 59.9958%\n",
      "total_backward_count 1781780 real_backward_count 133129   7.472%\n",
      "lif layer 1 self.abs_max_v: 19514.0\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.406229/  1.642511, val:  65.42%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6957%\n",
      "layer   2  Sparsity: 75.1821%\n",
      "layer   3  Sparsity: 60.0206%\n",
      "total_backward_count 1791570 real_backward_count 133739   7.465%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.408775/  1.671772, val:  55.83%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6814%\n",
      "layer   2  Sparsity: 75.5861%\n",
      "layer   3  Sparsity: 58.9395%\n",
      "total_backward_count 1801360 real_backward_count 134283   7.455%\n",
      "fc layer 2 self.abs_max_out: 3002.0\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.411906/  1.659366, val:  65.42%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6974%\n",
      "layer   2  Sparsity: 75.7333%\n",
      "layer   3  Sparsity: 59.3183%\n",
      "total_backward_count 1811150 real_backward_count 134842   7.445%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.418226/  1.651725, val:  68.33%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6935%\n",
      "layer   2  Sparsity: 75.6617%\n",
      "layer   3  Sparsity: 60.1466%\n",
      "total_backward_count 1820940 real_backward_count 135465   7.439%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.402115/  1.634147, val:  68.33%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6811%\n",
      "layer   2  Sparsity: 75.7048%\n",
      "layer   3  Sparsity: 59.7951%\n",
      "total_backward_count 1830730 real_backward_count 136077   7.433%\n",
      "lif layer 2 self.abs_max_v: 5568.5\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.394718/  1.664485, val:  73.33%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.36 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 83.6700%\n",
      "layer   2  Sparsity: 75.5583%\n",
      "layer   3  Sparsity: 59.3806%\n",
      "total_backward_count 1840520 real_backward_count 136687   7.427%\n",
      "lif layer 2 self.abs_max_v: 5620.5\n",
      "fc layer 2 self.abs_max_out: 3056.0\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.428328/  1.663863, val:  67.50%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6922%\n",
      "layer   2  Sparsity: 75.5080%\n",
      "layer   3  Sparsity: 59.4388%\n",
      "total_backward_count 1850310 real_backward_count 137298   7.420%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.421963/  1.663081, val:  66.67%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 83.6915%\n",
      "layer   2  Sparsity: 75.8512%\n",
      "layer   3  Sparsity: 59.7472%\n",
      "total_backward_count 1860100 real_backward_count 137863   7.412%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.395844/  1.636415, val:  72.50%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 83.6827%\n",
      "layer   2  Sparsity: 75.7303%\n",
      "layer   3  Sparsity: 59.8712%\n",
      "total_backward_count 1869890 real_backward_count 138409   7.402%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.402404/  1.665279, val:  55.00%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6924%\n",
      "layer   2  Sparsity: 75.6171%\n",
      "layer   3  Sparsity: 60.0401%\n",
      "total_backward_count 1879680 real_backward_count 139035   7.397%\n",
      "fc layer 2 self.abs_max_out: 3068.0\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.389013/  1.637969, val:  68.33%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.6611%\n",
      "layer   2  Sparsity: 75.4752%\n",
      "layer   3  Sparsity: 59.8435%\n",
      "total_backward_count 1889470 real_backward_count 139631   7.390%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.396774/  1.651494, val:  64.17%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.7012%\n",
      "layer   2  Sparsity: 75.0147%\n",
      "layer   3  Sparsity: 59.6158%\n",
      "total_backward_count 1899260 real_backward_count 140190   7.381%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.403255/  1.647443, val:  59.58%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6867%\n",
      "layer   2  Sparsity: 75.0075%\n",
      "layer   3  Sparsity: 58.6939%\n",
      "total_backward_count 1909050 real_backward_count 140769   7.374%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.389838/  1.641663, val:  62.50%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6878%\n",
      "layer   2  Sparsity: 75.3662%\n",
      "layer   3  Sparsity: 59.9177%\n",
      "total_backward_count 1918840 real_backward_count 141311   7.364%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.398814/  1.635356, val:  72.92%, val_best:  73.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7100%\n",
      "layer   2  Sparsity: 75.0308%\n",
      "layer   3  Sparsity: 60.1065%\n",
      "total_backward_count 1928630 real_backward_count 141912   7.358%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.396319/  1.642323, val:  62.92%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6879%\n",
      "layer   2  Sparsity: 75.2209%\n",
      "layer   3  Sparsity: 59.4886%\n",
      "total_backward_count 1938420 real_backward_count 142510   7.352%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.392881/  1.651195, val:  65.42%, val_best:  73.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 83.7071%\n",
      "layer   2  Sparsity: 75.8909%\n",
      "layer   3  Sparsity: 58.6189%\n",
      "total_backward_count 1948210 real_backward_count 143087   7.345%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.409862/  1.668723, val:  61.25%, val_best:  73.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 83.6786%\n",
      "layer   2  Sparsity: 75.6400%\n",
      "layer   3  Sparsity: 58.5303%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf4bad6bad441608b07393a2ab14602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñà‚ñÜ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñà‚ñÜ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99796</td></tr><tr><td>tr_epoch_loss</td><td>1.40986</td></tr><tr><td>val_acc_best</td><td>0.73333</td></tr><tr><td>val_acc_now</td><td>0.6125</td></tr><tr><td>val_loss</td><td>1.66872</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rare-sweep-82</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3udg0bjz' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3udg0bjz</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251115_235036-3udg0bjz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k6dafwt8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_041701-k6dafwt8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k6dafwt8' target=\"_blank\">wild-sweep-86</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k6dafwt8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k6dafwt8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251116_041710_275', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 4, 'leaky_temporal_filter': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 458.0\n",
      "lif layer 1 self.abs_max_v: 458.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 531.0\n",
      "lif layer 1 self.abs_max_v: 710.0\n",
      "fc layer 2 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "lif layer 1 self.abs_max_v: 815.0\n",
      "fc layer 2 self.abs_max_out: 305.0\n",
      "lif layer 2 self.abs_max_v: 300.0\n",
      "fc layer 1 self.abs_max_out: 573.0\n",
      "fc layer 1 self.abs_max_out: 597.0\n",
      "lif layer 2 self.abs_max_v: 366.0\n",
      "fc layer 1 self.abs_max_out: 647.0\n",
      "lif layer 1 self.abs_max_v: 823.5\n",
      "fc layer 2 self.abs_max_out: 316.0\n",
      "lif layer 2 self.abs_max_v: 435.0\n",
      "fc layer 1 self.abs_max_out: 788.0\n",
      "fc layer 2 self.abs_max_out: 331.0\n",
      "lif layer 2 self.abs_max_v: 466.5\n",
      "fc layer 1 self.abs_max_out: 793.0\n",
      "fc layer 2 self.abs_max_out: 335.0\n",
      "lif layer 1 self.abs_max_v: 834.5\n",
      "fc layer 2 self.abs_max_out: 390.0\n",
      "lif layer 2 self.abs_max_v: 541.0\n",
      "fc layer 1 self.abs_max_out: 1016.0\n",
      "lif layer 1 self.abs_max_v: 1016.0\n",
      "fc layer 2 self.abs_max_out: 483.0\n",
      "lif layer 2 self.abs_max_v: 621.5\n",
      "fc layer 3 self.abs_max_out: 36.0\n",
      "lif layer 1 self.abs_max_v: 1121.0\n",
      "fc layer 1 self.abs_max_out: 1225.0\n",
      "lif layer 1 self.abs_max_v: 1227.5\n",
      "fc layer 2 self.abs_max_out: 510.0\n",
      "fc layer 3 self.abs_max_out: 50.0\n",
      "lif layer 2 self.abs_max_v: 684.0\n",
      "fc layer 3 self.abs_max_out: 107.0\n",
      "fc layer 1 self.abs_max_out: 1390.0\n",
      "lif layer 1 self.abs_max_v: 1390.0\n",
      "fc layer 2 self.abs_max_out: 604.0\n",
      "lif layer 2 self.abs_max_v: 782.5\n",
      "fc layer 3 self.abs_max_out: 112.0\n",
      "lif layer 2 self.abs_max_v: 864.0\n",
      "fc layer 1 self.abs_max_out: 1844.0\n",
      "lif layer 1 self.abs_max_v: 1844.0\n",
      "fc layer 3 self.abs_max_out: 124.0\n",
      "fc layer 3 self.abs_max_out: 137.0\n",
      "fc layer 2 self.abs_max_out: 684.0\n",
      "lif layer 2 self.abs_max_v: 911.0\n",
      "fc layer 1 self.abs_max_out: 2004.0\n",
      "lif layer 1 self.abs_max_v: 2004.0\n",
      "fc layer 2 self.abs_max_out: 696.0\n",
      "lif layer 2 self.abs_max_v: 990.5\n",
      "lif layer 2 self.abs_max_v: 1000.5\n",
      "fc layer 2 self.abs_max_out: 706.0\n",
      "lif layer 2 self.abs_max_v: 1111.0\n",
      "fc layer 3 self.abs_max_out: 146.0\n",
      "fc layer 2 self.abs_max_out: 828.0\n",
      "lif layer 2 self.abs_max_v: 1320.0\n",
      "fc layer 3 self.abs_max_out: 151.0\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "lif layer 2 self.abs_max_v: 1357.0\n",
      "fc layer 1 self.abs_max_out: 2008.0\n",
      "lif layer 1 self.abs_max_v: 2008.0\n",
      "lif layer 2 self.abs_max_v: 1388.5\n",
      "fc layer 3 self.abs_max_out: 228.0\n",
      "fc layer 2 self.abs_max_out: 873.0\n",
      "fc layer 2 self.abs_max_out: 928.0\n",
      "fc layer 2 self.abs_max_out: 1107.0\n",
      "fc layer 1 self.abs_max_out: 2025.0\n",
      "lif layer 1 self.abs_max_v: 2025.0\n",
      "fc layer 1 self.abs_max_out: 2122.0\n",
      "lif layer 1 self.abs_max_v: 2122.0\n",
      "lif layer 2 self.abs_max_v: 1429.5\n",
      "fc layer 1 self.abs_max_out: 2196.0\n",
      "lif layer 1 self.abs_max_v: 2196.0\n",
      "fc layer 1 self.abs_max_out: 2322.0\n",
      "lif layer 1 self.abs_max_v: 2322.0\n",
      "fc layer 1 self.abs_max_out: 2659.0\n",
      "lif layer 1 self.abs_max_v: 2659.0\n",
      "fc layer 2 self.abs_max_out: 1170.0\n",
      "fc layer 2 self.abs_max_out: 1202.0\n",
      "fc layer 2 self.abs_max_out: 1232.0\n",
      "fc layer 2 self.abs_max_out: 1257.0\n",
      "fc layer 3 self.abs_max_out: 244.0\n",
      "fc layer 2 self.abs_max_out: 1286.0\n",
      "fc layer 3 self.abs_max_out: 258.0\n",
      "lif layer 2 self.abs_max_v: 1430.5\n",
      "lif layer 2 self.abs_max_v: 1684.5\n",
      "lif layer 2 self.abs_max_v: 1765.5\n",
      "fc layer 3 self.abs_max_out: 264.0\n",
      "lif layer 2 self.abs_max_v: 1776.0\n",
      "fc layer 1 self.abs_max_out: 2955.0\n",
      "lif layer 1 self.abs_max_v: 2955.0\n",
      "fc layer 1 self.abs_max_out: 3032.0\n",
      "lif layer 1 self.abs_max_v: 3032.0\n",
      "fc layer 3 self.abs_max_out: 278.0\n",
      "fc layer 3 self.abs_max_out: 286.0\n",
      "lif layer 1 self.abs_max_v: 3107.5\n",
      "fc layer 2 self.abs_max_out: 1297.0\n",
      "fc layer 3 self.abs_max_out: 305.0\n",
      "fc layer 3 self.abs_max_out: 313.0\n",
      "fc layer 2 self.abs_max_out: 1313.0\n",
      "fc layer 2 self.abs_max_out: 1318.0\n",
      "fc layer 3 self.abs_max_out: 314.0\n",
      "fc layer 2 self.abs_max_out: 1335.0\n",
      "fc layer 2 self.abs_max_out: 1391.0\n",
      "fc layer 2 self.abs_max_out: 1393.0\n",
      "fc layer 3 self.abs_max_out: 348.0\n",
      "fc layer 3 self.abs_max_out: 354.0\n",
      "fc layer 2 self.abs_max_out: 1407.0\n",
      "lif layer 1 self.abs_max_v: 3256.5\n",
      "lif layer 1 self.abs_max_v: 3307.5\n",
      "fc layer 2 self.abs_max_out: 1420.0\n",
      "fc layer 2 self.abs_max_out: 1430.0\n",
      "fc layer 2 self.abs_max_out: 1431.0\n",
      "lif layer 2 self.abs_max_v: 1850.0\n",
      "fc layer 2 self.abs_max_out: 1441.0\n",
      "fc layer 2 self.abs_max_out: 1479.0\n",
      "lif layer 1 self.abs_max_v: 3373.0\n",
      "lif layer 2 self.abs_max_v: 1906.5\n",
      "lif layer 2 self.abs_max_v: 1918.5\n",
      "lif layer 2 self.abs_max_v: 1995.0\n",
      "lif layer 2 self.abs_max_v: 2082.5\n",
      "fc layer 3 self.abs_max_out: 357.0\n",
      "fc layer 2 self.abs_max_out: 1540.0\n",
      "lif layer 2 self.abs_max_v: 2288.5\n",
      "fc layer 3 self.abs_max_out: 380.0\n",
      "fc layer 2 self.abs_max_out: 1562.0\n",
      "lif layer 1 self.abs_max_v: 3540.0\n",
      "lif layer 1 self.abs_max_v: 3578.0\n",
      "lif layer 1 self.abs_max_v: 3896.5\n",
      "fc layer 2 self.abs_max_out: 1618.0\n",
      "fc layer 3 self.abs_max_out: 393.0\n",
      "fc layer 2 self.abs_max_out: 1654.0\n",
      "fc layer 2 self.abs_max_out: 1661.0\n",
      "fc layer 1 self.abs_max_out: 3189.0\n",
      "fc layer 2 self.abs_max_out: 1690.0\n",
      "lif layer 1 self.abs_max_v: 4057.0\n",
      "lif layer 1 self.abs_max_v: 4060.0\n",
      "lif layer 1 self.abs_max_v: 4195.0\n",
      "fc layer 1 self.abs_max_out: 3249.0\n",
      "fc layer 1 self.abs_max_out: 3325.0\n",
      "lif layer 1 self.abs_max_v: 4331.5\n",
      "lif layer 1 self.abs_max_v: 4362.0\n",
      "fc layer 3 self.abs_max_out: 441.0\n",
      "fc layer 1 self.abs_max_out: 3527.0\n",
      "fc layer 2 self.abs_max_out: 1698.0\n",
      "fc layer 1 self.abs_max_out: 3644.0\n",
      "fc layer 1 self.abs_max_out: 3675.0\n",
      "fc layer 2 self.abs_max_out: 1737.0\n",
      "lif layer 1 self.abs_max_v: 4799.5\n",
      "lif layer 1 self.abs_max_v: 4804.0\n",
      "lif layer 1 self.abs_max_v: 5135.5\n",
      "fc layer 1 self.abs_max_out: 3680.0\n",
      "fc layer 1 self.abs_max_out: 4033.0\n",
      "fc layer 2 self.abs_max_out: 1745.0\n",
      "fc layer 3 self.abs_max_out: 464.0\n",
      "lif layer 1 self.abs_max_v: 5149.5\n",
      "lif layer 1 self.abs_max_v: 5372.5\n",
      "lif layer 1 self.abs_max_v: 5607.0\n",
      "lif layer 1 self.abs_max_v: 5777.5\n",
      "fc layer 3 self.abs_max_out: 483.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.074763/  2.096252, val:  51.67%, val_best:  51.67%, tr:  80.29%, tr_best:  80.29%, epoch time: 80.41 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.8213%\n",
      "layer   2  Sparsity: 85.4222%\n",
      "layer   3  Sparsity: 88.1795%\n",
      "total_backward_count 9790 real_backward_count 3829  39.111%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1750.0\n",
      "fc layer 2 self.abs_max_out: 1839.0\n",
      "fc layer 1 self.abs_max_out: 4036.0\n",
      "fc layer 1 self.abs_max_out: 4068.0\n",
      "fc layer 2 self.abs_max_out: 1840.0\n",
      "fc layer 2 self.abs_max_out: 1849.0\n",
      "fc layer 1 self.abs_max_out: 4091.0\n",
      "fc layer 1 self.abs_max_out: 4218.0\n",
      "fc layer 1 self.abs_max_out: 4310.0\n",
      "fc layer 2 self.abs_max_out: 1860.0\n",
      "fc layer 2 self.abs_max_out: 1864.0\n",
      "fc layer 2 self.abs_max_out: 1868.0\n",
      "fc layer 2 self.abs_max_out: 1888.0\n",
      "lif layer 1 self.abs_max_v: 5897.5\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.009513/  2.103604, val:  46.25%, val_best:  51.67%, tr:  96.32%, tr_best:  96.32%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7897%\n",
      "layer   2  Sparsity: 84.7672%\n",
      "layer   3  Sparsity: 84.2870%\n",
      "total_backward_count 19580 real_backward_count 6040  30.848%\n",
      "fc layer 1 self.abs_max_out: 4443.0\n",
      "fc layer 2 self.abs_max_out: 1917.0\n",
      "fc layer 1 self.abs_max_out: 4599.0\n",
      "fc layer 2 self.abs_max_out: 1989.0\n",
      "fc layer 2 self.abs_max_out: 2038.0\n",
      "lif layer 1 self.abs_max_v: 5929.0\n",
      "lif layer 1 self.abs_max_v: 6092.5\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  2.015885/  2.106674, val:  52.50%, val_best:  52.50%, tr:  98.16%, tr_best:  98.16%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8099%\n",
      "layer   2  Sparsity: 84.7820%\n",
      "layer   3  Sparsity: 83.3774%\n",
      "total_backward_count 29370 real_backward_count 7921  26.970%\n",
      "fc layer 1 self.abs_max_out: 4605.0\n",
      "fc layer 2 self.abs_max_out: 2042.0\n",
      "fc layer 1 self.abs_max_out: 4629.0\n",
      "fc layer 2 self.abs_max_out: 2124.0\n",
      "fc layer 2 self.abs_max_out: 2191.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  2.021379/  2.105241, val:  59.17%, val_best:  59.17%, tr:  98.98%, tr_best:  98.98%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8031%\n",
      "layer   2  Sparsity: 85.2325%\n",
      "layer   3  Sparsity: 83.3434%\n",
      "total_backward_count 39160 real_backward_count 9605  24.528%\n",
      "fc layer 1 self.abs_max_out: 4726.0\n",
      "fc layer 1 self.abs_max_out: 4737.0\n",
      "fc layer 1 self.abs_max_out: 4751.0\n",
      "fc layer 1 self.abs_max_out: 4790.0\n",
      "fc layer 1 self.abs_max_out: 4791.0\n",
      "fc layer 1 self.abs_max_out: 4909.0\n",
      "fc layer 2 self.abs_max_out: 2230.0\n",
      "fc layer 2 self.abs_max_out: 2259.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  2.022355/  2.113388, val:  45.83%, val_best:  59.17%, tr:  99.39%, tr_best:  99.39%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8097%\n",
      "layer   2  Sparsity: 84.6529%\n",
      "layer   3  Sparsity: 82.4285%\n",
      "total_backward_count 48950 real_backward_count 11183  22.846%\n",
      "fc layer 1 self.abs_max_out: 4991.0\n",
      "fc layer 1 self.abs_max_out: 5012.0\n",
      "fc layer 2 self.abs_max_out: 2296.0\n",
      "lif layer 2 self.abs_max_v: 2296.0\n",
      "fc layer 1 self.abs_max_out: 5268.0\n",
      "lif layer 1 self.abs_max_v: 6227.5\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  2.026805/  2.119547, val:  51.25%, val_best:  59.17%, tr:  99.18%, tr_best:  99.39%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8216%\n",
      "layer   2  Sparsity: 84.7122%\n",
      "layer   3  Sparsity: 82.6645%\n",
      "total_backward_count 58740 real_backward_count 12762  21.726%\n",
      "lif layer 1 self.abs_max_v: 6436.5\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  2.035725/  2.124993, val:  60.00%, val_best:  60.00%, tr:  99.39%, tr_best:  99.39%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8222%\n",
      "layer   2  Sparsity: 84.9964%\n",
      "layer   3  Sparsity: 83.1854%\n",
      "total_backward_count 68530 real_backward_count 14265  20.816%\n",
      "fc layer 1 self.abs_max_out: 5269.0\n",
      "fc layer 2 self.abs_max_out: 2298.0\n",
      "lif layer 2 self.abs_max_v: 2298.0\n",
      "fc layer 2 self.abs_max_out: 2302.0\n",
      "lif layer 2 self.abs_max_v: 2302.0\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  2.048047/  2.134502, val:  58.75%, val_best:  60.00%, tr:  99.39%, tr_best:  99.39%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8074%\n",
      "layer   2  Sparsity: 85.1114%\n",
      "layer   3  Sparsity: 83.6533%\n",
      "total_backward_count 78320 real_backward_count 15686  20.028%\n",
      "fc layer 2 self.abs_max_out: 2312.0\n",
      "lif layer 2 self.abs_max_v: 2312.0\n",
      "fc layer 2 self.abs_max_out: 2337.0\n",
      "lif layer 2 self.abs_max_v: 2337.0\n",
      "fc layer 2 self.abs_max_out: 2376.0\n",
      "lif layer 2 self.abs_max_v: 2376.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  2.050288/  2.132360, val:  54.58%, val_best:  60.00%, tr:  98.98%, tr_best:  99.39%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8078%\n",
      "layer   2  Sparsity: 84.3265%\n",
      "layer   3  Sparsity: 83.5512%\n",
      "total_backward_count 88110 real_backward_count 17173  19.490%\n",
      "lif layer 1 self.abs_max_v: 6448.5\n",
      "lif layer 1 self.abs_max_v: 6559.5\n",
      "fc layer 2 self.abs_max_out: 2379.0\n",
      "lif layer 2 self.abs_max_v: 2379.0\n",
      "fc layer 2 self.abs_max_out: 2497.0\n",
      "lif layer 2 self.abs_max_v: 2497.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  2.050470/  2.146160, val:  62.08%, val_best:  62.08%, tr:  99.49%, tr_best:  99.49%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8057%\n",
      "layer   2  Sparsity: 84.1603%\n",
      "layer   3  Sparsity: 83.2905%\n",
      "total_backward_count 97900 real_backward_count 18480  18.876%\n",
      "fc layer 2 self.abs_max_out: 2552.0\n",
      "lif layer 2 self.abs_max_v: 2552.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  2.060072/  2.133679, val:  55.00%, val_best:  62.08%, tr:  99.28%, tr_best:  99.49%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8086%\n",
      "layer   2  Sparsity: 84.1682%\n",
      "layer   3  Sparsity: 83.6885%\n",
      "total_backward_count 107690 real_backward_count 19810  18.395%\n",
      "lif layer 1 self.abs_max_v: 6723.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  2.056907/  2.126099, val:  58.75%, val_best:  62.08%, tr:  99.28%, tr_best:  99.49%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8022%\n",
      "layer   2  Sparsity: 83.7241%\n",
      "layer   3  Sparsity: 83.2524%\n",
      "total_backward_count 117480 real_backward_count 21197  18.043%\n",
      "lif layer 1 self.abs_max_v: 6831.0\n",
      "lif layer 1 self.abs_max_v: 7038.5\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  2.057033/  2.148644, val:  46.67%, val_best:  62.08%, tr:  99.39%, tr_best:  99.49%, epoch time: 80.09 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8032%\n",
      "layer   2  Sparsity: 83.8468%\n",
      "layer   3  Sparsity: 83.7830%\n",
      "total_backward_count 127270 real_backward_count 22486  17.668%\n",
      "fc layer 1 self.abs_max_out: 5339.0\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  2.059102/  2.129978, val:  56.25%, val_best:  62.08%, tr:  99.49%, tr_best:  99.49%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8122%\n",
      "layer   2  Sparsity: 83.8443%\n",
      "layer   3  Sparsity: 83.7334%\n",
      "total_backward_count 137060 real_backward_count 23794  17.360%\n",
      "fc layer 1 self.abs_max_out: 5432.0\n",
      "lif layer 1 self.abs_max_v: 7053.0\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  2.056537/  2.136350, val:  55.00%, val_best:  62.08%, tr:  99.80%, tr_best:  99.80%, epoch time: 80.23 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.8053%\n",
      "layer   2  Sparsity: 83.5718%\n",
      "layer   3  Sparsity: 83.6786%\n",
      "total_backward_count 146850 real_backward_count 25043  17.053%\n",
      "fc layer 1 self.abs_max_out: 5638.0\n",
      "lif layer 1 self.abs_max_v: 7081.5\n",
      "lif layer 1 self.abs_max_v: 7206.0\n",
      "lif layer 1 self.abs_max_v: 7271.5\n",
      "lif layer 2 self.abs_max_v: 2576.5\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  2.047235/  2.118515, val:  60.83%, val_best:  62.08%, tr:  99.59%, tr_best:  99.80%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8061%\n",
      "layer   2  Sparsity: 83.8505%\n",
      "layer   3  Sparsity: 83.9265%\n",
      "total_backward_count 156640 real_backward_count 26303  16.792%\n",
      "lif layer 2 self.abs_max_v: 2595.5\n",
      "lif layer 2 self.abs_max_v: 2610.5\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  2.052500/  2.130932, val:  58.33%, val_best:  62.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8080%\n",
      "layer   2  Sparsity: 83.6581%\n",
      "layer   3  Sparsity: 83.8038%\n",
      "total_backward_count 166430 real_backward_count 27532  16.543%\n",
      "fc layer 1 self.abs_max_out: 5661.0\n",
      "lif layer 2 self.abs_max_v: 2623.0\n",
      "lif layer 2 self.abs_max_v: 2641.5\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  2.059810/  2.138980, val:  56.67%, val_best:  62.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.04 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8109%\n",
      "layer   2  Sparsity: 83.7459%\n",
      "layer   3  Sparsity: 84.2104%\n",
      "total_backward_count 176220 real_backward_count 28773  16.328%\n",
      "lif layer 2 self.abs_max_v: 2673.5\n",
      "lif layer 2 self.abs_max_v: 2687.5\n",
      "lif layer 1 self.abs_max_v: 7373.0\n",
      "lif layer 1 self.abs_max_v: 7452.5\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  2.074903/  2.123920, val:  62.50%, val_best:  62.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8034%\n",
      "layer   2  Sparsity: 83.8555%\n",
      "layer   3  Sparsity: 84.4173%\n",
      "total_backward_count 186010 real_backward_count 30035  16.147%\n",
      "fc layer 1 self.abs_max_out: 6059.0\n",
      "lif layer 1 self.abs_max_v: 7787.0\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  2.063020/  2.143959, val:  45.42%, val_best:  62.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8183%\n",
      "layer   2  Sparsity: 83.4932%\n",
      "layer   3  Sparsity: 84.1432%\n",
      "total_backward_count 195800 real_backward_count 31228  15.949%\n",
      "fc layer 2 self.abs_max_out: 2559.0\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  2.063612/  2.138375, val:  56.67%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8159%\n",
      "layer   2  Sparsity: 83.1284%\n",
      "layer   3  Sparsity: 84.1110%\n",
      "total_backward_count 205590 real_backward_count 32384  15.752%\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  2.068482/  2.138643, val:  51.67%, val_best:  62.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8012%\n",
      "layer   2  Sparsity: 83.1519%\n",
      "layer   3  Sparsity: 84.0646%\n",
      "total_backward_count 215380 real_backward_count 33691  15.643%\n",
      "fc layer 1 self.abs_max_out: 6312.0\n",
      "fc layer 2 self.abs_max_out: 2560.0\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  2.068991/  2.131935, val:  60.83%, val_best:  62.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8137%\n",
      "layer   2  Sparsity: 82.7986%\n",
      "layer   3  Sparsity: 84.2468%\n",
      "total_backward_count 225170 real_backward_count 34936  15.515%\n",
      "lif layer 1 self.abs_max_v: 8007.0\n",
      "lif layer 2 self.abs_max_v: 2729.5\n",
      "lif layer 2 self.abs_max_v: 2756.5\n",
      "lif layer 2 self.abs_max_v: 2800.5\n",
      "lif layer 2 self.abs_max_v: 3038.0\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  2.062018/  2.131477, val:  59.17%, val_best:  62.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8132%\n",
      "layer   2  Sparsity: 82.5606%\n",
      "layer   3  Sparsity: 84.2825%\n",
      "total_backward_count 234960 real_backward_count 36101  15.365%\n",
      "lif layer 2 self.abs_max_v: 3092.0\n",
      "lif layer 2 self.abs_max_v: 3155.0\n",
      "lif layer 2 self.abs_max_v: 3174.0\n",
      "lif layer 1 self.abs_max_v: 8218.5\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  2.062732/  2.128211, val:  61.67%, val_best:  62.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.7985%\n",
      "layer   2  Sparsity: 82.2614%\n",
      "layer   3  Sparsity: 84.4330%\n",
      "total_backward_count 244750 real_backward_count 37270  15.228%\n",
      "fc layer 2 self.abs_max_out: 2572.0\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  2.056729/  2.121522, val:  62.50%, val_best:  62.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8213%\n",
      "layer   2  Sparsity: 82.1277%\n",
      "layer   3  Sparsity: 84.3287%\n",
      "total_backward_count 254540 real_backward_count 38477  15.116%\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  2.049643/  2.121951, val:  59.58%, val_best:  62.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7998%\n",
      "layer   2  Sparsity: 82.1630%\n",
      "layer   3  Sparsity: 84.0796%\n",
      "total_backward_count 264330 real_backward_count 39624  14.990%\n",
      "lif layer 1 self.abs_max_v: 8359.0\n",
      "fc layer 1 self.abs_max_out: 6625.0\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  2.060700/  2.122473, val:  65.00%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8110%\n",
      "layer   2  Sparsity: 82.6192%\n",
      "layer   3  Sparsity: 84.7181%\n",
      "total_backward_count 274120 real_backward_count 40781  14.877%\n",
      "lif layer 2 self.abs_max_v: 3266.5\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  2.066188/  2.132645, val:  59.17%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8114%\n",
      "layer   2  Sparsity: 82.3208%\n",
      "layer   3  Sparsity: 84.8726%\n",
      "total_backward_count 283910 real_backward_count 41918  14.765%\n",
      "fc layer 1 self.abs_max_out: 7006.0\n",
      "lif layer 1 self.abs_max_v: 8668.0\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  2.064256/  2.135015, val:  55.00%, val_best:  65.00%, tr:  99.39%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8085%\n",
      "layer   2  Sparsity: 82.6360%\n",
      "layer   3  Sparsity: 84.6932%\n",
      "total_backward_count 293700 real_backward_count 43065  14.663%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  2.061292/  2.131888, val:  62.50%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.8060%\n",
      "layer   2  Sparsity: 82.5373%\n",
      "layer   3  Sparsity: 84.3815%\n",
      "total_backward_count 303490 real_backward_count 44193  14.562%\n",
      "lif layer 1 self.abs_max_v: 8727.5\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  2.063982/  2.141248, val:  56.67%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8040%\n",
      "layer   2  Sparsity: 82.0557%\n",
      "layer   3  Sparsity: 84.1733%\n",
      "total_backward_count 313280 real_backward_count 45354  14.477%\n",
      "lif layer 2 self.abs_max_v: 3270.5\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  2.068220/  2.145236, val:  60.00%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.77 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.7982%\n",
      "layer   2  Sparsity: 82.3409%\n",
      "layer   3  Sparsity: 84.4678%\n",
      "total_backward_count 323070 real_backward_count 46452  14.378%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  2.075177/  2.141196, val:  55.83%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8035%\n",
      "layer   2  Sparsity: 82.4148%\n",
      "layer   3  Sparsity: 84.5142%\n",
      "total_backward_count 332860 real_backward_count 47584  14.295%\n",
      "lif layer 1 self.abs_max_v: 8910.5\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  2.063898/  2.137362, val:  61.25%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.7964%\n",
      "layer   2  Sparsity: 82.2587%\n",
      "layer   3  Sparsity: 84.9368%\n",
      "total_backward_count 342650 real_backward_count 48679  14.207%\n",
      "lif layer 2 self.abs_max_v: 3292.5\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  2.067720/  2.133352, val:  62.92%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7977%\n",
      "layer   2  Sparsity: 82.2316%\n",
      "layer   3  Sparsity: 85.1019%\n",
      "total_backward_count 352440 real_backward_count 49805  14.131%\n",
      "lif layer 2 self.abs_max_v: 3335.0\n",
      "lif layer 2 self.abs_max_v: 3516.0\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  2.065021/  2.138235, val:  65.42%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.78 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8151%\n",
      "layer   2  Sparsity: 82.2100%\n",
      "layer   3  Sparsity: 85.0958%\n",
      "total_backward_count 362230 real_backward_count 50930  14.060%\n",
      "lif layer 2 self.abs_max_v: 3579.0\n",
      "lif layer 2 self.abs_max_v: 3611.5\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  2.064482/  2.132833, val:  62.50%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8091%\n",
      "layer   2  Sparsity: 82.0693%\n",
      "layer   3  Sparsity: 84.9664%\n",
      "total_backward_count 372020 real_backward_count 51985  13.974%\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  2.067587/  2.137707, val:  64.58%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.09 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8093%\n",
      "layer   2  Sparsity: 82.3091%\n",
      "layer   3  Sparsity: 84.9252%\n",
      "total_backward_count 381810 real_backward_count 53108  13.910%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  2.076026/  2.147226, val:  58.75%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8099%\n",
      "layer   2  Sparsity: 82.0257%\n",
      "layer   3  Sparsity: 85.0552%\n",
      "total_backward_count 391600 real_backward_count 54193  13.839%\n",
      "lif layer 2 self.abs_max_v: 3651.0\n",
      "lif layer 2 self.abs_max_v: 3698.0\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  2.070469/  2.136838, val:  61.25%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8079%\n",
      "layer   2  Sparsity: 81.9328%\n",
      "layer   3  Sparsity: 85.1687%\n",
      "total_backward_count 401390 real_backward_count 55327  13.784%\n",
      "lif layer 2 self.abs_max_v: 3871.0\n",
      "fc layer 1 self.abs_max_out: 7078.0\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  2.073534/  2.132694, val:  64.17%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8105%\n",
      "layer   2  Sparsity: 82.0315%\n",
      "layer   3  Sparsity: 84.8248%\n",
      "total_backward_count 411180 real_backward_count 56408  13.719%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  2.065263/  2.132379, val:  65.83%, val_best:  65.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7914%\n",
      "layer   2  Sparsity: 81.9129%\n",
      "layer   3  Sparsity: 84.7492%\n",
      "total_backward_count 420970 real_backward_count 57426  13.641%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  2.064393/  2.143572, val:  66.67%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7847%\n",
      "layer   2  Sparsity: 81.6955%\n",
      "layer   3  Sparsity: 84.8964%\n",
      "total_backward_count 430760 real_backward_count 58477  13.575%\n",
      "lif layer 1 self.abs_max_v: 9057.5\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  2.064346/  2.137079, val:  62.50%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8208%\n",
      "layer   2  Sparsity: 81.7282%\n",
      "layer   3  Sparsity: 85.2602%\n",
      "total_backward_count 440550 real_backward_count 59521  13.511%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  2.058102/  2.124319, val:  64.17%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8129%\n",
      "layer   2  Sparsity: 81.6774%\n",
      "layer   3  Sparsity: 85.4661%\n",
      "total_backward_count 450340 real_backward_count 60548  13.445%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  2.066044/  2.135284, val:  61.25%, val_best:  66.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.47 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.8105%\n",
      "layer   2  Sparsity: 81.9402%\n",
      "layer   3  Sparsity: 85.2139%\n",
      "total_backward_count 460130 real_backward_count 61546  13.376%\n",
      "lif layer 2 self.abs_max_v: 3941.0\n",
      "fc layer 1 self.abs_max_out: 7206.0\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  2.061363/  2.128639, val:  58.75%, val_best:  66.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.48 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.8158%\n",
      "layer   2  Sparsity: 81.7315%\n",
      "layer   3  Sparsity: 85.0425%\n",
      "total_backward_count 469920 real_backward_count 62624  13.327%\n",
      "lif layer 1 self.abs_max_v: 9784.5\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  2.067667/  2.130548, val:  62.92%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7999%\n",
      "layer   2  Sparsity: 81.5215%\n",
      "layer   3  Sparsity: 85.0678%\n",
      "total_backward_count 479710 real_backward_count 63654  13.269%\n",
      "lif layer 2 self.abs_max_v: 3984.5\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  2.052650/  2.131437, val:  55.42%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8139%\n",
      "layer   2  Sparsity: 81.7489%\n",
      "layer   3  Sparsity: 85.2167%\n",
      "total_backward_count 489500 real_backward_count 64729  13.223%\n",
      "lif layer 2 self.abs_max_v: 4098.0\n",
      "lif layer 2 self.abs_max_v: 4100.0\n",
      "lif layer 1 self.abs_max_v: 9976.0\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  2.051899/  2.121467, val:  60.42%, val_best:  66.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8230%\n",
      "layer   2  Sparsity: 81.3768%\n",
      "layer   3  Sparsity: 85.5587%\n",
      "total_backward_count 499290 real_backward_count 65761  13.171%\n",
      "fc layer 1 self.abs_max_out: 7414.0\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  2.052102/  2.123574, val:  63.75%, val_best:  66.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.7807%\n",
      "layer   2  Sparsity: 81.5137%\n",
      "layer   3  Sparsity: 85.2194%\n",
      "total_backward_count 509080 real_backward_count 66818  13.125%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  2.052011/  2.127535, val:  65.83%, val_best:  66.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.8050%\n",
      "layer   2  Sparsity: 81.3262%\n",
      "layer   3  Sparsity: 84.8002%\n",
      "total_backward_count 518870 real_backward_count 67842  13.075%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  2.047523/  2.128752, val:  67.50%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 80.07 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8133%\n",
      "layer   2  Sparsity: 81.3287%\n",
      "layer   3  Sparsity: 85.0881%\n",
      "total_backward_count 528660 real_backward_count 68906  13.034%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  2.064106/  2.124029, val:  67.08%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8130%\n",
      "layer   2  Sparsity: 81.2894%\n",
      "layer   3  Sparsity: 84.9787%\n",
      "total_backward_count 538450 real_backward_count 69935  12.988%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  2.053875/  2.123434, val:  70.00%, val_best:  70.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8214%\n",
      "layer   2  Sparsity: 81.2711%\n",
      "layer   3  Sparsity: 85.0971%\n",
      "total_backward_count 548240 real_backward_count 70995  12.950%\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  2.052707/  2.126404, val:  57.08%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8035%\n",
      "layer   2  Sparsity: 81.4634%\n",
      "layer   3  Sparsity: 84.8189%\n",
      "total_backward_count 558030 real_backward_count 72037  12.909%\n",
      "fc layer 2 self.abs_max_out: 2573.0\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  2.050598/  2.128089, val:  63.75%, val_best:  70.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7980%\n",
      "layer   2  Sparsity: 81.1546%\n",
      "layer   3  Sparsity: 84.5162%\n",
      "total_backward_count 567820 real_backward_count 73014  12.859%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  2.050146/  2.124079, val:  63.33%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8076%\n",
      "layer   2  Sparsity: 81.4055%\n",
      "layer   3  Sparsity: 84.8725%\n",
      "total_backward_count 577610 real_backward_count 73987  12.809%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  2.054873/  2.130324, val:  61.67%, val_best:  70.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8048%\n",
      "layer   2  Sparsity: 81.2364%\n",
      "layer   3  Sparsity: 84.9194%\n",
      "total_backward_count 587400 real_backward_count 75027  12.773%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  2.062184/  2.116820, val:  59.17%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8080%\n",
      "layer   2  Sparsity: 81.5242%\n",
      "layer   3  Sparsity: 84.9090%\n",
      "total_backward_count 597190 real_backward_count 76030  12.731%\n",
      "lif layer 1 self.abs_max_v: 10168.5\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  2.061965/  2.122441, val:  69.17%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7956%\n",
      "layer   2  Sparsity: 81.7253%\n",
      "layer   3  Sparsity: 85.3999%\n",
      "total_backward_count 606980 real_backward_count 77035  12.692%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  2.057971/  2.137820, val:  65.83%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7946%\n",
      "layer   2  Sparsity: 81.5084%\n",
      "layer   3  Sparsity: 85.4963%\n",
      "total_backward_count 616770 real_backward_count 78059  12.656%\n",
      "lif layer 1 self.abs_max_v: 10278.5\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  2.060498/  2.134356, val:  62.50%, val_best:  70.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8002%\n",
      "layer   2  Sparsity: 81.5424%\n",
      "layer   3  Sparsity: 85.5156%\n",
      "total_backward_count 626560 real_backward_count 79023  12.612%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  2.063997/  2.126105, val:  62.08%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.11 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.8145%\n",
      "layer   2  Sparsity: 81.0348%\n",
      "layer   3  Sparsity: 85.4146%\n",
      "total_backward_count 636350 real_backward_count 79973  12.567%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  2.050251/  2.136882, val:  63.33%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8237%\n",
      "layer   2  Sparsity: 81.1148%\n",
      "layer   3  Sparsity: 85.4712%\n",
      "total_backward_count 646140 real_backward_count 80921  12.524%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  2.061433/  2.128951, val:  64.58%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8060%\n",
      "layer   2  Sparsity: 81.1923%\n",
      "layer   3  Sparsity: 85.5694%\n",
      "total_backward_count 655930 real_backward_count 81905  12.487%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  2.059687/  2.133280, val:  60.00%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8041%\n",
      "layer   2  Sparsity: 81.2618%\n",
      "layer   3  Sparsity: 85.4330%\n",
      "total_backward_count 665720 real_backward_count 82910  12.454%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  2.052820/  2.119254, val:  67.92%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8069%\n",
      "layer   2  Sparsity: 81.0437%\n",
      "layer   3  Sparsity: 85.1863%\n",
      "total_backward_count 675510 real_backward_count 83892  12.419%\n",
      "fc layer 1 self.abs_max_out: 7422.0\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  2.037979/  2.118819, val:  60.83%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8079%\n",
      "layer   2  Sparsity: 81.1236%\n",
      "layer   3  Sparsity: 84.7954%\n",
      "total_backward_count 685300 real_backward_count 84834  12.379%\n",
      "lif layer 1 self.abs_max_v: 10637.5\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  2.052987/  2.117651, val:  69.17%, val_best:  70.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8100%\n",
      "layer   2  Sparsity: 80.8277%\n",
      "layer   3  Sparsity: 84.9866%\n",
      "total_backward_count 695090 real_backward_count 85832  12.348%\n",
      "lif layer 1 self.abs_max_v: 10716.5\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  2.044700/  2.124768, val:  62.08%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8031%\n",
      "layer   2  Sparsity: 80.6986%\n",
      "layer   3  Sparsity: 85.0619%\n",
      "total_backward_count 704880 real_backward_count 86765  12.309%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  2.051812/  2.120484, val:  69.58%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.06 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8103%\n",
      "layer   2  Sparsity: 80.9753%\n",
      "layer   3  Sparsity: 85.3221%\n",
      "total_backward_count 714670 real_backward_count 87769  12.281%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  2.050566/  2.118755, val:  60.83%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8079%\n",
      "layer   2  Sparsity: 81.2311%\n",
      "layer   3  Sparsity: 85.5906%\n",
      "total_backward_count 724460 real_backward_count 88674  12.240%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  2.056069/  2.118724, val:  60.00%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.8018%\n",
      "layer   2  Sparsity: 80.8202%\n",
      "layer   3  Sparsity: 85.2804%\n",
      "total_backward_count 734250 real_backward_count 89686  12.215%\n",
      "lif layer 1 self.abs_max_v: 10821.5\n",
      "fc layer 1 self.abs_max_out: 7542.0\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  2.048494/  2.117500, val:  67.08%, val_best:  70.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8019%\n",
      "layer   2  Sparsity: 80.8378%\n",
      "layer   3  Sparsity: 85.1668%\n",
      "total_backward_count 744040 real_backward_count 90646  12.183%\n",
      "fc layer 1 self.abs_max_out: 7735.0\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  2.050672/  2.119419, val:  66.25%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8177%\n",
      "layer   2  Sparsity: 80.7832%\n",
      "layer   3  Sparsity: 85.3219%\n",
      "total_backward_count 753830 real_backward_count 91628  12.155%\n",
      "fc layer 1 self.abs_max_out: 7754.0\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  2.045125/  2.125534, val:  61.25%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8081%\n",
      "layer   2  Sparsity: 80.7061%\n",
      "layer   3  Sparsity: 84.9901%\n",
      "total_backward_count 763620 real_backward_count 92545  12.119%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  2.050433/  2.123806, val:  68.33%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8055%\n",
      "layer   2  Sparsity: 80.4491%\n",
      "layer   3  Sparsity: 84.9041%\n",
      "total_backward_count 773410 real_backward_count 93520  12.092%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  2.050734/  2.121543, val:  63.75%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.7986%\n",
      "layer   2  Sparsity: 80.2297%\n",
      "layer   3  Sparsity: 84.8725%\n",
      "total_backward_count 783200 real_backward_count 94479  12.063%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  2.053635/  2.129722, val:  68.75%, val_best:  70.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8085%\n",
      "layer   2  Sparsity: 80.4563%\n",
      "layer   3  Sparsity: 85.1564%\n",
      "total_backward_count 792990 real_backward_count 95433  12.035%\n",
      "fc layer 1 self.abs_max_out: 8101.0\n",
      "fc layer 2 self.abs_max_out: 2641.0\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  2.049745/  2.123682, val:  61.25%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8068%\n",
      "layer   2  Sparsity: 80.3533%\n",
      "layer   3  Sparsity: 85.2465%\n",
      "total_backward_count 802780 real_backward_count 96416  12.010%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  2.044661/  2.115456, val:  62.50%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8058%\n",
      "layer   2  Sparsity: 80.1011%\n",
      "layer   3  Sparsity: 84.7621%\n",
      "total_backward_count 812570 real_backward_count 97390  11.985%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  2.033787/  2.106407, val:  67.50%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8117%\n",
      "layer   2  Sparsity: 80.1792%\n",
      "layer   3  Sparsity: 85.1678%\n",
      "total_backward_count 822360 real_backward_count 98423  11.968%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  2.029596/  2.111159, val:  67.08%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8084%\n",
      "layer   2  Sparsity: 80.1555%\n",
      "layer   3  Sparsity: 85.0865%\n",
      "total_backward_count 832150 real_backward_count 99380  11.943%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  2.038919/  2.128274, val:  60.83%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8064%\n",
      "layer   2  Sparsity: 80.0110%\n",
      "layer   3  Sparsity: 85.2382%\n",
      "total_backward_count 841940 real_backward_count 100298  11.913%\n",
      "fc layer 1 self.abs_max_out: 8136.0\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  2.054178/  2.117165, val:  66.25%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8065%\n",
      "layer   2  Sparsity: 79.8535%\n",
      "layer   3  Sparsity: 85.2659%\n",
      "total_backward_count 851730 real_backward_count 101231  11.885%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  2.039449/  2.105386, val:  69.58%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.7961%\n",
      "layer   2  Sparsity: 80.0377%\n",
      "layer   3  Sparsity: 85.0826%\n",
      "total_backward_count 861520 real_backward_count 102224  11.866%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  2.028202/  2.102210, val:  62.50%, val_best:  70.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.28 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8062%\n",
      "layer   2  Sparsity: 80.1652%\n",
      "layer   3  Sparsity: 85.0413%\n",
      "total_backward_count 871310 real_backward_count 103226  11.847%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  2.033883/  2.109612, val:  65.42%, val_best:  70.00%, tr:  99.39%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8083%\n",
      "layer   2  Sparsity: 79.7573%\n",
      "layer   3  Sparsity: 84.8769%\n",
      "total_backward_count 881100 real_backward_count 104214  11.828%\n",
      "lif layer 1 self.abs_max_v: 11152.0\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  2.033728/  2.100750, val:  65.42%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8171%\n",
      "layer   2  Sparsity: 79.8534%\n",
      "layer   3  Sparsity: 84.3251%\n",
      "total_backward_count 890890 real_backward_count 105180  11.806%\n",
      "lif layer 1 self.abs_max_v: 11359.0\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  2.033956/  2.108924, val:  67.08%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8136%\n",
      "layer   2  Sparsity: 80.0266%\n",
      "layer   3  Sparsity: 84.5823%\n",
      "total_backward_count 900680 real_backward_count 106111  11.781%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  2.038056/  2.115072, val:  64.17%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.94 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.7929%\n",
      "layer   2  Sparsity: 80.1858%\n",
      "layer   3  Sparsity: 85.0726%\n",
      "total_backward_count 910470 real_backward_count 107084  11.761%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  2.039608/  2.117848, val:  62.50%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8035%\n",
      "layer   2  Sparsity: 80.0685%\n",
      "layer   3  Sparsity: 84.9664%\n",
      "total_backward_count 920260 real_backward_count 108085  11.745%\n",
      "fc layer 1 self.abs_max_out: 8480.0\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  2.046955/  2.113785, val:  69.17%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8002%\n",
      "layer   2  Sparsity: 79.9324%\n",
      "layer   3  Sparsity: 84.9743%\n",
      "total_backward_count 930050 real_backward_count 109022  11.722%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  2.042162/  2.111890, val:  67.92%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.91 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8090%\n",
      "layer   2  Sparsity: 79.9128%\n",
      "layer   3  Sparsity: 84.6791%\n",
      "total_backward_count 939840 real_backward_count 109978  11.702%\n",
      "fc layer 2 self.abs_max_out: 2663.0\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  2.029983/  2.103900, val:  60.42%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8015%\n",
      "layer   2  Sparsity: 80.0554%\n",
      "layer   3  Sparsity: 84.8202%\n",
      "total_backward_count 949630 real_backward_count 110911  11.679%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  2.032387/  2.103162, val:  68.33%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8016%\n",
      "layer   2  Sparsity: 80.3150%\n",
      "layer   3  Sparsity: 84.9138%\n",
      "total_backward_count 959420 real_backward_count 111771  11.650%\n",
      "fc layer 1 self.abs_max_out: 8651.0\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  2.026996/  2.111707, val:  66.25%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8164%\n",
      "layer   2  Sparsity: 79.9114%\n",
      "layer   3  Sparsity: 84.6162%\n",
      "total_backward_count 969210 real_backward_count 112679  11.626%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  2.023507/  2.105277, val:  65.83%, val_best:  70.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8162%\n",
      "layer   2  Sparsity: 80.1043%\n",
      "layer   3  Sparsity: 84.8969%\n",
      "total_backward_count 979000 real_backward_count 113571  11.601%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  2.029683/  2.103665, val:  67.92%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7980%\n",
      "layer   2  Sparsity: 79.8788%\n",
      "layer   3  Sparsity: 84.8187%\n",
      "total_backward_count 988790 real_backward_count 114485  11.578%\n",
      "fc layer 2 self.abs_max_out: 2707.0\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  2.030833/  2.098463, val:  66.67%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.7920%\n",
      "layer   2  Sparsity: 79.7609%\n",
      "layer   3  Sparsity: 84.6804%\n",
      "total_backward_count 998580 real_backward_count 115396  11.556%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  2.027081/  2.103519, val:  65.00%, val_best:  70.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.10 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8180%\n",
      "layer   2  Sparsity: 79.8753%\n",
      "layer   3  Sparsity: 84.9781%\n",
      "total_backward_count 1008370 real_backward_count 116288  11.532%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  2.019416/  2.101084, val:  72.92%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8211%\n",
      "layer   2  Sparsity: 79.9405%\n",
      "layer   3  Sparsity: 85.1331%\n",
      "total_backward_count 1018160 real_backward_count 117183  11.509%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  2.032470/  2.102754, val:  72.08%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8147%\n",
      "layer   2  Sparsity: 79.3105%\n",
      "layer   3  Sparsity: 84.7891%\n",
      "total_backward_count 1027950 real_backward_count 118061  11.485%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  2.031229/  2.104092, val:  69.17%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.8110%\n",
      "layer   2  Sparsity: 79.2359%\n",
      "layer   3  Sparsity: 84.9117%\n",
      "total_backward_count 1037740 real_backward_count 118959  11.463%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  2.032949/  2.109474, val:  69.58%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8203%\n",
      "layer   2  Sparsity: 79.1704%\n",
      "layer   3  Sparsity: 85.0150%\n",
      "total_backward_count 1047530 real_backward_count 119860  11.442%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  2.035234/  2.112026, val:  62.50%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8092%\n",
      "layer   2  Sparsity: 79.2113%\n",
      "layer   3  Sparsity: 85.1444%\n",
      "total_backward_count 1057320 real_backward_count 120805  11.426%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  2.040906/  2.108465, val:  68.33%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8039%\n",
      "layer   2  Sparsity: 79.0652%\n",
      "layer   3  Sparsity: 85.2268%\n",
      "total_backward_count 1067110 real_backward_count 121688  11.404%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  2.038389/  2.103635, val:  65.83%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8137%\n",
      "layer   2  Sparsity: 79.1653%\n",
      "layer   3  Sparsity: 84.7101%\n",
      "total_backward_count 1076900 real_backward_count 122567  11.381%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  2.028070/  2.108975, val:  66.67%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8006%\n",
      "layer   2  Sparsity: 79.3664%\n",
      "layer   3  Sparsity: 84.5591%\n",
      "total_backward_count 1086690 real_backward_count 123436  11.359%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  2.034087/  2.105400, val:  64.58%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8208%\n",
      "layer   2  Sparsity: 79.5070%\n",
      "layer   3  Sparsity: 84.7304%\n",
      "total_backward_count 1096480 real_backward_count 124323  11.338%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  2.038061/  2.109156, val:  72.50%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8090%\n",
      "layer   2  Sparsity: 79.7778%\n",
      "layer   3  Sparsity: 84.8020%\n",
      "total_backward_count 1106270 real_backward_count 125201  11.317%\n",
      "lif layer 1 self.abs_max_v: 12230.0\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  2.030961/  2.102113, val:  67.50%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7982%\n",
      "layer   2  Sparsity: 79.3169%\n",
      "layer   3  Sparsity: 84.8697%\n",
      "total_backward_count 1116060 real_backward_count 126062  11.295%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  2.031206/  2.112873, val:  65.00%, val_best:  72.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.82 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8308%\n",
      "layer   2  Sparsity: 79.4438%\n",
      "layer   3  Sparsity: 84.6648%\n",
      "total_backward_count 1125850 real_backward_count 126967  11.277%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  2.034535/  2.111108, val:  67.50%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7996%\n",
      "layer   2  Sparsity: 79.2253%\n",
      "layer   3  Sparsity: 84.7901%\n",
      "total_backward_count 1135640 real_backward_count 127851  11.258%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  2.032296/  2.100834, val:  61.25%, val_best:  72.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8035%\n",
      "layer   2  Sparsity: 79.3241%\n",
      "layer   3  Sparsity: 84.8639%\n",
      "total_backward_count 1145430 real_backward_count 128728  11.238%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  2.031388/  2.114875, val:  65.83%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.8136%\n",
      "layer   2  Sparsity: 79.3782%\n",
      "layer   3  Sparsity: 84.7688%\n",
      "total_backward_count 1155220 real_backward_count 129642  11.222%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  2.033663/  2.100286, val:  61.67%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8220%\n",
      "layer   2  Sparsity: 79.0680%\n",
      "layer   3  Sparsity: 84.5665%\n",
      "total_backward_count 1165010 real_backward_count 130548  11.206%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  2.023838/  2.108876, val:  66.67%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8045%\n",
      "layer   2  Sparsity: 78.9988%\n",
      "layer   3  Sparsity: 85.0984%\n",
      "total_backward_count 1174800 real_backward_count 131495  11.193%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  2.037307/  2.112714, val:  67.92%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.7924%\n",
      "layer   2  Sparsity: 79.1936%\n",
      "layer   3  Sparsity: 85.3385%\n",
      "total_backward_count 1184590 real_backward_count 132342  11.172%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  2.032380/  2.103819, val:  70.00%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7922%\n",
      "layer   2  Sparsity: 79.3101%\n",
      "layer   3  Sparsity: 84.5466%\n",
      "total_backward_count 1194380 real_backward_count 133237  11.155%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  2.024725/  2.104499, val:  72.92%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8116%\n",
      "layer   2  Sparsity: 78.8612%\n",
      "layer   3  Sparsity: 84.6557%\n",
      "total_backward_count 1204170 real_backward_count 134126  11.138%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  2.029946/  2.113341, val:  67.50%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.7877%\n",
      "layer   2  Sparsity: 78.6129%\n",
      "layer   3  Sparsity: 85.0026%\n",
      "total_backward_count 1213960 real_backward_count 135024  11.123%\n",
      "lif layer 1 self.abs_max_v: 12377.5\n",
      "fc layer 1 self.abs_max_out: 8812.0\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  2.023910/  2.092208, val:  68.75%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8075%\n",
      "layer   2  Sparsity: 78.7724%\n",
      "layer   3  Sparsity: 84.6379%\n",
      "total_backward_count 1223750 real_backward_count 135941  11.109%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  2.016309/  2.095663, val:  65.83%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.23 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.8181%\n",
      "layer   2  Sparsity: 78.7042%\n",
      "layer   3  Sparsity: 84.5582%\n",
      "total_backward_count 1233540 real_backward_count 136815  11.091%\n",
      "fc layer 1 self.abs_max_out: 9454.0\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  2.011843/  2.085414, val:  65.00%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.96 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8207%\n",
      "layer   2  Sparsity: 78.9522%\n",
      "layer   3  Sparsity: 84.2653%\n",
      "total_backward_count 1243330 real_backward_count 137647  11.071%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  2.001487/  2.076516, val:  69.58%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8081%\n",
      "layer   2  Sparsity: 78.8724%\n",
      "layer   3  Sparsity: 84.3191%\n",
      "total_backward_count 1253120 real_backward_count 138517  11.054%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  2.011042/  2.086019, val:  71.25%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8163%\n",
      "layer   2  Sparsity: 79.0649%\n",
      "layer   3  Sparsity: 84.0374%\n",
      "total_backward_count 1262910 real_backward_count 139386  11.037%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  2.002965/  2.083402, val:  63.75%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.00 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8094%\n",
      "layer   2  Sparsity: 78.8231%\n",
      "layer   3  Sparsity: 83.7748%\n",
      "total_backward_count 1272700 real_backward_count 140259  11.021%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  2.000622/  2.085983, val:  62.92%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8154%\n",
      "layer   2  Sparsity: 79.0061%\n",
      "layer   3  Sparsity: 84.1608%\n",
      "total_backward_count 1282490 real_backward_count 141131  11.004%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  2.004456/  2.082232, val:  66.67%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8020%\n",
      "layer   2  Sparsity: 79.0634%\n",
      "layer   3  Sparsity: 83.8543%\n",
      "total_backward_count 1292280 real_backward_count 142090  10.995%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  2.000866/  2.095681, val:  64.17%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8079%\n",
      "layer   2  Sparsity: 78.5247%\n",
      "layer   3  Sparsity: 83.9325%\n",
      "total_backward_count 1302070 real_backward_count 142926  10.977%\n",
      "lif layer 1 self.abs_max_v: 13704.0\n",
      "lif layer 1 self.abs_max_v: 14068.0\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  2.008006/  2.085874, val:  65.83%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.7908%\n",
      "layer   2  Sparsity: 78.5118%\n",
      "layer   3  Sparsity: 83.6907%\n",
      "total_backward_count 1311860 real_backward_count 143776  10.960%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  2.000057/  2.085640, val:  61.67%, val_best:  72.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8122%\n",
      "layer   2  Sparsity: 78.5631%\n",
      "layer   3  Sparsity: 83.2516%\n",
      "total_backward_count 1321650 real_backward_count 144628  10.943%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  2.008171/  2.086560, val:  64.17%, val_best:  72.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8023%\n",
      "layer   2  Sparsity: 78.2311%\n",
      "layer   3  Sparsity: 83.6130%\n",
      "total_backward_count 1331440 real_backward_count 145507  10.929%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  2.012664/  2.088483, val:  64.58%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8016%\n",
      "layer   2  Sparsity: 78.7727%\n",
      "layer   3  Sparsity: 84.3699%\n",
      "total_backward_count 1341230 real_backward_count 146355  10.912%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  2.011638/  2.087054, val:  65.83%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8053%\n",
      "layer   2  Sparsity: 79.0349%\n",
      "layer   3  Sparsity: 84.2925%\n",
      "total_backward_count 1351020 real_backward_count 147191  10.895%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  2.005122/  2.091678, val:  67.92%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.8125%\n",
      "layer   2  Sparsity: 79.0296%\n",
      "layer   3  Sparsity: 84.1221%\n",
      "total_backward_count 1360810 real_backward_count 148061  10.880%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  2.006300/  2.086932, val:  68.75%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8156%\n",
      "layer   2  Sparsity: 78.9758%\n",
      "layer   3  Sparsity: 83.9578%\n",
      "total_backward_count 1370600 real_backward_count 148921  10.865%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  2.006666/  2.084370, val:  65.83%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8081%\n",
      "layer   2  Sparsity: 78.7751%\n",
      "layer   3  Sparsity: 83.7182%\n",
      "total_backward_count 1380390 real_backward_count 149781  10.851%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  2.007792/  2.089615, val:  62.92%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.7984%\n",
      "layer   2  Sparsity: 78.8068%\n",
      "layer   3  Sparsity: 83.3438%\n",
      "total_backward_count 1390180 real_backward_count 150650  10.837%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  2.012276/  2.095258, val:  61.25%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.19 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.8050%\n",
      "layer   2  Sparsity: 78.7330%\n",
      "layer   3  Sparsity: 83.5331%\n",
      "total_backward_count 1399970 real_backward_count 151514  10.823%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  2.011042/  2.081769, val:  64.58%, val_best:  72.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7992%\n",
      "layer   2  Sparsity: 78.8021%\n",
      "layer   3  Sparsity: 83.5075%\n",
      "total_backward_count 1409760 real_backward_count 152369  10.808%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.998309/  2.078655, val:  60.42%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7995%\n",
      "layer   2  Sparsity: 78.7330%\n",
      "layer   3  Sparsity: 83.1831%\n",
      "total_backward_count 1419550 real_backward_count 153215  10.793%\n",
      "fc layer 2 self.abs_max_out: 2775.0\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  2.004380/  2.086987, val:  62.08%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7957%\n",
      "layer   2  Sparsity: 78.7788%\n",
      "layer   3  Sparsity: 83.5660%\n",
      "total_backward_count 1429340 real_backward_count 154046  10.777%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  2.006376/  2.092628, val:  65.83%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8146%\n",
      "layer   2  Sparsity: 78.8957%\n",
      "layer   3  Sparsity: 84.1136%\n",
      "total_backward_count 1439130 real_backward_count 154934  10.766%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  2.010862/  2.091185, val:  68.75%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.27 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.8117%\n",
      "layer   2  Sparsity: 79.0987%\n",
      "layer   3  Sparsity: 84.5173%\n",
      "total_backward_count 1448920 real_backward_count 155741  10.749%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  2.013527/  2.094308, val:  65.00%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8047%\n",
      "layer   2  Sparsity: 79.2025%\n",
      "layer   3  Sparsity: 84.2262%\n",
      "total_backward_count 1458710 real_backward_count 156603  10.736%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  2.015314/  2.095763, val:  65.00%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8176%\n",
      "layer   2  Sparsity: 78.9822%\n",
      "layer   3  Sparsity: 84.0732%\n",
      "total_backward_count 1468500 real_backward_count 157449  10.722%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  2.009272/  2.080228, val:  71.67%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8093%\n",
      "layer   2  Sparsity: 78.8110%\n",
      "layer   3  Sparsity: 83.8751%\n",
      "total_backward_count 1478290 real_backward_count 158230  10.704%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  2.002628/  2.079275, val:  66.25%, val_best:  72.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8037%\n",
      "layer   2  Sparsity: 78.7469%\n",
      "layer   3  Sparsity: 83.7170%\n",
      "total_backward_count 1488080 real_backward_count 159146  10.695%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  2.004223/  2.089083, val:  61.25%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8136%\n",
      "layer   2  Sparsity: 78.6277%\n",
      "layer   3  Sparsity: 83.9889%\n",
      "total_backward_count 1497870 real_backward_count 159950  10.678%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  2.000378/  2.081856, val:  70.83%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.13 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.8103%\n",
      "layer   2  Sparsity: 78.8525%\n",
      "layer   3  Sparsity: 83.7463%\n",
      "total_backward_count 1507660 real_backward_count 160780  10.664%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.995628/  2.085851, val:  65.83%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8015%\n",
      "layer   2  Sparsity: 78.7576%\n",
      "layer   3  Sparsity: 83.5190%\n",
      "total_backward_count 1517450 real_backward_count 161610  10.650%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  2.001407/  2.087946, val:  60.00%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8106%\n",
      "layer   2  Sparsity: 78.9608%\n",
      "layer   3  Sparsity: 83.3091%\n",
      "total_backward_count 1527240 real_backward_count 162380  10.632%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.998941/  2.076694, val:  67.92%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.7993%\n",
      "layer   2  Sparsity: 79.0573%\n",
      "layer   3  Sparsity: 83.2957%\n",
      "total_backward_count 1537030 real_backward_count 163208  10.618%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.988637/  2.076996, val:  74.17%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8086%\n",
      "layer   2  Sparsity: 79.0646%\n",
      "layer   3  Sparsity: 83.3135%\n",
      "total_backward_count 1546820 real_backward_count 164016  10.603%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.991588/  2.071619, val:  69.58%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.81 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8061%\n",
      "layer   2  Sparsity: 79.0436%\n",
      "layer   3  Sparsity: 83.3557%\n",
      "total_backward_count 1556610 real_backward_count 164866  10.591%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.995791/  2.078599, val:  69.58%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.01 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8320%\n",
      "layer   2  Sparsity: 78.9941%\n",
      "layer   3  Sparsity: 83.6915%\n",
      "total_backward_count 1566400 real_backward_count 165716  10.579%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.994233/  2.069181, val:  69.17%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8084%\n",
      "layer   2  Sparsity: 79.1169%\n",
      "layer   3  Sparsity: 83.6231%\n",
      "total_backward_count 1576190 real_backward_count 166553  10.567%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.995406/  2.077410, val:  70.83%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8125%\n",
      "layer   2  Sparsity: 79.3754%\n",
      "layer   3  Sparsity: 83.6694%\n",
      "total_backward_count 1585980 real_backward_count 167376  10.553%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.987811/  2.066468, val:  63.33%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8100%\n",
      "layer   2  Sparsity: 78.8755%\n",
      "layer   3  Sparsity: 83.5892%\n",
      "total_backward_count 1595770 real_backward_count 168234  10.542%\n",
      "lif layer 2 self.abs_max_v: 4151.0\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.990191/  2.080338, val:  76.25%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.8060%\n",
      "layer   2  Sparsity: 78.9259%\n",
      "layer   3  Sparsity: 83.6985%\n",
      "total_backward_count 1605560 real_backward_count 169032  10.528%\n",
      "lif layer 1 self.abs_max_v: 14162.0\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.992527/  2.071646, val:  62.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8198%\n",
      "layer   2  Sparsity: 79.0039%\n",
      "layer   3  Sparsity: 83.6836%\n",
      "total_backward_count 1615350 real_backward_count 169860  10.515%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.982624/  2.080739, val:  63.33%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8078%\n",
      "layer   2  Sparsity: 79.2082%\n",
      "layer   3  Sparsity: 83.8256%\n",
      "total_backward_count 1625140 real_backward_count 170676  10.502%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.981994/  2.065095, val:  62.08%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7947%\n",
      "layer   2  Sparsity: 78.9755%\n",
      "layer   3  Sparsity: 83.4270%\n",
      "total_backward_count 1634930 real_backward_count 171499  10.490%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.972242/  2.062022, val:  65.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.65 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8173%\n",
      "layer   2  Sparsity: 78.6893%\n",
      "layer   3  Sparsity: 82.9553%\n",
      "total_backward_count 1644720 real_backward_count 172303  10.476%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.981055/  2.067099, val:  66.67%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7925%\n",
      "layer   2  Sparsity: 78.6548%\n",
      "layer   3  Sparsity: 83.7571%\n",
      "total_backward_count 1654510 real_backward_count 173103  10.462%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.984671/  2.065790, val:  64.17%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.8070%\n",
      "layer   2  Sparsity: 78.7975%\n",
      "layer   3  Sparsity: 83.8025%\n",
      "total_backward_count 1664300 real_backward_count 173929  10.451%\n",
      "fc layer 2 self.abs_max_out: 2776.0\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.982626/  2.070724, val:  68.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.7972%\n",
      "layer   2  Sparsity: 78.8255%\n",
      "layer   3  Sparsity: 83.3657%\n",
      "total_backward_count 1674090 real_backward_count 174711  10.436%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.983932/  2.070536, val:  59.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.7889%\n",
      "layer   2  Sparsity: 78.6109%\n",
      "layer   3  Sparsity: 83.1888%\n",
      "total_backward_count 1683880 real_backward_count 175582  10.427%\n",
      "lif layer 1 self.abs_max_v: 14434.5\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.978660/  2.070729, val:  62.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8099%\n",
      "layer   2  Sparsity: 78.9481%\n",
      "layer   3  Sparsity: 83.1641%\n",
      "total_backward_count 1693670 real_backward_count 176421  10.416%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.981271/  2.073187, val:  65.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.16 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.8224%\n",
      "layer   2  Sparsity: 78.8296%\n",
      "layer   3  Sparsity: 83.2362%\n",
      "total_backward_count 1703460 real_backward_count 177210  10.403%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.987425/  2.071942, val:  72.92%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.90 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8087%\n",
      "layer   2  Sparsity: 78.8565%\n",
      "layer   3  Sparsity: 83.3643%\n",
      "total_backward_count 1713250 real_backward_count 178026  10.391%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.993127/  2.072474, val:  70.83%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7851%\n",
      "layer   2  Sparsity: 78.7726%\n",
      "layer   3  Sparsity: 82.9836%\n",
      "total_backward_count 1723040 real_backward_count 178850  10.380%\n",
      "lif layer 1 self.abs_max_v: 14529.5\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.990442/  2.069541, val:  71.25%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8060%\n",
      "layer   2  Sparsity: 78.6216%\n",
      "layer   3  Sparsity: 83.6881%\n",
      "total_backward_count 1732830 real_backward_count 179659  10.368%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.987338/  2.067738, val:  62.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.19 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.8104%\n",
      "layer   2  Sparsity: 78.6164%\n",
      "layer   3  Sparsity: 83.6966%\n",
      "total_backward_count 1742620 real_backward_count 180418  10.353%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.978641/  2.069416, val:  66.67%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8008%\n",
      "layer   2  Sparsity: 78.9161%\n",
      "layer   3  Sparsity: 83.5665%\n",
      "total_backward_count 1752410 real_backward_count 181183  10.339%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.990832/  2.071366, val:  67.50%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.7960%\n",
      "layer   2  Sparsity: 78.9727%\n",
      "layer   3  Sparsity: 83.6112%\n",
      "total_backward_count 1762200 real_backward_count 182025  10.329%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.985917/  2.061472, val:  70.83%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8070%\n",
      "layer   2  Sparsity: 78.8778%\n",
      "layer   3  Sparsity: 83.2498%\n",
      "total_backward_count 1771990 real_backward_count 182832  10.318%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.987428/  2.077641, val:  67.92%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8131%\n",
      "layer   2  Sparsity: 78.9841%\n",
      "layer   3  Sparsity: 83.7098%\n",
      "total_backward_count 1781780 real_backward_count 183577  10.303%\n",
      "lif layer 1 self.abs_max_v: 14539.5\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.984780/  2.070785, val:  70.00%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.10 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8039%\n",
      "layer   2  Sparsity: 78.9580%\n",
      "layer   3  Sparsity: 83.1424%\n",
      "total_backward_count 1791570 real_backward_count 184358  10.290%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.983281/  2.069134, val:  69.17%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8076%\n",
      "layer   2  Sparsity: 79.1441%\n",
      "layer   3  Sparsity: 82.9885%\n",
      "total_backward_count 1801360 real_backward_count 185132  10.277%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.981527/  2.077805, val:  67.92%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8118%\n",
      "layer   2  Sparsity: 79.0689%\n",
      "layer   3  Sparsity: 82.8522%\n",
      "total_backward_count 1811150 real_backward_count 185872  10.263%\n",
      "fc layer 2 self.abs_max_out: 2898.0\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.986750/  2.075071, val:  66.25%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.67 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.7943%\n",
      "layer   2  Sparsity: 78.7041%\n",
      "layer   3  Sparsity: 82.7799%\n",
      "total_backward_count 1820940 real_backward_count 186684  10.252%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.979645/  2.061932, val:  75.42%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8088%\n",
      "layer   2  Sparsity: 78.7023%\n",
      "layer   3  Sparsity: 82.4343%\n",
      "total_backward_count 1830730 real_backward_count 187504  10.242%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.975417/  2.067548, val:  73.75%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.7990%\n",
      "layer   2  Sparsity: 78.8715%\n",
      "layer   3  Sparsity: 82.9687%\n",
      "total_backward_count 1840520 real_backward_count 188356  10.234%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.980816/  2.065537, val:  69.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.11 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 87.8116%\n",
      "layer   2  Sparsity: 78.7646%\n",
      "layer   3  Sparsity: 82.6630%\n",
      "total_backward_count 1850310 real_backward_count 189150  10.223%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.973710/  2.055321, val:  68.33%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.90 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8067%\n",
      "layer   2  Sparsity: 78.8357%\n",
      "layer   3  Sparsity: 82.6624%\n",
      "total_backward_count 1860100 real_backward_count 189969  10.213%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.973283/  2.068870, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8176%\n",
      "layer   2  Sparsity: 79.2731%\n",
      "layer   3  Sparsity: 83.0815%\n",
      "total_backward_count 1869890 real_backward_count 190814  10.205%\n",
      "lif layer 1 self.abs_max_v: 14596.0\n",
      "lif layer 1 self.abs_max_v: 15112.5\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.981546/  2.069330, val:  61.25%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8052%\n",
      "layer   2  Sparsity: 78.8341%\n",
      "layer   3  Sparsity: 82.7580%\n",
      "total_backward_count 1879680 real_backward_count 191601  10.193%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.971325/  2.064527, val:  71.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 87.8138%\n",
      "layer   2  Sparsity: 78.8336%\n",
      "layer   3  Sparsity: 82.5439%\n",
      "total_backward_count 1889470 real_backward_count 192360  10.181%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.975925/  2.069332, val:  67.92%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8110%\n",
      "layer   2  Sparsity: 78.8425%\n",
      "layer   3  Sparsity: 82.7785%\n",
      "total_backward_count 1899260 real_backward_count 193111  10.168%\n",
      "fc layer 1 self.abs_max_out: 9718.0\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.968038/  2.059647, val:  64.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 87.8069%\n",
      "layer   2  Sparsity: 78.9087%\n",
      "layer   3  Sparsity: 82.1015%\n",
      "total_backward_count 1909050 real_backward_count 193901  10.157%\n",
      "lif layer 1 self.abs_max_v: 15806.0\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.971960/  2.066353, val:  62.08%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.7958%\n",
      "layer   2  Sparsity: 78.8977%\n",
      "layer   3  Sparsity: 82.3894%\n",
      "total_backward_count 1918840 real_backward_count 194694  10.146%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.969816/  2.057336, val:  70.83%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8076%\n",
      "layer   2  Sparsity: 78.6872%\n",
      "layer   3  Sparsity: 82.5479%\n",
      "total_backward_count 1928630 real_backward_count 195511  10.137%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.959088/  2.050799, val:  67.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.66 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 87.8122%\n",
      "layer   2  Sparsity: 78.4569%\n",
      "layer   3  Sparsity: 82.2580%\n",
      "total_backward_count 1938420 real_backward_count 196300  10.127%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.963661/  2.050366, val:  66.67%, val_best:  76.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8151%\n",
      "layer   2  Sparsity: 78.3244%\n",
      "layer   3  Sparsity: 82.3143%\n",
      "total_backward_count 1948210 real_backward_count 197115  10.118%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.960966/  2.059415, val:  63.33%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 87.8101%\n",
      "layer   2  Sparsity: 78.7052%\n",
      "layer   3  Sparsity: 82.2151%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0145f8a825714990a42b36b9f1065067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.96097</td></tr><tr><td>val_acc_best</td><td>0.7625</td></tr><tr><td>val_acc_now</td><td>0.63333</td></tr><tr><td>val_loss</td><td>2.05941</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wild-sweep-86</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k6dafwt8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/k6dafwt8</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_041701-k6dafwt8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 07tlf5qa with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_084246-07tlf5qa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/07tlf5qa' target=\"_blank\">laced-sweep-90</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/07tlf5qa' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/07tlf5qa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251116_084255_263', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 255.0\n",
      "lif layer 1 self.abs_max_v: 255.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 284.0\n",
      "lif layer 1 self.abs_max_v: 382.5\n",
      "fc layer 2 self.abs_max_out: 202.0\n",
      "lif layer 2 self.abs_max_v: 202.0\n",
      "fc layer 1 self.abs_max_out: 472.0\n",
      "lif layer 1 self.abs_max_v: 585.0\n",
      "fc layer 2 self.abs_max_out: 216.0\n",
      "lif layer 2 self.abs_max_v: 266.5\n",
      "fc layer 1 self.abs_max_out: 619.0\n",
      "lif layer 1 self.abs_max_v: 619.0\n",
      "fc layer 1 self.abs_max_out: 871.0\n",
      "lif layer 1 self.abs_max_v: 871.0\n",
      "lif layer 2 self.abs_max_v: 275.0\n",
      "fc layer 3 self.abs_max_out: 10.0\n",
      "fc layer 1 self.abs_max_out: 948.0\n",
      "lif layer 1 self.abs_max_v: 948.0\n",
      "lif layer 2 self.abs_max_v: 277.5\n",
      "fc layer 3 self.abs_max_out: 18.0\n",
      "fc layer 2 self.abs_max_out: 235.0\n",
      "lif layer 2 self.abs_max_v: 343.0\n",
      "fc layer 3 self.abs_max_out: 54.0\n",
      "lif layer 1 self.abs_max_v: 966.0\n",
      "lif layer 2 self.abs_max_v: 362.0\n",
      "fc layer 2 self.abs_max_out: 255.0\n",
      "fc layer 2 self.abs_max_out: 281.0\n",
      "lif layer 2 self.abs_max_v: 386.5\n",
      "fc layer 1 self.abs_max_out: 1153.0\n",
      "lif layer 1 self.abs_max_v: 1153.0\n",
      "fc layer 2 self.abs_max_out: 336.0\n",
      "lif layer 2 self.abs_max_v: 410.0\n",
      "fc layer 2 self.abs_max_out: 419.0\n",
      "lif layer 2 self.abs_max_v: 419.0\n",
      "fc layer 2 self.abs_max_out: 428.0\n",
      "lif layer 2 self.abs_max_v: 503.0\n",
      "fc layer 3 self.abs_max_out: 72.0\n",
      "fc layer 2 self.abs_max_out: 429.0\n",
      "fc layer 1 self.abs_max_out: 1266.0\n",
      "lif layer 1 self.abs_max_v: 1266.0\n",
      "fc layer 2 self.abs_max_out: 458.0\n",
      "fc layer 3 self.abs_max_out: 76.0\n",
      "fc layer 3 self.abs_max_out: 108.0\n",
      "fc layer 1 self.abs_max_out: 1453.0\n",
      "lif layer 1 self.abs_max_v: 1453.0\n",
      "fc layer 2 self.abs_max_out: 463.0\n",
      "lif layer 1 self.abs_max_v: 1482.5\n",
      "lif layer 2 self.abs_max_v: 542.0\n",
      "lif layer 1 self.abs_max_v: 1633.5\n",
      "lif layer 2 self.abs_max_v: 562.0\n",
      "fc layer 1 self.abs_max_out: 1685.0\n",
      "lif layer 1 self.abs_max_v: 1880.0\n",
      "lif layer 2 self.abs_max_v: 616.0\n",
      "lif layer 1 self.abs_max_v: 1944.0\n",
      "fc layer 2 self.abs_max_out: 480.0\n",
      "fc layer 2 self.abs_max_out: 495.0\n",
      "fc layer 2 self.abs_max_out: 528.0\n",
      "fc layer 2 self.abs_max_out: 537.0\n",
      "fc layer 2 self.abs_max_out: 590.0\n",
      "fc layer 3 self.abs_max_out: 110.0\n",
      "lif layer 2 self.abs_max_v: 631.0\n",
      "fc layer 1 self.abs_max_out: 1711.0\n",
      "fc layer 3 self.abs_max_out: 111.0\n",
      "lif layer 2 self.abs_max_v: 633.5\n",
      "fc layer 3 self.abs_max_out: 128.0\n",
      "lif layer 2 self.abs_max_v: 703.0\n",
      "fc layer 1 self.abs_max_out: 2433.0\n",
      "lif layer 1 self.abs_max_v: 2433.0\n",
      "fc layer 2 self.abs_max_out: 631.0\n",
      "lif layer 1 self.abs_max_v: 2482.5\n",
      "lif layer 2 self.abs_max_v: 758.5\n",
      "lif layer 1 self.abs_max_v: 2753.5\n",
      "fc layer 2 self.abs_max_out: 647.0\n",
      "lif layer 2 self.abs_max_v: 768.5\n",
      "fc layer 3 self.abs_max_out: 135.0\n",
      "lif layer 1 self.abs_max_v: 2897.0\n",
      "fc layer 2 self.abs_max_out: 656.0\n",
      "lif layer 2 self.abs_max_v: 793.5\n",
      "fc layer 2 self.abs_max_out: 672.0\n",
      "fc layer 3 self.abs_max_out: 137.0\n",
      "fc layer 3 self.abs_max_out: 142.0\n",
      "fc layer 3 self.abs_max_out: 148.0\n",
      "fc layer 3 self.abs_max_out: 165.0\n",
      "lif layer 1 self.abs_max_v: 2969.0\n",
      "lif layer 1 self.abs_max_v: 3195.5\n",
      "fc layer 2 self.abs_max_out: 709.0\n",
      "lif layer 2 self.abs_max_v: 818.5\n",
      "lif layer 2 self.abs_max_v: 822.0\n",
      "fc layer 2 self.abs_max_out: 722.0\n",
      "fc layer 2 self.abs_max_out: 724.0\n",
      "lif layer 1 self.abs_max_v: 3236.0\n",
      "fc layer 3 self.abs_max_out: 191.0\n",
      "fc layer 2 self.abs_max_out: 748.0\n",
      "lif layer 1 self.abs_max_v: 3296.0\n",
      "fc layer 2 self.abs_max_out: 786.0\n",
      "lif layer 1 self.abs_max_v: 3327.0\n",
      "lif layer 2 self.abs_max_v: 843.5\n",
      "fc layer 2 self.abs_max_out: 810.0\n",
      "fc layer 2 self.abs_max_out: 820.0\n",
      "fc layer 2 self.abs_max_out: 860.0\n",
      "lif layer 2 self.abs_max_v: 860.0\n",
      "lif layer 1 self.abs_max_v: 3504.0\n",
      "lif layer 1 self.abs_max_v: 3555.0\n",
      "lif layer 1 self.abs_max_v: 3646.5\n",
      "lif layer 1 self.abs_max_v: 3910.5\n",
      "fc layer 3 self.abs_max_out: 194.0\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "fc layer 3 self.abs_max_out: 211.0\n",
      "lif layer 2 self.abs_max_v: 862.0\n",
      "lif layer 2 self.abs_max_v: 906.5\n",
      "lif layer 2 self.abs_max_v: 924.5\n",
      "lif layer 2 self.abs_max_v: 928.5\n",
      "lif layer 2 self.abs_max_v: 929.5\n",
      "fc layer 2 self.abs_max_out: 866.0\n",
      "fc layer 3 self.abs_max_out: 243.0\n",
      "fc layer 3 self.abs_max_out: 254.0\n",
      "fc layer 2 self.abs_max_out: 878.0\n",
      "fc layer 2 self.abs_max_out: 913.0\n",
      "fc layer 2 self.abs_max_out: 926.0\n",
      "lif layer 2 self.abs_max_v: 933.5\n",
      "lif layer 2 self.abs_max_v: 954.0\n",
      "lif layer 1 self.abs_max_v: 4113.5\n",
      "lif layer 2 self.abs_max_v: 968.0\n",
      "fc layer 1 self.abs_max_out: 2611.0\n",
      "fc layer 2 self.abs_max_out: 928.0\n",
      "fc layer 2 self.abs_max_out: 933.0\n",
      "fc layer 2 self.abs_max_out: 961.0\n",
      "fc layer 2 self.abs_max_out: 1022.0\n",
      "lif layer 2 self.abs_max_v: 1022.0\n",
      "fc layer 2 self.abs_max_out: 1028.0\n",
      "lif layer 2 self.abs_max_v: 1028.0\n",
      "fc layer 2 self.abs_max_out: 1033.0\n",
      "lif layer 2 self.abs_max_v: 1033.0\n",
      "fc layer 2 self.abs_max_out: 1039.0\n",
      "lif layer 2 self.abs_max_v: 1039.0\n",
      "lif layer 1 self.abs_max_v: 4194.0\n",
      "lif layer 1 self.abs_max_v: 4377.5\n",
      "fc layer 2 self.abs_max_out: 1063.0\n",
      "lif layer 2 self.abs_max_v: 1063.0\n",
      "fc layer 2 self.abs_max_out: 1106.0\n",
      "lif layer 2 self.abs_max_v: 1106.0\n",
      "fc layer 1 self.abs_max_out: 2631.0\n",
      "fc layer 1 self.abs_max_out: 2719.0\n",
      "lif layer 1 self.abs_max_v: 4407.5\n",
      "fc layer 1 self.abs_max_out: 2786.0\n",
      "lif layer 1 self.abs_max_v: 4420.5\n",
      "lif layer 1 self.abs_max_v: 4513.5\n",
      "lif layer 1 self.abs_max_v: 4643.0\n",
      "lif layer 1 self.abs_max_v: 4856.5\n",
      "fc layer 1 self.abs_max_out: 2929.0\n",
      "lif layer 1 self.abs_max_v: 4978.5\n",
      "fc layer 1 self.abs_max_out: 3106.0\n",
      "lif layer 1 self.abs_max_v: 4990.0\n",
      "lif layer 1 self.abs_max_v: 5190.5\n",
      "lif layer 1 self.abs_max_v: 5499.5\n",
      "lif layer 2 self.abs_max_v: 1201.0\n",
      "lif layer 2 self.abs_max_v: 1216.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.941533/  2.088138, val:  29.58%, val_best:  29.58%, tr:  96.53%, tr_best:  96.53%, epoch time: 80.25 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 86.6035%\n",
      "layer   2  Sparsity: 88.6577%\n",
      "layer   3  Sparsity: 89.3998%\n",
      "total_backward_count 9790 real_backward_count 2006  20.490%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 3338.0\n",
      "fc layer 1 self.abs_max_out: 3403.0\n",
      "lif layer 2 self.abs_max_v: 1223.5\n",
      "lif layer 2 self.abs_max_v: 1251.0\n",
      "lif layer 1 self.abs_max_v: 5516.0\n",
      "lif layer 1 self.abs_max_v: 5544.5\n",
      "lif layer 1 self.abs_max_v: 5568.5\n",
      "lif layer 1 self.abs_max_v: 5705.5\n",
      "lif layer 1 self.abs_max_v: 5873.5\n",
      "lif layer 1 self.abs_max_v: 5960.0\n",
      "lif layer 1 self.abs_max_v: 6000.0\n",
      "lif layer 1 self.abs_max_v: 6108.0\n",
      "lif layer 1 self.abs_max_v: 6177.0\n",
      "lif layer 2 self.abs_max_v: 1262.5\n",
      "lif layer 2 self.abs_max_v: 1299.5\n",
      "lif layer 2 self.abs_max_v: 1330.0\n",
      "lif layer 2 self.abs_max_v: 1380.0\n",
      "lif layer 2 self.abs_max_v: 1406.5\n",
      "lif layer 2 self.abs_max_v: 1465.5\n",
      "lif layer 2 self.abs_max_v: 1515.0\n",
      "fc layer 1 self.abs_max_out: 3485.0\n",
      "fc layer 3 self.abs_max_out: 255.0\n",
      "fc layer 1 self.abs_max_out: 3488.0\n",
      "fc layer 1 self.abs_max_out: 3609.0\n",
      "lif layer 2 self.abs_max_v: 1551.5\n",
      "lif layer 1 self.abs_max_v: 6199.0\n",
      "fc layer 2 self.abs_max_out: 1121.0\n",
      "fc layer 2 self.abs_max_out: 1138.0\n",
      "fc layer 1 self.abs_max_out: 3774.0\n",
      "lif layer 1 self.abs_max_v: 6541.0\n",
      "fc layer 1 self.abs_max_out: 3781.0\n",
      "lif layer 1 self.abs_max_v: 6835.5\n",
      "fc layer 1 self.abs_max_out: 3800.0\n",
      "fc layer 2 self.abs_max_out: 1208.0\n",
      "fc layer 2 self.abs_max_out: 1213.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.889779/  2.078068, val:  30.00%, val_best:  30.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6031%\n",
      "layer   2  Sparsity: 88.7829%\n",
      "layer   3  Sparsity: 88.8374%\n",
      "total_backward_count 19580 real_backward_count 3532  18.039%\n",
      "fc layer 1 self.abs_max_out: 3837.0\n",
      "fc layer 1 self.abs_max_out: 3905.0\n",
      "fc layer 2 self.abs_max_out: 1226.0\n",
      "fc layer 3 self.abs_max_out: 256.0\n",
      "fc layer 3 self.abs_max_out: 258.0\n",
      "fc layer 2 self.abs_max_out: 1287.0\n",
      "fc layer 2 self.abs_max_out: 1323.0\n",
      "fc layer 3 self.abs_max_out: 260.0\n",
      "fc layer 3 self.abs_max_out: 261.0\n",
      "lif layer 2 self.abs_max_v: 1555.5\n",
      "lif layer 2 self.abs_max_v: 1573.0\n",
      "lif layer 2 self.abs_max_v: 1579.0\n",
      "fc layer 3 self.abs_max_out: 267.0\n",
      "fc layer 1 self.abs_max_out: 4053.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.847726/  2.034358, val:  44.58%, val_best:  44.58%, tr:  99.39%, tr_best:  99.59%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5936%\n",
      "layer   2  Sparsity: 87.7891%\n",
      "layer   3  Sparsity: 88.4508%\n",
      "total_backward_count 29370 real_backward_count 5046  17.181%\n",
      "fc layer 1 self.abs_max_out: 4054.0\n",
      "fc layer 3 self.abs_max_out: 270.0\n",
      "fc layer 3 self.abs_max_out: 271.0\n",
      "fc layer 1 self.abs_max_out: 4232.0\n",
      "fc layer 1 self.abs_max_out: 4344.0\n",
      "fc layer 3 self.abs_max_out: 278.0\n",
      "lif layer 2 self.abs_max_v: 1618.0\n",
      "fc layer 3 self.abs_max_out: 282.0\n",
      "lif layer 1 self.abs_max_v: 7093.0\n",
      "lif layer 1 self.abs_max_v: 7273.5\n",
      "lif layer 2 self.abs_max_v: 1627.5\n",
      "lif layer 2 self.abs_max_v: 1687.5\n",
      "lif layer 1 self.abs_max_v: 7304.0\n",
      "fc layer 3 self.abs_max_out: 289.0\n",
      "fc layer 3 self.abs_max_out: 320.0\n",
      "lif layer 1 self.abs_max_v: 7378.0\n",
      "lif layer 1 self.abs_max_v: 7871.0\n",
      "lif layer 1 self.abs_max_v: 7963.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.830567/  2.012085, val:  39.17%, val_best:  44.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 80.09 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6108%\n",
      "layer   2  Sparsity: 87.5272%\n",
      "layer   3  Sparsity: 88.2255%\n",
      "total_backward_count 39160 real_backward_count 6497  16.591%\n",
      "fc layer 3 self.abs_max_out: 321.0\n",
      "fc layer 1 self.abs_max_out: 4378.0\n",
      "fc layer 1 self.abs_max_out: 4609.0\n",
      "fc layer 3 self.abs_max_out: 342.0\n",
      "lif layer 2 self.abs_max_v: 1692.5\n",
      "lif layer 2 self.abs_max_v: 1705.0\n",
      "fc layer 3 self.abs_max_out: 357.0\n",
      "lif layer 2 self.abs_max_v: 1757.5\n",
      "lif layer 2 self.abs_max_v: 1786.5\n",
      "lif layer 2 self.abs_max_v: 1790.5\n",
      "lif layer 2 self.abs_max_v: 1875.5\n",
      "lif layer 2 self.abs_max_v: 1924.0\n",
      "lif layer 2 self.abs_max_v: 1938.0\n",
      "lif layer 2 self.abs_max_v: 1947.0\n",
      "lif layer 2 self.abs_max_v: 1948.0\n",
      "lif layer 2 self.abs_max_v: 1998.0\n",
      "lif layer 2 self.abs_max_v: 2042.0\n",
      "lif layer 1 self.abs_max_v: 8152.5\n",
      "fc layer 2 self.abs_max_out: 1332.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.804332/  2.026335, val:  46.67%, val_best:  46.67%, tr:  99.08%, tr_best:  99.59%, epoch time: 79.79 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6041%\n",
      "layer   2  Sparsity: 86.6481%\n",
      "layer   3  Sparsity: 88.6581%\n",
      "total_backward_count 48950 real_backward_count 7932  16.204%\n",
      "fc layer 1 self.abs_max_out: 4672.0\n",
      "fc layer 2 self.abs_max_out: 1336.0\n",
      "fc layer 2 self.abs_max_out: 1350.0\n",
      "fc layer 1 self.abs_max_out: 4923.0\n",
      "fc layer 1 self.abs_max_out: 5048.0\n",
      "lif layer 1 self.abs_max_v: 8794.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.843292/  2.050894, val:  37.92%, val_best:  46.67%, tr:  99.39%, tr_best:  99.59%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6075%\n",
      "layer   2  Sparsity: 86.4237%\n",
      "layer   3  Sparsity: 89.1199%\n",
      "total_backward_count 58740 real_backward_count 9377  15.964%\n",
      "lif layer 2 self.abs_max_v: 2055.5\n",
      "fc layer 2 self.abs_max_out: 1382.0\n",
      "fc layer 2 self.abs_max_out: 1400.0\n",
      "fc layer 2 self.abs_max_out: 1435.0\n",
      "fc layer 2 self.abs_max_out: 1548.0\n",
      "lif layer 2 self.abs_max_v: 2425.5\n",
      "lif layer 2 self.abs_max_v: 2451.5\n",
      "fc layer 1 self.abs_max_out: 5359.0\n",
      "lif layer 1 self.abs_max_v: 8936.0\n",
      "lif layer 1 self.abs_max_v: 9027.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.813060/  2.012619, val:  45.83%, val_best:  46.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6100%\n",
      "layer   2  Sparsity: 85.9370%\n",
      "layer   3  Sparsity: 87.8620%\n",
      "total_backward_count 68530 real_backward_count 10748  15.684%\n",
      "fc layer 1 self.abs_max_out: 5404.0\n",
      "fc layer 3 self.abs_max_out: 359.0\n",
      "fc layer 3 self.abs_max_out: 364.0\n",
      "fc layer 3 self.abs_max_out: 370.0\n",
      "fc layer 3 self.abs_max_out: 374.0\n",
      "lif layer 1 self.abs_max_v: 9276.5\n",
      "lif layer 1 self.abs_max_v: 9562.5\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "fc layer 3 self.abs_max_out: 407.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.756826/  1.920773, val:  45.42%, val_best:  46.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6143%\n",
      "layer   2  Sparsity: 85.7827%\n",
      "layer   3  Sparsity: 87.4083%\n",
      "total_backward_count 78320 real_backward_count 12053  15.389%\n",
      "fc layer 2 self.abs_max_out: 1558.0\n",
      "fc layer 2 self.abs_max_out: 1609.0\n",
      "fc layer 2 self.abs_max_out: 1624.0\n",
      "fc layer 1 self.abs_max_out: 5809.0\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.704364/  1.934760, val:  53.75%, val_best:  53.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5946%\n",
      "layer   2  Sparsity: 85.4958%\n",
      "layer   3  Sparsity: 86.5739%\n",
      "total_backward_count 88110 real_backward_count 13393  15.200%\n",
      "fc layer 3 self.abs_max_out: 414.0\n",
      "fc layer 2 self.abs_max_out: 1625.0\n",
      "fc layer 2 self.abs_max_out: 1654.0\n",
      "lif layer 2 self.abs_max_v: 2478.0\n",
      "lif layer 2 self.abs_max_v: 2610.0\n",
      "fc layer 3 self.abs_max_out: 421.0\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.701831/  1.948092, val:  48.75%, val_best:  53.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.97 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6049%\n",
      "layer   2  Sparsity: 85.5457%\n",
      "layer   3  Sparsity: 87.1307%\n",
      "total_backward_count 97900 real_backward_count 14682  14.997%\n",
      "fc layer 3 self.abs_max_out: 434.0\n",
      "fc layer 2 self.abs_max_out: 1662.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.725903/  1.939702, val:  47.92%, val_best:  53.75%, tr:  99.18%, tr_best:  99.90%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6017%\n",
      "layer   2  Sparsity: 85.2934%\n",
      "layer   3  Sparsity: 86.9736%\n",
      "total_backward_count 107690 real_backward_count 15918  14.781%\n",
      "lif layer 1 self.abs_max_v: 9687.0\n",
      "lif layer 1 self.abs_max_v: 9947.5\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.714028/  1.914849, val:  49.17%, val_best:  53.75%, tr:  99.59%, tr_best:  99.90%, epoch time: 80.00 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5997%\n",
      "layer   2  Sparsity: 84.6986%\n",
      "layer   3  Sparsity: 86.5686%\n",
      "total_backward_count 117480 real_backward_count 17172  14.617%\n",
      "fc layer 2 self.abs_max_out: 1694.0\n",
      "fc layer 2 self.abs_max_out: 1799.0\n",
      "fc layer 2 self.abs_max_out: 1802.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.708008/  1.956067, val:  47.08%, val_best:  53.75%, tr:  99.39%, tr_best:  99.90%, epoch time: 79.94 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6106%\n",
      "layer   2  Sparsity: 85.0784%\n",
      "layer   3  Sparsity: 87.1441%\n",
      "total_backward_count 127270 real_backward_count 18403  14.460%\n",
      "lif layer 2 self.abs_max_v: 2700.5\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.660359/  1.895828, val:  45.83%, val_best:  53.75%, tr:  99.49%, tr_best:  99.90%, epoch time: 79.55 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5974%\n",
      "layer   2  Sparsity: 85.4383%\n",
      "layer   3  Sparsity: 85.9593%\n",
      "total_backward_count 137060 real_backward_count 19606  14.305%\n",
      "lif layer 2 self.abs_max_v: 2701.5\n",
      "lif layer 2 self.abs_max_v: 2731.5\n",
      "lif layer 2 self.abs_max_v: 2745.0\n",
      "lif layer 1 self.abs_max_v: 10010.0\n",
      "lif layer 1 self.abs_max_v: 10305.0\n",
      "lif layer 1 self.abs_max_v: 10604.5\n",
      "lif layer 1 self.abs_max_v: 10609.5\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.655922/  1.855364, val:  60.00%, val_best:  60.00%, tr:  99.28%, tr_best:  99.90%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6137%\n",
      "layer   2  Sparsity: 85.8404%\n",
      "layer   3  Sparsity: 86.1800%\n",
      "total_backward_count 146850 real_backward_count 20783  14.153%\n",
      "fc layer 3 self.abs_max_out: 440.0\n",
      "fc layer 3 self.abs_max_out: 458.0\n",
      "lif layer 2 self.abs_max_v: 2771.5\n",
      "lif layer 2 self.abs_max_v: 2921.0\n",
      "lif layer 2 self.abs_max_v: 3035.5\n",
      "lif layer 2 self.abs_max_v: 3104.5\n",
      "fc layer 1 self.abs_max_out: 6012.0\n",
      "lif layer 1 self.abs_max_v: 10940.0\n",
      "lif layer 1 self.abs_max_v: 11007.5\n",
      "lif layer 1 self.abs_max_v: 11327.0\n",
      "lif layer 1 self.abs_max_v: 11388.5\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.650699/  1.883214, val:  53.33%, val_best:  60.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6055%\n",
      "layer   2  Sparsity: 84.9846%\n",
      "layer   3  Sparsity: 86.0536%\n",
      "total_backward_count 156640 real_backward_count 22017  14.056%\n",
      "lif layer 2 self.abs_max_v: 3224.0\n",
      "lif layer 2 self.abs_max_v: 3255.0\n",
      "lif layer 2 self.abs_max_v: 3263.5\n",
      "fc layer 2 self.abs_max_out: 1897.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.652567/  1.898200, val:  49.17%, val_best:  60.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6104%\n",
      "layer   2  Sparsity: 85.0460%\n",
      "layer   3  Sparsity: 85.9943%\n",
      "total_backward_count 166430 real_backward_count 23176  13.925%\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.652865/  1.845887, val:  60.83%, val_best:  60.83%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6043%\n",
      "layer   2  Sparsity: 85.2728%\n",
      "layer   3  Sparsity: 86.5061%\n",
      "total_backward_count 176220 real_backward_count 24363  13.825%\n",
      "fc layer 3 self.abs_max_out: 464.0\n",
      "fc layer 3 self.abs_max_out: 486.0\n",
      "fc layer 3 self.abs_max_out: 494.0\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.606053/  1.853231, val:  46.25%, val_best:  60.83%, tr:  99.49%, tr_best:  99.90%, epoch time: 79.90 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5980%\n",
      "layer   2  Sparsity: 85.2699%\n",
      "layer   3  Sparsity: 85.0374%\n",
      "total_backward_count 186010 real_backward_count 25591  13.758%\n",
      "fc layer 1 self.abs_max_out: 6144.0\n",
      "fc layer 3 self.abs_max_out: 537.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.563582/  1.844426, val:  49.17%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6126%\n",
      "layer   2  Sparsity: 84.3782%\n",
      "layer   3  Sparsity: 83.9717%\n",
      "total_backward_count 195800 real_backward_count 26736  13.655%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.564158/  1.835235, val:  61.67%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6068%\n",
      "layer   2  Sparsity: 84.4550%\n",
      "layer   3  Sparsity: 85.2366%\n",
      "total_backward_count 205590 real_backward_count 27921  13.581%\n",
      "fc layer 1 self.abs_max_out: 6201.0\n",
      "lif layer 2 self.abs_max_v: 3392.5\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.593593/  1.876168, val:  55.83%, val_best:  61.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5946%\n",
      "layer   2  Sparsity: 85.2919%\n",
      "layer   3  Sparsity: 85.3173%\n",
      "total_backward_count 215380 real_backward_count 29118  13.519%\n",
      "lif layer 2 self.abs_max_v: 3465.0\n",
      "lif layer 2 self.abs_max_v: 3508.5\n",
      "fc layer 2 self.abs_max_out: 1898.0\n",
      "fc layer 2 self.abs_max_out: 1944.0\n",
      "fc layer 2 self.abs_max_out: 2023.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.607602/  1.811714, val:  57.08%, val_best:  61.67%, tr:  99.39%, tr_best:  99.90%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6115%\n",
      "layer   2  Sparsity: 85.2847%\n",
      "layer   3  Sparsity: 84.8805%\n",
      "total_backward_count 225170 real_backward_count 30269  13.443%\n",
      "fc layer 1 self.abs_max_out: 6222.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.556263/  1.828819, val:  60.00%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6158%\n",
      "layer   2  Sparsity: 84.8650%\n",
      "layer   3  Sparsity: 84.3114%\n",
      "total_backward_count 234960 real_backward_count 31436  13.379%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.546130/  1.806827, val:  49.58%, val_best:  61.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6079%\n",
      "layer   2  Sparsity: 85.0026%\n",
      "layer   3  Sparsity: 83.7332%\n",
      "total_backward_count 244750 real_backward_count 32611  13.324%\n",
      "fc layer 1 self.abs_max_out: 6279.0\n",
      "fc layer 1 self.abs_max_out: 6311.0\n",
      "fc layer 2 self.abs_max_out: 2024.0\n",
      "fc layer 1 self.abs_max_out: 6443.0\n",
      "fc layer 1 self.abs_max_out: 6897.0\n",
      "lif layer 1 self.abs_max_v: 11597.0\n",
      "lif layer 1 self.abs_max_v: 11688.5\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.534704/  1.822624, val:  52.92%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5954%\n",
      "layer   2  Sparsity: 84.8489%\n",
      "layer   3  Sparsity: 84.0849%\n",
      "total_backward_count 254540 real_backward_count 33853  13.300%\n",
      "fc layer 1 self.abs_max_out: 7066.0\n",
      "lif layer 1 self.abs_max_v: 11742.5\n",
      "fc layer 1 self.abs_max_out: 7067.0\n",
      "fc layer 1 self.abs_max_out: 7204.0\n",
      "lif layer 1 self.abs_max_v: 12469.0\n",
      "lif layer 1 self.abs_max_v: 12517.5\n",
      "fc layer 2 self.abs_max_out: 2085.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.534587/  1.805030, val:  59.58%, val_best:  61.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6014%\n",
      "layer   2  Sparsity: 85.2133%\n",
      "layer   3  Sparsity: 84.4466%\n",
      "total_backward_count 264330 real_backward_count 35010  13.245%\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.549018/  1.795710, val:  55.00%, val_best:  61.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5996%\n",
      "layer   2  Sparsity: 84.9580%\n",
      "layer   3  Sparsity: 84.1266%\n",
      "total_backward_count 274120 real_backward_count 36163  13.192%\n",
      "fc layer 3 self.abs_max_out: 562.0\n",
      "fc layer 3 self.abs_max_out: 566.0\n",
      "fc layer 3 self.abs_max_out: 570.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.512965/  1.781162, val:  57.92%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6007%\n",
      "layer   2  Sparsity: 84.9781%\n",
      "layer   3  Sparsity: 83.8227%\n",
      "total_backward_count 283910 real_backward_count 37330  13.149%\n",
      "fc layer 1 self.abs_max_out: 7327.0\n",
      "fc layer 1 self.abs_max_out: 7802.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.534234/  1.805847, val:  56.25%, val_best:  61.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6162%\n",
      "layer   2  Sparsity: 84.8064%\n",
      "layer   3  Sparsity: 84.7221%\n",
      "total_backward_count 293700 real_backward_count 38486  13.104%\n",
      "fc layer 1 self.abs_max_out: 8341.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.567507/  1.792162, val:  56.67%, val_best:  61.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6032%\n",
      "layer   2  Sparsity: 84.4722%\n",
      "layer   3  Sparsity: 85.5880%\n",
      "total_backward_count 303490 real_backward_count 39667  13.070%\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.529973/  1.775594, val:  52.92%, val_best:  61.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.85 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6019%\n",
      "layer   2  Sparsity: 84.4050%\n",
      "layer   3  Sparsity: 85.0131%\n",
      "total_backward_count 313280 real_backward_count 40804  13.025%\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.492941/  1.807484, val:  53.75%, val_best:  61.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6080%\n",
      "layer   2  Sparsity: 84.8087%\n",
      "layer   3  Sparsity: 83.6938%\n",
      "total_backward_count 323070 real_backward_count 41935  12.980%\n",
      "fc layer 2 self.abs_max_out: 2111.0\n",
      "fc layer 2 self.abs_max_out: 2146.0\n",
      "fc layer 3 self.abs_max_out: 573.0\n",
      "fc layer 3 self.abs_max_out: 610.0\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.506685/  1.781496, val:  52.08%, val_best:  61.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5944%\n",
      "layer   2  Sparsity: 84.9580%\n",
      "layer   3  Sparsity: 83.5902%\n",
      "total_backward_count 332860 real_backward_count 43022  12.925%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.456271/  1.726471, val:  56.25%, val_best:  61.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5873%\n",
      "layer   2  Sparsity: 84.6709%\n",
      "layer   3  Sparsity: 83.9486%\n",
      "total_backward_count 342650 real_backward_count 44126  12.878%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.468651/  1.753016, val:  60.00%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6062%\n",
      "layer   2  Sparsity: 84.4579%\n",
      "layer   3  Sparsity: 84.6492%\n",
      "total_backward_count 352440 real_backward_count 45258  12.841%\n",
      "lif layer 1 self.abs_max_v: 13171.0\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.493570/  1.747692, val:  61.25%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.87 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5964%\n",
      "layer   2  Sparsity: 84.4281%\n",
      "layer   3  Sparsity: 84.5799%\n",
      "total_backward_count 362230 real_backward_count 46368  12.801%\n",
      "lif layer 1 self.abs_max_v: 13776.0\n",
      "fc layer 3 self.abs_max_out: 625.0\n",
      "fc layer 3 self.abs_max_out: 629.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.454982/  1.765170, val:  48.75%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6003%\n",
      "layer   2  Sparsity: 83.8042%\n",
      "layer   3  Sparsity: 83.6339%\n",
      "total_backward_count 372020 real_backward_count 47458  12.757%\n",
      "lif layer 2 self.abs_max_v: 3540.5\n",
      "lif layer 2 self.abs_max_v: 3578.5\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.441618/  1.707521, val:  61.25%, val_best:  61.67%, tr:  99.49%, tr_best:  99.90%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6050%\n",
      "layer   2  Sparsity: 82.9735%\n",
      "layer   3  Sparsity: 82.7377%\n",
      "total_backward_count 381810 real_backward_count 48625  12.735%\n",
      "fc layer 2 self.abs_max_out: 2166.0\n",
      "fc layer 2 self.abs_max_out: 2276.0\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.384484/  1.731179, val:  54.58%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6038%\n",
      "layer   2  Sparsity: 82.5098%\n",
      "layer   3  Sparsity: 82.0812%\n",
      "total_backward_count 391600 real_backward_count 49700  12.692%\n",
      "lif layer 2 self.abs_max_v: 3582.5\n",
      "lif layer 2 self.abs_max_v: 3624.5\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.379806/  1.681773, val:  62.08%, val_best:  62.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6084%\n",
      "layer   2  Sparsity: 82.2306%\n",
      "layer   3  Sparsity: 82.6000%\n",
      "total_backward_count 401390 real_backward_count 50788  12.653%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.371546/  1.708216, val:  57.92%, val_best:  62.08%, tr:  99.59%, tr_best:  99.90%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6097%\n",
      "layer   2  Sparsity: 82.5290%\n",
      "layer   3  Sparsity: 81.8155%\n",
      "total_backward_count 411180 real_backward_count 51824  12.604%\n",
      "lif layer 2 self.abs_max_v: 3641.5\n",
      "fc layer 3 self.abs_max_out: 635.0\n",
      "lif layer 2 self.abs_max_v: 3777.5\n",
      "fc layer 3 self.abs_max_out: 636.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.367356/  1.691909, val:  58.75%, val_best:  62.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 79.72 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6109%\n",
      "layer   2  Sparsity: 82.9223%\n",
      "layer   3  Sparsity: 82.0537%\n",
      "total_backward_count 420970 real_backward_count 52846  12.553%\n",
      "lif layer 1 self.abs_max_v: 13932.0\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.388332/  1.690716, val:  61.25%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6050%\n",
      "layer   2  Sparsity: 83.2710%\n",
      "layer   3  Sparsity: 81.9863%\n",
      "total_backward_count 430760 real_backward_count 53908  12.515%\n",
      "lif layer 2 self.abs_max_v: 3930.0\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.357492/  1.651250, val:  60.42%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.57 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6038%\n",
      "layer   2  Sparsity: 83.1503%\n",
      "layer   3  Sparsity: 81.9459%\n",
      "total_backward_count 440550 real_backward_count 54989  12.482%\n",
      "lif layer 2 self.abs_max_v: 3998.5\n",
      "lif layer 1 self.abs_max_v: 14101.0\n",
      "lif layer 1 self.abs_max_v: 14306.5\n",
      "lif layer 1 self.abs_max_v: 14423.5\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.334192/  1.637778, val:  55.00%, val_best:  62.08%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6096%\n",
      "layer   2  Sparsity: 83.0919%\n",
      "layer   3  Sparsity: 80.8609%\n",
      "total_backward_count 450340 real_backward_count 56072  12.451%\n",
      "lif layer 1 self.abs_max_v: 14746.0\n",
      "lif layer 1 self.abs_max_v: 15260.0\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.311891/  1.686574, val:  60.00%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5935%\n",
      "layer   2  Sparsity: 83.5468%\n",
      "layer   3  Sparsity: 80.4049%\n",
      "total_backward_count 460130 real_backward_count 57191  12.429%\n",
      "fc layer 2 self.abs_max_out: 2310.0\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.353235/  1.685757, val:  52.50%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6042%\n",
      "layer   2  Sparsity: 83.4052%\n",
      "layer   3  Sparsity: 80.9347%\n",
      "total_backward_count 469920 real_backward_count 58272  12.400%\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.304750/  1.662926, val:  60.00%, val_best:  62.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6003%\n",
      "layer   2  Sparsity: 83.3033%\n",
      "layer   3  Sparsity: 80.6243%\n",
      "total_backward_count 479710 real_backward_count 59319  12.366%\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.317271/  1.620122, val:  61.67%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6069%\n",
      "layer   2  Sparsity: 83.5694%\n",
      "layer   3  Sparsity: 81.1174%\n",
      "total_backward_count 489500 real_backward_count 60415  12.342%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.297581/  1.615644, val:  56.25%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.5960%\n",
      "layer   2  Sparsity: 83.7850%\n",
      "layer   3  Sparsity: 80.5454%\n",
      "total_backward_count 499290 real_backward_count 61492  12.316%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.287438/  1.650545, val:  55.83%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.97 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6061%\n",
      "layer   2  Sparsity: 84.3055%\n",
      "layer   3  Sparsity: 80.8680%\n",
      "total_backward_count 509080 real_backward_count 62542  12.285%\n",
      "fc layer 3 self.abs_max_out: 638.0\n",
      "fc layer 3 self.abs_max_out: 642.0\n",
      "fc layer 3 self.abs_max_out: 648.0\n",
      "fc layer 3 self.abs_max_out: 669.0\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.271700/  1.611618, val:  59.17%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6085%\n",
      "layer   2  Sparsity: 84.1054%\n",
      "layer   3  Sparsity: 80.7870%\n",
      "total_backward_count 518870 real_backward_count 63628  12.263%\n",
      "lif layer 2 self.abs_max_v: 4012.5\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.269439/  1.630886, val:  55.42%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6019%\n",
      "layer   2  Sparsity: 83.8601%\n",
      "layer   3  Sparsity: 80.3160%\n",
      "total_backward_count 528660 real_backward_count 64679  12.235%\n",
      "lif layer 2 self.abs_max_v: 4028.5\n",
      "fc layer 3 self.abs_max_out: 679.0\n",
      "fc layer 3 self.abs_max_out: 712.0\n",
      "fc layer 3 self.abs_max_out: 716.0\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.285257/  1.592789, val:  60.00%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6070%\n",
      "layer   2  Sparsity: 83.7872%\n",
      "layer   3  Sparsity: 80.1527%\n",
      "total_backward_count 538450 real_backward_count 65738  12.209%\n",
      "lif layer 2 self.abs_max_v: 4266.0\n",
      "lif layer 2 self.abs_max_v: 4334.0\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.282529/  1.579786, val:  62.50%, val_best:  62.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6105%\n",
      "layer   2  Sparsity: 83.3594%\n",
      "layer   3  Sparsity: 79.9713%\n",
      "total_backward_count 548240 real_backward_count 66783  12.181%\n",
      "fc layer 3 self.abs_max_out: 726.0\n",
      "fc layer 3 self.abs_max_out: 746.0\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.224895/  1.601934, val:  53.75%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6079%\n",
      "layer   2  Sparsity: 82.9409%\n",
      "layer   3  Sparsity: 79.8049%\n",
      "total_backward_count 558030 real_backward_count 67844  12.158%\n",
      "fc layer 1 self.abs_max_out: 8713.0\n",
      "fc layer 2 self.abs_max_out: 2315.0\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.206187/  1.542425, val:  61.67%, val_best:  62.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6042%\n",
      "layer   2  Sparsity: 82.3017%\n",
      "layer   3  Sparsity: 79.3553%\n",
      "total_backward_count 567820 real_backward_count 68860  12.127%\n",
      "lif layer 2 self.abs_max_v: 4391.5\n",
      "fc layer 3 self.abs_max_out: 748.0\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.170256/  1.555605, val:  55.83%, val_best:  62.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5985%\n",
      "layer   2  Sparsity: 82.4877%\n",
      "layer   3  Sparsity: 78.0731%\n",
      "total_backward_count 577610 real_backward_count 69849  12.093%\n",
      "fc layer 3 self.abs_max_out: 774.0\n",
      "fc layer 3 self.abs_max_out: 785.0\n",
      "fc layer 1 self.abs_max_out: 9078.0\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.189196/  1.585475, val:  55.42%, val_best:  62.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6044%\n",
      "layer   2  Sparsity: 82.3125%\n",
      "layer   3  Sparsity: 78.2425%\n",
      "total_backward_count 587400 real_backward_count 70859  12.063%\n",
      "fc layer 3 self.abs_max_out: 858.0\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.193381/  1.595438, val:  60.00%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5991%\n",
      "layer   2  Sparsity: 82.5312%\n",
      "layer   3  Sparsity: 79.5627%\n",
      "total_backward_count 597190 real_backward_count 71892  12.038%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.202014/  1.546800, val:  58.75%, val_best:  62.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.5978%\n",
      "layer   2  Sparsity: 82.7989%\n",
      "layer   3  Sparsity: 80.0363%\n",
      "total_backward_count 606980 real_backward_count 72908  12.012%\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.188443/  1.600757, val:  58.33%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6045%\n",
      "layer   2  Sparsity: 82.9111%\n",
      "layer   3  Sparsity: 79.0549%\n",
      "total_backward_count 616770 real_backward_count 73909  11.983%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.177802/  1.563281, val:  55.00%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6032%\n",
      "layer   2  Sparsity: 82.9412%\n",
      "layer   3  Sparsity: 79.3012%\n",
      "total_backward_count 626560 real_backward_count 74874  11.950%\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.174399/  1.577012, val:  53.33%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6090%\n",
      "layer   2  Sparsity: 83.0845%\n",
      "layer   3  Sparsity: 79.1454%\n",
      "total_backward_count 636350 real_backward_count 75851  11.920%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.156476/  1.591811, val:  56.67%, val_best:  62.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5969%\n",
      "layer   2  Sparsity: 83.5555%\n",
      "layer   3  Sparsity: 79.8678%\n",
      "total_backward_count 646140 real_backward_count 76827  11.890%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.180445/  1.588394, val:  57.08%, val_best:  62.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6012%\n",
      "layer   2  Sparsity: 83.4963%\n",
      "layer   3  Sparsity: 80.5911%\n",
      "total_backward_count 655930 real_backward_count 77788  11.859%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.216360/  1.606676, val:  52.08%, val_best:  62.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5966%\n",
      "layer   2  Sparsity: 83.3282%\n",
      "layer   3  Sparsity: 79.7411%\n",
      "total_backward_count 665720 real_backward_count 78774  11.833%\n",
      "fc layer 2 self.abs_max_out: 2322.0\n",
      "fc layer 1 self.abs_max_out: 9138.0\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.202978/  1.556304, val:  64.58%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5918%\n",
      "layer   2  Sparsity: 82.8111%\n",
      "layer   3  Sparsity: 79.9357%\n",
      "total_backward_count 675510 real_backward_count 79758  11.807%\n",
      "fc layer 1 self.abs_max_out: 9278.0\n",
      "fc layer 1 self.abs_max_out: 9328.0\n",
      "fc layer 2 self.abs_max_out: 2331.0\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.205482/  1.606979, val:  57.92%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6083%\n",
      "layer   2  Sparsity: 82.6281%\n",
      "layer   3  Sparsity: 80.8855%\n",
      "total_backward_count 685300 real_backward_count 80745  11.782%\n",
      "fc layer 1 self.abs_max_out: 10004.0\n",
      "fc layer 2 self.abs_max_out: 2375.0\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.197634/  1.613864, val:  55.83%, val_best:  64.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5995%\n",
      "layer   2  Sparsity: 82.6667%\n",
      "layer   3  Sparsity: 80.5516%\n",
      "total_backward_count 695090 real_backward_count 81702  11.754%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.212609/  1.593154, val:  55.00%, val_best:  64.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5969%\n",
      "layer   2  Sparsity: 83.1177%\n",
      "layer   3  Sparsity: 81.0011%\n",
      "total_backward_count 704880 real_backward_count 82729  11.737%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.209228/  1.503084, val:  67.50%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.5988%\n",
      "layer   2  Sparsity: 83.4845%\n",
      "layer   3  Sparsity: 80.8982%\n",
      "total_backward_count 714670 real_backward_count 83717  11.714%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.181477/  1.553573, val:  57.92%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5894%\n",
      "layer   2  Sparsity: 83.7383%\n",
      "layer   3  Sparsity: 80.9006%\n",
      "total_backward_count 724460 real_backward_count 84681  11.689%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.183381/  1.581436, val:  56.25%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.10 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6046%\n",
      "layer   2  Sparsity: 83.7484%\n",
      "layer   3  Sparsity: 81.5242%\n",
      "total_backward_count 734250 real_backward_count 85723  11.675%\n",
      "fc layer 1 self.abs_max_out: 10158.0\n",
      "lif layer 1 self.abs_max_v: 15330.5\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.196975/  1.556865, val:  62.08%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5943%\n",
      "layer   2  Sparsity: 83.7626%\n",
      "layer   3  Sparsity: 80.9550%\n",
      "total_backward_count 744040 real_backward_count 86738  11.658%\n",
      "lif layer 1 self.abs_max_v: 15752.5\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.187492/  1.525763, val:  62.08%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6128%\n",
      "layer   2  Sparsity: 84.1342%\n",
      "layer   3  Sparsity: 80.7581%\n",
      "total_backward_count 753830 real_backward_count 87723  11.637%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.169119/  1.552159, val:  55.83%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6009%\n",
      "layer   2  Sparsity: 83.6288%\n",
      "layer   3  Sparsity: 80.8717%\n",
      "total_backward_count 763620 real_backward_count 88687  11.614%\n",
      "lif layer 1 self.abs_max_v: 16521.0\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.165461/  1.582607, val:  53.75%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6027%\n",
      "layer   2  Sparsity: 83.8434%\n",
      "layer   3  Sparsity: 80.5542%\n",
      "total_backward_count 773410 real_backward_count 89682  11.596%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.198737/  1.563861, val:  60.83%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6066%\n",
      "layer   2  Sparsity: 83.8400%\n",
      "layer   3  Sparsity: 80.3537%\n",
      "total_backward_count 783200 real_backward_count 90632  11.572%\n",
      "lif layer 1 self.abs_max_v: 16674.0\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.233420/  1.577519, val:  65.42%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6153%\n",
      "layer   2  Sparsity: 83.6898%\n",
      "layer   3  Sparsity: 80.9113%\n",
      "total_backward_count 792990 real_backward_count 91613  11.553%\n",
      "lif layer 1 self.abs_max_v: 16692.0\n",
      "lif layer 1 self.abs_max_v: 16888.0\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.239834/  1.645508, val:  50.00%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6090%\n",
      "layer   2  Sparsity: 83.6654%\n",
      "layer   3  Sparsity: 81.6867%\n",
      "total_backward_count 802780 real_backward_count 92558  11.530%\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.239669/  1.604774, val:  56.25%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5994%\n",
      "layer   2  Sparsity: 83.6637%\n",
      "layer   3  Sparsity: 81.3883%\n",
      "total_backward_count 812570 real_backward_count 93576  11.516%\n",
      "lif layer 2 self.abs_max_v: 4400.0\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.229865/  1.607922, val:  57.50%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6142%\n",
      "layer   2  Sparsity: 83.5855%\n",
      "layer   3  Sparsity: 81.7840%\n",
      "total_backward_count 822360 real_backward_count 94599  11.503%\n",
      "lif layer 1 self.abs_max_v: 17517.0\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.226657/  1.608107, val:  58.33%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.5980%\n",
      "layer   2  Sparsity: 83.5559%\n",
      "layer   3  Sparsity: 81.7228%\n",
      "total_backward_count 832150 real_backward_count 95614  11.490%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.217558/  1.585436, val:  55.00%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6071%\n",
      "layer   2  Sparsity: 83.5820%\n",
      "layer   3  Sparsity: 80.8305%\n",
      "total_backward_count 841940 real_backward_count 96663  11.481%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.208871/  1.575015, val:  55.83%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6130%\n",
      "layer   2  Sparsity: 84.2467%\n",
      "layer   3  Sparsity: 80.9723%\n",
      "total_backward_count 851730 real_backward_count 97677  11.468%\n",
      "fc layer 1 self.abs_max_out: 10304.0\n",
      "lif layer 1 self.abs_max_v: 17675.5\n",
      "lif layer 1 self.abs_max_v: 18058.0\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.184265/  1.576907, val:  57.92%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6002%\n",
      "layer   2  Sparsity: 84.3677%\n",
      "layer   3  Sparsity: 80.2907%\n",
      "total_backward_count 861520 real_backward_count 98692  11.456%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.175621/  1.554340, val:  58.75%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6212%\n",
      "layer   2  Sparsity: 83.6924%\n",
      "layer   3  Sparsity: 80.8295%\n",
      "total_backward_count 871310 real_backward_count 99680  11.440%\n",
      "fc layer 1 self.abs_max_out: 10477.0\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.170978/  1.532923, val:  53.75%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6000%\n",
      "layer   2  Sparsity: 83.4167%\n",
      "layer   3  Sparsity: 79.6381%\n",
      "total_backward_count 881100 real_backward_count 100659  11.424%\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.169464/  1.564452, val:  55.42%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6069%\n",
      "layer   2  Sparsity: 83.5766%\n",
      "layer   3  Sparsity: 79.9590%\n",
      "total_backward_count 890890 real_backward_count 101662  11.411%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.173257/  1.526710, val:  64.17%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6084%\n",
      "layer   2  Sparsity: 83.5509%\n",
      "layer   3  Sparsity: 80.7903%\n",
      "total_backward_count 900680 real_backward_count 102650  11.397%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.174545/  1.551183, val:  60.83%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6076%\n",
      "layer   2  Sparsity: 83.5887%\n",
      "layer   3  Sparsity: 80.4993%\n",
      "total_backward_count 910470 real_backward_count 103626  11.382%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.173298/  1.558365, val:  57.50%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6091%\n",
      "layer   2  Sparsity: 83.5133%\n",
      "layer   3  Sparsity: 80.4059%\n",
      "total_backward_count 920260 real_backward_count 104635  11.370%\n",
      "lif layer 1 self.abs_max_v: 18267.5\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.154184/  1.558320, val:  58.33%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5990%\n",
      "layer   2  Sparsity: 83.2753%\n",
      "layer   3  Sparsity: 79.8372%\n",
      "total_backward_count 930050 real_backward_count 105620  11.356%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.159381/  1.552805, val:  56.25%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6060%\n",
      "layer   2  Sparsity: 83.1865%\n",
      "layer   3  Sparsity: 80.5651%\n",
      "total_backward_count 939840 real_backward_count 106593  11.342%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.174729/  1.658608, val:  52.92%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6071%\n",
      "layer   2  Sparsity: 83.4826%\n",
      "layer   3  Sparsity: 80.4249%\n",
      "total_backward_count 949630 real_backward_count 107530  11.323%\n",
      "fc layer 2 self.abs_max_out: 2418.0\n",
      "lif layer 2 self.abs_max_v: 4575.5\n",
      "lif layer 2 self.abs_max_v: 4579.0\n",
      "fc layer 2 self.abs_max_out: 2429.0\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.170154/  1.512285, val:  66.25%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5947%\n",
      "layer   2  Sparsity: 83.7194%\n",
      "layer   3  Sparsity: 79.8624%\n",
      "total_backward_count 959420 real_backward_count 108480  11.307%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.123625/  1.557438, val:  62.92%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6037%\n",
      "layer   2  Sparsity: 83.5010%\n",
      "layer   3  Sparsity: 79.2271%\n",
      "total_backward_count 969210 real_backward_count 109385  11.286%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.141788/  1.527360, val:  62.50%, val_best:  67.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6048%\n",
      "layer   2  Sparsity: 83.6440%\n",
      "layer   3  Sparsity: 79.3762%\n",
      "total_backward_count 979000 real_backward_count 110333  11.270%\n",
      "lif layer 2 self.abs_max_v: 4589.0\n",
      "lif layer 2 self.abs_max_v: 4688.5\n",
      "lif layer 2 self.abs_max_v: 4738.5\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.122527/  1.550736, val:  56.25%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6058%\n",
      "layer   2  Sparsity: 83.7749%\n",
      "layer   3  Sparsity: 79.6077%\n",
      "total_backward_count 988790 real_backward_count 111233  11.249%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.099791/  1.499291, val:  61.67%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6039%\n",
      "layer   2  Sparsity: 83.8368%\n",
      "layer   3  Sparsity: 78.3750%\n",
      "total_backward_count 998580 real_backward_count 112175  11.233%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.084411/  1.516321, val:  58.33%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5944%\n",
      "layer   2  Sparsity: 83.7760%\n",
      "layer   3  Sparsity: 78.6708%\n",
      "total_backward_count 1008370 real_backward_count 113146  11.221%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.095970/  1.484311, val:  65.00%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6021%\n",
      "layer   2  Sparsity: 83.6657%\n",
      "layer   3  Sparsity: 79.2407%\n",
      "total_backward_count 1018160 real_backward_count 114074  11.204%\n",
      "fc layer 2 self.abs_max_out: 2494.0\n",
      "lif layer 2 self.abs_max_v: 4775.0\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.109638/  1.504407, val:  64.17%, val_best:  67.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 80.12 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 86.6129%\n",
      "layer   2  Sparsity: 83.5491%\n",
      "layer   3  Sparsity: 79.2844%\n",
      "total_backward_count 1027950 real_backward_count 115000  11.187%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.098015/  1.511916, val:  63.75%, val_best:  67.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 78.40 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6056%\n",
      "layer   2  Sparsity: 82.9682%\n",
      "layer   3  Sparsity: 79.3521%\n",
      "total_backward_count 1037740 real_backward_count 115997  11.178%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (HTTPError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc layer 2 self.abs_max_out: 2538.0\n",
      "fc layer 2 self.abs_max_out: 2546.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.098141/  1.539505, val:  53.33%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.5988%\n",
      "layer   2  Sparsity: 83.0836%\n",
      "layer   3  Sparsity: 79.6188%\n",
      "total_backward_count 1047530 real_backward_count 116968  11.166%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: context deadline exceeded (<Response [500]>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.098483/  1.466608, val:  57.08%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.6081%\n",
      "layer   2  Sparsity: 83.4249%\n",
      "layer   3  Sparsity: 79.6799%\n",
      "total_backward_count 1057320 real_backward_count 117979  11.158%\n",
      "lif layer 2 self.abs_max_v: 4829.0\n",
      "fc layer 2 self.abs_max_out: 2556.0\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.064649/  1.466807, val:  66.67%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5958%\n",
      "layer   2  Sparsity: 83.3514%\n",
      "layer   3  Sparsity: 79.7942%\n",
      "total_backward_count 1067110 real_backward_count 118874  11.140%\n",
      "fc layer 2 self.abs_max_out: 2642.0\n",
      "fc layer 2 self.abs_max_out: 2780.0\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.093549/  1.514276, val:  60.83%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6096%\n",
      "layer   2  Sparsity: 83.2464%\n",
      "layer   3  Sparsity: 80.2492%\n",
      "total_backward_count 1076900 real_backward_count 119808  11.125%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.085870/  1.494001, val:  62.92%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5987%\n",
      "layer   2  Sparsity: 83.1651%\n",
      "layer   3  Sparsity: 80.2399%\n",
      "total_backward_count 1086690 real_backward_count 120777  11.114%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.083335/  1.484666, val:  60.00%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6003%\n",
      "layer   2  Sparsity: 83.2205%\n",
      "layer   3  Sparsity: 80.2425%\n",
      "total_backward_count 1096480 real_backward_count 121723  11.101%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.076571/  1.431489, val:  65.83%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.6112%\n",
      "layer   2  Sparsity: 82.8790%\n",
      "layer   3  Sparsity: 79.4017%\n",
      "total_backward_count 1106270 real_backward_count 122695  11.091%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.072389/  1.508729, val:  56.67%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6085%\n",
      "layer   2  Sparsity: 82.8144%\n",
      "layer   3  Sparsity: 79.3200%\n",
      "total_backward_count 1116060 real_backward_count 123639  11.078%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.034680/  1.492415, val:  52.92%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5990%\n",
      "layer   2  Sparsity: 82.6632%\n",
      "layer   3  Sparsity: 78.5772%\n",
      "total_backward_count 1125850 real_backward_count 124551  11.063%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.036305/  1.457302, val:  57.50%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5976%\n",
      "layer   2  Sparsity: 82.6052%\n",
      "layer   3  Sparsity: 78.1555%\n",
      "total_backward_count 1135640 real_backward_count 125445  11.046%\n",
      "fc layer 3 self.abs_max_out: 906.0\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.032711/  1.458669, val:  62.50%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6023%\n",
      "layer   2  Sparsity: 82.9631%\n",
      "layer   3  Sparsity: 78.6167%\n",
      "total_backward_count 1145430 real_backward_count 126377  11.033%\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.013953/  1.460795, val:  62.92%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6034%\n",
      "layer   2  Sparsity: 83.0846%\n",
      "layer   3  Sparsity: 78.8203%\n",
      "total_backward_count 1155220 real_backward_count 127294  11.019%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.015966/  1.429112, val:  61.67%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6125%\n",
      "layer   2  Sparsity: 83.0869%\n",
      "layer   3  Sparsity: 78.9311%\n",
      "total_backward_count 1165010 real_backward_count 128216  11.006%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.040518/  1.425343, val:  62.08%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5912%\n",
      "layer   2  Sparsity: 83.0015%\n",
      "layer   3  Sparsity: 77.7565%\n",
      "total_backward_count 1174800 real_backward_count 129155  10.994%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.042354/  1.469197, val:  61.67%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5911%\n",
      "layer   2  Sparsity: 82.7441%\n",
      "layer   3  Sparsity: 77.2938%\n",
      "total_backward_count 1184590 real_backward_count 130038  10.977%\n",
      "fc layer 1 self.abs_max_out: 10561.0\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  0.996464/  1.503900, val:  57.08%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.42 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6078%\n",
      "layer   2  Sparsity: 82.9264%\n",
      "layer   3  Sparsity: 76.7604%\n",
      "total_backward_count 1194380 real_backward_count 130939  10.963%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  0.997105/  1.457587, val:  67.92%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6043%\n",
      "layer   2  Sparsity: 82.6107%\n",
      "layer   3  Sparsity: 77.3481%\n",
      "total_backward_count 1204170 real_backward_count 131826  10.947%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.023964/  1.424444, val:  65.00%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6013%\n",
      "layer   2  Sparsity: 82.4300%\n",
      "layer   3  Sparsity: 77.7063%\n",
      "total_backward_count 1213960 real_backward_count 132764  10.936%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.010416/  1.410776, val:  66.67%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5958%\n",
      "layer   2  Sparsity: 82.7263%\n",
      "layer   3  Sparsity: 78.7057%\n",
      "total_backward_count 1223750 real_backward_count 133715  10.927%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.002260/  1.442613, val:  60.83%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6010%\n",
      "layer   2  Sparsity: 82.9277%\n",
      "layer   3  Sparsity: 78.1194%\n",
      "total_backward_count 1233540 real_backward_count 134626  10.914%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  0.998842/  1.498335, val:  64.17%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6051%\n",
      "layer   2  Sparsity: 82.5927%\n",
      "layer   3  Sparsity: 78.0734%\n",
      "total_backward_count 1243330 real_backward_count 135523  10.900%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.018882/  1.445819, val:  63.33%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5966%\n",
      "layer   2  Sparsity: 81.9584%\n",
      "layer   3  Sparsity: 77.5953%\n",
      "total_backward_count 1253120 real_backward_count 136431  10.887%\n",
      "lif layer 1 self.abs_max_v: 18548.5\n",
      "fc layer 1 self.abs_max_out: 10725.0\n",
      "lif layer 1 self.abs_max_v: 18669.5\n",
      "lif layer 1 self.abs_max_v: 18674.0\n",
      "lif layer 1 self.abs_max_v: 18747.5\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.015282/  1.445701, val:  59.58%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6068%\n",
      "layer   2  Sparsity: 82.4130%\n",
      "layer   3  Sparsity: 77.6788%\n",
      "total_backward_count 1262910 real_backward_count 137341  10.875%\n",
      "fc layer 1 self.abs_max_out: 10776.0\n",
      "lif layer 1 self.abs_max_v: 19108.5\n",
      "fc layer 1 self.abs_max_out: 11262.0\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.020805/  1.450710, val:  60.42%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6066%\n",
      "layer   2  Sparsity: 82.0885%\n",
      "layer   3  Sparsity: 77.6648%\n",
      "total_backward_count 1272700 real_backward_count 138260  10.864%\n",
      "lif layer 1 self.abs_max_v: 19171.5\n",
      "lif layer 1 self.abs_max_v: 19257.0\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.002721/  1.483698, val:  60.42%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6158%\n",
      "layer   2  Sparsity: 82.1207%\n",
      "layer   3  Sparsity: 77.6026%\n",
      "total_backward_count 1282490 real_backward_count 139144  10.850%\n",
      "lif layer 1 self.abs_max_v: 19432.0\n",
      "fc layer 3 self.abs_max_out: 915.0\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  0.990867/  1.434138, val:  61.67%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6141%\n",
      "layer   2  Sparsity: 82.0736%\n",
      "layer   3  Sparsity: 76.4957%\n",
      "total_backward_count 1292280 real_backward_count 140118  10.843%\n",
      "lif layer 1 self.abs_max_v: 19631.5\n",
      "lif layer 1 self.abs_max_v: 20742.0\n",
      "fc layer 1 self.abs_max_out: 11567.0\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  0.972889/  1.444131, val:  62.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6055%\n",
      "layer   2  Sparsity: 82.0060%\n",
      "layer   3  Sparsity: 76.8653%\n",
      "total_backward_count 1302070 real_backward_count 140995  10.829%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  0.959462/  1.430213, val:  60.83%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5972%\n",
      "layer   2  Sparsity: 82.2301%\n",
      "layer   3  Sparsity: 76.6807%\n",
      "total_backward_count 1311860 real_backward_count 141887  10.816%\n",
      "fc layer 3 self.abs_max_out: 946.0\n",
      "fc layer 3 self.abs_max_out: 964.0\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  0.930781/  1.415323, val:  58.75%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6033%\n",
      "layer   2  Sparsity: 82.3232%\n",
      "layer   3  Sparsity: 76.8715%\n",
      "total_backward_count 1321650 real_backward_count 142782  10.803%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  0.935314/  1.382509, val:  63.33%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6024%\n",
      "layer   2  Sparsity: 82.1647%\n",
      "layer   3  Sparsity: 76.4666%\n",
      "total_backward_count 1331440 real_backward_count 143725  10.795%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  0.933451/  1.380737, val:  64.58%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.6042%\n",
      "layer   2  Sparsity: 81.9283%\n",
      "layer   3  Sparsity: 75.3770%\n",
      "total_backward_count 1341230 real_backward_count 144600  10.781%\n",
      "lif layer 1 self.abs_max_v: 21399.5\n",
      "lif layer 1 self.abs_max_v: 21766.0\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  0.895345/  1.388322, val:  55.00%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.6211%\n",
      "layer   2  Sparsity: 82.0296%\n",
      "layer   3  Sparsity: 75.5594%\n",
      "total_backward_count 1351020 real_backward_count 145502  10.770%\n",
      "fc layer 1 self.abs_max_out: 12462.0\n",
      "lif layer 1 self.abs_max_v: 21874.0\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  0.915450/  1.391813, val:  60.00%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5884%\n",
      "layer   2  Sparsity: 82.1314%\n",
      "layer   3  Sparsity: 75.9845%\n",
      "total_backward_count 1360810 real_backward_count 146427  10.760%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  0.918177/  1.375825, val:  68.75%, val_best:  68.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6108%\n",
      "layer   2  Sparsity: 82.5002%\n",
      "layer   3  Sparsity: 76.5727%\n",
      "total_backward_count 1370600 real_backward_count 147392  10.754%\n",
      "lif layer 1 self.abs_max_v: 22300.0\n",
      "lif layer 1 self.abs_max_v: 22317.5\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  0.944758/  1.439466, val:  58.75%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6016%\n",
      "layer   2  Sparsity: 82.5819%\n",
      "layer   3  Sparsity: 77.2069%\n",
      "total_backward_count 1380390 real_backward_count 148305  10.744%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  0.962527/  1.398622, val:  59.58%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6030%\n",
      "layer   2  Sparsity: 82.3626%\n",
      "layer   3  Sparsity: 76.1540%\n",
      "total_backward_count 1390180 real_backward_count 149225  10.734%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  0.951572/  1.366593, val:  63.75%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6090%\n",
      "layer   2  Sparsity: 82.2273%\n",
      "layer   3  Sparsity: 76.2594%\n",
      "total_backward_count 1399970 real_backward_count 150176  10.727%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  0.946225/  1.422522, val:  53.75%, val_best:  68.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5954%\n",
      "layer   2  Sparsity: 82.3935%\n",
      "layer   3  Sparsity: 77.0042%\n",
      "total_backward_count 1409760 real_backward_count 151077  10.717%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  0.958700/  1.425356, val:  58.75%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5983%\n",
      "layer   2  Sparsity: 82.2534%\n",
      "layer   3  Sparsity: 77.0171%\n",
      "total_backward_count 1419550 real_backward_count 151930  10.703%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  0.923609/  1.412118, val:  57.08%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5967%\n",
      "layer   2  Sparsity: 82.1861%\n",
      "layer   3  Sparsity: 76.0968%\n",
      "total_backward_count 1429340 real_backward_count 152840  10.693%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  0.931509/  1.417931, val:  62.50%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6102%\n",
      "layer   2  Sparsity: 82.4143%\n",
      "layer   3  Sparsity: 76.6131%\n",
      "total_backward_count 1439130 real_backward_count 153739  10.683%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  0.912112/  1.436634, val:  56.25%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6029%\n",
      "layer   2  Sparsity: 82.6878%\n",
      "layer   3  Sparsity: 75.5200%\n",
      "total_backward_count 1448920 real_backward_count 154585  10.669%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  0.921202/  1.388350, val:  61.25%, val_best:  68.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6010%\n",
      "layer   2  Sparsity: 82.5708%\n",
      "layer   3  Sparsity: 76.0440%\n",
      "total_backward_count 1458710 real_backward_count 155504  10.660%\n",
      "fc layer 3 self.abs_max_out: 984.0\n",
      "fc layer 3 self.abs_max_out: 990.0\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  0.909434/  1.381736, val:  62.08%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.76 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6012%\n",
      "layer   2  Sparsity: 82.4721%\n",
      "layer   3  Sparsity: 75.9567%\n",
      "total_backward_count 1468500 real_backward_count 156432  10.653%\n",
      "fc layer 3 self.abs_max_out: 1008.0\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  0.918337/  1.457670, val:  55.83%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6055%\n",
      "layer   2  Sparsity: 82.1819%\n",
      "layer   3  Sparsity: 75.7071%\n",
      "total_backward_count 1478290 real_backward_count 157273  10.639%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  0.910658/  1.367617, val:  60.42%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6085%\n",
      "layer   2  Sparsity: 82.0682%\n",
      "layer   3  Sparsity: 76.1219%\n",
      "total_backward_count 1488080 real_backward_count 158134  10.627%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  0.910532/  1.378847, val:  58.75%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6001%\n",
      "layer   2  Sparsity: 82.0992%\n",
      "layer   3  Sparsity: 75.7751%\n",
      "total_backward_count 1497870 real_backward_count 158964  10.613%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  0.903146/  1.317975, val:  71.25%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.5985%\n",
      "layer   2  Sparsity: 82.0613%\n",
      "layer   3  Sparsity: 76.4235%\n",
      "total_backward_count 1507660 real_backward_count 159883  10.605%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  0.898633/  1.373829, val:  60.00%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.5988%\n",
      "layer   2  Sparsity: 82.3061%\n",
      "layer   3  Sparsity: 76.4555%\n",
      "total_backward_count 1517450 real_backward_count 160749  10.593%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  0.906802/  1.424285, val:  56.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.64 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6008%\n",
      "layer   2  Sparsity: 82.3903%\n",
      "layer   3  Sparsity: 76.4066%\n",
      "total_backward_count 1527240 real_backward_count 161601  10.581%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  0.909991/  1.375767, val:  65.42%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6008%\n",
      "layer   2  Sparsity: 82.0309%\n",
      "layer   3  Sparsity: 75.9309%\n",
      "total_backward_count 1537030 real_backward_count 162538  10.575%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  0.916336/  1.360757, val:  69.58%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.5978%\n",
      "layer   2  Sparsity: 82.2742%\n",
      "layer   3  Sparsity: 76.1432%\n",
      "total_backward_count 1546820 real_backward_count 163398  10.563%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  0.932803/  1.368166, val:  61.25%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6095%\n",
      "layer   2  Sparsity: 82.1647%\n",
      "layer   3  Sparsity: 75.8870%\n",
      "total_backward_count 1556610 real_backward_count 164298  10.555%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  0.899222/  1.376519, val:  62.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6001%\n",
      "layer   2  Sparsity: 82.2257%\n",
      "layer   3  Sparsity: 76.4290%\n",
      "total_backward_count 1566400 real_backward_count 165180  10.545%\n",
      "fc layer 3 self.abs_max_out: 1010.0\n",
      "fc layer 3 self.abs_max_out: 1074.0\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  0.896235/  1.362946, val:  62.50%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 86.6108%\n",
      "layer   2  Sparsity: 82.2446%\n",
      "layer   3  Sparsity: 76.2484%\n",
      "total_backward_count 1576190 real_backward_count 166106  10.538%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  0.883951/  1.314486, val:  68.33%, val_best:  71.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5913%\n",
      "layer   2  Sparsity: 82.5461%\n",
      "layer   3  Sparsity: 75.3578%\n",
      "total_backward_count 1585980 real_backward_count 166984  10.529%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  0.889249/  1.388845, val:  63.75%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6114%\n",
      "layer   2  Sparsity: 82.8139%\n",
      "layer   3  Sparsity: 76.6818%\n",
      "total_backward_count 1595770 real_backward_count 167906  10.522%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  0.890065/  1.363902, val:  65.42%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.5954%\n",
      "layer   2  Sparsity: 82.8083%\n",
      "layer   3  Sparsity: 76.2002%\n",
      "total_backward_count 1605560 real_backward_count 168780  10.512%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  0.901492/  1.387602, val:  63.33%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6088%\n",
      "layer   2  Sparsity: 82.2887%\n",
      "layer   3  Sparsity: 76.4519%\n",
      "total_backward_count 1615350 real_backward_count 169643  10.502%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  0.916110/  1.367347, val:  62.50%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6005%\n",
      "layer   2  Sparsity: 82.7125%\n",
      "layer   3  Sparsity: 76.7250%\n",
      "total_backward_count 1625140 real_backward_count 170602  10.498%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  0.886667/  1.384907, val:  62.08%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6146%\n",
      "layer   2  Sparsity: 83.2554%\n",
      "layer   3  Sparsity: 76.7954%\n",
      "total_backward_count 1634930 real_backward_count 171483  10.489%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  0.873240/  1.397794, val:  54.17%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.79 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6073%\n",
      "layer   2  Sparsity: 83.4415%\n",
      "layer   3  Sparsity: 75.9692%\n",
      "total_backward_count 1644720 real_backward_count 172384  10.481%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  0.892040/  1.364545, val:  62.08%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.5971%\n",
      "layer   2  Sparsity: 83.6079%\n",
      "layer   3  Sparsity: 75.5268%\n",
      "total_backward_count 1654510 real_backward_count 173292  10.474%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  0.892204/  1.367173, val:  60.00%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6049%\n",
      "layer   2  Sparsity: 83.8528%\n",
      "layer   3  Sparsity: 76.2576%\n",
      "total_backward_count 1664300 real_backward_count 174200  10.467%\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  0.892319/  1.382214, val:  57.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6046%\n",
      "layer   2  Sparsity: 83.2094%\n",
      "layer   3  Sparsity: 76.1706%\n",
      "total_backward_count 1674090 real_backward_count 175058  10.457%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  0.886457/  1.381954, val:  61.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6032%\n",
      "layer   2  Sparsity: 82.9053%\n",
      "layer   3  Sparsity: 76.1857%\n",
      "total_backward_count 1683880 real_backward_count 175892  10.446%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  0.881600/  1.323491, val:  65.83%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5880%\n",
      "layer   2  Sparsity: 82.9183%\n",
      "layer   3  Sparsity: 75.8549%\n",
      "total_backward_count 1693670 real_backward_count 176762  10.437%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  0.862523/  1.335323, val:  63.75%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6019%\n",
      "layer   2  Sparsity: 83.2960%\n",
      "layer   3  Sparsity: 75.6092%\n",
      "total_backward_count 1703460 real_backward_count 177564  10.424%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  0.877205/  1.379865, val:  61.25%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6027%\n",
      "layer   2  Sparsity: 83.4463%\n",
      "layer   3  Sparsity: 76.1577%\n",
      "total_backward_count 1713250 real_backward_count 178415  10.414%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  0.875113/  1.354695, val:  62.92%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6034%\n",
      "layer   2  Sparsity: 83.2606%\n",
      "layer   3  Sparsity: 76.1217%\n",
      "total_backward_count 1723040 real_backward_count 179257  10.404%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  0.844037/  1.291048, val:  64.17%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6024%\n",
      "layer   2  Sparsity: 83.1118%\n",
      "layer   3  Sparsity: 75.7359%\n",
      "total_backward_count 1732830 real_backward_count 180094  10.393%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  0.849882/  1.347369, val:  62.08%, val_best:  71.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.5981%\n",
      "layer   2  Sparsity: 82.8115%\n",
      "layer   3  Sparsity: 75.7392%\n",
      "total_backward_count 1742620 real_backward_count 180910  10.381%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  0.861682/  1.370658, val:  61.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.40 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6110%\n",
      "layer   2  Sparsity: 82.7790%\n",
      "layer   3  Sparsity: 75.4254%\n",
      "total_backward_count 1752410 real_backward_count 181705  10.369%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  0.829851/  1.391675, val:  58.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6068%\n",
      "layer   2  Sparsity: 82.9657%\n",
      "layer   3  Sparsity: 75.5295%\n",
      "total_backward_count 1762200 real_backward_count 182557  10.360%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  0.833556/  1.340440, val:  60.00%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6041%\n",
      "layer   2  Sparsity: 83.1303%\n",
      "layer   3  Sparsity: 75.2011%\n",
      "total_backward_count 1771990 real_backward_count 183404  10.350%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  0.832907/  1.342604, val:  62.50%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6013%\n",
      "layer   2  Sparsity: 83.2760%\n",
      "layer   3  Sparsity: 75.4459%\n",
      "total_backward_count 1781780 real_backward_count 184216  10.339%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  0.855354/  1.371072, val:  62.08%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6050%\n",
      "layer   2  Sparsity: 83.0419%\n",
      "layer   3  Sparsity: 75.6306%\n",
      "total_backward_count 1791570 real_backward_count 185081  10.331%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  0.854109/  1.370929, val:  62.92%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6093%\n",
      "layer   2  Sparsity: 82.9851%\n",
      "layer   3  Sparsity: 76.3035%\n",
      "total_backward_count 1801360 real_backward_count 185910  10.321%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  0.843251/  1.380214, val:  60.00%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6081%\n",
      "layer   2  Sparsity: 82.7509%\n",
      "layer   3  Sparsity: 75.3516%\n",
      "total_backward_count 1811150 real_backward_count 186731  10.310%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  0.852041/  1.358170, val:  64.17%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6001%\n",
      "layer   2  Sparsity: 82.6772%\n",
      "layer   3  Sparsity: 75.1916%\n",
      "total_backward_count 1820940 real_backward_count 187614  10.303%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  0.855321/  1.330182, val:  63.75%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 86.6190%\n",
      "layer   2  Sparsity: 82.2405%\n",
      "layer   3  Sparsity: 75.1812%\n",
      "total_backward_count 1830730 real_backward_count 188494  10.296%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  0.841588/  1.389097, val:  63.75%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6039%\n",
      "layer   2  Sparsity: 82.4892%\n",
      "layer   3  Sparsity: 75.0128%\n",
      "total_backward_count 1840520 real_backward_count 189393  10.290%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  0.864295/  1.335282, val:  66.25%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5922%\n",
      "layer   2  Sparsity: 82.3906%\n",
      "layer   3  Sparsity: 75.2123%\n",
      "total_backward_count 1850310 real_backward_count 190268  10.283%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  0.834072/  1.367861, val:  57.08%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5981%\n",
      "layer   2  Sparsity: 82.3471%\n",
      "layer   3  Sparsity: 75.1151%\n",
      "total_backward_count 1860100 real_backward_count 191095  10.273%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  0.840942/  1.315859, val:  69.58%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.5940%\n",
      "layer   2  Sparsity: 82.2912%\n",
      "layer   3  Sparsity: 74.2563%\n",
      "total_backward_count 1869890 real_backward_count 191926  10.264%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  0.840015/  1.387185, val:  56.25%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5952%\n",
      "layer   2  Sparsity: 82.0478%\n",
      "layer   3  Sparsity: 74.3168%\n",
      "total_backward_count 1879680 real_backward_count 192761  10.255%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  0.830767/  1.405898, val:  62.50%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.70 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6037%\n",
      "layer   2  Sparsity: 82.3385%\n",
      "layer   3  Sparsity: 74.6879%\n",
      "total_backward_count 1889470 real_backward_count 193636  10.248%\n",
      "fc layer 2 self.abs_max_out: 2842.0\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  0.800753/  1.327472, val:  63.33%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6019%\n",
      "layer   2  Sparsity: 82.3094%\n",
      "layer   3  Sparsity: 74.5531%\n",
      "total_backward_count 1899260 real_backward_count 194433  10.237%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  0.816699/  1.319588, val:  60.00%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6043%\n",
      "layer   2  Sparsity: 82.2176%\n",
      "layer   3  Sparsity: 74.8567%\n",
      "total_backward_count 1909050 real_backward_count 195249  10.228%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  0.793565/  1.334488, val:  62.50%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.5907%\n",
      "layer   2  Sparsity: 82.2231%\n",
      "layer   3  Sparsity: 74.3852%\n",
      "total_backward_count 1918840 real_backward_count 196092  10.219%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  0.819874/  1.326823, val:  65.83%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.5955%\n",
      "layer   2  Sparsity: 82.0493%\n",
      "layer   3  Sparsity: 74.2737%\n",
      "total_backward_count 1928630 real_backward_count 196975  10.213%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  0.818704/  1.350358, val:  63.33%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.5990%\n",
      "layer   2  Sparsity: 82.1507%\n",
      "layer   3  Sparsity: 74.3820%\n",
      "total_backward_count 1938420 real_backward_count 197836  10.206%\n",
      "fc layer 2 self.abs_max_out: 2877.0\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  0.802437/  1.307737, val:  66.25%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 86.6053%\n",
      "layer   2  Sparsity: 82.1667%\n",
      "layer   3  Sparsity: 74.8588%\n",
      "total_backward_count 1948210 real_backward_count 198646  10.196%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  0.817079/  1.330215, val:  60.42%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.66 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 86.6058%\n",
      "layer   2  Sparsity: 82.1229%\n",
      "layer   3  Sparsity: 75.7000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266611637eb84d50acbde686a803d1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñá‚ñá‚ñÖ‚ñÖ‚ñá‚ñà‚ñÜ‚ñà‚ñà‚ñÜ‚ñà‚ñÉ‚ñá‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñá‚ñÜ‚ñá</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>0.81708</td></tr><tr><td>val_acc_best</td><td>0.7125</td></tr><tr><td>val_acc_now</td><td>0.60417</td></tr><tr><td>val_loss</td><td>1.33021</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">laced-sweep-90</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/07tlf5qa' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/07tlf5qa</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_084246-07tlf5qa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fbwrlyzc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_130729-fbwrlyzc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fbwrlyzc' target=\"_blank\">celestial-sweep-94</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fbwrlyzc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fbwrlyzc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251116_130738_759', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 2, 'leaky_temporal_filter': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 1452.0\n",
      "lif layer 1 self.abs_max_v: 1452.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1896.0\n",
      "lif layer 2 self.abs_max_v: 1896.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 1106.0\n",
      "fc layer 1 self.abs_max_out: 1718.0\n",
      "lif layer 1 self.abs_max_v: 2280.0\n",
      "fc layer 2 self.abs_max_out: 1959.0\n",
      "lif layer 2 self.abs_max_v: 2449.5\n",
      "fc layer 2 self.abs_max_out: 2185.0\n",
      "lif layer 2 self.abs_max_v: 3410.0\n",
      "lif layer 1 self.abs_max_v: 2599.0\n",
      "fc layer 2 self.abs_max_out: 2345.0\n",
      "lif layer 2 self.abs_max_v: 3679.0\n",
      "lif layer 2 self.abs_max_v: 3686.5\n",
      "lif layer 2 self.abs_max_v: 3699.0\n",
      "lif layer 2 self.abs_max_v: 3721.5\n",
      "fc layer 1 self.abs_max_out: 2636.0\n",
      "lif layer 1 self.abs_max_v: 2636.0\n",
      "lif layer 2 self.abs_max_v: 3766.0\n",
      "lif layer 1 self.abs_max_v: 3341.0\n",
      "fc layer 2 self.abs_max_out: 2406.0\n",
      "fc layer 1 self.abs_max_out: 2698.0\n",
      "fc layer 1 self.abs_max_out: 2703.0\n",
      "lif layer 1 self.abs_max_v: 3893.5\n",
      "fc layer 3 self.abs_max_out: 1118.0\n",
      "lif layer 1 self.abs_max_v: 3955.0\n",
      "fc layer 3 self.abs_max_out: 1120.0\n",
      "lif layer 1 self.abs_max_v: 3972.0\n",
      "lif layer 2 self.abs_max_v: 3954.0\n",
      "fc layer 1 self.abs_max_out: 3035.0\n",
      "lif layer 1 self.abs_max_v: 4175.5\n",
      "fc layer 2 self.abs_max_out: 2462.0\n",
      "lif layer 2 self.abs_max_v: 4087.5\n",
      "lif layer 2 self.abs_max_v: 4119.0\n",
      "fc layer 2 self.abs_max_out: 2509.0\n",
      "lif layer 2 self.abs_max_v: 4159.0\n",
      "fc layer 1 self.abs_max_out: 3422.0\n",
      "lif layer 1 self.abs_max_v: 4250.5\n",
      "lif layer 1 self.abs_max_v: 4389.5\n",
      "fc layer 2 self.abs_max_out: 2519.0\n",
      "fc layer 2 self.abs_max_out: 2727.0\n",
      "lif layer 2 self.abs_max_v: 4199.0\n",
      "lif layer 2 self.abs_max_v: 4339.5\n",
      "fc layer 2 self.abs_max_out: 2878.0\n",
      "fc layer 3 self.abs_max_out: 1139.0\n",
      "lif layer 2 self.abs_max_v: 4382.0\n",
      "lif layer 2 self.abs_max_v: 4440.5\n",
      "lif layer 2 self.abs_max_v: 4474.5\n",
      "lif layer 2 self.abs_max_v: 4807.5\n",
      "lif layer 1 self.abs_max_v: 4484.5\n",
      "lif layer 2 self.abs_max_v: 4820.0\n",
      "lif layer 1 self.abs_max_v: 4591.5\n",
      "lif layer 2 self.abs_max_v: 4838.0\n",
      "lif layer 2 self.abs_max_v: 5163.0\n",
      "fc layer 1 self.abs_max_out: 3508.0\n",
      "lif layer 1 self.abs_max_v: 4828.0\n",
      "lif layer 1 self.abs_max_v: 4861.0\n",
      "lif layer 1 self.abs_max_v: 4981.0\n",
      "fc layer 2 self.abs_max_out: 2937.0\n",
      "lif layer 1 self.abs_max_v: 5275.5\n",
      "lif layer 1 self.abs_max_v: 5371.5\n",
      "fc layer 1 self.abs_max_out: 3593.0\n",
      "fc layer 2 self.abs_max_out: 3316.0\n",
      "lif layer 1 self.abs_max_v: 5761.5\n",
      "lif layer 1 self.abs_max_v: 5962.0\n",
      "fc layer 1 self.abs_max_out: 3906.0\n",
      "fc layer 1 self.abs_max_out: 4013.0\n",
      "lif layer 2 self.abs_max_v: 5258.5\n",
      "lif layer 2 self.abs_max_v: 5315.0\n",
      "lif layer 2 self.abs_max_v: 5327.0\n",
      "lif layer 2 self.abs_max_v: 5495.5\n",
      "lif layer 1 self.abs_max_v: 5988.0\n",
      "lif layer 1 self.abs_max_v: 6528.0\n",
      "lif layer 1 self.abs_max_v: 6599.0\n",
      "fc layer 1 self.abs_max_out: 4485.0\n",
      "lif layer 1 self.abs_max_v: 6607.5\n",
      "lif layer 1 self.abs_max_v: 6725.5\n",
      "lif layer 1 self.abs_max_v: 7569.0\n",
      "fc layer 1 self.abs_max_out: 4666.0\n",
      "fc layer 1 self.abs_max_out: 4990.0\n",
      "lif layer 1 self.abs_max_v: 7601.0\n",
      "lif layer 1 self.abs_max_v: 7920.5\n",
      "lif layer 1 self.abs_max_v: 8314.5\n",
      "lif layer 1 self.abs_max_v: 8545.5\n",
      "lif layer 1 self.abs_max_v: 8549.5\n",
      "lif layer 1 self.abs_max_v: 8562.0\n",
      "fc layer 1 self.abs_max_out: 5188.0\n",
      "lif layer 1 self.abs_max_v: 9369.0\n",
      "fc layer 1 self.abs_max_out: 5264.0\n",
      "fc layer 3 self.abs_max_out: 1151.0\n",
      "fc layer 3 self.abs_max_out: 1166.0\n",
      "fc layer 3 self.abs_max_out: 1232.0\n",
      "fc layer 3 self.abs_max_out: 1235.0\n",
      "fc layer 1 self.abs_max_out: 5283.0\n",
      "lif layer 1 self.abs_max_v: 9371.5\n",
      "lif layer 1 self.abs_max_v: 9437.0\n",
      "lif layer 1 self.abs_max_v: 9595.0\n",
      "fc layer 1 self.abs_max_out: 5672.0\n",
      "fc layer 1 self.abs_max_out: 6177.0\n",
      "lif layer 1 self.abs_max_v: 10861.5\n",
      "fc layer 1 self.abs_max_out: 6216.0\n",
      "fc layer 1 self.abs_max_out: 6324.0\n",
      "lif layer 1 self.abs_max_v: 10949.5\n",
      "lif layer 1 self.abs_max_v: 11146.0\n",
      "lif layer 1 self.abs_max_v: 11194.0\n",
      "fc layer 1 self.abs_max_out: 6574.0\n",
      "lif layer 1 self.abs_max_v: 11384.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  1.988531/  2.079357, val:  38.75%, val_best:  38.75%, tr:  80.39%, tr_best:  80.39%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 75.8444%\n",
      "layer   2  Sparsity: 70.8062%\n",
      "layer   3  Sparsity: 63.6405%\n",
      "total_backward_count 9790 real_backward_count 3404  34.770%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 1248.0\n",
      "fc layer 3 self.abs_max_out: 1255.0\n",
      "fc layer 1 self.abs_max_out: 6764.0\n",
      "fc layer 1 self.abs_max_out: 7311.0\n",
      "lif layer 1 self.abs_max_v: 11456.0\n",
      "fc layer 1 self.abs_max_out: 7658.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.915115/  2.035341, val:  42.92%, val_best:  42.92%, tr:  93.77%, tr_best:  93.77%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8315%\n",
      "layer   2  Sparsity: 75.7491%\n",
      "layer   3  Sparsity: 64.8112%\n",
      "total_backward_count 19580 real_backward_count 5732  29.275%\n",
      "lif layer 1 self.abs_max_v: 11692.5\n",
      "lif layer 1 self.abs_max_v: 11999.0\n",
      "fc layer 1 self.abs_max_out: 7988.0\n",
      "lif layer 1 self.abs_max_v: 12102.5\n",
      "fc layer 3 self.abs_max_out: 1280.0\n",
      "lif layer 2 self.abs_max_v: 5526.0\n",
      "lif layer 1 self.abs_max_v: 12311.0\n",
      "lif layer 1 self.abs_max_v: 12400.5\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.888676/  2.013526, val:  57.92%, val_best:  57.92%, tr:  95.81%, tr_best:  95.81%, epoch time: 78.69 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8476%\n",
      "layer   2  Sparsity: 76.2538%\n",
      "layer   3  Sparsity: 64.8662%\n",
      "total_backward_count 29370 real_backward_count 7815  26.609%\n",
      "lif layer 1 self.abs_max_v: 12470.0\n",
      "lif layer 1 self.abs_max_v: 12637.0\n",
      "lif layer 1 self.abs_max_v: 12799.0\n",
      "lif layer 1 self.abs_max_v: 12963.5\n",
      "lif layer 1 self.abs_max_v: 13065.5\n",
      "lif layer 1 self.abs_max_v: 13073.5\n",
      "lif layer 1 self.abs_max_v: 13252.0\n",
      "lif layer 2 self.abs_max_v: 5639.0\n",
      "lif layer 2 self.abs_max_v: 5731.0\n",
      "lif layer 1 self.abs_max_v: 13360.5\n",
      "lif layer 1 self.abs_max_v: 13701.5\n",
      "fc layer 3 self.abs_max_out: 1289.0\n",
      "fc layer 3 self.abs_max_out: 1342.0\n",
      "lif layer 1 self.abs_max_v: 14373.5\n",
      "fc layer 3 self.abs_max_out: 1488.0\n",
      "lif layer 1 self.abs_max_v: 14572.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  1.900906/  2.017542, val:  54.17%, val_best:  57.92%, tr:  97.55%, tr_best:  97.55%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8502%\n",
      "layer   2  Sparsity: 76.9926%\n",
      "layer   3  Sparsity: 63.7138%\n",
      "total_backward_count 39160 real_backward_count 9599  24.512%\n",
      "fc layer 1 self.abs_max_out: 8040.0\n",
      "lif layer 1 self.abs_max_v: 14582.0\n",
      "lif layer 1 self.abs_max_v: 15063.5\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  1.901149/  2.021440, val:  44.17%, val_best:  57.92%, tr:  98.77%, tr_best:  98.77%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8618%\n",
      "layer   2  Sparsity: 77.0865%\n",
      "layer   3  Sparsity: 64.0349%\n",
      "total_backward_count 48950 real_backward_count 11150  22.778%\n",
      "fc layer 1 self.abs_max_out: 8045.0\n",
      "lif layer 1 self.abs_max_v: 15178.0\n",
      "lif layer 1 self.abs_max_v: 15312.0\n",
      "fc layer 1 self.abs_max_out: 8213.0\n",
      "lif layer 1 self.abs_max_v: 15350.5\n",
      "lif layer 1 self.abs_max_v: 15415.5\n",
      "lif layer 1 self.abs_max_v: 15773.0\n",
      "fc layer 1 self.abs_max_out: 8451.0\n",
      "lif layer 1 self.abs_max_v: 16337.5\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  1.915770/  2.055634, val:  44.17%, val_best:  57.92%, tr:  99.28%, tr_best:  99.28%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8401%\n",
      "layer   2  Sparsity: 76.1123%\n",
      "layer   3  Sparsity: 63.6603%\n",
      "total_backward_count 58740 real_backward_count 12673  21.575%\n",
      "fc layer 1 self.abs_max_out: 8466.0\n",
      "fc layer 1 self.abs_max_out: 8546.0\n",
      "fc layer 1 self.abs_max_out: 8621.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  1.930776/  2.041963, val:  53.33%, val_best:  57.92%, tr:  99.49%, tr_best:  99.49%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8496%\n",
      "layer   2  Sparsity: 76.2261%\n",
      "layer   3  Sparsity: 64.0794%\n",
      "total_backward_count 68530 real_backward_count 14122  20.607%\n",
      "fc layer 1 self.abs_max_out: 8691.0\n",
      "fc layer 1 self.abs_max_out: 8950.0\n",
      "lif layer 1 self.abs_max_v: 16394.5\n",
      "fc layer 1 self.abs_max_out: 9042.0\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  1.911053/  1.994560, val:  51.25%, val_best:  57.92%, tr:  99.49%, tr_best:  99.49%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8455%\n",
      "layer   2  Sparsity: 78.4686%\n",
      "layer   3  Sparsity: 63.9391%\n",
      "total_backward_count 78320 real_backward_count 15451  19.728%\n",
      "lif layer 2 self.abs_max_v: 5772.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  1.908271/  2.041185, val:  50.42%, val_best:  57.92%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8395%\n",
      "layer   2  Sparsity: 77.3681%\n",
      "layer   3  Sparsity: 63.3115%\n",
      "total_backward_count 88110 real_backward_count 16786  19.051%\n",
      "lif layer 2 self.abs_max_v: 5965.0\n",
      "lif layer 2 self.abs_max_v: 6195.5\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  1.914102/  2.037140, val:  55.00%, val_best:  57.92%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.39 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8313%\n",
      "layer   2  Sparsity: 76.4972%\n",
      "layer   3  Sparsity: 62.3214%\n",
      "total_backward_count 97900 real_backward_count 17998  18.384%\n",
      "lif layer 1 self.abs_max_v: 16399.5\n",
      "fc layer 1 self.abs_max_out: 9691.0\n",
      "fc layer 2 self.abs_max_out: 3344.0\n",
      "lif layer 1 self.abs_max_v: 16674.0\n",
      "fc layer 2 self.abs_max_out: 3569.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  1.921004/  2.039125, val:  50.42%, val_best:  57.92%, tr:  99.59%, tr_best:  99.80%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8512%\n",
      "layer   2  Sparsity: 75.1165%\n",
      "layer   3  Sparsity: 62.4448%\n",
      "total_backward_count 107690 real_backward_count 19193  17.822%\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  1.926737/  2.033006, val:  61.67%, val_best:  61.67%, tr:  99.69%, tr_best:  99.80%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8358%\n",
      "layer   2  Sparsity: 75.1631%\n",
      "layer   3  Sparsity: 62.5469%\n",
      "total_backward_count 117480 real_backward_count 20385  17.352%\n",
      "fc layer 2 self.abs_max_out: 3635.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  1.913578/  2.045975, val:  48.75%, val_best:  61.67%, tr:  99.69%, tr_best:  99.80%, epoch time: 79.31 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8765%\n",
      "layer   2  Sparsity: 75.8390%\n",
      "layer   3  Sparsity: 62.3542%\n",
      "total_backward_count 127270 real_backward_count 21496  16.890%\n",
      "lif layer 1 self.abs_max_v: 16945.5\n",
      "lif layer 1 self.abs_max_v: 17193.5\n",
      "fc layer 1 self.abs_max_out: 9827.0\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  1.927421/  2.055830, val:  47.08%, val_best:  61.67%, tr:  99.69%, tr_best:  99.80%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8552%\n",
      "layer   2  Sparsity: 74.6667%\n",
      "layer   3  Sparsity: 62.0763%\n",
      "total_backward_count 137060 real_backward_count 22624  16.507%\n",
      "lif layer 1 self.abs_max_v: 17284.0\n",
      "lif layer 1 self.abs_max_v: 17553.0\n",
      "fc layer 1 self.abs_max_out: 10634.0\n",
      "lif layer 1 self.abs_max_v: 18040.0\n",
      "lif layer 1 self.abs_max_v: 18749.5\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  1.930896/  2.022564, val:  52.92%, val_best:  61.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8532%\n",
      "layer   2  Sparsity: 74.1724%\n",
      "layer   3  Sparsity: 62.1348%\n",
      "total_backward_count 146850 real_backward_count 23712  16.147%\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  1.907678/  2.018607, val:  57.92%, val_best:  61.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8419%\n",
      "layer   2  Sparsity: 72.8209%\n",
      "layer   3  Sparsity: 62.7945%\n",
      "total_backward_count 156640 real_backward_count 24877  15.882%\n",
      "lif layer 2 self.abs_max_v: 6197.5\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  1.906926/  2.020250, val:  57.50%, val_best:  61.67%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8673%\n",
      "layer   2  Sparsity: 72.4142%\n",
      "layer   3  Sparsity: 60.8826%\n",
      "total_backward_count 166430 real_backward_count 25908  15.567%\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  1.913090/  2.014424, val:  50.83%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8576%\n",
      "layer   2  Sparsity: 72.7247%\n",
      "layer   3  Sparsity: 59.6524%\n",
      "total_backward_count 176220 real_backward_count 27001  15.322%\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  1.915822/  2.021591, val:  55.00%, val_best:  61.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 80.08 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8545%\n",
      "layer   2  Sparsity: 73.1843%\n",
      "layer   3  Sparsity: 59.5907%\n",
      "total_backward_count 186010 real_backward_count 28071  15.091%\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  1.899389/  2.014382, val:  46.67%, val_best:  61.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.80 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8719%\n",
      "layer   2  Sparsity: 74.6953%\n",
      "layer   3  Sparsity: 60.5817%\n",
      "total_backward_count 195800 real_backward_count 29125  14.875%\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  1.907168/  2.031661, val:  46.67%, val_best:  61.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8708%\n",
      "layer   2  Sparsity: 74.4995%\n",
      "layer   3  Sparsity: 61.0180%\n",
      "total_backward_count 205590 real_backward_count 30183  14.681%\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  1.920455/  2.040874, val:  52.08%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8584%\n",
      "layer   2  Sparsity: 74.0691%\n",
      "layer   3  Sparsity: 61.2024%\n",
      "total_backward_count 215380 real_backward_count 31223  14.497%\n",
      "lif layer 2 self.abs_max_v: 6303.0\n",
      "lif layer 2 self.abs_max_v: 6308.5\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  1.928915/  2.028919, val:  50.42%, val_best:  61.67%, tr:  99.28%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8498%\n",
      "layer   2  Sparsity: 73.3797%\n",
      "layer   3  Sparsity: 61.1316%\n",
      "total_backward_count 225170 real_backward_count 32230  14.314%\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  1.918527/  2.031117, val:  59.17%, val_best:  61.67%, tr:  99.39%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8631%\n",
      "layer   2  Sparsity: 73.0536%\n",
      "layer   3  Sparsity: 61.1310%\n",
      "total_backward_count 234960 real_backward_count 33302  14.173%\n",
      "lif layer 1 self.abs_max_v: 18903.0\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  1.921630/  2.048169, val:  53.75%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8556%\n",
      "layer   2  Sparsity: 73.7183%\n",
      "layer   3  Sparsity: 61.0552%\n",
      "total_backward_count 244750 real_backward_count 34282  14.007%\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  1.921749/  2.026366, val:  59.17%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.08 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8586%\n",
      "layer   2  Sparsity: 74.3900%\n",
      "layer   3  Sparsity: 61.5842%\n",
      "total_backward_count 254540 real_backward_count 35380  13.900%\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  1.924337/  2.030848, val:  58.75%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8615%\n",
      "layer   2  Sparsity: 74.1445%\n",
      "layer   3  Sparsity: 61.9668%\n",
      "total_backward_count 264330 real_backward_count 36455  13.791%\n",
      "lif layer 2 self.abs_max_v: 6366.5\n",
      "lif layer 2 self.abs_max_v: 6379.5\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  1.925316/  2.033377, val:  52.92%, val_best:  61.67%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8466%\n",
      "layer   2  Sparsity: 73.6733%\n",
      "layer   3  Sparsity: 61.7918%\n",
      "total_backward_count 274120 real_backward_count 37519  13.687%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  1.916311/  2.051018, val:  57.50%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.61 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8413%\n",
      "layer   2  Sparsity: 74.4635%\n",
      "layer   3  Sparsity: 62.2212%\n",
      "total_backward_count 283910 real_backward_count 38481  13.554%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  1.941713/  2.025789, val:  50.42%, val_best:  61.67%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8597%\n",
      "layer   2  Sparsity: 73.7824%\n",
      "layer   3  Sparsity: 62.0519%\n",
      "total_backward_count 293700 real_backward_count 39477  13.441%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  1.929459/  2.048440, val:  54.58%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8354%\n",
      "layer   2  Sparsity: 74.2007%\n",
      "layer   3  Sparsity: 61.3612%\n",
      "total_backward_count 303490 real_backward_count 40468  13.334%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  1.927849/  2.019449, val:  54.17%, val_best:  61.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8542%\n",
      "layer   2  Sparsity: 73.2553%\n",
      "layer   3  Sparsity: 61.9222%\n",
      "total_backward_count 313280 real_backward_count 41459  13.234%\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  1.922719/  2.052510, val:  56.67%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8425%\n",
      "layer   2  Sparsity: 72.1631%\n",
      "layer   3  Sparsity: 61.2815%\n",
      "total_backward_count 323070 real_backward_count 42391  13.121%\n",
      "fc layer 2 self.abs_max_out: 3690.0\n",
      "lif layer 2 self.abs_max_v: 6493.0\n",
      "lif layer 2 self.abs_max_v: 6803.5\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  1.937215/  2.049008, val:  52.50%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8408%\n",
      "layer   2  Sparsity: 71.6656%\n",
      "layer   3  Sparsity: 61.7116%\n",
      "total_backward_count 332860 real_backward_count 43375  13.031%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  1.935163/  2.044449, val:  51.67%, val_best:  61.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.26 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8333%\n",
      "layer   2  Sparsity: 71.2993%\n",
      "layer   3  Sparsity: 61.7208%\n",
      "total_backward_count 342650 real_backward_count 44339  12.940%\n",
      "fc layer 2 self.abs_max_out: 3939.0\n",
      "lif layer 2 self.abs_max_v: 7122.0\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  1.934710/  2.055084, val:  54.58%, val_best:  61.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8568%\n",
      "layer   2  Sparsity: 71.9784%\n",
      "layer   3  Sparsity: 61.9413%\n",
      "total_backward_count 352440 real_backward_count 45317  12.858%\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  1.936528/  2.051328, val:  62.08%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8507%\n",
      "layer   2  Sparsity: 72.1045%\n",
      "layer   3  Sparsity: 61.6683%\n",
      "total_backward_count 362230 real_backward_count 46324  12.789%\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  1.941167/  2.046814, val:  54.58%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.86 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8468%\n",
      "layer   2  Sparsity: 71.2959%\n",
      "layer   3  Sparsity: 62.3265%\n",
      "total_backward_count 372020 real_backward_count 47273  12.707%\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  1.924431/  2.021762, val:  57.50%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8480%\n",
      "layer   2  Sparsity: 70.8775%\n",
      "layer   3  Sparsity: 62.3605%\n",
      "total_backward_count 381810 real_backward_count 48254  12.638%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  1.923395/  2.041153, val:  50.00%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8349%\n",
      "layer   2  Sparsity: 71.3795%\n",
      "layer   3  Sparsity: 62.0452%\n",
      "total_backward_count 391600 real_backward_count 49210  12.566%\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  1.921132/  2.020789, val:  64.58%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8484%\n",
      "layer   2  Sparsity: 71.8678%\n",
      "layer   3  Sparsity: 62.3084%\n",
      "total_backward_count 401390 real_backward_count 50165  12.498%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  1.916646/  2.020939, val:  55.42%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8492%\n",
      "layer   2  Sparsity: 71.5515%\n",
      "layer   3  Sparsity: 62.0445%\n",
      "total_backward_count 411180 real_backward_count 51111  12.430%\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  1.918264/  2.025324, val:  57.08%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8506%\n",
      "layer   2  Sparsity: 71.7293%\n",
      "layer   3  Sparsity: 62.5459%\n",
      "total_backward_count 420970 real_backward_count 52056  12.366%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  1.919296/  2.040041, val:  54.58%, val_best:  64.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.63 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8481%\n",
      "layer   2  Sparsity: 71.1992%\n",
      "layer   3  Sparsity: 62.8705%\n",
      "total_backward_count 430760 real_backward_count 53008  12.306%\n",
      "lif layer 1 self.abs_max_v: 19296.0\n",
      "lif layer 1 self.abs_max_v: 19609.0\n",
      "fc layer 1 self.abs_max_out: 10789.0\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  1.914834/  2.027689, val:  62.92%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8593%\n",
      "layer   2  Sparsity: 71.4627%\n",
      "layer   3  Sparsity: 62.2910%\n",
      "total_backward_count 440550 real_backward_count 53963  12.249%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  1.923250/  2.045437, val:  50.83%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8627%\n",
      "layer   2  Sparsity: 71.5068%\n",
      "layer   3  Sparsity: 61.3142%\n",
      "total_backward_count 450340 real_backward_count 54969  12.206%\n",
      "fc layer 1 self.abs_max_out: 10833.0\n",
      "fc layer 1 self.abs_max_out: 11077.0\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  1.917254/  2.040871, val:  55.42%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8196%\n",
      "layer   2  Sparsity: 72.4385%\n",
      "layer   3  Sparsity: 61.4607%\n",
      "total_backward_count 460130 real_backward_count 55971  12.164%\n",
      "fc layer 1 self.abs_max_out: 11212.0\n",
      "lif layer 1 self.abs_max_v: 20021.5\n",
      "fc layer 1 self.abs_max_out: 11325.0\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  1.907889/  2.031644, val:  52.92%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8493%\n",
      "layer   2  Sparsity: 72.2070%\n",
      "layer   3  Sparsity: 60.5515%\n",
      "total_backward_count 469920 real_backward_count 56926  12.114%\n",
      "lif layer 1 self.abs_max_v: 20132.0\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  1.908939/  2.022425, val:  62.08%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8423%\n",
      "layer   2  Sparsity: 71.9815%\n",
      "layer   3  Sparsity: 59.9726%\n",
      "total_backward_count 479710 real_backward_count 57800  12.049%\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  1.905428/  2.022325, val:  56.25%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8427%\n",
      "layer   2  Sparsity: 70.9900%\n",
      "layer   3  Sparsity: 59.6552%\n",
      "total_backward_count 489500 real_backward_count 58722  11.996%\n",
      "fc layer 1 self.abs_max_out: 11783.0\n",
      "lif layer 1 self.abs_max_v: 20277.5\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  1.909517/  2.019183, val:  57.92%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.95 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8382%\n",
      "layer   2  Sparsity: 71.3432%\n",
      "layer   3  Sparsity: 58.8975%\n",
      "total_backward_count 499290 real_backward_count 59643  11.946%\n",
      "lif layer 1 self.abs_max_v: 20366.0\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  1.903316/  2.027538, val:  54.58%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8535%\n",
      "layer   2  Sparsity: 71.7515%\n",
      "layer   3  Sparsity: 60.3680%\n",
      "total_backward_count 509080 real_backward_count 60538  11.892%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  1.913125/  2.040910, val:  61.25%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8532%\n",
      "layer   2  Sparsity: 71.3961%\n",
      "layer   3  Sparsity: 61.4790%\n",
      "total_backward_count 518870 real_backward_count 61531  11.859%\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  1.909519/  2.021967, val:  64.58%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.51 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8499%\n",
      "layer   2  Sparsity: 71.0002%\n",
      "layer   3  Sparsity: 61.7923%\n",
      "total_backward_count 528660 real_backward_count 62424  11.808%\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  1.908139/  2.019527, val:  60.83%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8453%\n",
      "layer   2  Sparsity: 71.5979%\n",
      "layer   3  Sparsity: 60.9093%\n",
      "total_backward_count 538450 real_backward_count 63289  11.754%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.907793/  2.022393, val:  56.25%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8454%\n",
      "layer   2  Sparsity: 71.0368%\n",
      "layer   3  Sparsity: 61.3908%\n",
      "total_backward_count 548240 real_backward_count 64192  11.709%\n",
      "lif layer 1 self.abs_max_v: 20533.0\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.905858/  2.022754, val:  47.08%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8570%\n",
      "layer   2  Sparsity: 70.6895%\n",
      "layer   3  Sparsity: 62.3896%\n",
      "total_backward_count 558030 real_backward_count 65114  11.669%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.902397/  2.024892, val:  49.58%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8427%\n",
      "layer   2  Sparsity: 70.6288%\n",
      "layer   3  Sparsity: 61.4043%\n",
      "total_backward_count 567820 real_backward_count 65984  11.621%\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.901043/  2.016237, val:  65.00%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8236%\n",
      "layer   2  Sparsity: 69.9082%\n",
      "layer   3  Sparsity: 61.8847%\n",
      "total_backward_count 577610 real_backward_count 66806  11.566%\n",
      "lif layer 1 self.abs_max_v: 20756.5\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.899418/  2.023272, val:  49.58%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.01 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8539%\n",
      "layer   2  Sparsity: 69.5166%\n",
      "layer   3  Sparsity: 61.4516%\n",
      "total_backward_count 587400 real_backward_count 67683  11.522%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.905607/  2.021818, val:  54.58%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8383%\n",
      "layer   2  Sparsity: 69.0604%\n",
      "layer   3  Sparsity: 60.9110%\n",
      "total_backward_count 597190 real_backward_count 68560  11.480%\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.906662/  2.007738, val:  61.67%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8382%\n",
      "layer   2  Sparsity: 68.3156%\n",
      "layer   3  Sparsity: 60.3912%\n",
      "total_backward_count 606980 real_backward_count 69417  11.436%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.894195/  2.032276, val:  52.08%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8419%\n",
      "layer   2  Sparsity: 69.5728%\n",
      "layer   3  Sparsity: 60.4377%\n",
      "total_backward_count 616770 real_backward_count 70282  11.395%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.903809/  2.009303, val:  57.50%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.37 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8458%\n",
      "layer   2  Sparsity: 68.4575%\n",
      "layer   3  Sparsity: 60.3582%\n",
      "total_backward_count 626560 real_backward_count 71142  11.354%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.898606/  2.020119, val:  47.50%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8368%\n",
      "layer   2  Sparsity: 69.3962%\n",
      "layer   3  Sparsity: 59.2267%\n",
      "total_backward_count 636350 real_backward_count 71986  11.312%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.893902/  2.031228, val:  57.92%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8575%\n",
      "layer   2  Sparsity: 69.5256%\n",
      "layer   3  Sparsity: 60.8560%\n",
      "total_backward_count 646140 real_backward_count 72821  11.270%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.904146/  2.018032, val:  55.42%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8377%\n",
      "layer   2  Sparsity: 69.6295%\n",
      "layer   3  Sparsity: 61.2525%\n",
      "total_backward_count 655930 real_backward_count 73685  11.234%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.892454/  2.009685, val:  60.83%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8316%\n",
      "layer   2  Sparsity: 69.8461%\n",
      "layer   3  Sparsity: 59.5253%\n",
      "total_backward_count 665720 real_backward_count 74513  11.193%\n",
      "lif layer 1 self.abs_max_v: 21053.5\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.890921/  1.999740, val:  61.25%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8411%\n",
      "layer   2  Sparsity: 69.0258%\n",
      "layer   3  Sparsity: 60.1436%\n",
      "total_backward_count 675510 real_backward_count 75372  11.158%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.881963/  2.010976, val:  57.08%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8794%\n",
      "layer   2  Sparsity: 69.6845%\n",
      "layer   3  Sparsity: 60.1636%\n",
      "total_backward_count 685300 real_backward_count 76187  11.117%\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.887377/  2.005533, val:  58.33%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8523%\n",
      "layer   2  Sparsity: 69.8072%\n",
      "layer   3  Sparsity: 60.3249%\n",
      "total_backward_count 695090 real_backward_count 77045  11.084%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.892398/  2.000769, val:  58.33%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8228%\n",
      "layer   2  Sparsity: 69.3135%\n",
      "layer   3  Sparsity: 60.6893%\n",
      "total_backward_count 704880 real_backward_count 77923  11.055%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.890175/  2.003647, val:  60.42%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8456%\n",
      "layer   2  Sparsity: 68.7381%\n",
      "layer   3  Sparsity: 62.0649%\n",
      "total_backward_count 714670 real_backward_count 78772  11.022%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.892290/  2.010350, val:  56.67%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8549%\n",
      "layer   2  Sparsity: 68.9504%\n",
      "layer   3  Sparsity: 59.9642%\n",
      "total_backward_count 724460 real_backward_count 79598  10.987%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.895619/  2.009007, val:  56.25%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8578%\n",
      "layer   2  Sparsity: 69.6162%\n",
      "layer   3  Sparsity: 59.3026%\n",
      "total_backward_count 734250 real_backward_count 80443  10.956%\n",
      "fc layer 2 self.abs_max_out: 3941.0\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.882250/  1.986635, val:  65.00%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8322%\n",
      "layer   2  Sparsity: 69.2014%\n",
      "layer   3  Sparsity: 59.8074%\n",
      "total_backward_count 744040 real_backward_count 81225  10.917%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.877541/  1.996930, val:  60.42%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8470%\n",
      "layer   2  Sparsity: 69.3637%\n",
      "layer   3  Sparsity: 60.7702%\n",
      "total_backward_count 753830 real_backward_count 82087  10.889%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.878697/  1.994819, val:  57.50%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8397%\n",
      "layer   2  Sparsity: 69.4646%\n",
      "layer   3  Sparsity: 61.6050%\n",
      "total_backward_count 763620 real_backward_count 82887  10.854%\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.878579/  2.008660, val:  53.33%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8502%\n",
      "layer   2  Sparsity: 68.7257%\n",
      "layer   3  Sparsity: 61.9764%\n",
      "total_backward_count 773410 real_backward_count 83708  10.823%\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.881985/  2.018921, val:  58.75%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8597%\n",
      "layer   2  Sparsity: 66.7987%\n",
      "layer   3  Sparsity: 60.7701%\n",
      "total_backward_count 783200 real_backward_count 84541  10.794%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.881615/  1.984962, val:  58.75%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8588%\n",
      "layer   2  Sparsity: 66.2392%\n",
      "layer   3  Sparsity: 61.3859%\n",
      "total_backward_count 792990 real_backward_count 85365  10.765%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.875532/  2.005815, val:  52.92%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8435%\n",
      "layer   2  Sparsity: 67.0038%\n",
      "layer   3  Sparsity: 62.3604%\n",
      "total_backward_count 802780 real_backward_count 86129  10.729%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.881853/  2.002566, val:  60.00%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8406%\n",
      "layer   2  Sparsity: 66.9091%\n",
      "layer   3  Sparsity: 61.1950%\n",
      "total_backward_count 812570 real_backward_count 86973  10.703%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.883165/  2.020292, val:  50.83%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8540%\n",
      "layer   2  Sparsity: 67.1587%\n",
      "layer   3  Sparsity: 62.1845%\n",
      "total_backward_count 822360 real_backward_count 87790  10.675%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.890750/  2.001518, val:  60.00%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.60 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8537%\n",
      "layer   2  Sparsity: 66.9967%\n",
      "layer   3  Sparsity: 62.6708%\n",
      "total_backward_count 832150 real_backward_count 88628  10.650%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.885986/  1.999929, val:  53.75%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8368%\n",
      "layer   2  Sparsity: 66.6644%\n",
      "layer   3  Sparsity: 62.6470%\n",
      "total_backward_count 841940 real_backward_count 89420  10.621%\n",
      "fc layer 1 self.abs_max_out: 12063.0\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.890189/  2.008831, val:  62.08%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8404%\n",
      "layer   2  Sparsity: 66.4694%\n",
      "layer   3  Sparsity: 62.4005%\n",
      "total_backward_count 851730 real_backward_count 90203  10.591%\n",
      "lif layer 1 self.abs_max_v: 21169.5\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.887636/  2.008889, val:  60.00%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8482%\n",
      "layer   2  Sparsity: 66.8785%\n",
      "layer   3  Sparsity: 61.4718%\n",
      "total_backward_count 861520 real_backward_count 91003  10.563%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.890129/  2.011926, val:  56.25%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.39 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 75.8784%\n",
      "layer   2  Sparsity: 66.8048%\n",
      "layer   3  Sparsity: 62.2445%\n",
      "total_backward_count 871310 real_backward_count 91834  10.540%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.892265/  2.002728, val:  62.50%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8511%\n",
      "layer   2  Sparsity: 66.8759%\n",
      "layer   3  Sparsity: 62.5642%\n",
      "total_backward_count 881100 real_backward_count 92718  10.523%\n",
      "fc layer 2 self.abs_max_out: 4519.0\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.896756/  2.010489, val:  56.25%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8538%\n",
      "layer   2  Sparsity: 65.5260%\n",
      "layer   3  Sparsity: 60.8053%\n",
      "total_backward_count 890890 real_backward_count 93576  10.504%\n",
      "fc layer 1 self.abs_max_out: 12362.0\n",
      "lif layer 1 self.abs_max_v: 21298.0\n",
      "lif layer 1 self.abs_max_v: 21560.5\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.881406/  1.992244, val:  63.33%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8617%\n",
      "layer   2  Sparsity: 65.7838%\n",
      "layer   3  Sparsity: 59.3884%\n",
      "total_backward_count 900680 real_backward_count 94389  10.480%\n",
      "fc layer 1 self.abs_max_out: 12394.0\n",
      "lif layer 1 self.abs_max_v: 22290.0\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.881474/  1.993017, val:  58.75%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.53 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8489%\n",
      "layer   2  Sparsity: 65.7495%\n",
      "layer   3  Sparsity: 60.9829%\n",
      "total_backward_count 910470 real_backward_count 95210  10.457%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.881030/  1.985781, val:  60.83%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 75.8626%\n",
      "layer   2  Sparsity: 65.7542%\n",
      "layer   3  Sparsity: 62.3027%\n",
      "total_backward_count 920260 real_backward_count 96077  10.440%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.878704/  2.000066, val:  61.25%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8570%\n",
      "layer   2  Sparsity: 65.5239%\n",
      "layer   3  Sparsity: 62.0874%\n",
      "total_backward_count 930050 real_backward_count 96909  10.420%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.879042/  1.981286, val:  65.42%, val_best:  65.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8557%\n",
      "layer   2  Sparsity: 66.2672%\n",
      "layer   3  Sparsity: 61.3774%\n",
      "total_backward_count 939840 real_backward_count 97711  10.397%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.869947/  1.991808, val:  50.83%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8491%\n",
      "layer   2  Sparsity: 66.1466%\n",
      "layer   3  Sparsity: 61.2280%\n",
      "total_backward_count 949630 real_backward_count 98604  10.383%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.877281/  1.984667, val:  61.67%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8391%\n",
      "layer   2  Sparsity: 65.8757%\n",
      "layer   3  Sparsity: 60.7183%\n",
      "total_backward_count 959420 real_backward_count 99432  10.364%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.873016/  2.006534, val:  60.00%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8309%\n",
      "layer   2  Sparsity: 65.5666%\n",
      "layer   3  Sparsity: 60.5920%\n",
      "total_backward_count 969210 real_backward_count 100209  10.339%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.868192/  1.996818, val:  58.33%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8502%\n",
      "layer   2  Sparsity: 65.0498%\n",
      "layer   3  Sparsity: 59.9178%\n",
      "total_backward_count 979000 real_backward_count 101029  10.320%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.863235/  1.991979, val:  62.50%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8444%\n",
      "layer   2  Sparsity: 65.3461%\n",
      "layer   3  Sparsity: 61.2616%\n",
      "total_backward_count 988790 real_backward_count 101842  10.300%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.869787/  1.985435, val:  57.92%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8527%\n",
      "layer   2  Sparsity: 65.8632%\n",
      "layer   3  Sparsity: 60.7075%\n",
      "total_backward_count 998580 real_backward_count 102637  10.278%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.864506/  1.985685, val:  56.67%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8360%\n",
      "layer   2  Sparsity: 65.4986%\n",
      "layer   3  Sparsity: 60.4808%\n",
      "total_backward_count 1008370 real_backward_count 103427  10.257%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.859007/  1.998725, val:  59.58%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8531%\n",
      "layer   2  Sparsity: 65.2016%\n",
      "layer   3  Sparsity: 61.9042%\n",
      "total_backward_count 1018160 real_backward_count 104180  10.232%\n",
      "lif layer 2 self.abs_max_v: 7278.0\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.863184/  1.995403, val:  54.58%, val_best:  65.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8513%\n",
      "layer   2  Sparsity: 65.6591%\n",
      "layer   3  Sparsity: 61.8938%\n",
      "total_backward_count 1027950 real_backward_count 104976  10.212%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.871785/  1.974160, val:  61.67%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8525%\n",
      "layer   2  Sparsity: 65.7066%\n",
      "layer   3  Sparsity: 60.5824%\n",
      "total_backward_count 1037740 real_backward_count 105855  10.201%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.859305/  1.985623, val:  55.42%, val_best:  65.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8499%\n",
      "layer   2  Sparsity: 65.9477%\n",
      "layer   3  Sparsity: 60.5532%\n",
      "total_backward_count 1047530 real_backward_count 106648  10.181%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.853832/  1.990967, val:  57.50%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8681%\n",
      "layer   2  Sparsity: 65.8498%\n",
      "layer   3  Sparsity: 59.8015%\n",
      "total_backward_count 1057320 real_backward_count 107461  10.164%\n",
      "fc layer 1 self.abs_max_out: 12680.0\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.866477/  2.014028, val:  61.67%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.21 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8286%\n",
      "layer   2  Sparsity: 65.6037%\n",
      "layer   3  Sparsity: 61.3139%\n",
      "total_backward_count 1067110 real_backward_count 108239  10.143%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.867001/  1.998498, val:  62.08%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.64 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8494%\n",
      "layer   2  Sparsity: 66.0487%\n",
      "layer   3  Sparsity: 60.4447%\n",
      "total_backward_count 1076900 real_backward_count 109078  10.129%\n",
      "lif layer 1 self.abs_max_v: 22592.5\n",
      "lif layer 2 self.abs_max_v: 7361.0\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.862554/  1.978479, val:  64.58%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8545%\n",
      "layer   2  Sparsity: 65.7868%\n",
      "layer   3  Sparsity: 61.1421%\n",
      "total_backward_count 1086690 real_backward_count 109859  10.110%\n",
      "lif layer 2 self.abs_max_v: 7380.0\n",
      "fc layer 1 self.abs_max_out: 12824.0\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.855470/  1.973289, val:  59.17%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8256%\n",
      "layer   2  Sparsity: 64.7521%\n",
      "layer   3  Sparsity: 61.2035%\n",
      "total_backward_count 1096480 real_backward_count 110682  10.094%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.849603/  1.967616, val:  59.58%, val_best:  65.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.14 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 75.8572%\n",
      "layer   2  Sparsity: 65.4414%\n",
      "layer   3  Sparsity: 62.1703%\n",
      "total_backward_count 1106270 real_backward_count 111473  10.076%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.855012/  1.985337, val:  55.83%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8426%\n",
      "layer   2  Sparsity: 65.0279%\n",
      "layer   3  Sparsity: 62.0477%\n",
      "total_backward_count 1116060 real_backward_count 112271  10.060%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.857020/  1.992892, val:  55.00%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8516%\n",
      "layer   2  Sparsity: 65.0155%\n",
      "layer   3  Sparsity: 63.4240%\n",
      "total_backward_count 1125850 real_backward_count 113099  10.046%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.861273/  1.982185, val:  60.83%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8279%\n",
      "layer   2  Sparsity: 64.9471%\n",
      "layer   3  Sparsity: 61.3136%\n",
      "total_backward_count 1135640 real_backward_count 113847  10.025%\n",
      "lif layer 2 self.abs_max_v: 7571.5\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.867367/  1.995247, val:  58.75%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8481%\n",
      "layer   2  Sparsity: 64.8744%\n",
      "layer   3  Sparsity: 61.5208%\n",
      "total_backward_count 1145430 real_backward_count 114653  10.010%\n",
      "lif layer 2 self.abs_max_v: 7637.5\n",
      "lif layer 2 self.abs_max_v: 7993.0\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.862380/  1.986374, val:  63.75%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8520%\n",
      "layer   2  Sparsity: 64.8492%\n",
      "layer   3  Sparsity: 62.5196%\n",
      "total_backward_count 1155220 real_backward_count 115460   9.995%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.859185/  1.993666, val:  57.08%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.43 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8635%\n",
      "layer   2  Sparsity: 64.9095%\n",
      "layer   3  Sparsity: 62.8155%\n",
      "total_backward_count 1165010 real_backward_count 116229   9.977%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.873381/  1.982467, val:  58.33%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8325%\n",
      "layer   2  Sparsity: 64.8553%\n",
      "layer   3  Sparsity: 62.5699%\n",
      "total_backward_count 1174800 real_backward_count 117045   9.963%\n",
      "lif layer 1 self.abs_max_v: 22631.5\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.863453/  1.981668, val:  61.25%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8240%\n",
      "layer   2  Sparsity: 64.6980%\n",
      "layer   3  Sparsity: 62.0514%\n",
      "total_backward_count 1184590 real_backward_count 117783   9.943%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.857295/  1.984699, val:  57.08%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8398%\n",
      "layer   2  Sparsity: 65.0026%\n",
      "layer   3  Sparsity: 61.7925%\n",
      "total_backward_count 1194380 real_backward_count 118560   9.926%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.861524/  1.977001, val:  59.17%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8614%\n",
      "layer   2  Sparsity: 64.2614%\n",
      "layer   3  Sparsity: 61.3028%\n",
      "total_backward_count 1204170 real_backward_count 119350   9.911%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.871296/  1.985309, val:  59.17%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.33 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8496%\n",
      "layer   2  Sparsity: 64.2557%\n",
      "layer   3  Sparsity: 61.0068%\n",
      "total_backward_count 1213960 real_backward_count 120132   9.896%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.868480/  1.981000, val:  67.08%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8487%\n",
      "layer   2  Sparsity: 64.6921%\n",
      "layer   3  Sparsity: 60.9753%\n",
      "total_backward_count 1223750 real_backward_count 120956   9.884%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.860889/  1.989412, val:  65.42%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8654%\n",
      "layer   2  Sparsity: 65.4687%\n",
      "layer   3  Sparsity: 59.9551%\n",
      "total_backward_count 1233540 real_backward_count 121772   9.872%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.858263/  1.989185, val:  64.17%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.50 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8374%\n",
      "layer   2  Sparsity: 65.8410%\n",
      "layer   3  Sparsity: 61.6180%\n",
      "total_backward_count 1243330 real_backward_count 122556   9.857%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.865758/  1.983887, val:  62.50%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8462%\n",
      "layer   2  Sparsity: 66.1919%\n",
      "layer   3  Sparsity: 62.1738%\n",
      "total_backward_count 1253120 real_backward_count 123305   9.840%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.871973/  1.986093, val:  59.58%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.02 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8460%\n",
      "layer   2  Sparsity: 65.7850%\n",
      "layer   3  Sparsity: 61.4741%\n",
      "total_backward_count 1262910 real_backward_count 124037   9.822%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.862523/  1.974948, val:  62.08%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.52 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8439%\n",
      "layer   2  Sparsity: 65.6700%\n",
      "layer   3  Sparsity: 62.0170%\n",
      "total_backward_count 1272700 real_backward_count 124777   9.804%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.859806/  1.982195, val:  65.42%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8648%\n",
      "layer   2  Sparsity: 65.4565%\n",
      "layer   3  Sparsity: 61.8800%\n",
      "total_backward_count 1282490 real_backward_count 125525   9.788%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.864336/  1.986332, val:  60.83%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8611%\n",
      "layer   2  Sparsity: 65.9482%\n",
      "layer   3  Sparsity: 61.1091%\n",
      "total_backward_count 1292280 real_backward_count 126294   9.773%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.866571/  1.998980, val:  59.17%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.90 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8535%\n",
      "layer   2  Sparsity: 65.4932%\n",
      "layer   3  Sparsity: 61.6922%\n",
      "total_backward_count 1302070 real_backward_count 127061   9.758%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.866607/  1.984144, val:  53.75%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.92 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8409%\n",
      "layer   2  Sparsity: 65.6363%\n",
      "layer   3  Sparsity: 59.7089%\n",
      "total_backward_count 1311860 real_backward_count 127869   9.747%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.869215/  1.981837, val:  57.92%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8698%\n",
      "layer   2  Sparsity: 65.2280%\n",
      "layer   3  Sparsity: 60.6802%\n",
      "total_backward_count 1321650 real_backward_count 128647   9.734%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.868683/  1.993988, val:  54.58%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.97 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8428%\n",
      "layer   2  Sparsity: 65.3780%\n",
      "layer   3  Sparsity: 60.3675%\n",
      "total_backward_count 1331440 real_backward_count 129408   9.719%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.868290/  1.977249, val:  66.67%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8736%\n",
      "layer   2  Sparsity: 65.1687%\n",
      "layer   3  Sparsity: 59.7600%\n",
      "total_backward_count 1341230 real_backward_count 130184   9.706%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.861593/  1.976556, val:  58.75%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.00 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8813%\n",
      "layer   2  Sparsity: 65.5034%\n",
      "layer   3  Sparsity: 60.4550%\n",
      "total_backward_count 1351020 real_backward_count 130916   9.690%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.856367/  1.970124, val:  62.08%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8483%\n",
      "layer   2  Sparsity: 65.1441%\n",
      "layer   3  Sparsity: 59.4810%\n",
      "total_backward_count 1360810 real_backward_count 131711   9.679%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.854956/  1.976226, val:  59.58%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8516%\n",
      "layer   2  Sparsity: 65.3094%\n",
      "layer   3  Sparsity: 59.5931%\n",
      "total_backward_count 1370600 real_backward_count 132487   9.666%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.855115/  1.980537, val:  59.17%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 75.8521%\n",
      "layer   2  Sparsity: 65.0482%\n",
      "layer   3  Sparsity: 61.1684%\n",
      "total_backward_count 1380390 real_backward_count 133265   9.654%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.850829/  1.970074, val:  60.00%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.27 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8464%\n",
      "layer   2  Sparsity: 64.6049%\n",
      "layer   3  Sparsity: 61.3208%\n",
      "total_backward_count 1390180 real_backward_count 133979   9.638%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.853583/  1.983070, val:  57.08%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.54 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8543%\n",
      "layer   2  Sparsity: 65.0187%\n",
      "layer   3  Sparsity: 59.7936%\n",
      "total_backward_count 1399970 real_backward_count 134785   9.628%\n",
      "fc layer 1 self.abs_max_out: 13053.0\n",
      "lif layer 1 self.abs_max_v: 22644.0\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.843329/  1.972852, val:  55.00%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8228%\n",
      "layer   2  Sparsity: 64.7931%\n",
      "layer   3  Sparsity: 59.3703%\n",
      "total_backward_count 1409760 real_backward_count 135526   9.613%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.846105/  1.966684, val:  55.83%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8285%\n",
      "layer   2  Sparsity: 64.5151%\n",
      "layer   3  Sparsity: 62.8568%\n",
      "total_backward_count 1419550 real_backward_count 136310   9.602%\n",
      "fc layer 1 self.abs_max_out: 13613.0\n",
      "lif layer 1 self.abs_max_v: 22906.0\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.838103/  1.968127, val:  62.92%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8329%\n",
      "layer   2  Sparsity: 64.0904%\n",
      "layer   3  Sparsity: 61.7523%\n",
      "total_backward_count 1429340 real_backward_count 137108   9.592%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.854987/  1.988842, val:  57.92%, val_best:  67.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 79.46 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8530%\n",
      "layer   2  Sparsity: 64.6262%\n",
      "layer   3  Sparsity: 61.6493%\n",
      "total_backward_count 1439130 real_backward_count 137877   9.581%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.852748/  1.974925, val:  59.58%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8469%\n",
      "layer   2  Sparsity: 64.5681%\n",
      "layer   3  Sparsity: 61.9888%\n",
      "total_backward_count 1448920 real_backward_count 138581   9.564%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.854257/  1.991615, val:  57.50%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.23 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8434%\n",
      "layer   2  Sparsity: 64.8846%\n",
      "layer   3  Sparsity: 60.5212%\n",
      "total_backward_count 1458710 real_backward_count 139315   9.551%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.861882/  1.983577, val:  60.42%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8495%\n",
      "layer   2  Sparsity: 64.7126%\n",
      "layer   3  Sparsity: 61.4958%\n",
      "total_backward_count 1468500 real_backward_count 140053   9.537%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.857876/  1.983695, val:  60.83%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.22 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8704%\n",
      "layer   2  Sparsity: 65.1207%\n",
      "layer   3  Sparsity: 61.1635%\n",
      "total_backward_count 1478290 real_backward_count 140772   9.523%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.864129/  1.990695, val:  55.83%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8535%\n",
      "layer   2  Sparsity: 66.0047%\n",
      "layer   3  Sparsity: 60.8500%\n",
      "total_backward_count 1488080 real_backward_count 141531   9.511%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.853903/  1.982771, val:  62.50%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.15 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8322%\n",
      "layer   2  Sparsity: 65.5579%\n",
      "layer   3  Sparsity: 61.4712%\n",
      "total_backward_count 1497870 real_backward_count 142263   9.498%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.847821/  1.971004, val:  59.17%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.41 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8550%\n",
      "layer   2  Sparsity: 66.1611%\n",
      "layer   3  Sparsity: 60.2144%\n",
      "total_backward_count 1507660 real_backward_count 143052   9.488%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.841617/  1.978171, val:  61.25%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8413%\n",
      "layer   2  Sparsity: 66.6575%\n",
      "layer   3  Sparsity: 60.8040%\n",
      "total_backward_count 1517450 real_backward_count 143771   9.475%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.838371/  1.960434, val:  64.58%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.53 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8626%\n",
      "layer   2  Sparsity: 66.2153%\n",
      "layer   3  Sparsity: 59.3801%\n",
      "total_backward_count 1527240 real_backward_count 144487   9.461%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.831838/  1.975183, val:  60.42%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8397%\n",
      "layer   2  Sparsity: 66.2197%\n",
      "layer   3  Sparsity: 59.6199%\n",
      "total_backward_count 1537030 real_backward_count 145224   9.448%\n",
      "lif layer 1 self.abs_max_v: 22909.5\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.838420/  1.960254, val:  63.33%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.47 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8593%\n",
      "layer   2  Sparsity: 67.2535%\n",
      "layer   3  Sparsity: 60.5881%\n",
      "total_backward_count 1546820 real_backward_count 145962   9.436%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.831556/  1.957520, val:  56.67%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8456%\n",
      "layer   2  Sparsity: 67.3218%\n",
      "layer   3  Sparsity: 61.3717%\n",
      "total_backward_count 1556610 real_backward_count 146704   9.425%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.837677/  1.977413, val:  59.58%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.14 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8497%\n",
      "layer   2  Sparsity: 67.3861%\n",
      "layer   3  Sparsity: 61.7740%\n",
      "total_backward_count 1566400 real_backward_count 147421   9.411%\n",
      "lif layer 1 self.abs_max_v: 23019.0\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.842527/  1.967230, val:  60.83%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8633%\n",
      "layer   2  Sparsity: 66.9397%\n",
      "layer   3  Sparsity: 60.3642%\n",
      "total_backward_count 1576190 real_backward_count 148148   9.399%\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.839264/  1.970899, val:  64.17%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.49 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8309%\n",
      "layer   2  Sparsity: 67.1003%\n",
      "layer   3  Sparsity: 60.6898%\n",
      "total_backward_count 1585980 real_backward_count 148880   9.387%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.840902/  1.964328, val:  60.00%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.03 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8567%\n",
      "layer   2  Sparsity: 67.3281%\n",
      "layer   3  Sparsity: 60.2237%\n",
      "total_backward_count 1595770 real_backward_count 149600   9.375%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.833297/  1.965256, val:  65.83%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.06 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8437%\n",
      "layer   2  Sparsity: 66.9445%\n",
      "layer   3  Sparsity: 59.7231%\n",
      "total_backward_count 1605560 real_backward_count 150349   9.364%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.837128/  1.966764, val:  60.83%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.35 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8500%\n",
      "layer   2  Sparsity: 66.7475%\n",
      "layer   3  Sparsity: 60.1058%\n",
      "total_backward_count 1615350 real_backward_count 151112   9.355%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.829842/  1.964576, val:  55.00%, val_best:  67.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8399%\n",
      "layer   2  Sparsity: 66.4341%\n",
      "layer   3  Sparsity: 60.9737%\n",
      "total_backward_count 1625140 real_backward_count 151871   9.345%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.822849/  1.968386, val:  59.17%, val_best:  67.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.74 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8677%\n",
      "layer   2  Sparsity: 66.7548%\n",
      "layer   3  Sparsity: 60.5049%\n",
      "total_backward_count 1634930 real_backward_count 152577   9.332%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.820306/  1.972540, val:  55.00%, val_best:  67.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8652%\n",
      "layer   2  Sparsity: 66.3160%\n",
      "layer   3  Sparsity: 61.2587%\n",
      "total_backward_count 1644720 real_backward_count 153297   9.321%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.828396/  1.947117, val:  67.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8318%\n",
      "layer   2  Sparsity: 66.2163%\n",
      "layer   3  Sparsity: 60.1128%\n",
      "total_backward_count 1654510 real_backward_count 154027   9.310%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.824651/  1.951820, val:  57.92%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8582%\n",
      "layer   2  Sparsity: 65.8906%\n",
      "layer   3  Sparsity: 60.2388%\n",
      "total_backward_count 1664300 real_backward_count 154783   9.300%\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.822040/  1.961687, val:  55.00%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.94 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8518%\n",
      "layer   2  Sparsity: 65.8560%\n",
      "layer   3  Sparsity: 60.7150%\n",
      "total_backward_count 1674090 real_backward_count 155510   9.289%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.826240/  1.956281, val:  58.33%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.44 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8439%\n",
      "layer   2  Sparsity: 65.5164%\n",
      "layer   3  Sparsity: 58.9701%\n",
      "total_backward_count 1683880 real_backward_count 156210   9.277%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.833316/  1.963119, val:  59.17%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8256%\n",
      "layer   2  Sparsity: 65.4995%\n",
      "layer   3  Sparsity: 60.1431%\n",
      "total_backward_count 1693670 real_backward_count 156946   9.267%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.833289/  1.974815, val:  61.67%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.24 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8427%\n",
      "layer   2  Sparsity: 65.7373%\n",
      "layer   3  Sparsity: 59.9224%\n",
      "total_backward_count 1703460 real_backward_count 157667   9.256%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.834439/  1.971884, val:  60.00%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8408%\n",
      "layer   2  Sparsity: 65.6760%\n",
      "layer   3  Sparsity: 59.7365%\n",
      "total_backward_count 1713250 real_backward_count 158376   9.244%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.831413/  1.965372, val:  69.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.86 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8382%\n",
      "layer   2  Sparsity: 65.3691%\n",
      "layer   3  Sparsity: 59.7062%\n",
      "total_backward_count 1723040 real_backward_count 159094   9.233%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.839949/  1.976954, val:  63.33%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8385%\n",
      "layer   2  Sparsity: 65.1691%\n",
      "layer   3  Sparsity: 60.8203%\n",
      "total_backward_count 1732830 real_backward_count 159818   9.223%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.841473/  1.967855, val:  57.08%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8279%\n",
      "layer   2  Sparsity: 64.9380%\n",
      "layer   3  Sparsity: 60.8986%\n",
      "total_backward_count 1742620 real_backward_count 160520   9.211%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.832972/  1.981804, val:  55.00%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8550%\n",
      "layer   2  Sparsity: 64.6605%\n",
      "layer   3  Sparsity: 61.1281%\n",
      "total_backward_count 1752410 real_backward_count 161221   9.200%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.834060/  1.978919, val:  56.25%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.57 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8619%\n",
      "layer   2  Sparsity: 64.4890%\n",
      "layer   3  Sparsity: 60.5057%\n",
      "total_backward_count 1762200 real_backward_count 161932   9.189%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.835623/  1.960681, val:  58.33%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8467%\n",
      "layer   2  Sparsity: 65.1242%\n",
      "layer   3  Sparsity: 61.5802%\n",
      "total_backward_count 1771990 real_backward_count 162623   9.177%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.826438/  1.967391, val:  66.25%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8319%\n",
      "layer   2  Sparsity: 64.8070%\n",
      "layer   3  Sparsity: 60.8933%\n",
      "total_backward_count 1781780 real_backward_count 163312   9.166%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.834847/  1.981820, val:  64.17%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.93 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8473%\n",
      "layer   2  Sparsity: 64.5599%\n",
      "layer   3  Sparsity: 61.0400%\n",
      "total_backward_count 1791570 real_backward_count 164001   9.154%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.843934/  1.959618, val:  58.75%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8392%\n",
      "layer   2  Sparsity: 64.2995%\n",
      "layer   3  Sparsity: 61.3255%\n",
      "total_backward_count 1801360 real_backward_count 164726   9.145%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.832398/  1.966904, val:  58.75%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.36 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8591%\n",
      "layer   2  Sparsity: 64.4345%\n",
      "layer   3  Sparsity: 61.0034%\n",
      "total_backward_count 1811150 real_backward_count 165402   9.132%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.825931/  1.965331, val:  62.08%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.18 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8248%\n",
      "layer   2  Sparsity: 64.7797%\n",
      "layer   3  Sparsity: 61.3061%\n",
      "total_backward_count 1820940 real_backward_count 166132   9.123%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.831345/  1.975925, val:  62.50%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8624%\n",
      "layer   2  Sparsity: 64.9626%\n",
      "layer   3  Sparsity: 61.8263%\n",
      "total_backward_count 1830730 real_backward_count 166837   9.113%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.837508/  1.977380, val:  60.42%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8514%\n",
      "layer   2  Sparsity: 64.8733%\n",
      "layer   3  Sparsity: 61.6407%\n",
      "total_backward_count 1840520 real_backward_count 167574   9.105%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.835037/  1.971035, val:  57.92%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8595%\n",
      "layer   2  Sparsity: 64.8408%\n",
      "layer   3  Sparsity: 60.5577%\n",
      "total_backward_count 1850310 real_backward_count 168290   9.095%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.833960/  1.956265, val:  60.83%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.04 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 75.8474%\n",
      "layer   2  Sparsity: 64.7173%\n",
      "layer   3  Sparsity: 59.3922%\n",
      "total_backward_count 1860100 real_backward_count 169006   9.086%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.824411/  1.946352, val:  67.92%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.19 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8483%\n",
      "layer   2  Sparsity: 64.8519%\n",
      "layer   3  Sparsity: 60.8875%\n",
      "total_backward_count 1869890 real_backward_count 169718   9.076%\n",
      "fc layer 1 self.abs_max_out: 13783.0\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.830799/  1.985975, val:  55.83%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8392%\n",
      "layer   2  Sparsity: 64.5792%\n",
      "layer   3  Sparsity: 59.6943%\n",
      "total_backward_count 1879680 real_backward_count 170432   9.067%\n",
      "lif layer 1 self.abs_max_v: 23043.5\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.825210/  1.949083, val:  65.42%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8517%\n",
      "layer   2  Sparsity: 64.1753%\n",
      "layer   3  Sparsity: 59.1678%\n",
      "total_backward_count 1889470 real_backward_count 171126   9.057%\n",
      "lif layer 1 self.abs_max_v: 23078.5\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.823298/  1.965246, val:  57.92%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.84 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8453%\n",
      "layer   2  Sparsity: 64.5058%\n",
      "layer   3  Sparsity: 59.8200%\n",
      "total_backward_count 1899260 real_backward_count 171789   9.045%\n",
      "lif layer 1 self.abs_max_v: 23123.0\n",
      "lif layer 1 self.abs_max_v: 23313.5\n",
      "lif layer 1 self.abs_max_v: 23540.0\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.821617/  1.954346, val:  60.00%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.32 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8406%\n",
      "layer   2  Sparsity: 63.9889%\n",
      "layer   3  Sparsity: 60.6630%\n",
      "total_backward_count 1909050 real_backward_count 172456   9.034%\n",
      "lif layer 1 self.abs_max_v: 23553.0\n",
      "lif layer 1 self.abs_max_v: 24270.5\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.832826/  1.962784, val:  54.17%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8226%\n",
      "layer   2  Sparsity: 63.6760%\n",
      "layer   3  Sparsity: 61.4576%\n",
      "total_backward_count 1918840 real_backward_count 173181   9.025%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.837072/  1.967639, val:  62.08%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.25 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 75.8296%\n",
      "layer   2  Sparsity: 63.9395%\n",
      "layer   3  Sparsity: 60.8347%\n",
      "total_backward_count 1928630 real_backward_count 173918   9.018%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.841971/  1.966220, val:  55.83%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8510%\n",
      "layer   2  Sparsity: 64.2156%\n",
      "layer   3  Sparsity: 60.5012%\n",
      "total_backward_count 1938420 real_backward_count 174628   9.009%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.832422/  1.959395, val:  59.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 75.8546%\n",
      "layer   2  Sparsity: 64.9463%\n",
      "layer   3  Sparsity: 60.1004%\n",
      "total_backward_count 1948210 real_backward_count 175313   8.999%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.828140/  1.951962, val:  59.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.74 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 75.8472%\n",
      "layer   2  Sparsity: 65.0802%\n",
      "layer   3  Sparsity: 60.4974%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cef9d4aed4447e49fddbe7302b242b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñá‚ñà‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99898</td></tr><tr><td>tr_epoch_loss</td><td>1.82814</td></tr><tr><td>val_acc_best</td><td>0.69167</td></tr><tr><td>val_acc_now</td><td>0.59167</td></tr><tr><td>val_loss</td><td>1.95196</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">celestial-sweep-94</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fbwrlyzc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/fbwrlyzc</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_130729-fbwrlyzc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jetvksx5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_173139-jetvksx5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jetvksx5' target=\"_blank\">dry-sweep-98</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jetvksx5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jetvksx5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251116_173148_826', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 30, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 8, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 186.0\n",
      "lif layer 1 self.abs_max_v: 186.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 338.0\n",
      "lif layer 2 self.abs_max_v: 338.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 177.0\n",
      "fc layer 2 self.abs_max_out: 360.0\n",
      "lif layer 2 self.abs_max_v: 371.0\n",
      "fc layer 1 self.abs_max_out: 189.0\n",
      "lif layer 1 self.abs_max_v: 231.5\n",
      "lif layer 2 self.abs_max_v: 455.0\n",
      "lif layer 1 self.abs_max_v: 274.0\n",
      "fc layer 1 self.abs_max_out: 196.0\n",
      "lif layer 1 self.abs_max_v: 333.0\n",
      "fc layer 3 self.abs_max_out: 178.0\n",
      "lif layer 2 self.abs_max_v: 464.0\n",
      "fc layer 2 self.abs_max_out: 377.0\n",
      "lif layer 2 self.abs_max_v: 576.5\n",
      "fc layer 1 self.abs_max_out: 248.0\n",
      "fc layer 2 self.abs_max_out: 410.0\n",
      "fc layer 1 self.abs_max_out: 307.0\n",
      "lif layer 1 self.abs_max_v: 427.0\n",
      "fc layer 2 self.abs_max_out: 467.0\n",
      "lif layer 2 self.abs_max_v: 638.5\n",
      "lif layer 1 self.abs_max_v: 429.0\n",
      "lif layer 2 self.abs_max_v: 656.0\n",
      "fc layer 2 self.abs_max_out: 501.0\n",
      "fc layer 2 self.abs_max_out: 565.0\n",
      "lif layer 2 self.abs_max_v: 738.0\n",
      "fc layer 3 self.abs_max_out: 232.0\n",
      "fc layer 1 self.abs_max_out: 335.0\n",
      "fc layer 1 self.abs_max_out: 356.0\n",
      "fc layer 2 self.abs_max_out: 599.0\n",
      "fc layer 2 self.abs_max_out: 629.0\n",
      "fc layer 3 self.abs_max_out: 245.0\n",
      "lif layer 1 self.abs_max_v: 450.5\n",
      "fc layer 1 self.abs_max_out: 386.0\n",
      "lif layer 1 self.abs_max_v: 463.0\n",
      "lif layer 1 self.abs_max_v: 550.0\n",
      "lif layer 1 self.abs_max_v: 634.0\n",
      "lif layer 2 self.abs_max_v: 763.5\n",
      "lif layer 2 self.abs_max_v: 819.0\n",
      "lif layer 2 self.abs_max_v: 849.5\n",
      "fc layer 1 self.abs_max_out: 515.0\n",
      "fc layer 3 self.abs_max_out: 277.0\n",
      "lif layer 2 self.abs_max_v: 875.5\n",
      "lif layer 2 self.abs_max_v: 912.0\n",
      "fc layer 3 self.abs_max_out: 304.0\n",
      "lif layer 2 self.abs_max_v: 959.0\n",
      "lif layer 1 self.abs_max_v: 668.0\n",
      "fc layer 2 self.abs_max_out: 631.0\n",
      "lif layer 2 self.abs_max_v: 1012.0\n",
      "fc layer 2 self.abs_max_out: 692.0\n",
      "lif layer 1 self.abs_max_v: 717.5\n",
      "fc layer 3 self.abs_max_out: 316.0\n",
      "lif layer 2 self.abs_max_v: 1052.0\n",
      "lif layer 2 self.abs_max_v: 1052.5\n",
      "fc layer 3 self.abs_max_out: 322.0\n",
      "lif layer 1 self.abs_max_v: 721.0\n",
      "lif layer 1 self.abs_max_v: 822.0\n",
      "fc layer 2 self.abs_max_out: 702.0\n",
      "fc layer 2 self.abs_max_out: 714.0\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.325998/  2.322866, val:  16.67%, val_best:  16.67%, tr:  16.14%, tr_best:  16.14%, epoch time: 80.69 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 93.7899%\n",
      "layer   2  Sparsity: 80.8717%\n",
      "layer   3  Sparsity: 76.0488%\n",
      "total_backward_count 9790 real_backward_count 8520  87.028%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 333.0\n",
      "lif layer 2 self.abs_max_v: 1086.5\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.321071/  2.322866, val:  16.67%, val_best:  16.67%, tr:  15.02%, tr_best:  16.14%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 93.7889%\n",
      "layer   2  Sparsity: 80.9032%\n",
      "layer   3  Sparsity: 76.0740%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c148f231f01d49e1ac4addfbd31da7ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>tr_acc</td><td>‚ñà‚ñÅ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.15015</td></tr><tr><td>tr_epoch_loss</td><td>2.32107</td></tr><tr><td>val_acc_best</td><td>0.16667</td></tr><tr><td>val_acc_now</td><td>0.16667</td></tr><tr><td>val_loss</td><td>2.32287</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dry-sweep-98</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jetvksx5' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/jetvksx5</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_173139-jetvksx5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run jetvksx5 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_15806/2189894352.py\", line 114, in hyper_iter\n",
      "    my_snn_system(\n",
      "  File \"/tmp/ipykernel_15806/256537533.py\", line 952, in my_snn_system\n",
      "    assert val_acc_best > 0.2\n",
      "AssertionError\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run jetvksx5 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/bhkim003/anaconda3/envs/aedat2/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 307, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_15806/2189894352.py\", line 114, in hyper_iter\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     my_snn_system(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_15806/256537533.py\", line 952, in my_snn_system\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     assert val_acc_best > 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AssertionError\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 756ifoym with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_173446-756ifoym</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/756ifoym' target=\"_blank\">sunny-sweep-99</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/756ifoym' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/756ifoym</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251116_173455_566', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 8, 'leaky_temporal_filter': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 1402.0\n",
      "lif layer 1 self.abs_max_v: 1402.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1982.0\n",
      "lif layer 2 self.abs_max_v: 1982.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 798.0\n",
      "lif layer 1 self.abs_max_v: 1434.5\n",
      "fc layer 2 self.abs_max_out: 2073.0\n",
      "lif layer 2 self.abs_max_v: 2473.5\n",
      "lif layer 1 self.abs_max_v: 1713.5\n",
      "lif layer 2 self.abs_max_v: 2878.0\n",
      "lif layer 1 self.abs_max_v: 1824.0\n",
      "fc layer 2 self.abs_max_out: 2214.0\n",
      "lif layer 2 self.abs_max_v: 3559.0\n",
      "lif layer 1 self.abs_max_v: 1963.5\n",
      "fc layer 2 self.abs_max_out: 2519.0\n",
      "lif layer 2 self.abs_max_v: 3860.5\n",
      "lif layer 2 self.abs_max_v: 4072.0\n",
      "lif layer 2 self.abs_max_v: 4115.0\n",
      "fc layer 1 self.abs_max_out: 1908.0\n",
      "fc layer 1 self.abs_max_out: 1911.0\n",
      "lif layer 1 self.abs_max_v: 2239.0\n",
      "fc layer 1 self.abs_max_out: 2026.0\n",
      "lif layer 1 self.abs_max_v: 2859.0\n",
      "fc layer 1 self.abs_max_out: 2126.0\n",
      "fc layer 3 self.abs_max_out: 939.0\n",
      "lif layer 1 self.abs_max_v: 2954.5\n",
      "fc layer 2 self.abs_max_out: 2649.0\n",
      "fc layer 1 self.abs_max_out: 2249.0\n",
      "lif layer 1 self.abs_max_v: 2988.0\n",
      "lif layer 1 self.abs_max_v: 3426.0\n",
      "lif layer 2 self.abs_max_v: 4257.0\n",
      "lif layer 1 self.abs_max_v: 3452.0\n",
      "lif layer 1 self.abs_max_v: 3462.0\n",
      "fc layer 3 self.abs_max_out: 1056.0\n",
      "fc layer 3 self.abs_max_out: 1097.0\n",
      "fc layer 1 self.abs_max_out: 2719.0\n",
      "lif layer 1 self.abs_max_v: 3661.5\n",
      "fc layer 2 self.abs_max_out: 2781.0\n",
      "lif layer 2 self.abs_max_v: 4523.5\n",
      "fc layer 2 self.abs_max_out: 2782.0\n",
      "fc layer 2 self.abs_max_out: 2902.0\n",
      "fc layer 2 self.abs_max_out: 3017.0\n",
      "lif layer 2 self.abs_max_v: 4811.5\n",
      "lif layer 2 self.abs_max_v: 4978.5\n",
      "lif layer 1 self.abs_max_v: 3888.5\n",
      "lif layer 1 self.abs_max_v: 4055.5\n",
      "fc layer 1 self.abs_max_out: 2761.0\n",
      "lif layer 1 self.abs_max_v: 4703.0\n",
      "fc layer 1 self.abs_max_out: 2802.0\n",
      "fc layer 1 self.abs_max_out: 2881.0\n",
      "fc layer 1 self.abs_max_out: 2967.0\n",
      "fc layer 1 self.abs_max_out: 3411.0\n",
      "fc layer 1 self.abs_max_out: 3470.0\n",
      "lif layer 1 self.abs_max_v: 4828.0\n",
      "lif layer 1 self.abs_max_v: 4830.5\n",
      "lif layer 2 self.abs_max_v: 5116.5\n",
      "lif layer 2 self.abs_max_v: 5132.0\n",
      "lif layer 2 self.abs_max_v: 5415.0\n",
      "lif layer 1 self.abs_max_v: 4993.0\n",
      "lif layer 1 self.abs_max_v: 5054.0\n",
      "lif layer 1 self.abs_max_v: 5588.5\n",
      "lif layer 1 self.abs_max_v: 5706.5\n",
      "fc layer 2 self.abs_max_out: 3103.0\n",
      "fc layer 1 self.abs_max_out: 3837.0\n",
      "fc layer 1 self.abs_max_out: 3839.0\n",
      "lif layer 1 self.abs_max_v: 5860.0\n",
      "lif layer 1 self.abs_max_v: 6079.5\n",
      "lif layer 1 self.abs_max_v: 6516.0\n",
      "fc layer 1 self.abs_max_out: 4022.0\n",
      "lif layer 1 self.abs_max_v: 6572.0\n",
      "fc layer 1 self.abs_max_out: 4024.0\n",
      "lif layer 1 self.abs_max_v: 6729.5\n",
      "lif layer 1 self.abs_max_v: 6938.0\n",
      "lif layer 1 self.abs_max_v: 7401.0\n",
      "fc layer 1 self.abs_max_out: 4035.0\n",
      "lif layer 1 self.abs_max_v: 7442.5\n",
      "fc layer 1 self.abs_max_out: 4515.0\n",
      "lif layer 1 self.abs_max_v: 7686.5\n",
      "lif layer 1 self.abs_max_v: 7726.0\n",
      "lif layer 1 self.abs_max_v: 8175.0\n",
      "fc layer 1 self.abs_max_out: 4729.0\n",
      "fc layer 3 self.abs_max_out: 1102.0\n",
      "fc layer 3 self.abs_max_out: 1103.0\n",
      "fc layer 3 self.abs_max_out: 1172.0\n",
      "fc layer 1 self.abs_max_out: 4792.0\n",
      "lif layer 1 self.abs_max_v: 8303.0\n",
      "fc layer 1 self.abs_max_out: 4901.0\n",
      "lif layer 1 self.abs_max_v: 9015.0\n",
      "fc layer 1 self.abs_max_out: 5037.0\n",
      "lif layer 1 self.abs_max_v: 9291.5\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  1.937546/  1.992080, val:  45.00%, val_best:  45.00%, tr:  83.86%, tr_best:  83.86%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 84.7881%\n",
      "layer   2  Sparsity: 71.5757%\n",
      "layer   3  Sparsity: 66.8771%\n",
      "total_backward_count 9790 real_backward_count 3314  33.851%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 1207.0\n",
      "fc layer 2 self.abs_max_out: 3162.0\n",
      "fc layer 3 self.abs_max_out: 1272.0\n",
      "fc layer 3 self.abs_max_out: 1319.0\n",
      "lif layer 2 self.abs_max_v: 5435.0\n",
      "lif layer 2 self.abs_max_v: 5543.5\n",
      "fc layer 1 self.abs_max_out: 5092.0\n",
      "fc layer 1 self.abs_max_out: 5150.0\n",
      "fc layer 1 self.abs_max_out: 5592.0\n",
      "lif layer 1 self.abs_max_v: 9807.5\n",
      "lif layer 1 self.abs_max_v: 10173.0\n",
      "lif layer 1 self.abs_max_v: 10300.0\n",
      "fc layer 2 self.abs_max_out: 3412.0\n",
      "fc layer 3 self.abs_max_out: 1394.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  1.845829/  1.971334, val:  50.42%, val_best:  50.42%, tr:  94.99%, tr_best:  94.99%, epoch time: 78.49 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 84.8058%\n",
      "layer   2  Sparsity: 75.1089%\n",
      "layer   3  Sparsity: 66.8819%\n",
      "total_backward_count 19580 real_backward_count 5321  27.176%\n",
      "lif layer 2 self.abs_max_v: 5608.0\n",
      "fc layer 1 self.abs_max_out: 5716.0\n",
      "lif layer 1 self.abs_max_v: 10731.5\n",
      "lif layer 1 self.abs_max_v: 10975.0\n",
      "lif layer 2 self.abs_max_v: 5699.5\n",
      "fc layer 3 self.abs_max_out: 1412.0\n",
      "fc layer 1 self.abs_max_out: 5748.0\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  1.839736/  1.965735, val:  58.75%, val_best:  58.75%, tr:  96.42%, tr_best:  96.42%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 84.7985%\n",
      "layer   2  Sparsity: 74.8166%\n",
      "layer   3  Sparsity: 66.8914%\n",
      "total_backward_count 29370 real_backward_count 7078  24.099%\n",
      "fc layer 1 self.abs_max_out: 6062.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  1.837535/  1.967549, val:  58.33%, val_best:  58.75%, tr:  98.57%, tr_best:  98.57%, epoch time: 79.20 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 84.7596%\n",
      "layer   2  Sparsity: 75.9377%\n",
      "layer   3  Sparsity: 66.8989%\n",
      "total_backward_count 39160 real_backward_count 8548  21.828%\n",
      "fc layer 1 self.abs_max_out: 6528.0\n",
      "lif layer 1 self.abs_max_v: 10987.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  1.835798/  1.959964, val:  53.33%, val_best:  58.75%, tr:  98.47%, tr_best:  98.57%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 84.7689%\n",
      "layer   2  Sparsity: 76.5967%\n",
      "layer   3  Sparsity: 66.6607%\n",
      "total_backward_count 48950 real_backward_count 9959  20.345%\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  1.834700/  1.986173, val:  57.92%, val_best:  58.75%, tr:  98.77%, tr_best:  98.77%, epoch time: 72.46 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7775%\n",
      "layer   2  Sparsity: 76.9214%\n",
      "layer   3  Sparsity: 66.4889%\n",
      "total_backward_count 58740 real_backward_count 11274  19.193%\n",
      "lif layer 1 self.abs_max_v: 11045.5\n",
      "lif layer 1 self.abs_max_v: 11208.5\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  1.844514/  1.959707, val:  59.58%, val_best:  59.58%, tr:  99.28%, tr_best:  99.28%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7790%\n",
      "layer   2  Sparsity: 77.2459%\n",
      "layer   3  Sparsity: 65.5397%\n",
      "total_backward_count 68530 real_backward_count 12516  18.264%\n",
      "fc layer 1 self.abs_max_out: 6806.0\n",
      "fc layer 2 self.abs_max_out: 3508.0\n",
      "lif layer 1 self.abs_max_v: 11266.5\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  1.845827/  1.990821, val:  51.67%, val_best:  59.58%, tr:  99.18%, tr_best:  99.28%, epoch time: 72.38 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7797%\n",
      "layer   2  Sparsity: 76.9142%\n",
      "layer   3  Sparsity: 65.4941%\n",
      "total_backward_count 78320 real_backward_count 13664  17.446%\n",
      "lif layer 1 self.abs_max_v: 11430.0\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  1.878753/  1.982323, val:  62.50%, val_best:  62.50%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7748%\n",
      "layer   2  Sparsity: 76.4574%\n",
      "layer   3  Sparsity: 66.3820%\n",
      "total_backward_count 88110 real_backward_count 14786  16.781%\n",
      "lif layer 1 self.abs_max_v: 11534.0\n",
      "fc layer 1 self.abs_max_out: 6866.0\n",
      "lif layer 1 self.abs_max_v: 11976.0\n",
      "fc layer 1 self.abs_max_out: 7063.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  1.870837/  1.996499, val:  58.33%, val_best:  62.50%, tr:  99.49%, tr_best:  99.80%, epoch time: 72.72 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8091%\n",
      "layer   2  Sparsity: 76.2747%\n",
      "layer   3  Sparsity: 66.9752%\n",
      "total_backward_count 97900 real_backward_count 15882  16.223%\n",
      "lif layer 1 self.abs_max_v: 12157.0\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  1.871620/  1.977623, val:  59.17%, val_best:  62.50%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.89 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7809%\n",
      "layer   2  Sparsity: 77.1797%\n",
      "layer   3  Sparsity: 66.7244%\n",
      "total_backward_count 107690 real_backward_count 16914  15.706%\n",
      "lif layer 1 self.abs_max_v: 12177.0\n",
      "lif layer 1 self.abs_max_v: 12377.5\n",
      "fc layer 1 self.abs_max_out: 7402.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  1.869791/  1.971346, val:  61.25%, val_best:  62.50%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7845%\n",
      "layer   2  Sparsity: 77.6893%\n",
      "layer   3  Sparsity: 66.3532%\n",
      "total_backward_count 117480 real_backward_count 17984  15.308%\n",
      "lif layer 2 self.abs_max_v: 5782.0\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  1.865016/  1.995585, val:  52.50%, val_best:  62.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7734%\n",
      "layer   2  Sparsity: 77.6211%\n",
      "layer   3  Sparsity: 66.0891%\n",
      "total_backward_count 127270 real_backward_count 18993  14.923%\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  1.879912/  1.993195, val:  54.58%, val_best:  62.50%, tr:  99.49%, tr_best:  99.90%, epoch time: 72.38 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8036%\n",
      "layer   2  Sparsity: 78.0837%\n",
      "layer   3  Sparsity: 66.2415%\n",
      "total_backward_count 137060 real_backward_count 19956  14.560%\n",
      "lif layer 1 self.abs_max_v: 12504.5\n",
      "lif layer 1 self.abs_max_v: 12676.5\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  1.882839/  1.984633, val:  58.75%, val_best:  62.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.8021%\n",
      "layer   2  Sparsity: 77.9624%\n",
      "layer   3  Sparsity: 66.1446%\n",
      "total_backward_count 146850 real_backward_count 20923  14.248%\n",
      "lif layer 1 self.abs_max_v: 12867.5\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  1.883092/  1.995222, val:  62.08%, val_best:  62.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 73.00 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.8058%\n",
      "layer   2  Sparsity: 78.5102%\n",
      "layer   3  Sparsity: 66.2799%\n",
      "total_backward_count 156640 real_backward_count 21919  13.993%\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  1.880662/  1.979622, val:  64.58%, val_best:  64.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7896%\n",
      "layer   2  Sparsity: 77.7864%\n",
      "layer   3  Sparsity: 65.5940%\n",
      "total_backward_count 166430 real_backward_count 22894  13.756%\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  1.876015/  1.978811, val:  64.58%, val_best:  64.58%, tr:  99.59%, tr_best:  99.90%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7955%\n",
      "layer   2  Sparsity: 78.1102%\n",
      "layer   3  Sparsity: 66.6891%\n",
      "total_backward_count 176220 real_backward_count 23858  13.539%\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  1.883673/  1.983506, val:  60.42%, val_best:  64.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7564%\n",
      "layer   2  Sparsity: 78.0712%\n",
      "layer   3  Sparsity: 66.1897%\n",
      "total_backward_count 186010 real_backward_count 24818  13.342%\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  1.883967/  2.010129, val:  54.17%, val_best:  64.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 72.47 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8093%\n",
      "layer   2  Sparsity: 77.3175%\n",
      "layer   3  Sparsity: 66.0478%\n",
      "total_backward_count 195800 real_backward_count 25743  13.148%\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  1.884166/  1.997499, val:  60.42%, val_best:  64.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7936%\n",
      "layer   2  Sparsity: 77.4658%\n",
      "layer   3  Sparsity: 67.0119%\n",
      "total_backward_count 205590 real_backward_count 26634  12.955%\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  1.897232/  1.994274, val:  57.92%, val_best:  64.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.37 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7934%\n",
      "layer   2  Sparsity: 77.6739%\n",
      "layer   3  Sparsity: 67.4529%\n",
      "total_backward_count 215380 real_backward_count 27631  12.829%\n",
      "lif layer 1 self.abs_max_v: 12874.0\n",
      "fc layer 1 self.abs_max_out: 7781.0\n",
      "lif layer 1 self.abs_max_v: 13040.0\n",
      "lif layer 1 self.abs_max_v: 13100.0\n",
      "lif layer 1 self.abs_max_v: 13385.0\n",
      "fc layer 1 self.abs_max_out: 8026.0\n",
      "lif layer 1 self.abs_max_v: 13743.5\n",
      "fc layer 1 self.abs_max_out: 8097.0\n",
      "lif layer 1 self.abs_max_v: 13867.5\n",
      "lif layer 2 self.abs_max_v: 5840.0\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  1.885501/  1.972432, val:  65.00%, val_best:  65.00%, tr:  99.39%, tr_best:  99.90%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7621%\n",
      "layer   2  Sparsity: 77.6441%\n",
      "layer   3  Sparsity: 66.9553%\n",
      "total_backward_count 225170 real_backward_count 28604  12.703%\n",
      "lif layer 1 self.abs_max_v: 13881.5\n",
      "lif layer 1 self.abs_max_v: 14498.0\n",
      "lif layer 2 self.abs_max_v: 6240.5\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  1.881699/  1.992133, val:  63.75%, val_best:  65.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7921%\n",
      "layer   2  Sparsity: 77.0484%\n",
      "layer   3  Sparsity: 65.9792%\n",
      "total_backward_count 234960 real_backward_count 29521  12.564%\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  1.885846/  1.982478, val:  59.17%, val_best:  65.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.39 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7847%\n",
      "layer   2  Sparsity: 77.1551%\n",
      "layer   3  Sparsity: 65.5635%\n",
      "total_backward_count 244750 real_backward_count 30418  12.428%\n",
      "lif layer 1 self.abs_max_v: 14751.5\n",
      "fc layer 1 self.abs_max_out: 8420.0\n",
      "lif layer 1 self.abs_max_v: 14821.5\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  1.884438/  1.976323, val:  67.92%, val_best:  67.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 71.86 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.7754%\n",
      "layer   2  Sparsity: 77.4971%\n",
      "layer   3  Sparsity: 65.7505%\n",
      "total_backward_count 254540 real_backward_count 31317  12.303%\n",
      "lif layer 1 self.abs_max_v: 15133.5\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  1.884608/  1.990228, val:  60.00%, val_best:  67.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 71.60 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 84.7896%\n",
      "layer   2  Sparsity: 77.8030%\n",
      "layer   3  Sparsity: 65.7440%\n",
      "total_backward_count 264330 real_backward_count 32197  12.181%\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  1.877604/  1.966903, val:  60.42%, val_best:  67.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 71.88 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.7750%\n",
      "layer   2  Sparsity: 77.4365%\n",
      "layer   3  Sparsity: 65.3461%\n",
      "total_backward_count 274120 real_backward_count 33099  12.075%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  1.863853/  1.978240, val:  67.50%, val_best:  67.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7807%\n",
      "layer   2  Sparsity: 77.0853%\n",
      "layer   3  Sparsity: 65.6066%\n",
      "total_backward_count 283910 real_backward_count 33959  11.961%\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  1.880777/  1.980866, val:  57.92%, val_best:  67.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 73.59 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.7999%\n",
      "layer   2  Sparsity: 77.4985%\n",
      "layer   3  Sparsity: 66.2802%\n",
      "total_backward_count 293700 real_backward_count 34845  11.864%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  1.878393/  1.978890, val:  63.75%, val_best:  67.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.8033%\n",
      "layer   2  Sparsity: 77.7198%\n",
      "layer   3  Sparsity: 65.2785%\n",
      "total_backward_count 303490 real_backward_count 35710  11.766%\n",
      "fc layer 1 self.abs_max_out: 8520.0\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  1.881705/  1.992659, val:  66.67%, val_best:  67.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 71.65 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 84.8011%\n",
      "layer   2  Sparsity: 77.3199%\n",
      "layer   3  Sparsity: 65.1807%\n",
      "total_backward_count 313280 real_backward_count 36600  11.683%\n",
      "fc layer 1 self.abs_max_out: 8780.0\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  1.895000/  2.004144, val:  65.83%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7877%\n",
      "layer   2  Sparsity: 76.8931%\n",
      "layer   3  Sparsity: 65.4940%\n",
      "total_backward_count 323070 real_backward_count 37390  11.573%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  1.895939/  1.993142, val:  67.08%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.38 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7922%\n",
      "layer   2  Sparsity: 76.9317%\n",
      "layer   3  Sparsity: 64.5616%\n",
      "total_backward_count 332860 real_backward_count 38265  11.496%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  1.884579/  1.986737, val:  62.08%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7760%\n",
      "layer   2  Sparsity: 77.4265%\n",
      "layer   3  Sparsity: 64.0664%\n",
      "total_backward_count 342650 real_backward_count 39080  11.405%\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  1.889433/  1.997740, val:  66.25%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.93 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7850%\n",
      "layer   2  Sparsity: 77.6840%\n",
      "layer   3  Sparsity: 63.6113%\n",
      "total_backward_count 352440 real_backward_count 39916  11.326%\n",
      "fc layer 1 self.abs_max_out: 9025.0\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  1.892362/  1.993603, val:  63.75%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7952%\n",
      "layer   2  Sparsity: 77.5530%\n",
      "layer   3  Sparsity: 63.5929%\n",
      "total_backward_count 362230 real_backward_count 40743  11.248%\n",
      "lif layer 1 self.abs_max_v: 15205.5\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  1.887031/  1.996300, val:  59.17%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.8067%\n",
      "layer   2  Sparsity: 77.1905%\n",
      "layer   3  Sparsity: 63.4444%\n",
      "total_backward_count 372020 real_backward_count 41533  11.164%\n",
      "lif layer 1 self.abs_max_v: 15406.0\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  1.879916/  1.987012, val:  62.50%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.85 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8020%\n",
      "layer   2  Sparsity: 77.1660%\n",
      "layer   3  Sparsity: 64.0210%\n",
      "total_backward_count 381810 real_backward_count 42352  11.092%\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  1.876913/  1.990431, val:  63.75%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.34 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7775%\n",
      "layer   2  Sparsity: 76.9420%\n",
      "layer   3  Sparsity: 64.4656%\n",
      "total_backward_count 391600 real_backward_count 43161  11.022%\n",
      "lif layer 1 self.abs_max_v: 15612.5\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  1.878382/  1.982959, val:  65.42%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7842%\n",
      "layer   2  Sparsity: 77.1362%\n",
      "layer   3  Sparsity: 64.1699%\n",
      "total_backward_count 401390 real_backward_count 43978  10.956%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  1.874084/  1.992126, val:  66.67%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.62 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7935%\n",
      "layer   2  Sparsity: 76.9559%\n",
      "layer   3  Sparsity: 63.7216%\n",
      "total_backward_count 411180 real_backward_count 44788  10.893%\n",
      "lif layer 1 self.abs_max_v: 15633.0\n",
      "lif layer 1 self.abs_max_v: 16003.0\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  1.889682/  1.994515, val:  61.67%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.8111%\n",
      "layer   2  Sparsity: 76.7654%\n",
      "layer   3  Sparsity: 64.7442%\n",
      "total_backward_count 420970 real_backward_count 45587  10.829%\n",
      "fc layer 2 self.abs_max_out: 3557.0\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  1.884637/  1.985008, val:  65.00%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7731%\n",
      "layer   2  Sparsity: 77.1213%\n",
      "layer   3  Sparsity: 64.4425%\n",
      "total_backward_count 430760 real_backward_count 46370  10.765%\n",
      "fc layer 2 self.abs_max_out: 3565.0\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  1.875099/  1.976699, val:  62.92%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.35 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7810%\n",
      "layer   2  Sparsity: 76.9470%\n",
      "layer   3  Sparsity: 63.8322%\n",
      "total_backward_count 440550 real_backward_count 47145  10.701%\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  1.871082/  1.984942, val:  60.83%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.42 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8067%\n",
      "layer   2  Sparsity: 76.1900%\n",
      "layer   3  Sparsity: 63.1325%\n",
      "total_backward_count 450340 real_backward_count 47932  10.644%\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  1.881197/  1.985695, val:  60.42%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.7911%\n",
      "layer   2  Sparsity: 75.8945%\n",
      "layer   3  Sparsity: 63.5983%\n",
      "total_backward_count 460130 real_backward_count 48692  10.582%\n",
      "fc layer 1 self.abs_max_out: 9362.0\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  1.877088/  1.978899, val:  61.25%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.8030%\n",
      "layer   2  Sparsity: 76.1497%\n",
      "layer   3  Sparsity: 63.6138%\n",
      "total_backward_count 469920 real_backward_count 49468  10.527%\n",
      "lif layer 1 self.abs_max_v: 16709.0\n",
      "fc layer 1 self.abs_max_out: 9397.0\n",
      "fc layer 1 self.abs_max_out: 9512.0\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  1.861167/  1.973712, val:  67.92%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8005%\n",
      "layer   2  Sparsity: 76.8724%\n",
      "layer   3  Sparsity: 63.4974%\n",
      "total_backward_count 479710 real_backward_count 50222  10.469%\n",
      "fc layer 1 self.abs_max_out: 9624.0\n",
      "fc layer 1 self.abs_max_out: 9811.0\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  1.865240/  1.979999, val:  60.42%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7965%\n",
      "layer   2  Sparsity: 77.2323%\n",
      "layer   3  Sparsity: 62.5374%\n",
      "total_backward_count 489500 real_backward_count 50987  10.416%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  1.868658/  1.980030, val:  63.33%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7737%\n",
      "layer   2  Sparsity: 76.9469%\n",
      "layer   3  Sparsity: 62.7626%\n",
      "total_backward_count 499290 real_backward_count 51734  10.362%\n",
      "lif layer 1 self.abs_max_v: 16871.5\n",
      "lif layer 1 self.abs_max_v: 17599.5\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  1.869907/  1.989921, val:  67.08%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.28 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.7871%\n",
      "layer   2  Sparsity: 76.1172%\n",
      "layer   3  Sparsity: 63.0013%\n",
      "total_backward_count 509080 real_backward_count 52472  10.307%\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  1.884309/  1.982036, val:  59.58%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.66 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7825%\n",
      "layer   2  Sparsity: 76.0372%\n",
      "layer   3  Sparsity: 63.2308%\n",
      "total_backward_count 518870 real_backward_count 53259  10.264%\n",
      "lif layer 1 self.abs_max_v: 17789.5\n",
      "lif layer 1 self.abs_max_v: 17925.0\n",
      "lif layer 1 self.abs_max_v: 18692.5\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  1.870436/  1.975285, val:  63.33%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7735%\n",
      "layer   2  Sparsity: 76.1558%\n",
      "layer   3  Sparsity: 62.3651%\n",
      "total_backward_count 528660 real_backward_count 54044  10.223%\n",
      "fc layer 1 self.abs_max_out: 9908.0\n",
      "lif layer 1 self.abs_max_v: 18713.0\n",
      "lif layer 1 self.abs_max_v: 19059.5\n",
      "fc layer 1 self.abs_max_out: 9948.0\n",
      "fc layer 1 self.abs_max_out: 10242.0\n",
      "lif layer 1 self.abs_max_v: 19297.5\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  1.874552/  1.988071, val:  60.42%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.67 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.7919%\n",
      "layer   2  Sparsity: 76.8442%\n",
      "layer   3  Sparsity: 61.8318%\n",
      "total_backward_count 538450 real_backward_count 54793  10.176%\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  1.873400/  1.981343, val:  66.25%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7834%\n",
      "layer   2  Sparsity: 77.0248%\n",
      "layer   3  Sparsity: 61.4738%\n",
      "total_backward_count 548240 real_backward_count 55559  10.134%\n",
      "fc layer 1 self.abs_max_out: 10416.0\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  1.871003/  1.990709, val:  58.33%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 71.74 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.7969%\n",
      "layer   2  Sparsity: 76.7235%\n",
      "layer   3  Sparsity: 62.2014%\n",
      "total_backward_count 558030 real_backward_count 56330  10.094%\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  1.874000/  1.986939, val:  64.17%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.12 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.7755%\n",
      "layer   2  Sparsity: 76.4024%\n",
      "layer   3  Sparsity: 63.1158%\n",
      "total_backward_count 567820 real_backward_count 57057  10.048%\n",
      "fc layer 1 self.abs_max_out: 10730.0\n",
      "lif layer 1 self.abs_max_v: 19734.0\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  1.869404/  1.972282, val:  67.08%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.37 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7681%\n",
      "layer   2  Sparsity: 76.1831%\n",
      "layer   3  Sparsity: 63.2433%\n",
      "total_backward_count 577610 real_backward_count 57773  10.002%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  1.863634/  1.989443, val:  61.67%, val_best:  67.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8123%\n",
      "layer   2  Sparsity: 76.5769%\n",
      "layer   3  Sparsity: 63.1368%\n",
      "total_backward_count 587400 real_backward_count 58490   9.957%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  1.868145/  1.988904, val:  63.33%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7817%\n",
      "layer   2  Sparsity: 76.2519%\n",
      "layer   3  Sparsity: 62.8678%\n",
      "total_backward_count 597190 real_backward_count 59215   9.916%\n",
      "fc layer 1 self.abs_max_out: 11524.0\n",
      "lif layer 1 self.abs_max_v: 19855.5\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  1.865542/  1.979937, val:  67.08%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.8083%\n",
      "layer   2  Sparsity: 76.0892%\n",
      "layer   3  Sparsity: 62.6740%\n",
      "total_backward_count 606980 real_backward_count 59978   9.881%\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  1.874590/  1.980089, val:  66.25%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8069%\n",
      "layer   2  Sparsity: 76.4272%\n",
      "layer   3  Sparsity: 63.6091%\n",
      "total_backward_count 616770 real_backward_count 60693   9.840%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  1.875250/  1.977486, val:  64.17%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7934%\n",
      "layer   2  Sparsity: 76.0963%\n",
      "layer   3  Sparsity: 63.2079%\n",
      "total_backward_count 626560 real_backward_count 61370   9.795%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  1.868362/  1.974054, val:  72.08%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7755%\n",
      "layer   2  Sparsity: 76.2413%\n",
      "layer   3  Sparsity: 63.3082%\n",
      "total_backward_count 636350 real_backward_count 62062   9.753%\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  1.871461/  1.996547, val:  60.42%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.16 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.7763%\n",
      "layer   2  Sparsity: 76.3578%\n",
      "layer   3  Sparsity: 62.9427%\n",
      "total_backward_count 646140 real_backward_count 62745   9.711%\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  1.866605/  1.976790, val:  62.50%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7824%\n",
      "layer   2  Sparsity: 75.4664%\n",
      "layer   3  Sparsity: 62.6542%\n",
      "total_backward_count 655930 real_backward_count 63468   9.676%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  1.864849/  1.977531, val:  66.25%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8211%\n",
      "layer   2  Sparsity: 75.4607%\n",
      "layer   3  Sparsity: 62.9552%\n",
      "total_backward_count 665720 real_backward_count 64212   9.645%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  1.863901/  1.970117, val:  69.17%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7680%\n",
      "layer   2  Sparsity: 75.8111%\n",
      "layer   3  Sparsity: 63.0041%\n",
      "total_backward_count 675510 real_backward_count 64907   9.609%\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  1.864134/  1.985092, val:  60.83%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.57 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7961%\n",
      "layer   2  Sparsity: 75.7530%\n",
      "layer   3  Sparsity: 62.8634%\n",
      "total_backward_count 685300 real_backward_count 65609   9.574%\n",
      "lif layer 2 self.abs_max_v: 6267.0\n",
      "lif layer 2 self.abs_max_v: 6374.5\n",
      "lif layer 2 self.abs_max_v: 6409.5\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  1.871238/  1.986021, val:  61.67%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7863%\n",
      "layer   2  Sparsity: 75.4600%\n",
      "layer   3  Sparsity: 63.5489%\n",
      "total_backward_count 695090 real_backward_count 66339   9.544%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  1.879700/  1.999324, val:  57.50%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.8029%\n",
      "layer   2  Sparsity: 75.8860%\n",
      "layer   3  Sparsity: 63.7932%\n",
      "total_backward_count 704880 real_backward_count 67034   9.510%\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  1.880776/  1.977069, val:  65.00%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7710%\n",
      "layer   2  Sparsity: 75.5126%\n",
      "layer   3  Sparsity: 63.2028%\n",
      "total_backward_count 714670 real_backward_count 67728   9.477%\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  1.871442/  1.983647, val:  65.83%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.82 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.7766%\n",
      "layer   2  Sparsity: 75.2228%\n",
      "layer   3  Sparsity: 63.3592%\n",
      "total_backward_count 724460 real_backward_count 68430   9.446%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  1.872614/  1.979314, val:  65.83%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.88 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8010%\n",
      "layer   2  Sparsity: 75.0809%\n",
      "layer   3  Sparsity: 63.4031%\n",
      "total_backward_count 734250 real_backward_count 69120   9.414%\n",
      "lif layer 1 self.abs_max_v: 20034.0\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  1.871927/  1.976350, val:  72.08%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7645%\n",
      "layer   2  Sparsity: 75.6893%\n",
      "layer   3  Sparsity: 63.1716%\n",
      "total_backward_count 744040 real_backward_count 69838   9.386%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  1.878151/  1.995712, val:  63.33%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.40 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8056%\n",
      "layer   2  Sparsity: 75.8755%\n",
      "layer   3  Sparsity: 64.0025%\n",
      "total_backward_count 753830 real_backward_count 70581   9.363%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  1.877494/  1.982226, val:  61.67%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7918%\n",
      "layer   2  Sparsity: 75.9893%\n",
      "layer   3  Sparsity: 63.7057%\n",
      "total_backward_count 763620 real_backward_count 71295   9.336%\n",
      "lif layer 1 self.abs_max_v: 20364.0\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  1.868547/  1.971556, val:  68.75%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7930%\n",
      "layer   2  Sparsity: 75.5108%\n",
      "layer   3  Sparsity: 63.0353%\n",
      "total_backward_count 773410 real_backward_count 71972   9.306%\n",
      "fc layer 1 self.abs_max_out: 12112.0\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  1.865595/  1.981467, val:  65.00%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7751%\n",
      "layer   2  Sparsity: 75.7962%\n",
      "layer   3  Sparsity: 63.4011%\n",
      "total_backward_count 783200 real_backward_count 72651   9.276%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  1.873623/  1.989484, val:  63.33%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7851%\n",
      "layer   2  Sparsity: 75.9913%\n",
      "layer   3  Sparsity: 64.0445%\n",
      "total_backward_count 792990 real_backward_count 73364   9.252%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  1.871845/  1.991946, val:  62.08%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.16 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.8162%\n",
      "layer   2  Sparsity: 75.5906%\n",
      "layer   3  Sparsity: 64.4636%\n",
      "total_backward_count 802780 real_backward_count 74080   9.228%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  1.881573/  1.994595, val:  65.83%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.64 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7816%\n",
      "layer   2  Sparsity: 75.9721%\n",
      "layer   3  Sparsity: 64.0495%\n",
      "total_backward_count 812570 real_backward_count 74828   9.209%\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  1.880617/  1.991376, val:  65.42%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7978%\n",
      "layer   2  Sparsity: 75.5116%\n",
      "layer   3  Sparsity: 64.6156%\n",
      "total_backward_count 822360 real_backward_count 75533   9.185%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  1.882822/  2.000982, val:  65.00%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7986%\n",
      "layer   2  Sparsity: 75.0332%\n",
      "layer   3  Sparsity: 64.4448%\n",
      "total_backward_count 832150 real_backward_count 76253   9.163%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  1.883295/  1.993377, val:  71.67%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.8058%\n",
      "layer   2  Sparsity: 74.5240%\n",
      "layer   3  Sparsity: 64.2650%\n",
      "total_backward_count 841940 real_backward_count 76953   9.140%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  1.879559/  1.991719, val:  64.17%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7761%\n",
      "layer   2  Sparsity: 74.3797%\n",
      "layer   3  Sparsity: 63.9941%\n",
      "total_backward_count 851730 real_backward_count 77626   9.114%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  1.880621/  1.985503, val:  68.33%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7979%\n",
      "layer   2  Sparsity: 74.0435%\n",
      "layer   3  Sparsity: 64.1884%\n",
      "total_backward_count 861520 real_backward_count 78316   9.090%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  1.877810/  1.984775, val:  63.33%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.18 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.7675%\n",
      "layer   2  Sparsity: 73.8278%\n",
      "layer   3  Sparsity: 64.3411%\n",
      "total_backward_count 871310 real_backward_count 79020   9.069%\n",
      "fc layer 2 self.abs_max_out: 3650.0\n",
      "lif layer 2 self.abs_max_v: 6422.0\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  1.873994/  1.981615, val:  68.75%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.31 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7958%\n",
      "layer   2  Sparsity: 73.9648%\n",
      "layer   3  Sparsity: 64.6498%\n",
      "total_backward_count 881100 real_backward_count 79725   9.048%\n",
      "lif layer 2 self.abs_max_v: 6744.0\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  1.868508/  1.989777, val:  62.50%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.57 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7944%\n",
      "layer   2  Sparsity: 73.8751%\n",
      "layer   3  Sparsity: 64.2338%\n",
      "total_backward_count 890890 real_backward_count 80418   9.027%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  1.880253/  1.985630, val:  67.92%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.8109%\n",
      "layer   2  Sparsity: 73.9713%\n",
      "layer   3  Sparsity: 64.4974%\n",
      "total_backward_count 900680 real_backward_count 81109   9.005%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  1.879343/  1.998333, val:  64.58%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.00 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7784%\n",
      "layer   2  Sparsity: 73.8473%\n",
      "layer   3  Sparsity: 63.8208%\n",
      "total_backward_count 910470 real_backward_count 81733   8.977%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  1.887041/  1.995867, val:  58.33%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.8087%\n",
      "layer   2  Sparsity: 73.4110%\n",
      "layer   3  Sparsity: 63.7162%\n",
      "total_backward_count 920260 real_backward_count 82480   8.963%\n",
      "lif layer 2 self.abs_max_v: 6791.5\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  1.872806/  1.986859, val:  67.08%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8053%\n",
      "layer   2  Sparsity: 73.3804%\n",
      "layer   3  Sparsity: 63.9658%\n",
      "total_backward_count 930050 real_backward_count 83187   8.944%\n",
      "fc layer 2 self.abs_max_out: 3653.0\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  1.872358/  1.983821, val:  72.50%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7666%\n",
      "layer   2  Sparsity: 73.3019%\n",
      "layer   3  Sparsity: 63.7335%\n",
      "total_backward_count 939840 real_backward_count 83919   8.929%\n",
      "fc layer 2 self.abs_max_out: 3741.0\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  1.870047/  1.991801, val:  58.75%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.8041%\n",
      "layer   2  Sparsity: 74.2062%\n",
      "layer   3  Sparsity: 64.1947%\n",
      "total_backward_count 949630 real_backward_count 84569   8.905%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  1.874132/  1.990649, val:  65.42%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7881%\n",
      "layer   2  Sparsity: 73.9873%\n",
      "layer   3  Sparsity: 63.7225%\n",
      "total_backward_count 959420 real_backward_count 85249   8.885%\n",
      "lif layer 2 self.abs_max_v: 6977.5\n",
      "lif layer 2 self.abs_max_v: 6998.0\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  1.869926/  1.991063, val:  67.92%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7769%\n",
      "layer   2  Sparsity: 74.2777%\n",
      "layer   3  Sparsity: 63.6290%\n",
      "total_backward_count 969210 real_backward_count 85902   8.863%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  1.871667/  1.990014, val:  73.75%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7925%\n",
      "layer   2  Sparsity: 74.4015%\n",
      "layer   3  Sparsity: 64.1549%\n",
      "total_backward_count 979000 real_backward_count 86571   8.843%\n",
      "fc layer 2 self.abs_max_out: 3794.0\n",
      "lif layer 2 self.abs_max_v: 7004.0\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  1.873147/  1.994222, val:  67.08%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.48 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7979%\n",
      "layer   2  Sparsity: 73.7613%\n",
      "layer   3  Sparsity: 63.9155%\n",
      "total_backward_count 988790 real_backward_count 87214   8.820%\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  1.875764/  1.985825, val:  64.58%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7706%\n",
      "layer   2  Sparsity: 73.9877%\n",
      "layer   3  Sparsity: 63.3970%\n",
      "total_backward_count 998580 real_backward_count 87867   8.799%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  1.871160/  1.988945, val:  65.42%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7901%\n",
      "layer   2  Sparsity: 74.1975%\n",
      "layer   3  Sparsity: 63.7419%\n",
      "total_backward_count 1008370 real_backward_count 88521   8.779%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  1.871580/  1.988930, val:  63.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7838%\n",
      "layer   2  Sparsity: 74.0308%\n",
      "layer   3  Sparsity: 64.0718%\n",
      "total_backward_count 1018160 real_backward_count 89179   8.759%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  1.875031/  1.987355, val:  63.33%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.85 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8165%\n",
      "layer   2  Sparsity: 74.1923%\n",
      "layer   3  Sparsity: 63.8480%\n",
      "total_backward_count 1027950 real_backward_count 89869   8.743%\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  1.872746/  1.985277, val:  66.25%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.50 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.8110%\n",
      "layer   2  Sparsity: 74.4133%\n",
      "layer   3  Sparsity: 63.5587%\n",
      "total_backward_count 1037740 real_backward_count 90519   8.723%\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  1.874058/  1.990279, val:  65.83%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.92 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.7774%\n",
      "layer   2  Sparsity: 74.6185%\n",
      "layer   3  Sparsity: 63.8086%\n",
      "total_backward_count 1047530 real_backward_count 91163   8.703%\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  1.873076/  1.996237, val:  57.92%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.56 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7764%\n",
      "layer   2  Sparsity: 74.3684%\n",
      "layer   3  Sparsity: 64.1240%\n",
      "total_backward_count 1057320 real_backward_count 91842   8.686%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  1.868146/  1.977118, val:  69.17%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8083%\n",
      "layer   2  Sparsity: 74.1301%\n",
      "layer   3  Sparsity: 63.2576%\n",
      "total_backward_count 1067110 real_backward_count 92496   8.668%\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  1.868013/  1.983508, val:  65.00%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7704%\n",
      "layer   2  Sparsity: 74.0865%\n",
      "layer   3  Sparsity: 64.2447%\n",
      "total_backward_count 1076900 real_backward_count 93175   8.652%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  1.868694/  1.987738, val:  66.25%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7825%\n",
      "layer   2  Sparsity: 74.0664%\n",
      "layer   3  Sparsity: 63.3730%\n",
      "total_backward_count 1086690 real_backward_count 93845   8.636%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  1.869407/  1.981023, val:  68.75%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7865%\n",
      "layer   2  Sparsity: 74.1183%\n",
      "layer   3  Sparsity: 63.4076%\n",
      "total_backward_count 1096480 real_backward_count 94497   8.618%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  1.872998/  1.997393, val:  67.50%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7709%\n",
      "layer   2  Sparsity: 74.0166%\n",
      "layer   3  Sparsity: 64.4940%\n",
      "total_backward_count 1106270 real_backward_count 95174   8.603%\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  1.877650/  1.989919, val:  65.00%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7916%\n",
      "layer   2  Sparsity: 73.6951%\n",
      "layer   3  Sparsity: 64.1322%\n",
      "total_backward_count 1116060 real_backward_count 95827   8.586%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  1.875567/  1.988745, val:  67.08%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7804%\n",
      "layer   2  Sparsity: 73.1968%\n",
      "layer   3  Sparsity: 64.8376%\n",
      "total_backward_count 1125850 real_backward_count 96516   8.573%\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  1.873330/  1.988339, val:  66.67%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7862%\n",
      "layer   2  Sparsity: 73.2354%\n",
      "layer   3  Sparsity: 64.6866%\n",
      "total_backward_count 1135640 real_backward_count 97166   8.556%\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  1.873149/  1.992506, val:  66.67%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.01 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7724%\n",
      "layer   2  Sparsity: 73.5268%\n",
      "layer   3  Sparsity: 64.5339%\n",
      "total_backward_count 1145430 real_backward_count 97853   8.543%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  1.881872/  1.993949, val:  68.33%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.83 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7685%\n",
      "layer   2  Sparsity: 73.0815%\n",
      "layer   3  Sparsity: 64.3176%\n",
      "total_backward_count 1155220 real_backward_count 98514   8.528%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  1.872210/  1.983502, val:  67.08%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.78 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7873%\n",
      "layer   2  Sparsity: 73.5410%\n",
      "layer   3  Sparsity: 64.2831%\n",
      "total_backward_count 1165010 real_backward_count 99149   8.511%\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  1.873431/  1.993922, val:  67.50%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.35 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7957%\n",
      "layer   2  Sparsity: 73.3043%\n",
      "layer   3  Sparsity: 64.3179%\n",
      "total_backward_count 1174800 real_backward_count 99794   8.495%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  1.875286/  1.995198, val:  70.42%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7822%\n",
      "layer   2  Sparsity: 73.3464%\n",
      "layer   3  Sparsity: 64.1276%\n",
      "total_backward_count 1184590 real_backward_count 100428   8.478%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  1.873644/  1.982494, val:  67.50%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.49 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7695%\n",
      "layer   2  Sparsity: 73.8402%\n",
      "layer   3  Sparsity: 64.8095%\n",
      "total_backward_count 1194380 real_backward_count 101086   8.463%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  1.873075/  1.992888, val:  67.92%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7919%\n",
      "layer   2  Sparsity: 73.8114%\n",
      "layer   3  Sparsity: 64.4730%\n",
      "total_backward_count 1204170 real_backward_count 101807   8.455%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  1.872436/  1.984932, val:  61.67%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.01 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.7839%\n",
      "layer   2  Sparsity: 74.2185%\n",
      "layer   3  Sparsity: 64.1208%\n",
      "total_backward_count 1213960 real_backward_count 102495   8.443%\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  1.869117/  1.982270, val:  64.17%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7938%\n",
      "layer   2  Sparsity: 73.9201%\n",
      "layer   3  Sparsity: 64.1333%\n",
      "total_backward_count 1223750 real_backward_count 103141   8.428%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  1.872009/  1.982034, val:  67.92%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.01 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.8049%\n",
      "layer   2  Sparsity: 74.0810%\n",
      "layer   3  Sparsity: 64.6021%\n",
      "total_backward_count 1233540 real_backward_count 103794   8.414%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  1.868971/  1.987013, val:  64.17%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7955%\n",
      "layer   2  Sparsity: 74.2311%\n",
      "layer   3  Sparsity: 64.6868%\n",
      "total_backward_count 1243330 real_backward_count 104441   8.400%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  1.864652/  1.981053, val:  62.92%, val_best:  73.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7781%\n",
      "layer   2  Sparsity: 73.9372%\n",
      "layer   3  Sparsity: 64.2234%\n",
      "total_backward_count 1253120 real_backward_count 105077   8.385%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  1.865394/  1.985082, val:  63.33%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.7845%\n",
      "layer   2  Sparsity: 74.0495%\n",
      "layer   3  Sparsity: 64.0938%\n",
      "total_backward_count 1262910 real_backward_count 105708   8.370%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  1.857806/  1.959816, val:  69.58%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.96 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.7992%\n",
      "layer   2  Sparsity: 74.7460%\n",
      "layer   3  Sparsity: 63.7033%\n",
      "total_backward_count 1272700 real_backward_count 106326   8.354%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  1.848503/  1.968734, val:  63.33%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7881%\n",
      "layer   2  Sparsity: 74.6970%\n",
      "layer   3  Sparsity: 63.9663%\n",
      "total_backward_count 1282490 real_backward_count 106934   8.338%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  1.859241/  1.981383, val:  66.25%, val_best:  73.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7849%\n",
      "layer   2  Sparsity: 74.7750%\n",
      "layer   3  Sparsity: 64.5392%\n",
      "total_backward_count 1292280 real_backward_count 107619   8.328%\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  1.858195/  1.981541, val:  66.25%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7896%\n",
      "layer   2  Sparsity: 74.4478%\n",
      "layer   3  Sparsity: 63.6715%\n",
      "total_backward_count 1302070 real_backward_count 108259   8.314%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  1.861878/  1.977165, val:  63.75%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.88 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.8005%\n",
      "layer   2  Sparsity: 74.2825%\n",
      "layer   3  Sparsity: 63.9657%\n",
      "total_backward_count 1311860 real_backward_count 108914   8.302%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  1.863670/  1.981522, val:  65.42%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.8061%\n",
      "layer   2  Sparsity: 74.6018%\n",
      "layer   3  Sparsity: 63.7504%\n",
      "total_backward_count 1321650 real_backward_count 109548   8.289%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  1.869095/  1.987527, val:  74.58%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.83 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.7891%\n",
      "layer   2  Sparsity: 74.3531%\n",
      "layer   3  Sparsity: 63.6105%\n",
      "total_backward_count 1331440 real_backward_count 110234   8.279%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  1.860260/  1.979047, val:  68.75%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7903%\n",
      "layer   2  Sparsity: 74.3793%\n",
      "layer   3  Sparsity: 63.7575%\n",
      "total_backward_count 1341230 real_backward_count 110863   8.266%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  1.858524/  1.965964, val:  60.42%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.54 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.7817%\n",
      "layer   2  Sparsity: 74.3176%\n",
      "layer   3  Sparsity: 63.5852%\n",
      "total_backward_count 1351020 real_backward_count 111474   8.251%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  1.851843/  1.975019, val:  62.92%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.59 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.7802%\n",
      "layer   2  Sparsity: 74.3384%\n",
      "layer   3  Sparsity: 64.1060%\n",
      "total_backward_count 1360810 real_backward_count 112161   8.242%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  1.862012/  1.976550, val:  68.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7958%\n",
      "layer   2  Sparsity: 74.2333%\n",
      "layer   3  Sparsity: 64.9088%\n",
      "total_backward_count 1370600 real_backward_count 112847   8.233%\n",
      "fc layer 2 self.abs_max_out: 3889.0\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  1.857530/  1.983442, val:  62.50%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.89 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7863%\n",
      "layer   2  Sparsity: 74.8201%\n",
      "layer   3  Sparsity: 64.3406%\n",
      "total_backward_count 1380390 real_backward_count 113488   8.221%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  1.861161/  1.981591, val:  64.17%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.7665%\n",
      "layer   2  Sparsity: 74.6806%\n",
      "layer   3  Sparsity: 64.6683%\n",
      "total_backward_count 1390180 real_backward_count 114148   8.211%\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  1.865376/  1.978567, val:  62.92%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.8009%\n",
      "layer   2  Sparsity: 74.2963%\n",
      "layer   3  Sparsity: 64.2772%\n",
      "total_backward_count 1399970 real_backward_count 114791   8.200%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  1.868288/  1.999576, val:  59.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.31 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 84.8108%\n",
      "layer   2  Sparsity: 74.1014%\n",
      "layer   3  Sparsity: 64.3927%\n",
      "total_backward_count 1409760 real_backward_count 115422   8.187%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  1.865687/  1.973898, val:  65.00%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7638%\n",
      "layer   2  Sparsity: 74.3502%\n",
      "layer   3  Sparsity: 63.1858%\n",
      "total_backward_count 1419550 real_backward_count 116088   8.178%\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  1.866384/  1.981937, val:  64.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.8030%\n",
      "layer   2  Sparsity: 74.4605%\n",
      "layer   3  Sparsity: 63.1910%\n",
      "total_backward_count 1429340 real_backward_count 116714   8.166%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  1.861077/  1.987208, val:  60.42%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.83 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7957%\n",
      "layer   2  Sparsity: 74.2270%\n",
      "layer   3  Sparsity: 63.7036%\n",
      "total_backward_count 1439130 real_backward_count 117319   8.152%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  1.859262/  1.983980, val:  66.67%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.82 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7837%\n",
      "layer   2  Sparsity: 74.1350%\n",
      "layer   3  Sparsity: 63.8948%\n",
      "total_backward_count 1448920 real_backward_count 117934   8.139%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  1.862413/  1.976059, val:  61.67%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.68 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.8007%\n",
      "layer   2  Sparsity: 73.9197%\n",
      "layer   3  Sparsity: 63.7479%\n",
      "total_backward_count 1458710 real_backward_count 118599   8.130%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  1.853697/  1.977965, val:  59.17%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.7856%\n",
      "layer   2  Sparsity: 74.1155%\n",
      "layer   3  Sparsity: 63.7576%\n",
      "total_backward_count 1468500 real_backward_count 119250   8.121%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  1.850976/  1.982848, val:  69.17%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7691%\n",
      "layer   2  Sparsity: 74.1492%\n",
      "layer   3  Sparsity: 63.0003%\n",
      "total_backward_count 1478290 real_backward_count 119872   8.109%\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  1.852127/  1.974448, val:  65.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.35 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7765%\n",
      "layer   2  Sparsity: 74.3026%\n",
      "layer   3  Sparsity: 63.8787%\n",
      "total_backward_count 1488080 real_backward_count 120508   8.098%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  1.848110/  1.970544, val:  63.33%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7596%\n",
      "layer   2  Sparsity: 74.4591%\n",
      "layer   3  Sparsity: 63.1562%\n",
      "total_backward_count 1497870 real_backward_count 121117   8.086%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  1.855261/  1.975946, val:  64.58%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7828%\n",
      "layer   2  Sparsity: 74.2658%\n",
      "layer   3  Sparsity: 63.8549%\n",
      "total_backward_count 1507660 real_backward_count 121755   8.076%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  1.856987/  1.983633, val:  65.83%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7910%\n",
      "layer   2  Sparsity: 74.0609%\n",
      "layer   3  Sparsity: 65.0221%\n",
      "total_backward_count 1517450 real_backward_count 122371   8.064%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  1.857254/  1.965510, val:  65.83%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.74 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.8005%\n",
      "layer   2  Sparsity: 74.0366%\n",
      "layer   3  Sparsity: 64.0732%\n",
      "total_backward_count 1527240 real_backward_count 123005   8.054%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  1.852582/  1.975687, val:  60.42%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.7921%\n",
      "layer   2  Sparsity: 74.1630%\n",
      "layer   3  Sparsity: 63.4381%\n",
      "total_backward_count 1537030 real_backward_count 123608   8.042%\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  1.857920/  1.986086, val:  65.00%, val_best:  74.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7996%\n",
      "layer   2  Sparsity: 73.8968%\n",
      "layer   3  Sparsity: 63.4799%\n",
      "total_backward_count 1546820 real_backward_count 124263   8.033%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  1.863161/  1.979459, val:  66.67%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7873%\n",
      "layer   2  Sparsity: 73.5370%\n",
      "layer   3  Sparsity: 63.5389%\n",
      "total_backward_count 1556610 real_backward_count 124920   8.025%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  1.860716/  1.986101, val:  63.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.53 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7832%\n",
      "layer   2  Sparsity: 73.7894%\n",
      "layer   3  Sparsity: 63.3426%\n",
      "total_backward_count 1566400 real_backward_count 125540   8.015%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  1.856424/  1.973327, val:  63.33%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.7791%\n",
      "layer   2  Sparsity: 74.1368%\n",
      "layer   3  Sparsity: 63.4022%\n",
      "total_backward_count 1576190 real_backward_count 126172   8.005%\n",
      "fc layer 1 self.abs_max_out: 12339.0\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  1.863650/  1.986261, val:  72.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7931%\n",
      "layer   2  Sparsity: 74.2055%\n",
      "layer   3  Sparsity: 63.6984%\n",
      "total_backward_count 1585980 real_backward_count 126801   7.995%\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  1.862188/  1.982211, val:  69.17%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.8109%\n",
      "layer   2  Sparsity: 73.9516%\n",
      "layer   3  Sparsity: 64.6514%\n",
      "total_backward_count 1595770 real_backward_count 127442   7.986%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  1.860765/  1.986907, val:  63.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8050%\n",
      "layer   2  Sparsity: 73.5628%\n",
      "layer   3  Sparsity: 65.0048%\n",
      "total_backward_count 1605560 real_backward_count 128046   7.975%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  1.862207/  1.988370, val:  61.67%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.17 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.7884%\n",
      "layer   2  Sparsity: 73.6977%\n",
      "layer   3  Sparsity: 64.0291%\n",
      "total_backward_count 1615350 real_backward_count 128669   7.965%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  1.857127/  1.991076, val:  67.50%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7981%\n",
      "layer   2  Sparsity: 74.1280%\n",
      "layer   3  Sparsity: 64.2934%\n",
      "total_backward_count 1625140 real_backward_count 129282   7.955%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  1.856057/  1.991457, val:  60.83%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8049%\n",
      "layer   2  Sparsity: 74.6134%\n",
      "layer   3  Sparsity: 63.8546%\n",
      "total_backward_count 1634930 real_backward_count 129911   7.946%\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  1.859102/  1.986139, val:  63.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7875%\n",
      "layer   2  Sparsity: 74.6667%\n",
      "layer   3  Sparsity: 63.9580%\n",
      "total_backward_count 1644720 real_backward_count 130487   7.934%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  1.855103/  1.981168, val:  63.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7838%\n",
      "layer   2  Sparsity: 74.3897%\n",
      "layer   3  Sparsity: 63.0144%\n",
      "total_backward_count 1654510 real_backward_count 131101   7.924%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  1.849424/  1.979270, val:  65.00%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.85 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7834%\n",
      "layer   2  Sparsity: 74.1680%\n",
      "layer   3  Sparsity: 62.9422%\n",
      "total_backward_count 1664300 real_backward_count 131680   7.912%\n",
      "fc layer 1 self.abs_max_out: 13094.0\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  1.853399/  1.975369, val:  65.42%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7813%\n",
      "layer   2  Sparsity: 74.5149%\n",
      "layer   3  Sparsity: 63.4229%\n",
      "total_backward_count 1674090 real_backward_count 132266   7.901%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  1.852956/  1.960558, val:  66.25%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7930%\n",
      "layer   2  Sparsity: 74.2639%\n",
      "layer   3  Sparsity: 63.2727%\n",
      "total_backward_count 1683880 real_backward_count 132894   7.892%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  1.848881/  1.972470, val:  61.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7794%\n",
      "layer   2  Sparsity: 74.0377%\n",
      "layer   3  Sparsity: 63.4630%\n",
      "total_backward_count 1693670 real_backward_count 133536   7.884%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.843640/  1.976244, val:  63.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7700%\n",
      "layer   2  Sparsity: 74.1287%\n",
      "layer   3  Sparsity: 63.1682%\n",
      "total_backward_count 1703460 real_backward_count 134157   7.876%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  1.841798/  1.966152, val:  70.00%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.55 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.8148%\n",
      "layer   2  Sparsity: 73.8908%\n",
      "layer   3  Sparsity: 63.3975%\n",
      "total_backward_count 1713250 real_backward_count 134739   7.865%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  1.843496/  1.962308, val:  70.42%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7786%\n",
      "layer   2  Sparsity: 73.7520%\n",
      "layer   3  Sparsity: 62.3654%\n",
      "total_backward_count 1723040 real_backward_count 135357   7.856%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  1.839274/  1.964139, val:  62.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.48 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8076%\n",
      "layer   2  Sparsity: 73.6646%\n",
      "layer   3  Sparsity: 62.7745%\n",
      "total_backward_count 1732830 real_backward_count 135960   7.846%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  1.840629/  1.974394, val:  66.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7817%\n",
      "layer   2  Sparsity: 73.8538%\n",
      "layer   3  Sparsity: 62.3640%\n",
      "total_backward_count 1742620 real_backward_count 136540   7.835%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  1.845745/  1.964374, val:  67.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7881%\n",
      "layer   2  Sparsity: 73.8502%\n",
      "layer   3  Sparsity: 62.7700%\n",
      "total_backward_count 1752410 real_backward_count 137136   7.826%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.853277/  1.971896, val:  61.67%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.37 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7981%\n",
      "layer   2  Sparsity: 73.8142%\n",
      "layer   3  Sparsity: 62.1890%\n",
      "total_backward_count 1762200 real_backward_count 137733   7.816%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.848081/  1.962049, val:  64.17%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8100%\n",
      "layer   2  Sparsity: 73.7057%\n",
      "layer   3  Sparsity: 62.2284%\n",
      "total_backward_count 1771990 real_backward_count 138311   7.805%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  1.846391/  1.973381, val:  62.08%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7859%\n",
      "layer   2  Sparsity: 73.5992%\n",
      "layer   3  Sparsity: 62.5913%\n",
      "total_backward_count 1781780 real_backward_count 138883   7.795%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  1.850070/  1.969498, val:  69.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.15 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.7999%\n",
      "layer   2  Sparsity: 73.7481%\n",
      "layer   3  Sparsity: 62.5500%\n",
      "total_backward_count 1791570 real_backward_count 139535   7.788%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  1.852254/  1.973807, val:  60.42%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.27 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.7797%\n",
      "layer   2  Sparsity: 73.6795%\n",
      "layer   3  Sparsity: 62.5899%\n",
      "total_backward_count 1801360 real_backward_count 140139   7.780%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  1.852894/  1.970051, val:  62.50%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.09 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.7671%\n",
      "layer   2  Sparsity: 73.5752%\n",
      "layer   3  Sparsity: 62.4512%\n",
      "total_backward_count 1811150 real_backward_count 140730   7.770%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  1.845289/  1.963317, val:  66.25%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8094%\n",
      "layer   2  Sparsity: 74.1657%\n",
      "layer   3  Sparsity: 62.5022%\n",
      "total_backward_count 1820940 real_backward_count 141349   7.762%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  1.841374/  1.961664, val:  70.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.47 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7984%\n",
      "layer   2  Sparsity: 74.0938%\n",
      "layer   3  Sparsity: 63.1415%\n",
      "total_backward_count 1830730 real_backward_count 141949   7.754%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  1.840106/  1.973268, val:  63.33%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7801%\n",
      "layer   2  Sparsity: 74.1845%\n",
      "layer   3  Sparsity: 63.3565%\n",
      "total_backward_count 1840520 real_backward_count 142566   7.746%\n",
      "lif layer 1 self.abs_max_v: 20843.5\n",
      "lif layer 1 self.abs_max_v: 21784.0\n",
      "lif layer 1 self.abs_max_v: 22039.0\n",
      "lif layer 1 self.abs_max_v: 22940.5\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  1.838387/  1.969919, val:  65.00%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7776%\n",
      "layer   2  Sparsity: 74.6535%\n",
      "layer   3  Sparsity: 62.5870%\n",
      "total_backward_count 1850310 real_backward_count 143187   7.739%\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.841197/  1.959926, val:  62.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.25 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 84.7893%\n",
      "layer   2  Sparsity: 74.5519%\n",
      "layer   3  Sparsity: 62.8478%\n",
      "total_backward_count 1860100 real_backward_count 143788   7.730%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  1.832455/  1.953601, val:  69.58%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7921%\n",
      "layer   2  Sparsity: 74.1375%\n",
      "layer   3  Sparsity: 62.7461%\n",
      "total_backward_count 1869890 real_backward_count 144368   7.721%\n",
      "fc layer 1 self.abs_max_out: 13382.0\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  1.837871/  1.973289, val:  59.17%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.8019%\n",
      "layer   2  Sparsity: 74.2874%\n",
      "layer   3  Sparsity: 62.7264%\n",
      "total_backward_count 1879680 real_backward_count 144985   7.713%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  1.842307/  1.966528, val:  69.17%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.01 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7701%\n",
      "layer   2  Sparsity: 74.3195%\n",
      "layer   3  Sparsity: 63.0948%\n",
      "total_backward_count 1889470 real_backward_count 145569   7.704%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  1.845038/  1.968971, val:  70.42%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.8070%\n",
      "layer   2  Sparsity: 74.0537%\n",
      "layer   3  Sparsity: 63.6067%\n",
      "total_backward_count 1899260 real_backward_count 146155   7.695%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  1.845623/  1.966823, val:  64.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7979%\n",
      "layer   2  Sparsity: 74.1253%\n",
      "layer   3  Sparsity: 62.9636%\n",
      "total_backward_count 1909050 real_backward_count 146748   7.687%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  1.848971/  1.981607, val:  58.33%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7952%\n",
      "layer   2  Sparsity: 73.9642%\n",
      "layer   3  Sparsity: 62.8931%\n",
      "total_backward_count 1918840 real_backward_count 147370   7.680%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  1.853706/  1.969972, val:  61.25%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7869%\n",
      "layer   2  Sparsity: 74.0422%\n",
      "layer   3  Sparsity: 62.0030%\n",
      "total_backward_count 1928630 real_backward_count 147986   7.673%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  1.845807/  1.968129, val:  61.67%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 84.7883%\n",
      "layer   2  Sparsity: 74.4522%\n",
      "layer   3  Sparsity: 62.2944%\n",
      "total_backward_count 1938420 real_backward_count 148592   7.666%\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  1.845953/  1.971038, val:  62.92%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 84.7725%\n",
      "layer   2  Sparsity: 74.4306%\n",
      "layer   3  Sparsity: 62.8232%\n",
      "total_backward_count 1948210 real_backward_count 149178   7.657%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  1.845374/  1.962232, val:  63.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 84.8132%\n",
      "layer   2  Sparsity: 73.9855%\n",
      "layer   3  Sparsity: 63.0420%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e30b6c405b488bba25d52c6829c294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñà‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñÇ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñà‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñÉ‚ñÖ‚ñÉ‚ñá‚ñá‚ñÑ‚ñá‚ñà‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.84537</td></tr><tr><td>val_acc_best</td><td>0.74583</td></tr><tr><td>val_acc_now</td><td>0.6375</td></tr><tr><td>val_loss</td><td>1.96223</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunny-sweep-99</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/756ifoym' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/756ifoym</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_173446-756ifoym/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sdptpsgh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251116_213851-sdptpsgh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sdptpsgh' target=\"_blank\">fresh-sweep-103</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sdptpsgh' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sdptpsgh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251116_213900_797', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 10, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 2, 'leaky_temporal_filter': 0.5} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 559.0\n",
      "lif layer 1 self.abs_max_v: 559.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 187.0\n",
      "lif layer 2 self.abs_max_v: 187.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 743.0\n",
      "lif layer 1 self.abs_max_v: 969.0\n",
      "fc layer 2 self.abs_max_out: 462.0\n",
      "lif layer 2 self.abs_max_v: 476.0\n",
      "fc layer 2 self.abs_max_out: 492.0\n",
      "lif layer 2 self.abs_max_v: 524.5\n",
      "fc layer 1 self.abs_max_out: 1271.0\n",
      "lif layer 1 self.abs_max_v: 1503.5\n",
      "fc layer 1 self.abs_max_out: 1536.0\n",
      "lif layer 1 self.abs_max_v: 1536.0\n",
      "fc layer 2 self.abs_max_out: 494.0\n",
      "lif layer 2 self.abs_max_v: 680.0\n",
      "fc layer 3 self.abs_max_out: 21.0\n",
      "fc layer 1 self.abs_max_out: 2201.0\n",
      "lif layer 1 self.abs_max_v: 2201.0\n",
      "fc layer 3 self.abs_max_out: 76.0\n",
      "lif layer 1 self.abs_max_v: 2383.5\n",
      "fc layer 2 self.abs_max_out: 556.0\n",
      "lif layer 2 self.abs_max_v: 761.5\n",
      "fc layer 3 self.abs_max_out: 113.0\n",
      "fc layer 1 self.abs_max_out: 2219.0\n",
      "lif layer 1 self.abs_max_v: 2722.0\n",
      "fc layer 1 self.abs_max_out: 2247.0\n",
      "lif layer 1 self.abs_max_v: 2861.0\n",
      "lif layer 2 self.abs_max_v: 865.5\n",
      "fc layer 1 self.abs_max_out: 2500.0\n",
      "fc layer 2 self.abs_max_out: 565.0\n",
      "fc layer 2 self.abs_max_out: 597.0\n",
      "fc layer 1 self.abs_max_out: 2787.0\n",
      "lif layer 1 self.abs_max_v: 3220.0\n",
      "fc layer 2 self.abs_max_out: 939.0\n",
      "lif layer 2 self.abs_max_v: 970.0\n",
      "fc layer 3 self.abs_max_out: 152.0\n",
      "fc layer 3 self.abs_max_out: 170.0\n",
      "lif layer 2 self.abs_max_v: 1111.5\n",
      "fc layer 3 self.abs_max_out: 204.0\n",
      "lif layer 2 self.abs_max_v: 1155.0\n",
      "lif layer 2 self.abs_max_v: 1199.5\n",
      "fc layer 1 self.abs_max_out: 2907.0\n",
      "lif layer 1 self.abs_max_v: 3257.5\n",
      "lif layer 2 self.abs_max_v: 1205.0\n",
      "fc layer 1 self.abs_max_out: 3201.0\n",
      "fc layer 2 self.abs_max_out: 978.0\n",
      "fc layer 2 self.abs_max_out: 1042.0\n",
      "lif layer 1 self.abs_max_v: 3517.0\n",
      "lif layer 2 self.abs_max_v: 1218.5\n",
      "lif layer 1 self.abs_max_v: 3619.5\n",
      "lif layer 1 self.abs_max_v: 3681.5\n",
      "lif layer 1 self.abs_max_v: 4015.0\n",
      "fc layer 2 self.abs_max_out: 1110.0\n",
      "fc layer 1 self.abs_max_out: 3749.0\n",
      "fc layer 3 self.abs_max_out: 239.0\n",
      "fc layer 1 self.abs_max_out: 3818.0\n",
      "lif layer 1 self.abs_max_v: 4921.0\n",
      "lif layer 2 self.abs_max_v: 1225.0\n",
      "fc layer 1 self.abs_max_out: 4106.0\n",
      "lif layer 1 self.abs_max_v: 5369.5\n",
      "fc layer 2 self.abs_max_out: 1335.0\n",
      "lif layer 2 self.abs_max_v: 1335.0\n",
      "fc layer 1 self.abs_max_out: 4654.0\n",
      "lif layer 1 self.abs_max_v: 6130.0\n",
      "lif layer 2 self.abs_max_v: 1351.5\n",
      "lif layer 1 self.abs_max_v: 6295.0\n",
      "lif layer 2 self.abs_max_v: 1398.0\n",
      "fc layer 2 self.abs_max_out: 1401.0\n",
      "lif layer 2 self.abs_max_v: 1534.5\n",
      "fc layer 2 self.abs_max_out: 1437.0\n",
      "lif layer 2 self.abs_max_v: 1597.5\n",
      "lif layer 2 self.abs_max_v: 1615.5\n",
      "lif layer 2 self.abs_max_v: 1787.0\n",
      "fc layer 2 self.abs_max_out: 1465.0\n",
      "lif layer 2 self.abs_max_v: 1799.5\n",
      "fc layer 2 self.abs_max_out: 1480.0\n",
      "lif layer 2 self.abs_max_v: 1815.5\n",
      "lif layer 2 self.abs_max_v: 1821.5\n",
      "fc layer 2 self.abs_max_out: 1553.0\n",
      "fc layer 2 self.abs_max_out: 1560.0\n",
      "fc layer 3 self.abs_max_out: 243.0\n",
      "fc layer 2 self.abs_max_out: 1615.0\n",
      "fc layer 3 self.abs_max_out: 285.0\n",
      "lif layer 2 self.abs_max_v: 1832.0\n",
      "lif layer 2 self.abs_max_v: 1896.0\n",
      "lif layer 2 self.abs_max_v: 1940.0\n",
      "lif layer 2 self.abs_max_v: 2199.0\n",
      "lif layer 2 self.abs_max_v: 2383.0\n",
      "fc layer 2 self.abs_max_out: 1672.0\n",
      "fc layer 2 self.abs_max_out: 1682.0\n",
      "fc layer 2 self.abs_max_out: 1706.0\n",
      "fc layer 1 self.abs_max_out: 5114.0\n",
      "fc layer 2 self.abs_max_out: 1739.0\n",
      "lif layer 2 self.abs_max_v: 2420.5\n",
      "lif layer 2 self.abs_max_v: 2523.5\n",
      "fc layer 3 self.abs_max_out: 289.0\n",
      "fc layer 1 self.abs_max_out: 5365.0\n",
      "fc layer 1 self.abs_max_out: 5556.0\n",
      "fc layer 3 self.abs_max_out: 316.0\n",
      "fc layer 2 self.abs_max_out: 1822.0\n",
      "fc layer 2 self.abs_max_out: 1839.0\n",
      "lif layer 2 self.abs_max_v: 2583.5\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "fc layer 2 self.abs_max_out: 1881.0\n",
      "lif layer 2 self.abs_max_v: 2750.0\n",
      "lif layer 2 self.abs_max_v: 2929.0\n",
      "lif layer 2 self.abs_max_v: 3107.5\n",
      "lif layer 2 self.abs_max_v: 3264.0\n",
      "fc layer 3 self.abs_max_out: 327.0\n",
      "fc layer 3 self.abs_max_out: 337.0\n",
      "fc layer 2 self.abs_max_out: 1984.0\n",
      "lif layer 2 self.abs_max_v: 3288.5\n",
      "fc layer 3 self.abs_max_out: 356.0\n",
      "lif layer 1 self.abs_max_v: 6443.0\n",
      "lif layer 1 self.abs_max_v: 6737.5\n",
      "fc layer 2 self.abs_max_out: 1990.0\n",
      "fc layer 2 self.abs_max_out: 2105.0\n",
      "fc layer 2 self.abs_max_out: 2178.0\n",
      "fc layer 3 self.abs_max_out: 363.0\n",
      "fc layer 3 self.abs_max_out: 393.0\n",
      "fc layer 2 self.abs_max_out: 2199.0\n",
      "fc layer 3 self.abs_max_out: 396.0\n",
      "lif layer 1 self.abs_max_v: 7034.0\n",
      "lif layer 1 self.abs_max_v: 7148.0\n",
      "fc layer 1 self.abs_max_out: 5650.0\n",
      "lif layer 1 self.abs_max_v: 7296.0\n",
      "fc layer 1 self.abs_max_out: 5653.0\n",
      "lif layer 1 self.abs_max_v: 7452.5\n",
      "lif layer 1 self.abs_max_v: 8180.5\n",
      "fc layer 1 self.abs_max_out: 5724.0\n",
      "fc layer 1 self.abs_max_out: 5794.0\n",
      "lif layer 1 self.abs_max_v: 8275.0\n",
      "lif layer 1 self.abs_max_v: 8722.5\n",
      "fc layer 3 self.abs_max_out: 444.0\n",
      "lif layer 1 self.abs_max_v: 8856.5\n",
      "lif layer 1 self.abs_max_v: 9046.0\n",
      "lif layer 1 self.abs_max_v: 9212.0\n",
      "lif layer 1 self.abs_max_v: 9275.0\n",
      "lif layer 1 self.abs_max_v: 9494.5\n",
      "fc layer 2 self.abs_max_out: 2276.0\n",
      "lif layer 1 self.abs_max_v: 9735.0\n",
      "lif layer 1 self.abs_max_v: 10027.0\n",
      "lif layer 1 self.abs_max_v: 10316.5\n",
      "fc layer 2 self.abs_max_out: 2337.0\n",
      "fc layer 2 self.abs_max_out: 2341.0\n",
      "fc layer 1 self.abs_max_out: 5840.0\n",
      "lif layer 1 self.abs_max_v: 10402.0\n",
      "fc layer 2 self.abs_max_out: 2408.0\n",
      "fc layer 2 self.abs_max_out: 2449.0\n",
      "lif layer 2 self.abs_max_v: 3335.0\n",
      "fc layer 2 self.abs_max_out: 2505.0\n",
      "fc layer 2 self.abs_max_out: 2662.0\n",
      "fc layer 1 self.abs_max_out: 6127.0\n",
      "fc layer 1 self.abs_max_out: 6258.0\n",
      "fc layer 1 self.abs_max_out: 6868.0\n",
      "lif layer 1 self.abs_max_v: 10552.0\n",
      "lif layer 1 self.abs_max_v: 10766.0\n",
      "lif layer 1 self.abs_max_v: 10784.0\n",
      "fc layer 2 self.abs_max_out: 2692.0\n",
      "fc layer 2 self.abs_max_out: 2746.0\n",
      "fc layer 3 self.abs_max_out: 446.0\n",
      "lif layer 1 self.abs_max_v: 10866.5\n",
      "lif layer 1 self.abs_max_v: 11394.5\n",
      "lif layer 1 self.abs_max_v: 11518.5\n",
      "fc layer 3 self.abs_max_out: 462.0\n",
      "fc layer 1 self.abs_max_out: 7068.0\n",
      "fc layer 3 self.abs_max_out: 476.0\n",
      "fc layer 3 self.abs_max_out: 514.0\n",
      "fc layer 1 self.abs_max_out: 7217.0\n",
      "lif layer 1 self.abs_max_v: 11870.5\n",
      "lif layer 1 self.abs_max_v: 12037.5\n",
      "lif layer 1 self.abs_max_v: 12317.0\n",
      "lif layer 1 self.abs_max_v: 12956.5\n",
      "lif layer 1 self.abs_max_v: 13248.5\n",
      "lif layer 1 self.abs_max_v: 13302.5\n",
      "lif layer 1 self.abs_max_v: 13750.5\n",
      "fc layer 1 self.abs_max_out: 7244.0\n",
      "lif layer 1 self.abs_max_v: 14119.5\n",
      "fc layer 1 self.abs_max_out: 7321.0\n",
      "lif layer 2 self.abs_max_v: 3521.0\n",
      "lif layer 2 self.abs_max_v: 3550.5\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.968769/  2.139158, val:  33.75%, val_best:  33.75%, tr:  98.67%, tr_best:  98.67%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.8042%\n",
      "layer   2  Sparsity: 79.9627%\n",
      "layer   3  Sparsity: 90.7394%\n",
      "total_backward_count 9790 real_backward_count 1791  18.294%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 3790.0\n",
      "lif layer 2 self.abs_max_v: 3895.0\n",
      "fc layer 1 self.abs_max_out: 7328.0\n",
      "fc layer 1 self.abs_max_out: 7693.0\n",
      "lif layer 1 self.abs_max_v: 14538.0\n",
      "fc layer 1 self.abs_max_out: 7729.0\n",
      "fc layer 1 self.abs_max_out: 7796.0\n",
      "fc layer 1 self.abs_max_out: 7904.0\n",
      "lif layer 1 self.abs_max_v: 15050.0\n",
      "lif layer 1 self.abs_max_v: 15078.0\n",
      "lif layer 1 self.abs_max_v: 15375.0\n",
      "fc layer 3 self.abs_max_out: 519.0\n",
      "fc layer 1 self.abs_max_out: 8037.0\n",
      "fc layer 1 self.abs_max_out: 8276.0\n",
      "fc layer 3 self.abs_max_out: 532.0\n",
      "fc layer 3 self.abs_max_out: 549.0\n",
      "fc layer 3 self.abs_max_out: 571.0\n",
      "fc layer 3 self.abs_max_out: 581.0\n",
      "lif layer 2 self.abs_max_v: 3918.5\n",
      "lif layer 2 self.abs_max_v: 4223.0\n",
      "lif layer 2 self.abs_max_v: 4504.5\n",
      "lif layer 2 self.abs_max_v: 4532.5\n",
      "lif layer 2 self.abs_max_v: 4643.5\n",
      "lif layer 2 self.abs_max_v: 4648.0\n",
      "lif layer 2 self.abs_max_v: 4848.0\n",
      "lif layer 2 self.abs_max_v: 4862.0\n",
      "lif layer 1 self.abs_max_v: 15402.0\n",
      "fc layer 1 self.abs_max_out: 8334.0\n",
      "fc layer 1 self.abs_max_out: 8770.0\n",
      "fc layer 1 self.abs_max_out: 8911.0\n",
      "lif layer 1 self.abs_max_v: 15515.5\n",
      "lif layer 1 self.abs_max_v: 16193.0\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  2.023219/  2.135281, val:  32.92%, val_best:  33.75%, tr:  98.77%, tr_best:  98.77%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7979%\n",
      "layer   2  Sparsity: 83.0335%\n",
      "layer   3  Sparsity: 93.1288%\n",
      "total_backward_count 19580 real_backward_count 3607  18.422%\n",
      "fc layer 1 self.abs_max_out: 9720.0\n",
      "fc layer 3 self.abs_max_out: 587.0\n",
      "fc layer 1 self.abs_max_out: 9730.0\n",
      "lif layer 1 self.abs_max_v: 16265.5\n",
      "lif layer 1 self.abs_max_v: 16421.0\n",
      "lif layer 1 self.abs_max_v: 17097.5\n",
      "fc layer 2 self.abs_max_out: 2851.0\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  2.027497/  2.125110, val:  46.25%, val_best:  46.25%, tr:  98.57%, tr_best:  98.77%, epoch time: 72.89 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7967%\n",
      "layer   2  Sparsity: 83.6203%\n",
      "layer   3  Sparsity: 93.2211%\n",
      "total_backward_count 29370 real_backward_count 5397  18.376%\n",
      "fc layer 2 self.abs_max_out: 2949.0\n",
      "lif layer 1 self.abs_max_v: 17166.0\n",
      "fc layer 1 self.abs_max_out: 9988.0\n",
      "lif layer 1 self.abs_max_v: 17777.0\n",
      "lif layer 1 self.abs_max_v: 17792.0\n",
      "lif layer 1 self.abs_max_v: 17973.5\n",
      "fc layer 1 self.abs_max_out: 10440.0\n",
      "lif layer 1 self.abs_max_v: 19427.0\n",
      "fc layer 1 self.abs_max_out: 10452.0\n",
      "lif layer 1 self.abs_max_v: 19919.5\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  2.011440/  2.142830, val:  32.50%, val_best:  46.25%, tr:  98.98%, tr_best:  98.98%, epoch time: 73.00 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8109%\n",
      "layer   2  Sparsity: 83.2742%\n",
      "layer   3  Sparsity: 92.5909%\n",
      "total_backward_count 39160 real_backward_count 7089  18.103%\n",
      "fc layer 2 self.abs_max_out: 3025.0\n",
      "fc layer 1 self.abs_max_out: 11584.0\n",
      "lif layer 1 self.abs_max_v: 20248.5\n",
      "fc layer 2 self.abs_max_out: 3029.0\n",
      "lif layer 1 self.abs_max_v: 20959.0\n",
      "lif layer 1 self.abs_max_v: 21843.5\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  2.022356/  2.127912, val:  41.25%, val_best:  46.25%, tr:  98.26%, tr_best:  98.98%, epoch time: 72.34 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8079%\n",
      "layer   2  Sparsity: 83.8009%\n",
      "layer   3  Sparsity: 93.2034%\n",
      "total_backward_count 48950 real_backward_count 8746  17.867%\n",
      "fc layer 1 self.abs_max_out: 12021.0\n",
      "lif layer 1 self.abs_max_v: 22232.5\n",
      "lif layer 1 self.abs_max_v: 23106.5\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  2.030973/  2.136663, val:  44.58%, val_best:  46.25%, tr:  98.37%, tr_best:  98.98%, epoch time: 72.72 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8052%\n",
      "layer   2  Sparsity: 83.3669%\n",
      "layer   3  Sparsity: 93.0349%\n",
      "total_backward_count 58740 real_backward_count 10465  17.816%\n",
      "fc layer 3 self.abs_max_out: 597.0\n",
      "fc layer 3 self.abs_max_out: 619.0\n",
      "fc layer 3 self.abs_max_out: 669.0\n",
      "fc layer 3 self.abs_max_out: 676.0\n",
      "fc layer 2 self.abs_max_out: 3064.0\n",
      "fc layer 2 self.abs_max_out: 3167.0\n",
      "lif layer 1 self.abs_max_v: 23167.0\n",
      "fc layer 1 self.abs_max_out: 12853.0\n",
      "lif layer 1 self.abs_max_v: 23661.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.992104/  2.146635, val:  46.25%, val_best:  46.25%, tr:  98.77%, tr_best:  98.98%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.8128%\n",
      "layer   2  Sparsity: 83.5521%\n",
      "layer   3  Sparsity: 92.4932%\n",
      "total_backward_count 68530 real_backward_count 12127  17.696%\n",
      "fc layer 3 self.abs_max_out: 685.0\n",
      "lif layer 1 self.abs_max_v: 23696.0\n",
      "fc layer 1 self.abs_max_out: 13073.0\n",
      "lif layer 1 self.abs_max_v: 24113.0\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.981346/  2.077569, val:  42.50%, val_best:  46.25%, tr:  98.67%, tr_best:  98.98%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8077%\n",
      "layer   2  Sparsity: 83.8784%\n",
      "layer   3  Sparsity: 92.3137%\n",
      "total_backward_count 78320 real_backward_count 13664  17.446%\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.971212/  2.076426, val:  45.00%, val_best:  46.25%, tr:  98.77%, tr_best:  98.98%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7968%\n",
      "layer   2  Sparsity: 84.3661%\n",
      "layer   3  Sparsity: 91.9652%\n",
      "total_backward_count 88110 real_backward_count 15282  17.344%\n",
      "fc layer 1 self.abs_max_out: 13222.0\n",
      "lif layer 1 self.abs_max_v: 24390.5\n",
      "fc layer 2 self.abs_max_out: 3259.0\n",
      "lif layer 1 self.abs_max_v: 24538.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.928156/  2.136199, val:  42.50%, val_best:  46.25%, tr:  99.28%, tr_best:  99.28%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.7930%\n",
      "layer   2  Sparsity: 83.4581%\n",
      "layer   3  Sparsity: 90.9661%\n",
      "total_backward_count 97900 real_backward_count 16748  17.107%\n",
      "lif layer 2 self.abs_max_v: 4911.0\n",
      "lif layer 2 self.abs_max_v: 5063.0\n",
      "lif layer 2 self.abs_max_v: 5172.5\n",
      "lif layer 2 self.abs_max_v: 5228.5\n",
      "lif layer 2 self.abs_max_v: 5314.0\n",
      "lif layer 2 self.abs_max_v: 5454.0\n",
      "lif layer 2 self.abs_max_v: 5609.0\n",
      "lif layer 1 self.abs_max_v: 25090.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.934395/  2.063700, val:  41.25%, val_best:  46.25%, tr:  99.28%, tr_best:  99.28%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8001%\n",
      "layer   2  Sparsity: 82.2886%\n",
      "layer   3  Sparsity: 90.7898%\n",
      "total_backward_count 107690 real_backward_count 18262  16.958%\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.933740/  2.042630, val:  45.42%, val_best:  46.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.46 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8013%\n",
      "layer   2  Sparsity: 82.4037%\n",
      "layer   3  Sparsity: 90.5994%\n",
      "total_backward_count 117480 real_backward_count 19744  16.806%\n",
      "lif layer 2 self.abs_max_v: 5665.0\n",
      "lif layer 2 self.abs_max_v: 5676.5\n",
      "lif layer 2 self.abs_max_v: 5699.5\n",
      "lif layer 2 self.abs_max_v: 5827.5\n",
      "lif layer 2 self.abs_max_v: 5912.0\n",
      "fc layer 2 self.abs_max_out: 3284.0\n",
      "fc layer 1 self.abs_max_out: 13448.0\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.894044/  2.118696, val:  38.75%, val_best:  46.25%, tr:  99.08%, tr_best:  99.80%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8195%\n",
      "layer   2  Sparsity: 82.3519%\n",
      "layer   3  Sparsity: 90.7905%\n",
      "total_backward_count 127270 real_backward_count 21183  16.644%\n",
      "fc layer 2 self.abs_max_out: 3322.0\n",
      "fc layer 1 self.abs_max_out: 13807.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.945338/  2.115419, val:  31.67%, val_best:  46.25%, tr:  98.77%, tr_best:  99.80%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8036%\n",
      "layer   2  Sparsity: 82.9126%\n",
      "layer   3  Sparsity: 91.6216%\n",
      "total_backward_count 137060 real_backward_count 22686  16.552%\n",
      "lif layer 2 self.abs_max_v: 5938.0\n",
      "fc layer 1 self.abs_max_out: 14296.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.922504/  2.056531, val:  45.00%, val_best:  46.25%, tr:  98.67%, tr_best:  99.80%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8143%\n",
      "layer   2  Sparsity: 82.4860%\n",
      "layer   3  Sparsity: 90.8990%\n",
      "total_backward_count 146850 real_backward_count 24182  16.467%\n",
      "lif layer 1 self.abs_max_v: 25395.0\n",
      "lif layer 1 self.abs_max_v: 25657.0\n",
      "fc layer 1 self.abs_max_out: 14356.0\n",
      "lif layer 1 self.abs_max_v: 26366.5\n",
      "fc layer 1 self.abs_max_out: 14453.0\n",
      "lif layer 1 self.abs_max_v: 27636.5\n",
      "fc layer 1 self.abs_max_out: 14585.0\n",
      "lif layer 1 self.abs_max_v: 28403.5\n",
      "lif layer 1 self.abs_max_v: 28440.5\n",
      "fc layer 1 self.abs_max_out: 14705.0\n",
      "lif layer 1 self.abs_max_v: 28863.5\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.887626/  2.036572, val:  53.33%, val_best:  53.33%, tr:  98.77%, tr_best:  99.80%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8052%\n",
      "layer   2  Sparsity: 82.1486%\n",
      "layer   3  Sparsity: 89.9103%\n",
      "total_backward_count 156640 real_backward_count 25585  16.334%\n",
      "fc layer 1 self.abs_max_out: 14829.0\n",
      "fc layer 3 self.abs_max_out: 686.0\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.888938/  2.024259, val:  58.33%, val_best:  58.33%, tr:  99.49%, tr_best:  99.80%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8038%\n",
      "layer   2  Sparsity: 83.2797%\n",
      "layer   3  Sparsity: 90.5476%\n",
      "total_backward_count 166430 real_backward_count 26977  16.209%\n",
      "fc layer 3 self.abs_max_out: 688.0\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.906129/  2.043702, val:  56.25%, val_best:  58.33%, tr:  98.77%, tr_best:  99.80%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8079%\n",
      "layer   2  Sparsity: 84.4137%\n",
      "layer   3  Sparsity: 90.5406%\n",
      "total_backward_count 176220 real_backward_count 28435  16.136%\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.894348/  2.030965, val:  49.17%, val_best:  58.33%, tr:  99.28%, tr_best:  99.80%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8006%\n",
      "layer   2  Sparsity: 84.6879%\n",
      "layer   3  Sparsity: 90.3078%\n",
      "total_backward_count 186010 real_backward_count 29961  16.107%\n",
      "fc layer 3 self.abs_max_out: 711.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.862556/  2.053839, val:  38.33%, val_best:  58.33%, tr:  99.39%, tr_best:  99.80%, epoch time: 73.17 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8202%\n",
      "layer   2  Sparsity: 84.1432%\n",
      "layer   3  Sparsity: 89.2480%\n",
      "total_backward_count 195800 real_backward_count 31310  15.991%\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.858914/  2.067381, val:  40.00%, val_best:  58.33%, tr:  99.28%, tr_best:  99.80%, epoch time: 72.48 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8076%\n",
      "layer   2  Sparsity: 83.7689%\n",
      "layer   3  Sparsity: 89.9179%\n",
      "total_backward_count 205590 real_backward_count 32726  15.918%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.854211/  2.038749, val:  49.58%, val_best:  58.33%, tr:  99.59%, tr_best:  99.80%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8034%\n",
      "layer   2  Sparsity: 84.5185%\n",
      "layer   3  Sparsity: 89.6950%\n",
      "total_backward_count 215380 real_backward_count 34143  15.852%\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.843794/  1.993791, val:  50.83%, val_best:  58.33%, tr:  98.88%, tr_best:  99.80%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8152%\n",
      "layer   2  Sparsity: 85.1001%\n",
      "layer   3  Sparsity: 89.0884%\n",
      "total_backward_count 225170 real_backward_count 35573  15.798%\n",
      "fc layer 1 self.abs_max_out: 14945.0\n",
      "fc layer 1 self.abs_max_out: 15947.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.830386/  1.988548, val:  60.83%, val_best:  60.83%, tr:  99.08%, tr_best:  99.80%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8212%\n",
      "layer   2  Sparsity: 84.1065%\n",
      "layer   3  Sparsity: 89.1692%\n",
      "total_backward_count 234960 real_backward_count 36970  15.735%\n",
      "fc layer 2 self.abs_max_out: 3399.0\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.818615/  1.981517, val:  52.92%, val_best:  60.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8110%\n",
      "layer   2  Sparsity: 83.6098%\n",
      "layer   3  Sparsity: 89.3760%\n",
      "total_backward_count 244750 real_backward_count 38376  15.680%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.833727/  2.010017, val:  57.50%, val_best:  60.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8022%\n",
      "layer   2  Sparsity: 82.9814%\n",
      "layer   3  Sparsity: 89.5420%\n",
      "total_backward_count 254540 real_backward_count 39789  15.632%\n",
      "fc layer 3 self.abs_max_out: 721.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.819295/  1.968955, val:  53.33%, val_best:  60.83%, tr:  99.28%, tr_best:  99.80%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8071%\n",
      "layer   2  Sparsity: 82.4630%\n",
      "layer   3  Sparsity: 88.9875%\n",
      "total_backward_count 264330 real_backward_count 41090  15.545%\n",
      "fc layer 2 self.abs_max_out: 3501.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.803974/  1.964519, val:  48.75%, val_best:  60.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8040%\n",
      "layer   2  Sparsity: 83.3386%\n",
      "layer   3  Sparsity: 88.5140%\n",
      "total_backward_count 274120 real_backward_count 42469  15.493%\n",
      "fc layer 2 self.abs_max_out: 3668.0\n",
      "lif layer 2 self.abs_max_v: 5977.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.806176/  2.016607, val:  42.92%, val_best:  60.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8035%\n",
      "layer   2  Sparsity: 82.8222%\n",
      "layer   3  Sparsity: 89.3582%\n",
      "total_backward_count 283910 real_backward_count 43847  15.444%\n",
      "fc layer 3 self.abs_max_out: 730.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.822180/  1.968942, val:  50.00%, val_best:  60.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8121%\n",
      "layer   2  Sparsity: 82.5781%\n",
      "layer   3  Sparsity: 88.6174%\n",
      "total_backward_count 293700 real_backward_count 45172  15.380%\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.765253/  1.973258, val:  55.00%, val_best:  60.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.33 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8054%\n",
      "layer   2  Sparsity: 82.7075%\n",
      "layer   3  Sparsity: 87.6924%\n",
      "total_backward_count 303490 real_backward_count 46468  15.311%\n",
      "lif layer 2 self.abs_max_v: 6011.5\n",
      "lif layer 2 self.abs_max_v: 6024.5\n",
      "fc layer 3 self.abs_max_out: 871.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.750072/  1.921102, val:  47.92%, val_best:  60.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 72.36 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8067%\n",
      "layer   2  Sparsity: 82.6571%\n",
      "layer   3  Sparsity: 87.7332%\n",
      "total_backward_count 313280 real_backward_count 47846  15.273%\n",
      "lif layer 2 self.abs_max_v: 6097.0\n",
      "lif layer 2 self.abs_max_v: 6268.5\n",
      "fc layer 1 self.abs_max_out: 15996.0\n",
      "fc layer 1 self.abs_max_out: 16213.0\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.795917/  1.984178, val:  53.33%, val_best:  60.83%, tr:  99.08%, tr_best:  99.80%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8064%\n",
      "layer   2  Sparsity: 82.0573%\n",
      "layer   3  Sparsity: 88.9360%\n",
      "total_backward_count 323070 real_backward_count 49216  15.234%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.804536/  2.007029, val:  45.83%, val_best:  60.83%, tr:  99.08%, tr_best:  99.80%, epoch time: 72.89 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7895%\n",
      "layer   2  Sparsity: 81.0320%\n",
      "layer   3  Sparsity: 88.6314%\n",
      "total_backward_count 332860 real_backward_count 50577  15.195%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.787844/  1.994265, val:  52.50%, val_best:  60.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7916%\n",
      "layer   2  Sparsity: 80.9626%\n",
      "layer   3  Sparsity: 89.2279%\n",
      "total_backward_count 342650 real_backward_count 51984  15.171%\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.832225/  1.992054, val:  49.17%, val_best:  60.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8048%\n",
      "layer   2  Sparsity: 81.0992%\n",
      "layer   3  Sparsity: 89.4787%\n",
      "total_backward_count 352440 real_backward_count 53400  15.152%\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.753225/  1.958203, val:  57.50%, val_best:  60.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8019%\n",
      "layer   2  Sparsity: 81.1062%\n",
      "layer   3  Sparsity: 87.9826%\n",
      "total_backward_count 362230 real_backward_count 54721  15.107%\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.742645/  1.959070, val:  44.58%, val_best:  60.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7988%\n",
      "layer   2  Sparsity: 80.6228%\n",
      "layer   3  Sparsity: 86.8630%\n",
      "total_backward_count 372020 real_backward_count 55946  15.038%\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.703396/  1.936008, val:  53.75%, val_best:  60.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8038%\n",
      "layer   2  Sparsity: 80.6382%\n",
      "layer   3  Sparsity: 86.6132%\n",
      "total_backward_count 381810 real_backward_count 57297  15.007%\n",
      "lif layer 2 self.abs_max_v: 6393.5\n",
      "lif layer 2 self.abs_max_v: 6517.0\n",
      "fc layer 1 self.abs_max_out: 16773.0\n",
      "lif layer 1 self.abs_max_v: 30007.5\n",
      "lif layer 1 self.abs_max_v: 30017.5\n",
      "fc layer 3 self.abs_max_out: 887.0\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.726616/  2.011227, val:  41.67%, val_best:  60.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.7949%\n",
      "layer   2  Sparsity: 79.9983%\n",
      "layer   3  Sparsity: 87.2787%\n",
      "total_backward_count 391600 real_backward_count 58610  14.967%\n",
      "lif layer 2 self.abs_max_v: 6584.0\n",
      "lif layer 2 self.abs_max_v: 6667.0\n",
      "lif layer 2 self.abs_max_v: 6760.5\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.736759/  1.941402, val:  50.42%, val_best:  60.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8093%\n",
      "layer   2  Sparsity: 78.8921%\n",
      "layer   3  Sparsity: 87.4879%\n",
      "total_backward_count 401390 real_backward_count 59950  14.936%\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.710922/  1.919825, val:  49.17%, val_best:  60.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8022%\n",
      "layer   2  Sparsity: 78.1891%\n",
      "layer   3  Sparsity: 87.1434%\n",
      "total_backward_count 411180 real_backward_count 61215  14.888%\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.703975/  1.958103, val:  45.42%, val_best:  60.83%, tr:  98.98%, tr_best:  99.80%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8108%\n",
      "layer   2  Sparsity: 79.3217%\n",
      "layer   3  Sparsity: 88.0153%\n",
      "total_backward_count 420970 real_backward_count 62525  14.853%\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.737474/  1.944701, val:  55.42%, val_best:  60.83%, tr:  99.08%, tr_best:  99.80%, epoch time: 72.30 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 82.8034%\n",
      "layer   2  Sparsity: 79.9722%\n",
      "layer   3  Sparsity: 87.8336%\n",
      "total_backward_count 430760 real_backward_count 63883  14.830%\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.759071/  1.943479, val:  46.25%, val_best:  60.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8089%\n",
      "layer   2  Sparsity: 80.1398%\n",
      "layer   3  Sparsity: 88.7121%\n",
      "total_backward_count 440550 real_backward_count 65211  14.802%\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.786179/  1.963117, val:  56.67%, val_best:  60.83%, tr:  99.28%, tr_best:  99.80%, epoch time: 72.12 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 82.8220%\n",
      "layer   2  Sparsity: 79.2828%\n",
      "layer   3  Sparsity: 89.0414%\n",
      "total_backward_count 450340 real_backward_count 66498  14.766%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.753240/  1.950091, val:  56.67%, val_best:  60.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 71.80 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 82.7935%\n",
      "layer   2  Sparsity: 78.7392%\n",
      "layer   3  Sparsity: 87.9508%\n",
      "total_backward_count 460130 real_backward_count 67814  14.738%\n",
      "fc layer 1 self.abs_max_out: 17063.0\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.760848/  2.004361, val:  41.67%, val_best:  60.83%, tr:  98.98%, tr_best:  99.80%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7994%\n",
      "layer   2  Sparsity: 78.7866%\n",
      "layer   3  Sparsity: 88.5296%\n",
      "total_backward_count 469920 real_backward_count 69140  14.713%\n",
      "lif layer 1 self.abs_max_v: 30026.5\n",
      "lif layer 1 self.abs_max_v: 30287.5\n",
      "lif layer 1 self.abs_max_v: 30386.0\n",
      "lif layer 1 self.abs_max_v: 30547.5\n",
      "lif layer 1 self.abs_max_v: 31767.0\n",
      "lif layer 1 self.abs_max_v: 32001.5\n",
      "lif layer 1 self.abs_max_v: 32280.0\n",
      "lif layer 1 self.abs_max_v: 32295.0\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.718400/  1.934678, val:  52.92%, val_best:  60.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7988%\n",
      "layer   2  Sparsity: 79.2305%\n",
      "layer   3  Sparsity: 87.6115%\n",
      "total_backward_count 479710 real_backward_count 70463  14.689%\n",
      "fc layer 1 self.abs_max_out: 17413.0\n",
      "lif layer 1 self.abs_max_v: 32797.5\n",
      "fc layer 1 self.abs_max_out: 17503.0\n",
      "lif layer 1 self.abs_max_v: 33214.0\n",
      "fc layer 1 self.abs_max_out: 17765.0\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.690479/  1.910328, val:  49.58%, val_best:  60.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.8061%\n",
      "layer   2  Sparsity: 79.3265%\n",
      "layer   3  Sparsity: 87.2675%\n",
      "total_backward_count 489500 real_backward_count 71779  14.664%\n",
      "lif layer 2 self.abs_max_v: 7004.5\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.741627/  1.885935, val:  49.58%, val_best:  60.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7919%\n",
      "layer   2  Sparsity: 79.5613%\n",
      "layer   3  Sparsity: 88.2545%\n",
      "total_backward_count 499290 real_backward_count 73122  14.645%\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.675998/  1.906685, val:  48.33%, val_best:  60.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 72.93 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8082%\n",
      "layer   2  Sparsity: 79.5708%\n",
      "layer   3  Sparsity: 86.8603%\n",
      "total_backward_count 509080 real_backward_count 74380  14.611%\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.627946/  1.916709, val:  49.17%, val_best:  60.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.8072%\n",
      "layer   2  Sparsity: 79.4618%\n",
      "layer   3  Sparsity: 85.8814%\n",
      "total_backward_count 518870 real_backward_count 75657  14.581%\n",
      "fc layer 2 self.abs_max_out: 3883.0\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.640663/  1.873582, val:  55.00%, val_best:  60.83%, tr:  98.98%, tr_best:  99.80%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7975%\n",
      "layer   2  Sparsity: 79.2153%\n",
      "layer   3  Sparsity: 86.0978%\n",
      "total_backward_count 528660 real_backward_count 76992  14.564%\n",
      "lif layer 2 self.abs_max_v: 7181.5\n",
      "lif layer 2 self.abs_max_v: 7183.5\n",
      "fc layer 2 self.abs_max_out: 3912.0\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.660961/  1.879758, val:  55.00%, val_best:  60.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8065%\n",
      "layer   2  Sparsity: 79.0155%\n",
      "layer   3  Sparsity: 86.6290%\n",
      "total_backward_count 538450 real_backward_count 78306  14.543%\n",
      "lif layer 2 self.abs_max_v: 7409.0\n",
      "fc layer 2 self.abs_max_out: 4096.0\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.664810/  1.903208, val:  57.92%, val_best:  60.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.89 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8084%\n",
      "layer   2  Sparsity: 79.2249%\n",
      "layer   3  Sparsity: 86.9573%\n",
      "total_backward_count 548240 real_backward_count 79610  14.521%\n",
      "lif layer 2 self.abs_max_v: 7421.0\n",
      "lif layer 2 self.abs_max_v: 7430.5\n",
      "lif layer 2 self.abs_max_v: 7506.0\n",
      "fc layer 2 self.abs_max_out: 4265.0\n",
      "lif layer 2 self.abs_max_v: 8010.0\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.652485/  1.930543, val:  37.50%, val_best:  60.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 72.10 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 82.8161%\n",
      "layer   2  Sparsity: 78.8816%\n",
      "layer   3  Sparsity: 85.9523%\n",
      "total_backward_count 558030 real_backward_count 80895  14.497%\n",
      "fc layer 3 self.abs_max_out: 940.0\n",
      "fc layer 2 self.abs_max_out: 4267.0\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.601891/  1.913919, val:  49.58%, val_best:  60.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8031%\n",
      "layer   2  Sparsity: 79.7172%\n",
      "layer   3  Sparsity: 85.7395%\n",
      "total_backward_count 567820 real_backward_count 82147  14.467%\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.628381/  1.853259, val:  47.50%, val_best:  60.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.56 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7942%\n",
      "layer   2  Sparsity: 79.3016%\n",
      "layer   3  Sparsity: 85.4895%\n",
      "total_backward_count 577610 real_backward_count 83380  14.435%\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.615240/  1.861372, val:  52.92%, val_best:  60.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 72.31 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8023%\n",
      "layer   2  Sparsity: 79.8283%\n",
      "layer   3  Sparsity: 85.5015%\n",
      "total_backward_count 587400 real_backward_count 84607  14.404%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.602192/  1.865802, val:  57.08%, val_best:  60.83%, tr:  99.49%, tr_best:  99.90%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7981%\n",
      "layer   2  Sparsity: 79.5496%\n",
      "layer   3  Sparsity: 85.7547%\n",
      "total_backward_count 597190 real_backward_count 85933  14.390%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.610416/  1.820639, val:  54.58%, val_best:  60.83%, tr:  99.39%, tr_best:  99.90%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7963%\n",
      "layer   2  Sparsity: 78.8671%\n",
      "layer   3  Sparsity: 85.7607%\n",
      "total_backward_count 606980 real_backward_count 87238  14.372%\n",
      "fc layer 3 self.abs_max_out: 960.0\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.563621/  1.877123, val:  45.00%, val_best:  60.83%, tr:  99.59%, tr_best:  99.90%, epoch time: 72.93 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7950%\n",
      "layer   2  Sparsity: 78.8355%\n",
      "layer   3  Sparsity: 84.9946%\n",
      "total_backward_count 616770 real_backward_count 88492  14.348%\n",
      "fc layer 3 self.abs_max_out: 983.0\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.604620/  1.892139, val:  52.08%, val_best:  60.83%, tr:  99.80%, tr_best:  99.90%, epoch time: 73.00 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8099%\n",
      "layer   2  Sparsity: 77.9870%\n",
      "layer   3  Sparsity: 85.2577%\n",
      "total_backward_count 626560 real_backward_count 89708  14.318%\n",
      "fc layer 1 self.abs_max_out: 17779.0\n",
      "fc layer 1 self.abs_max_out: 18395.0\n",
      "fc layer 1 self.abs_max_out: 20289.0\n",
      "lif layer 1 self.abs_max_v: 34970.5\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.559116/  1.846413, val:  47.08%, val_best:  60.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.59 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.7992%\n",
      "layer   2  Sparsity: 77.8507%\n",
      "layer   3  Sparsity: 84.6562%\n",
      "total_backward_count 636350 real_backward_count 90907  14.286%\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.568059/  1.845745, val:  57.08%, val_best:  60.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8092%\n",
      "layer   2  Sparsity: 78.2652%\n",
      "layer   3  Sparsity: 86.0975%\n",
      "total_backward_count 646140 real_backward_count 92106  14.255%\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.649686/  1.924367, val:  48.33%, val_best:  60.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8009%\n",
      "layer   2  Sparsity: 78.0227%\n",
      "layer   3  Sparsity: 87.2322%\n",
      "total_backward_count 655930 real_backward_count 93361  14.233%\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.661490/  1.881131, val:  55.42%, val_best:  60.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7891%\n",
      "layer   2  Sparsity: 77.4406%\n",
      "layer   3  Sparsity: 86.4887%\n",
      "total_backward_count 665720 real_backward_count 94620  14.213%\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.616030/  1.848405, val:  54.58%, val_best:  60.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7934%\n",
      "layer   2  Sparsity: 77.2575%\n",
      "layer   3  Sparsity: 85.6128%\n",
      "total_backward_count 675510 real_backward_count 95860  14.191%\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.535632/  1.822169, val:  52.92%, val_best:  60.83%, tr:  99.28%, tr_best: 100.00%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8164%\n",
      "layer   2  Sparsity: 77.7969%\n",
      "layer   3  Sparsity: 84.2993%\n",
      "total_backward_count 685300 real_backward_count 97067  14.164%\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.503947/  1.798726, val:  56.67%, val_best:  60.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.8072%\n",
      "layer   2  Sparsity: 77.3355%\n",
      "layer   3  Sparsity: 83.8228%\n",
      "total_backward_count 695090 real_backward_count 98315  14.144%\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.481561/  1.820769, val:  55.00%, val_best:  60.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.7895%\n",
      "layer   2  Sparsity: 76.7343%\n",
      "layer   3  Sparsity: 83.8584%\n",
      "total_backward_count 704880 real_backward_count 99470  14.112%\n",
      "fc layer 3 self.abs_max_out: 1018.0\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.508412/  1.765557, val:  53.33%, val_best:  60.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.7915%\n",
      "layer   2  Sparsity: 76.3397%\n",
      "layer   3  Sparsity: 84.7271%\n",
      "total_backward_count 714670 real_backward_count 100656  14.084%\n",
      "fc layer 3 self.abs_max_out: 1029.0\n",
      "fc layer 3 self.abs_max_out: 1033.0\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.512498/  1.818680, val:  50.42%, val_best:  60.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7948%\n",
      "layer   2  Sparsity: 75.6059%\n",
      "layer   3  Sparsity: 85.1947%\n",
      "total_backward_count 724460 real_backward_count 101866  14.061%\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.491771/  1.805790, val:  49.58%, val_best:  60.83%, tr:  99.39%, tr_best: 100.00%, epoch time: 74.25 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 82.8057%\n",
      "layer   2  Sparsity: 75.8654%\n",
      "layer   3  Sparsity: 84.6178%\n",
      "total_backward_count 734250 real_backward_count 103108  14.043%\n",
      "fc layer 3 self.abs_max_out: 1039.0\n",
      "fc layer 3 self.abs_max_out: 1070.0\n",
      "fc layer 3 self.abs_max_out: 1074.0\n",
      "fc layer 3 self.abs_max_out: 1110.0\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.489602/  1.723860, val:  55.00%, val_best:  60.83%, tr:  99.39%, tr_best: 100.00%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7844%\n",
      "layer   2  Sparsity: 75.3461%\n",
      "layer   3  Sparsity: 84.2065%\n",
      "total_backward_count 744040 real_backward_count 104319  14.021%\n",
      "fc layer 3 self.abs_max_out: 1122.0\n",
      "fc layer 3 self.abs_max_out: 1136.0\n",
      "fc layer 3 self.abs_max_out: 1162.0\n",
      "fc layer 3 self.abs_max_out: 1173.0\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.431344/  1.704936, val:  59.58%, val_best:  60.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.16 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 82.8131%\n",
      "layer   2  Sparsity: 75.7142%\n",
      "layer   3  Sparsity: 83.0451%\n",
      "total_backward_count 753830 real_backward_count 105541  14.001%\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.456500/  1.855788, val:  46.67%, val_best:  60.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 72.19 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 82.7972%\n",
      "layer   2  Sparsity: 75.6077%\n",
      "layer   3  Sparsity: 84.6822%\n",
      "total_backward_count 763620 real_backward_count 106767  13.982%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.513068/  1.825554, val:  44.17%, val_best:  60.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.47 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7968%\n",
      "layer   2  Sparsity: 76.5263%\n",
      "layer   3  Sparsity: 84.4541%\n",
      "total_backward_count 773410 real_backward_count 107972  13.961%\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.493071/  1.826388, val:  53.33%, val_best:  60.83%, tr:  99.18%, tr_best: 100.00%, epoch time: 72.68 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8080%\n",
      "layer   2  Sparsity: 75.7395%\n",
      "layer   3  Sparsity: 84.7725%\n",
      "total_backward_count 783200 real_backward_count 109175  13.940%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.491232/  1.747636, val:  55.83%, val_best:  60.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.23 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 82.8150%\n",
      "layer   2  Sparsity: 75.6237%\n",
      "layer   3  Sparsity: 83.9149%\n",
      "total_backward_count 792990 real_backward_count 110359  13.917%\n",
      "fc layer 2 self.abs_max_out: 4485.0\n",
      "lif layer 2 self.abs_max_v: 8037.0\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.478308/  1.884856, val:  38.33%, val_best:  60.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.42 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8117%\n",
      "layer   2  Sparsity: 76.0435%\n",
      "layer   3  Sparsity: 84.9291%\n",
      "total_backward_count 802780 real_backward_count 111551  13.896%\n",
      "lif layer 2 self.abs_max_v: 8227.0\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.537712/  1.805869, val:  53.75%, val_best:  60.83%, tr:  99.39%, tr_best: 100.00%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7972%\n",
      "layer   2  Sparsity: 76.1046%\n",
      "layer   3  Sparsity: 84.7008%\n",
      "total_backward_count 812570 real_backward_count 112778  13.879%\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.544801/  1.793585, val:  50.83%, val_best:  60.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8068%\n",
      "layer   2  Sparsity: 76.5087%\n",
      "layer   3  Sparsity: 84.1329%\n",
      "total_backward_count 822360 real_backward_count 114060  13.870%\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.540738/  1.879415, val:  49.17%, val_best:  60.83%, tr:  98.88%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8009%\n",
      "layer   2  Sparsity: 76.6233%\n",
      "layer   3  Sparsity: 84.9746%\n",
      "total_backward_count 832150 real_backward_count 115350  13.862%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.579677/  1.848580, val:  48.33%, val_best:  60.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8041%\n",
      "layer   2  Sparsity: 77.0812%\n",
      "layer   3  Sparsity: 85.0739%\n",
      "total_backward_count 841940 real_backward_count 116645  13.854%\n",
      "fc layer 3 self.abs_max_out: 1191.0\n",
      "fc layer 3 self.abs_max_out: 1223.0\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.523899/  1.808192, val:  47.50%, val_best:  60.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8078%\n",
      "layer   2  Sparsity: 77.0045%\n",
      "layer   3  Sparsity: 84.2598%\n",
      "total_backward_count 851730 real_backward_count 117842  13.836%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.579284/  1.900154, val:  53.75%, val_best:  60.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.19 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7993%\n",
      "layer   2  Sparsity: 77.8611%\n",
      "layer   3  Sparsity: 85.7857%\n",
      "total_backward_count 861520 real_backward_count 119128  13.828%\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.568193/  1.793545, val:  55.42%, val_best:  60.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8248%\n",
      "layer   2  Sparsity: 77.4671%\n",
      "layer   3  Sparsity: 85.1276%\n",
      "total_backward_count 871310 real_backward_count 120350  13.813%\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.565596/  1.839119, val:  57.50%, val_best:  60.83%, tr:  98.67%, tr_best: 100.00%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.7981%\n",
      "layer   2  Sparsity: 77.7046%\n",
      "layer   3  Sparsity: 85.9226%\n",
      "total_backward_count 881100 real_backward_count 121613  13.802%\n",
      "fc layer 3 self.abs_max_out: 1241.0\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.589124/  1.864766, val:  49.17%, val_best:  60.83%, tr:  98.98%, tr_best: 100.00%, epoch time: 73.96 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.8063%\n",
      "layer   2  Sparsity: 79.1334%\n",
      "layer   3  Sparsity: 85.8208%\n",
      "total_backward_count 890890 real_backward_count 122913  13.797%\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.618362/  1.855145, val:  56.25%, val_best:  60.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.55 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.8104%\n",
      "layer   2  Sparsity: 79.5606%\n",
      "layer   3  Sparsity: 86.3540%\n",
      "total_backward_count 900680 real_backward_count 124268  13.797%\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.581388/  1.850518, val:  52.50%, val_best:  60.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8056%\n",
      "layer   2  Sparsity: 79.3910%\n",
      "layer   3  Sparsity: 85.5155%\n",
      "total_backward_count 910470 real_backward_count 125542  13.789%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.574505/  1.839678, val:  55.83%, val_best:  60.83%, tr:  99.28%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8174%\n",
      "layer   2  Sparsity: 79.8781%\n",
      "layer   3  Sparsity: 85.6703%\n",
      "total_backward_count 920260 real_backward_count 126812  13.780%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.558925/  1.809231, val:  55.42%, val_best:  60.83%, tr:  99.39%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8029%\n",
      "layer   2  Sparsity: 79.9449%\n",
      "layer   3  Sparsity: 84.7985%\n",
      "total_backward_count 930050 real_backward_count 128072  13.770%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.540988/  1.731404, val:  52.92%, val_best:  60.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8056%\n",
      "layer   2  Sparsity: 79.6255%\n",
      "layer   3  Sparsity: 84.3678%\n",
      "total_backward_count 939840 real_backward_count 129345  13.762%\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.488585/  1.883864, val:  45.00%, val_best:  60.83%, tr:  99.18%, tr_best: 100.00%, epoch time: 74.04 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.8064%\n",
      "layer   2  Sparsity: 80.2913%\n",
      "layer   3  Sparsity: 85.0425%\n",
      "total_backward_count 949630 real_backward_count 130603  13.753%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.556143/  1.845221, val:  51.67%, val_best:  60.83%, tr:  99.08%, tr_best: 100.00%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.8004%\n",
      "layer   2  Sparsity: 80.7988%\n",
      "layer   3  Sparsity: 85.1571%\n",
      "total_backward_count 959420 real_backward_count 131944  13.752%\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.549228/  1.847708, val:  42.50%, val_best:  60.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.54 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.7895%\n",
      "layer   2  Sparsity: 80.2562%\n",
      "layer   3  Sparsity: 84.8233%\n",
      "total_backward_count 969210 real_backward_count 133180  13.741%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.507214/  1.794080, val:  59.17%, val_best:  60.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8010%\n",
      "layer   2  Sparsity: 80.5370%\n",
      "layer   3  Sparsity: 84.2223%\n",
      "total_backward_count 979000 real_backward_count 134405  13.729%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.558016/  1.885386, val:  49.17%, val_best:  60.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.8088%\n",
      "layer   2  Sparsity: 80.1595%\n",
      "layer   3  Sparsity: 85.3157%\n",
      "total_backward_count 988790 real_backward_count 135622  13.716%\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.534593/  1.790751, val:  52.08%, val_best:  60.83%, tr:  99.28%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8053%\n",
      "layer   2  Sparsity: 80.3660%\n",
      "layer   3  Sparsity: 84.1934%\n",
      "total_backward_count 998580 real_backward_count 136861  13.706%\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.528134/  1.842525, val:  49.58%, val_best:  60.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7930%\n",
      "layer   2  Sparsity: 80.4680%\n",
      "layer   3  Sparsity: 84.5370%\n",
      "total_backward_count 1008370 real_backward_count 138093  13.695%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.511168/  1.783195, val:  50.00%, val_best:  60.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8046%\n",
      "layer   2  Sparsity: 80.9794%\n",
      "layer   3  Sparsity: 85.0212%\n",
      "total_backward_count 1018160 real_backward_count 139259  13.678%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.498960/  1.874448, val:  44.58%, val_best:  60.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 72.78 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8060%\n",
      "layer   2  Sparsity: 80.8688%\n",
      "layer   3  Sparsity: 84.2616%\n",
      "total_backward_count 1027950 real_backward_count 140411  13.659%\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.526350/  1.764403, val:  50.42%, val_best:  60.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.29 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 82.8014%\n",
      "layer   2  Sparsity: 80.1546%\n",
      "layer   3  Sparsity: 83.8857%\n",
      "total_backward_count 1037740 real_backward_count 141670  13.652%\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.492764/  1.797194, val:  40.83%, val_best:  60.83%, tr:  99.39%, tr_best: 100.00%, epoch time: 72.07 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 82.7966%\n",
      "layer   2  Sparsity: 80.0924%\n",
      "layer   3  Sparsity: 84.2715%\n",
      "total_backward_count 1047530 real_backward_count 142928  13.644%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.502830/  1.818098, val:  50.83%, val_best:  60.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.66 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8053%\n",
      "layer   2  Sparsity: 79.5626%\n",
      "layer   3  Sparsity: 84.5982%\n",
      "total_backward_count 1057320 real_backward_count 144130  13.632%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.516217/  1.787916, val:  55.00%, val_best:  60.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.34 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7868%\n",
      "layer   2  Sparsity: 79.7992%\n",
      "layer   3  Sparsity: 84.0927%\n",
      "total_backward_count 1067110 real_backward_count 145366  13.622%\n",
      "fc layer 3 self.abs_max_out: 1262.0\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.457226/  1.745862, val:  57.08%, val_best:  60.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8061%\n",
      "layer   2  Sparsity: 79.8326%\n",
      "layer   3  Sparsity: 81.8078%\n",
      "total_backward_count 1076900 real_backward_count 146510  13.605%\n",
      "epoch-110 lr=['0.0078125'], tr/val_loss:  1.462594/  1.780530, val:  52.08%, val_best:  60.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.14 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7917%\n",
      "layer   2  Sparsity: 80.2139%\n",
      "layer   3  Sparsity: 82.6303%\n",
      "total_backward_count 1086690 real_backward_count 147687  13.591%\n",
      "epoch-111 lr=['0.0078125'], tr/val_loss:  1.463896/  1.768944, val:  45.42%, val_best:  60.83%, tr:  99.28%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7892%\n",
      "layer   2  Sparsity: 79.7893%\n",
      "layer   3  Sparsity: 83.4082%\n",
      "total_backward_count 1096480 real_backward_count 148920  13.582%\n",
      "epoch-112 lr=['0.0078125'], tr/val_loss:  1.413800/  1.690457, val:  61.25%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.08 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8065%\n",
      "layer   2  Sparsity: 79.9518%\n",
      "layer   3  Sparsity: 82.6876%\n",
      "total_backward_count 1106270 real_backward_count 150103  13.568%\n",
      "epoch-113 lr=['0.0078125'], tr/val_loss:  1.473050/  1.782431, val:  55.83%, val_best:  61.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.01 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8035%\n",
      "layer   2  Sparsity: 80.4687%\n",
      "layer   3  Sparsity: 83.9247%\n",
      "total_backward_count 1116060 real_backward_count 151316  13.558%\n",
      "epoch-114 lr=['0.0078125'], tr/val_loss:  1.462388/  1.820365, val:  45.42%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.77 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.8010%\n",
      "layer   2  Sparsity: 80.8230%\n",
      "layer   3  Sparsity: 83.3145%\n",
      "total_backward_count 1125850 real_backward_count 152465  13.542%\n",
      "epoch-115 lr=['0.0078125'], tr/val_loss:  1.473862/  1.767238, val:  52.08%, val_best:  61.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.7925%\n",
      "layer   2  Sparsity: 80.5165%\n",
      "layer   3  Sparsity: 82.6634%\n",
      "total_backward_count 1135640 real_backward_count 153625  13.528%\n",
      "epoch-116 lr=['0.0078125'], tr/val_loss:  1.444613/  1.728054, val:  57.50%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7973%\n",
      "layer   2  Sparsity: 80.4392%\n",
      "layer   3  Sparsity: 81.3555%\n",
      "total_backward_count 1145430 real_backward_count 154784  13.513%\n",
      "fc layer 3 self.abs_max_out: 1270.0\n",
      "fc layer 3 self.abs_max_out: 1349.0\n",
      "epoch-117 lr=['0.0078125'], tr/val_loss:  1.423964/  1.773299, val:  50.00%, val_best:  61.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8071%\n",
      "layer   2  Sparsity: 80.9542%\n",
      "layer   3  Sparsity: 81.2949%\n",
      "total_backward_count 1155220 real_backward_count 156000  13.504%\n",
      "epoch-118 lr=['0.0078125'], tr/val_loss:  1.404169/  1.748487, val:  56.25%, val_best:  61.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 72.49 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8154%\n",
      "layer   2  Sparsity: 80.1461%\n",
      "layer   3  Sparsity: 81.5548%\n",
      "total_backward_count 1165010 real_backward_count 157141  13.488%\n",
      "epoch-119 lr=['0.0078125'], tr/val_loss:  1.430200/  1.719650, val:  60.42%, val_best:  61.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7915%\n",
      "layer   2  Sparsity: 79.3874%\n",
      "layer   3  Sparsity: 82.1422%\n",
      "total_backward_count 1174800 real_backward_count 158320  13.476%\n",
      "epoch-120 lr=['0.0078125'], tr/val_loss:  1.447114/  1.758577, val:  52.50%, val_best:  61.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.7927%\n",
      "layer   2  Sparsity: 80.0668%\n",
      "layer   3  Sparsity: 83.1454%\n",
      "total_backward_count 1184590 real_backward_count 159470  13.462%\n",
      "epoch-121 lr=['0.0078125'], tr/val_loss:  1.435050/  1.770545, val:  55.42%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8047%\n",
      "layer   2  Sparsity: 80.2530%\n",
      "layer   3  Sparsity: 81.8417%\n",
      "total_backward_count 1194380 real_backward_count 160643  13.450%\n",
      "epoch-122 lr=['0.0078125'], tr/val_loss:  1.432596/  1.730345, val:  52.08%, val_best:  61.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 72.93 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8112%\n",
      "layer   2  Sparsity: 79.5991%\n",
      "layer   3  Sparsity: 81.3381%\n",
      "total_backward_count 1204170 real_backward_count 161862  13.442%\n",
      "epoch-123 lr=['0.0078125'], tr/val_loss:  1.392907/  1.740727, val:  56.67%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8027%\n",
      "layer   2  Sparsity: 78.9246%\n",
      "layer   3  Sparsity: 81.9953%\n",
      "total_backward_count 1213960 real_backward_count 163073  13.433%\n",
      "epoch-124 lr=['0.0078125'], tr/val_loss:  1.433053/  1.684615, val:  58.75%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7920%\n",
      "layer   2  Sparsity: 79.0990%\n",
      "layer   3  Sparsity: 82.5626%\n",
      "total_backward_count 1223750 real_backward_count 164297  13.426%\n",
      "epoch-125 lr=['0.0078125'], tr/val_loss:  1.460799/  1.795404, val:  47.92%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.48 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8066%\n",
      "layer   2  Sparsity: 79.3699%\n",
      "layer   3  Sparsity: 84.5887%\n",
      "total_backward_count 1233540 real_backward_count 165486  13.416%\n",
      "epoch-126 lr=['0.0078125'], tr/val_loss:  1.449389/  1.796850, val:  52.50%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7984%\n",
      "layer   2  Sparsity: 78.9729%\n",
      "layer   3  Sparsity: 82.9133%\n",
      "total_backward_count 1243330 real_backward_count 166683  13.406%\n",
      "epoch-127 lr=['0.0078125'], tr/val_loss:  1.445432/  1.746289, val:  56.25%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.48 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8009%\n",
      "layer   2  Sparsity: 79.3678%\n",
      "layer   3  Sparsity: 83.1374%\n",
      "total_backward_count 1253120 real_backward_count 167847  13.394%\n",
      "epoch-128 lr=['0.0078125'], tr/val_loss:  1.430233/  1.764021, val:  47.92%, val_best:  61.25%, tr:  99.08%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8047%\n",
      "layer   2  Sparsity: 79.5125%\n",
      "layer   3  Sparsity: 83.6399%\n",
      "total_backward_count 1262910 real_backward_count 169045  13.385%\n",
      "epoch-129 lr=['0.0078125'], tr/val_loss:  1.445909/  1.754854, val:  53.75%, val_best:  61.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8011%\n",
      "layer   2  Sparsity: 79.7479%\n",
      "layer   3  Sparsity: 84.6957%\n",
      "total_backward_count 1272700 real_backward_count 170222  13.375%\n",
      "epoch-130 lr=['0.0078125'], tr/val_loss:  1.454813/  1.775559, val:  57.08%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8149%\n",
      "layer   2  Sparsity: 80.1799%\n",
      "layer   3  Sparsity: 84.2233%\n",
      "total_backward_count 1282490 real_backward_count 171440  13.368%\n",
      "epoch-131 lr=['0.0078125'], tr/val_loss:  1.510298/  1.781058, val:  57.08%, val_best:  61.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 72.16 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 82.8176%\n",
      "layer   2  Sparsity: 78.9674%\n",
      "layer   3  Sparsity: 84.1987%\n",
      "total_backward_count 1292280 real_backward_count 172698  13.364%\n",
      "epoch-132 lr=['0.0078125'], tr/val_loss:  1.504555/  1.773297, val:  51.25%, val_best:  61.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8126%\n",
      "layer   2  Sparsity: 78.7014%\n",
      "layer   3  Sparsity: 83.8432%\n",
      "total_backward_count 1302070 real_backward_count 173878  13.354%\n",
      "epoch-133 lr=['0.0078125'], tr/val_loss:  1.444803/  1.760013, val:  55.00%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8010%\n",
      "layer   2  Sparsity: 78.4559%\n",
      "layer   3  Sparsity: 83.3715%\n",
      "total_backward_count 1311860 real_backward_count 175060  13.344%\n",
      "epoch-134 lr=['0.0078125'], tr/val_loss:  1.438675/  1.749201, val:  49.17%, val_best:  61.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8116%\n",
      "layer   2  Sparsity: 77.9073%\n",
      "layer   3  Sparsity: 82.7292%\n",
      "total_backward_count 1321650 real_backward_count 176283  13.338%\n",
      "epoch-135 lr=['0.0078125'], tr/val_loss:  1.413396/  1.748284, val:  59.17%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.49 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8020%\n",
      "layer   2  Sparsity: 78.3246%\n",
      "layer   3  Sparsity: 82.7464%\n",
      "total_backward_count 1331440 real_backward_count 177523  13.333%\n",
      "epoch-136 lr=['0.0078125'], tr/val_loss:  1.444054/  1.730476, val:  54.17%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8124%\n",
      "layer   2  Sparsity: 78.3695%\n",
      "layer   3  Sparsity: 83.5908%\n",
      "total_backward_count 1341230 real_backward_count 178713  13.325%\n",
      "epoch-137 lr=['0.0078125'], tr/val_loss:  1.385101/  1.699915, val:  52.08%, val_best:  61.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8295%\n",
      "layer   2  Sparsity: 78.4700%\n",
      "layer   3  Sparsity: 81.7708%\n",
      "total_backward_count 1351020 real_backward_count 179806  13.309%\n",
      "epoch-138 lr=['0.0078125'], tr/val_loss:  1.415787/  1.723204, val:  52.50%, val_best:  61.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7932%\n",
      "layer   2  Sparsity: 78.4801%\n",
      "layer   3  Sparsity: 83.3497%\n",
      "total_backward_count 1360810 real_backward_count 181002  13.301%\n",
      "epoch-139 lr=['0.0078125'], tr/val_loss:  1.430525/  1.734837, val:  56.67%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.8078%\n",
      "layer   2  Sparsity: 77.8004%\n",
      "layer   3  Sparsity: 84.0852%\n",
      "total_backward_count 1370600 real_backward_count 182262  13.298%\n",
      "epoch-140 lr=['0.0078125'], tr/val_loss:  1.490821/  1.771443, val:  50.83%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 71.95 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 82.8070%\n",
      "layer   2  Sparsity: 78.5126%\n",
      "layer   3  Sparsity: 84.4307%\n",
      "total_backward_count 1380390 real_backward_count 183434  13.289%\n",
      "epoch-141 lr=['0.0078125'], tr/val_loss:  1.468640/  1.770243, val:  51.67%, val_best:  61.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8048%\n",
      "layer   2  Sparsity: 78.2460%\n",
      "layer   3  Sparsity: 83.4475%\n",
      "total_backward_count 1390180 real_backward_count 184665  13.284%\n",
      "epoch-142 lr=['0.0078125'], tr/val_loss:  1.452467/  1.746046, val:  52.08%, val_best:  61.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.99 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.8146%\n",
      "layer   2  Sparsity: 78.6317%\n",
      "layer   3  Sparsity: 82.9189%\n",
      "total_backward_count 1399970 real_backward_count 185884  13.278%\n",
      "epoch-143 lr=['0.0078125'], tr/val_loss:  1.413530/  1.783508, val:  50.42%, val_best:  61.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 73.81 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.7891%\n",
      "layer   2  Sparsity: 78.2838%\n",
      "layer   3  Sparsity: 82.4541%\n",
      "total_backward_count 1409760 real_backward_count 187049  13.268%\n",
      "epoch-144 lr=['0.0078125'], tr/val_loss:  1.431541/  1.709577, val:  52.92%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.98 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.7884%\n",
      "layer   2  Sparsity: 78.0655%\n",
      "layer   3  Sparsity: 81.6098%\n",
      "total_backward_count 1419550 real_backward_count 188202  13.258%\n",
      "epoch-145 lr=['0.0078125'], tr/val_loss:  1.406157/  1.767520, val:  47.92%, val_best:  61.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 73.50 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7976%\n",
      "layer   2  Sparsity: 78.7490%\n",
      "layer   3  Sparsity: 82.5341%\n",
      "total_backward_count 1429340 real_backward_count 189397  13.251%\n",
      "epoch-146 lr=['0.0078125'], tr/val_loss:  1.402983/  1.766680, val:  47.92%, val_best:  61.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8105%\n",
      "layer   2  Sparsity: 78.8112%\n",
      "layer   3  Sparsity: 81.3779%\n",
      "total_backward_count 1439130 real_backward_count 190591  13.243%\n",
      "epoch-147 lr=['0.0078125'], tr/val_loss:  1.399426/  1.747016, val:  56.25%, val_best:  61.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.34 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8043%\n",
      "layer   2  Sparsity: 79.2163%\n",
      "layer   3  Sparsity: 81.3037%\n",
      "total_backward_count 1448920 real_backward_count 191683  13.229%\n",
      "epoch-148 lr=['0.0078125'], tr/val_loss:  1.396059/  1.718416, val:  53.33%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7932%\n",
      "layer   2  Sparsity: 79.1747%\n",
      "layer   3  Sparsity: 81.8163%\n",
      "total_backward_count 1458710 real_backward_count 192900  13.224%\n",
      "epoch-149 lr=['0.0078125'], tr/val_loss:  1.427315/  1.742331, val:  55.42%, val_best:  61.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.37 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8040%\n",
      "layer   2  Sparsity: 79.0400%\n",
      "layer   3  Sparsity: 83.4182%\n",
      "total_backward_count 1468500 real_backward_count 194110  13.218%\n",
      "epoch-150 lr=['0.0078125'], tr/val_loss:  1.424171/  1.760391, val:  50.00%, val_best:  61.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8123%\n",
      "layer   2  Sparsity: 79.6462%\n",
      "layer   3  Sparsity: 83.5144%\n",
      "total_backward_count 1478290 real_backward_count 195288  13.210%\n",
      "epoch-151 lr=['0.0078125'], tr/val_loss:  1.420771/  1.691110, val:  56.67%, val_best:  61.25%, tr:  99.28%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8129%\n",
      "layer   2  Sparsity: 79.0354%\n",
      "layer   3  Sparsity: 83.3502%\n",
      "total_backward_count 1488080 real_backward_count 196512  13.206%\n",
      "epoch-152 lr=['0.0078125'], tr/val_loss:  1.379910/  1.712547, val:  50.00%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.7921%\n",
      "layer   2  Sparsity: 79.2320%\n",
      "layer   3  Sparsity: 81.6142%\n",
      "total_backward_count 1497870 real_backward_count 197582  13.191%\n",
      "epoch-153 lr=['0.0078125'], tr/val_loss:  1.396996/  1.653571, val:  60.00%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8043%\n",
      "layer   2  Sparsity: 79.7357%\n",
      "layer   3  Sparsity: 81.9059%\n",
      "total_backward_count 1507660 real_backward_count 198788  13.185%\n",
      "epoch-154 lr=['0.0078125'], tr/val_loss:  1.359444/  1.717244, val:  52.92%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.83 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7990%\n",
      "layer   2  Sparsity: 80.1976%\n",
      "layer   3  Sparsity: 82.1164%\n",
      "total_backward_count 1517450 real_backward_count 199945  13.176%\n",
      "epoch-155 lr=['0.0078125'], tr/val_loss:  1.419460/  1.695340, val:  56.25%, val_best:  61.25%, tr:  99.49%, tr_best: 100.00%, epoch time: 72.53 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8030%\n",
      "layer   2  Sparsity: 80.4891%\n",
      "layer   3  Sparsity: 83.3230%\n",
      "total_backward_count 1527240 real_backward_count 201069  13.166%\n",
      "epoch-156 lr=['0.0078125'], tr/val_loss:  1.365594/  1.778376, val:  46.67%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.66 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7996%\n",
      "layer   2  Sparsity: 79.9225%\n",
      "layer   3  Sparsity: 82.5425%\n",
      "total_backward_count 1537030 real_backward_count 202286  13.161%\n",
      "epoch-157 lr=['0.0078125'], tr/val_loss:  1.397247/  1.713217, val:  60.00%, val_best:  61.25%, tr:  99.08%, tr_best: 100.00%, epoch time: 72.82 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8087%\n",
      "layer   2  Sparsity: 78.8179%\n",
      "layer   3  Sparsity: 81.8050%\n",
      "total_backward_count 1546820 real_backward_count 203436  13.152%\n",
      "epoch-158 lr=['0.0078125'], tr/val_loss:  1.404727/  1.722333, val:  50.42%, val_best:  61.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8074%\n",
      "layer   2  Sparsity: 79.1618%\n",
      "layer   3  Sparsity: 82.6407%\n",
      "total_backward_count 1556610 real_backward_count 204627  13.146%\n",
      "epoch-159 lr=['0.0078125'], tr/val_loss:  1.384683/  1.743297, val:  56.67%, val_best:  61.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8033%\n",
      "layer   2  Sparsity: 79.1813%\n",
      "layer   3  Sparsity: 82.1281%\n",
      "total_backward_count 1566400 real_backward_count 205795  13.138%\n",
      "epoch-160 lr=['0.0078125'], tr/val_loss:  1.452945/  1.710363, val:  55.42%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8199%\n",
      "layer   2  Sparsity: 78.8321%\n",
      "layer   3  Sparsity: 82.7366%\n",
      "total_backward_count 1576190 real_backward_count 206961  13.130%\n",
      "epoch-161 lr=['0.0078125'], tr/val_loss:  1.381842/  1.705989, val:  57.08%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7944%\n",
      "layer   2  Sparsity: 78.6566%\n",
      "layer   3  Sparsity: 82.3469%\n",
      "total_backward_count 1585980 real_backward_count 208132  13.123%\n",
      "epoch-162 lr=['0.0078125'], tr/val_loss:  1.411858/  1.711478, val:  55.83%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8171%\n",
      "layer   2  Sparsity: 77.6350%\n",
      "layer   3  Sparsity: 83.4578%\n",
      "total_backward_count 1595770 real_backward_count 209391  13.122%\n",
      "epoch-163 lr=['0.0078125'], tr/val_loss:  1.370010/  1.666860, val:  55.00%, val_best:  61.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8027%\n",
      "layer   2  Sparsity: 77.6127%\n",
      "layer   3  Sparsity: 81.8441%\n",
      "total_backward_count 1605560 real_backward_count 210539  13.113%\n",
      "epoch-164 lr=['0.0078125'], tr/val_loss:  1.367019/  1.707186, val:  54.17%, val_best:  61.25%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8132%\n",
      "layer   2  Sparsity: 77.5292%\n",
      "layer   3  Sparsity: 82.3246%\n",
      "total_backward_count 1615350 real_backward_count 211720  13.107%\n",
      "epoch-165 lr=['0.0078125'], tr/val_loss:  1.406260/  1.703856, val:  54.17%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7994%\n",
      "layer   2  Sparsity: 78.2252%\n",
      "layer   3  Sparsity: 82.5258%\n",
      "total_backward_count 1625140 real_backward_count 212926  13.102%\n",
      "epoch-166 lr=['0.0078125'], tr/val_loss:  1.367152/  1.711971, val:  50.00%, val_best:  61.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8222%\n",
      "layer   2  Sparsity: 78.6566%\n",
      "layer   3  Sparsity: 82.2445%\n",
      "total_backward_count 1634930 real_backward_count 214107  13.096%\n",
      "epoch-167 lr=['0.0078125'], tr/val_loss:  1.373653/  1.781333, val:  41.67%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8077%\n",
      "layer   2  Sparsity: 77.9284%\n",
      "layer   3  Sparsity: 81.9879%\n",
      "total_backward_count 1644720 real_backward_count 215276  13.089%\n",
      "epoch-168 lr=['0.0078125'], tr/val_loss:  1.393522/  1.671713, val:  62.50%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.63 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7938%\n",
      "layer   2  Sparsity: 77.3107%\n",
      "layer   3  Sparsity: 83.2021%\n",
      "total_backward_count 1654510 real_backward_count 216463  13.083%\n",
      "epoch-169 lr=['0.0078125'], tr/val_loss:  1.344737/  1.682770, val:  52.50%, val_best:  62.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8134%\n",
      "layer   2  Sparsity: 78.1241%\n",
      "layer   3  Sparsity: 81.6833%\n",
      "total_backward_count 1664300 real_backward_count 217631  13.076%\n",
      "fc layer 3 self.abs_max_out: 1398.0\n",
      "epoch-170 lr=['0.0078125'], tr/val_loss:  1.428482/  1.730338, val:  60.42%, val_best:  62.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8091%\n",
      "layer   2  Sparsity: 78.7489%\n",
      "layer   3  Sparsity: 83.9317%\n",
      "total_backward_count 1674090 real_backward_count 218848  13.073%\n",
      "epoch-171 lr=['0.0078125'], tr/val_loss:  1.449043/  1.775557, val:  49.17%, val_best:  62.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.37 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8041%\n",
      "layer   2  Sparsity: 79.1267%\n",
      "layer   3  Sparsity: 83.5386%\n",
      "total_backward_count 1683880 real_backward_count 220109  13.072%\n",
      "epoch-172 lr=['0.0078125'], tr/val_loss:  1.418738/  1.757853, val:  57.08%, val_best:  62.50%, tr:  99.39%, tr_best: 100.00%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7805%\n",
      "layer   2  Sparsity: 79.3536%\n",
      "layer   3  Sparsity: 82.5837%\n",
      "total_backward_count 1693670 real_backward_count 221301  13.066%\n",
      "epoch-173 lr=['0.0078125'], tr/val_loss:  1.402884/  1.782461, val:  51.25%, val_best:  62.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.62 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7961%\n",
      "layer   2  Sparsity: 79.1010%\n",
      "layer   3  Sparsity: 82.8826%\n",
      "total_backward_count 1703460 real_backward_count 222457  13.059%\n",
      "epoch-174 lr=['0.0078125'], tr/val_loss:  1.422922/  1.761559, val:  52.50%, val_best:  62.50%, tr:  99.28%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8037%\n",
      "layer   2  Sparsity: 78.9379%\n",
      "layer   3  Sparsity: 82.7879%\n",
      "total_backward_count 1713250 real_backward_count 223608  13.052%\n",
      "epoch-175 lr=['0.0078125'], tr/val_loss:  1.388216/  1.657839, val:  65.00%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8035%\n",
      "layer   2  Sparsity: 78.4498%\n",
      "layer   3  Sparsity: 82.1090%\n",
      "total_backward_count 1723040 real_backward_count 224783  13.046%\n",
      "epoch-176 lr=['0.0078125'], tr/val_loss:  1.398590/  1.687285, val:  52.08%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.60 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7986%\n",
      "layer   2  Sparsity: 77.9249%\n",
      "layer   3  Sparsity: 82.5449%\n",
      "total_backward_count 1732830 real_backward_count 225976  13.041%\n",
      "epoch-177 lr=['0.0078125'], tr/val_loss:  1.366012/  1.747167, val:  57.92%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7907%\n",
      "layer   2  Sparsity: 77.8896%\n",
      "layer   3  Sparsity: 81.7508%\n",
      "total_backward_count 1742620 real_backward_count 227125  13.034%\n",
      "epoch-178 lr=['0.0078125'], tr/val_loss:  1.397471/  1.727071, val:  49.17%, val_best:  65.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.62 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8121%\n",
      "layer   2  Sparsity: 77.1296%\n",
      "layer   3  Sparsity: 82.3678%\n",
      "total_backward_count 1752410 real_backward_count 228282  13.027%\n",
      "epoch-179 lr=['0.0078125'], tr/val_loss:  1.341600/  1.655699, val:  52.92%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8084%\n",
      "layer   2  Sparsity: 76.4921%\n",
      "layer   3  Sparsity: 81.0651%\n",
      "total_backward_count 1762200 real_backward_count 229416  13.019%\n",
      "epoch-180 lr=['0.0078125'], tr/val_loss:  1.360911/  1.719807, val:  52.50%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8052%\n",
      "layer   2  Sparsity: 77.0820%\n",
      "layer   3  Sparsity: 81.6335%\n",
      "total_backward_count 1771990 real_backward_count 230551  13.011%\n",
      "epoch-181 lr=['0.0078125'], tr/val_loss:  1.402905/  1.768885, val:  59.17%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.68 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7967%\n",
      "layer   2  Sparsity: 77.9519%\n",
      "layer   3  Sparsity: 82.6868%\n",
      "total_backward_count 1781780 real_backward_count 231709  13.004%\n",
      "epoch-182 lr=['0.0078125'], tr/val_loss:  1.445858/  1.732087, val:  58.33%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8057%\n",
      "layer   2  Sparsity: 78.4279%\n",
      "layer   3  Sparsity: 83.5284%\n",
      "total_backward_count 1791570 real_backward_count 232915  13.001%\n",
      "epoch-183 lr=['0.0078125'], tr/val_loss:  1.386987/  1.731678, val:  51.67%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8097%\n",
      "layer   2  Sparsity: 79.1849%\n",
      "layer   3  Sparsity: 82.0287%\n",
      "total_backward_count 1801360 real_backward_count 234064  12.994%\n",
      "epoch-184 lr=['0.0078125'], tr/val_loss:  1.379245/  1.771236, val:  41.67%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.72 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8089%\n",
      "layer   2  Sparsity: 79.3289%\n",
      "layer   3  Sparsity: 81.2986%\n",
      "total_backward_count 1811150 real_backward_count 235182  12.985%\n",
      "epoch-185 lr=['0.0078125'], tr/val_loss:  1.374009/  1.728037, val:  52.50%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 71.95 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 82.7956%\n",
      "layer   2  Sparsity: 78.7945%\n",
      "layer   3  Sparsity: 81.6295%\n",
      "total_backward_count 1820940 real_backward_count 236356  12.980%\n",
      "epoch-186 lr=['0.0078125'], tr/val_loss:  1.360501/  1.676212, val:  58.75%, val_best:  65.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.82 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8247%\n",
      "layer   2  Sparsity: 78.9516%\n",
      "layer   3  Sparsity: 81.3708%\n",
      "total_backward_count 1830730 real_backward_count 237543  12.975%\n",
      "epoch-187 lr=['0.0078125'], tr/val_loss:  1.343904/  1.754914, val:  49.58%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.17 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8069%\n",
      "layer   2  Sparsity: 78.7811%\n",
      "layer   3  Sparsity: 82.1623%\n",
      "total_backward_count 1840520 real_backward_count 238761  12.972%\n",
      "epoch-188 lr=['0.0078125'], tr/val_loss:  1.382718/  1.691202, val:  59.17%, val_best:  65.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7936%\n",
      "layer   2  Sparsity: 78.5767%\n",
      "layer   3  Sparsity: 81.5064%\n",
      "total_backward_count 1850310 real_backward_count 239925  12.967%\n",
      "epoch-189 lr=['0.0078125'], tr/val_loss:  1.337138/  1.706884, val:  47.92%, val_best:  65.00%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7984%\n",
      "layer   2  Sparsity: 79.5993%\n",
      "layer   3  Sparsity: 81.3077%\n",
      "total_backward_count 1860100 real_backward_count 241025  12.958%\n",
      "epoch-190 lr=['0.0078125'], tr/val_loss:  1.344842/  1.672772, val:  67.92%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7989%\n",
      "layer   2  Sparsity: 79.8216%\n",
      "layer   3  Sparsity: 81.1651%\n",
      "total_backward_count 1869890 real_backward_count 242132  12.949%\n",
      "epoch-191 lr=['0.0078125'], tr/val_loss:  1.351951/  1.737213, val:  43.75%, val_best:  67.92%, tr:  99.49%, tr_best: 100.00%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7917%\n",
      "layer   2  Sparsity: 79.5864%\n",
      "layer   3  Sparsity: 80.9611%\n",
      "total_backward_count 1879680 real_backward_count 243317  12.945%\n",
      "epoch-192 lr=['0.0078125'], tr/val_loss:  1.348797/  1.751702, val:  58.75%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.33 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.8107%\n",
      "layer   2  Sparsity: 79.3968%\n",
      "layer   3  Sparsity: 81.3789%\n",
      "total_backward_count 1889470 real_backward_count 244459  12.938%\n",
      "fc layer 3 self.abs_max_out: 1455.0\n",
      "epoch-193 lr=['0.0078125'], tr/val_loss:  1.316296/  1.696496, val:  53.33%, val_best:  67.92%, tr:  99.39%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8023%\n",
      "layer   2  Sparsity: 79.4873%\n",
      "layer   3  Sparsity: 80.2685%\n",
      "total_backward_count 1899260 real_backward_count 245570  12.930%\n",
      "epoch-194 lr=['0.0078125'], tr/val_loss:  1.358124/  1.683504, val:  52.08%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8046%\n",
      "layer   2  Sparsity: 79.6748%\n",
      "layer   3  Sparsity: 81.9936%\n",
      "total_backward_count 1909050 real_backward_count 246676  12.921%\n",
      "epoch-195 lr=['0.0078125'], tr/val_loss:  1.384099/  1.776472, val:  50.42%, val_best:  67.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7867%\n",
      "layer   2  Sparsity: 79.7054%\n",
      "layer   3  Sparsity: 82.5066%\n",
      "total_backward_count 1918840 real_backward_count 247888  12.919%\n",
      "epoch-196 lr=['0.0078125'], tr/val_loss:  1.412224/  1.713708, val:  58.33%, val_best:  67.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.93 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.7937%\n",
      "layer   2  Sparsity: 79.4971%\n",
      "layer   3  Sparsity: 82.8269%\n",
      "total_backward_count 1928630 real_backward_count 249121  12.917%\n",
      "epoch-197 lr=['0.0078125'], tr/val_loss:  1.352141/  1.708521, val:  55.42%, val_best:  67.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.89 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 82.7932%\n",
      "layer   2  Sparsity: 79.3818%\n",
      "layer   3  Sparsity: 80.7685%\n",
      "total_backward_count 1938420 real_backward_count 250296  12.912%\n",
      "epoch-198 lr=['0.0078125'], tr/val_loss:  1.315199/  1.649706, val:  50.00%, val_best:  67.92%, tr:  99.39%, tr_best: 100.00%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 82.8126%\n",
      "layer   2  Sparsity: 79.4509%\n",
      "layer   3  Sparsity: 80.0710%\n",
      "total_backward_count 1948210 real_backward_count 251443  12.906%\n",
      "epoch-199 lr=['0.0078125'], tr/val_loss:  1.321512/  1.710646, val:  56.25%, val_best:  67.92%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 82.8145%\n",
      "layer   2  Sparsity: 79.8871%\n",
      "layer   3  Sparsity: 81.4062%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd34c3dff6143d48f31b2a27a8ce50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÇ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñÅ‚ñà‚ñÉ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÑ‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÇ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99489</td></tr><tr><td>tr_epoch_loss</td><td>1.32151</td></tr><tr><td>val_acc_best</td><td>0.67917</td></tr><tr><td>val_acc_now</td><td>0.5625</td></tr><tr><td>val_loss</td><td>1.71065</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-sweep-103</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sdptpsgh' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sdptpsgh</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251116_213851-sdptpsgh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9v7u2kug with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009765625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_014256-9v7u2kug</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9v7u2kug' target=\"_blank\">pretty-sweep-106</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9v7u2kug' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9v7u2kug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251117_014305_587', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0009765625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 20, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 2, 'leaky_temporal_filter': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0009765625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 711.0\n",
      "lif layer 1 self.abs_max_v: 711.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 719.5\n",
      "fc layer 2 self.abs_max_out: 353.0\n",
      "lif layer 2 self.abs_max_v: 373.0\n",
      "fc layer 1 self.abs_max_out: 739.0\n",
      "lif layer 1 self.abs_max_v: 984.0\n",
      "fc layer 2 self.abs_max_out: 410.0\n",
      "lif layer 2 self.abs_max_v: 446.5\n",
      "fc layer 2 self.abs_max_out: 420.0\n",
      "lif layer 2 self.abs_max_v: 493.0\n",
      "lif layer 1 self.abs_max_v: 1028.0\n",
      "lif layer 2 self.abs_max_v: 537.5\n",
      "fc layer 3 self.abs_max_out: 32.0\n",
      "fc layer 1 self.abs_max_out: 855.0\n",
      "fc layer 1 self.abs_max_out: 1181.0\n",
      "lif layer 1 self.abs_max_v: 1181.0\n",
      "fc layer 2 self.abs_max_out: 517.0\n",
      "fc layer 1 self.abs_max_out: 1484.0\n",
      "lif layer 1 self.abs_max_v: 1484.0\n",
      "lif layer 2 self.abs_max_v: 623.0\n",
      "fc layer 3 self.abs_max_out: 56.0\n",
      "fc layer 2 self.abs_max_out: 532.0\n",
      "lif layer 2 self.abs_max_v: 676.0\n",
      "fc layer 3 self.abs_max_out: 59.0\n",
      "fc layer 3 self.abs_max_out: 142.0\n",
      "lif layer 2 self.abs_max_v: 691.5\n",
      "fc layer 1 self.abs_max_out: 1518.0\n",
      "lif layer 1 self.abs_max_v: 1518.0\n",
      "fc layer 2 self.abs_max_out: 533.0\n",
      "lif layer 2 self.abs_max_v: 800.0\n",
      "fc layer 2 self.abs_max_out: 587.0\n",
      "fc layer 2 self.abs_max_out: 648.0\n",
      "lif layer 2 self.abs_max_v: 893.0\n",
      "fc layer 3 self.abs_max_out: 149.0\n",
      "fc layer 1 self.abs_max_out: 1552.0\n",
      "lif layer 1 self.abs_max_v: 1552.0\n",
      "lif layer 1 self.abs_max_v: 1604.0\n",
      "lif layer 2 self.abs_max_v: 895.0\n",
      "lif layer 1 self.abs_max_v: 1638.5\n",
      "lif layer 2 self.abs_max_v: 943.5\n",
      "lif layer 2 self.abs_max_v: 954.5\n",
      "fc layer 1 self.abs_max_out: 1612.0\n",
      "lif layer 2 self.abs_max_v: 1016.5\n",
      "lif layer 1 self.abs_max_v: 1705.0\n",
      "lif layer 1 self.abs_max_v: 1735.0\n",
      "fc layer 2 self.abs_max_out: 784.0\n",
      "lif layer 2 self.abs_max_v: 1021.5\n",
      "fc layer 3 self.abs_max_out: 155.0\n",
      "fc layer 2 self.abs_max_out: 807.0\n",
      "fc layer 2 self.abs_max_out: 859.0\n",
      "lif layer 2 self.abs_max_v: 1105.5\n",
      "fc layer 2 self.abs_max_out: 864.0\n",
      "fc layer 1 self.abs_max_out: 1692.0\n",
      "fc layer 1 self.abs_max_out: 1786.0\n",
      "lif layer 1 self.abs_max_v: 1786.0\n",
      "fc layer 1 self.abs_max_out: 1829.0\n",
      "lif layer 1 self.abs_max_v: 1829.0\n",
      "fc layer 2 self.abs_max_out: 999.0\n",
      "fc layer 3 self.abs_max_out: 156.0\n",
      "fc layer 1 self.abs_max_out: 1834.0\n",
      "lif layer 1 self.abs_max_v: 1834.0\n",
      "fc layer 1 self.abs_max_out: 1849.0\n",
      "lif layer 1 self.abs_max_v: 1849.0\n",
      "fc layer 3 self.abs_max_out: 175.0\n",
      "fc layer 3 self.abs_max_out: 193.0\n",
      "fc layer 1 self.abs_max_out: 1860.0\n",
      "lif layer 1 self.abs_max_v: 1860.0\n",
      "lif layer 2 self.abs_max_v: 1154.0\n",
      "lif layer 2 self.abs_max_v: 1244.0\n",
      "fc layer 1 self.abs_max_out: 1977.0\n",
      "lif layer 1 self.abs_max_v: 1977.0\n",
      "fc layer 3 self.abs_max_out: 198.0\n",
      "lif layer 1 self.abs_max_v: 2006.0\n",
      "lif layer 2 self.abs_max_v: 1273.5\n",
      "fc layer 1 self.abs_max_out: 2041.0\n",
      "lif layer 1 self.abs_max_v: 2041.0\n",
      "lif layer 1 self.abs_max_v: 2042.0\n",
      "fc layer 1 self.abs_max_out: 2340.0\n",
      "lif layer 1 self.abs_max_v: 2340.0\n",
      "lif layer 2 self.abs_max_v: 1307.5\n",
      "fc layer 2 self.abs_max_out: 1064.0\n",
      "fc layer 2 self.abs_max_out: 1117.0\n",
      "fc layer 2 self.abs_max_out: 1188.0\n",
      "fc layer 3 self.abs_max_out: 237.0\n",
      "fc layer 1 self.abs_max_out: 2414.0\n",
      "lif layer 1 self.abs_max_v: 2414.0\n",
      "lif layer 2 self.abs_max_v: 1361.5\n",
      "lif layer 2 self.abs_max_v: 1377.0\n",
      "fc layer 2 self.abs_max_out: 1211.0\n",
      "fc layer 1 self.abs_max_out: 2694.0\n",
      "lif layer 1 self.abs_max_v: 2694.0\n",
      "lif layer 2 self.abs_max_v: 1394.5\n",
      "lif layer 2 self.abs_max_v: 1417.0\n",
      "lif layer 2 self.abs_max_v: 1433.0\n",
      "lif layer 2 self.abs_max_v: 1477.0\n",
      "lif layer 2 self.abs_max_v: 1504.0\n",
      "lif layer 2 self.abs_max_v: 1581.0\n",
      "fc layer 3 self.abs_max_out: 247.0\n",
      "fc layer 1 self.abs_max_out: 2760.0\n",
      "lif layer 1 self.abs_max_v: 2760.0\n",
      "fc layer 1 self.abs_max_out: 2983.0\n",
      "lif layer 1 self.abs_max_v: 2983.0\n",
      "fc layer 2 self.abs_max_out: 1264.0\n",
      "fc layer 3 self.abs_max_out: 249.0\n",
      "lif layer 1 self.abs_max_v: 3080.5\n",
      "fc layer 3 self.abs_max_out: 285.0\n",
      "lif layer 1 self.abs_max_v: 3237.0\n",
      "lif layer 1 self.abs_max_v: 3268.5\n",
      "lif layer 1 self.abs_max_v: 3435.5\n",
      "lif layer 1 self.abs_max_v: 3485.0\n",
      "fc layer 1 self.abs_max_out: 3070.0\n",
      "fc layer 1 self.abs_max_out: 3225.0\n",
      "lif layer 1 self.abs_max_v: 3569.0\n",
      "lif layer 1 self.abs_max_v: 3701.5\n",
      "lif layer 1 self.abs_max_v: 3835.0\n",
      "lif layer 1 self.abs_max_v: 3849.5\n",
      "lif layer 1 self.abs_max_v: 4003.0\n",
      "lif layer 1 self.abs_max_v: 4009.0\n",
      "lif layer 1 self.abs_max_v: 4112.5\n",
      "lif layer 1 self.abs_max_v: 4143.5\n",
      "epoch-0   lr=['0.0009766'], tr/val_loss:  2.207403/  2.227056, val:  44.17%, val_best:  44.17%, tr:  64.96%, tr_best:  64.96%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9417%\n",
      "layer   2  Sparsity: 90.9274%\n",
      "layer   3  Sparsity: 95.4639%\n",
      "total_backward_count 9790 real_backward_count 5225  53.371%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1291.0\n",
      "lif layer 1 self.abs_max_v: 4187.5\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "lif layer 1 self.abs_max_v: 4202.5\n",
      "fc layer 2 self.abs_max_out: 1345.0\n",
      "fc layer 1 self.abs_max_out: 3362.0\n",
      "fc layer 2 self.abs_max_out: 1362.0\n",
      "fc layer 1 self.abs_max_out: 3397.0\n",
      "fc layer 2 self.abs_max_out: 1377.0\n",
      "lif layer 1 self.abs_max_v: 4297.0\n",
      "lif layer 1 self.abs_max_v: 4397.5\n",
      "fc layer 2 self.abs_max_out: 1397.0\n",
      "fc layer 2 self.abs_max_out: 1413.0\n",
      "lif layer 1 self.abs_max_v: 4571.0\n",
      "lif layer 1 self.abs_max_v: 4806.5\n",
      "lif layer 1 self.abs_max_v: 4862.0\n",
      "lif layer 1 self.abs_max_v: 4894.5\n",
      "lif layer 1 self.abs_max_v: 5215.5\n",
      "lif layer 1 self.abs_max_v: 5252.0\n",
      "lif layer 1 self.abs_max_v: 5286.0\n",
      "epoch-1   lr=['0.0009766'], tr/val_loss:  2.184657/  2.214812, val:  46.25%, val_best:  46.25%, tr:  88.97%, tr_best:  88.97%, epoch time: 73.55 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9393%\n",
      "layer   2  Sparsity: 91.4955%\n",
      "layer   3  Sparsity: 93.7733%\n",
      "total_backward_count 19580 real_backward_count 8313  42.457%\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "fc layer 1 self.abs_max_out: 3559.0\n",
      "fc layer 2 self.abs_max_out: 1418.0\n",
      "lif layer 1 self.abs_max_v: 5297.0\n",
      "lif layer 1 self.abs_max_v: 5378.0\n",
      "lif layer 1 self.abs_max_v: 5563.0\n",
      "lif layer 1 self.abs_max_v: 5780.0\n",
      "lif layer 1 self.abs_max_v: 6161.0\n",
      "lif layer 1 self.abs_max_v: 6191.0\n",
      "lif layer 1 self.abs_max_v: 6433.5\n",
      "epoch-2   lr=['0.0009766'], tr/val_loss:  2.162791/  2.202374, val:  49.17%, val_best:  49.17%, tr:  94.18%, tr_best:  94.18%, epoch time: 71.70 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9376%\n",
      "layer   2  Sparsity: 91.4215%\n",
      "layer   3  Sparsity: 91.9777%\n",
      "total_backward_count 29370 real_backward_count 10973  37.361%\n",
      "lif layer 2 self.abs_max_v: 1646.5\n",
      "fc layer 1 self.abs_max_out: 3580.0\n",
      "fc layer 2 self.abs_max_out: 1487.0\n",
      "fc layer 2 self.abs_max_out: 1502.0\n",
      "epoch-3   lr=['0.0009766'], tr/val_loss:  2.155755/  2.202432, val:  38.33%, val_best:  49.17%, tr:  96.63%, tr_best:  96.63%, epoch time: 71.82 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9418%\n",
      "layer   2  Sparsity: 90.6352%\n",
      "layer   3  Sparsity: 90.6412%\n",
      "total_backward_count 39160 real_backward_count 13206  33.723%\n",
      "fc layer 1 self.abs_max_out: 3607.0\n",
      "epoch-4   lr=['0.0009766'], tr/val_loss:  2.157678/  2.208340, val:  46.67%, val_best:  49.17%, tr:  98.06%, tr_best:  98.06%, epoch time: 72.56 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9387%\n",
      "layer   2  Sparsity: 90.8954%\n",
      "layer   3  Sparsity: 90.4904%\n",
      "total_backward_count 48950 real_backward_count 15223  31.099%\n",
      "fc layer 1 self.abs_max_out: 3638.0\n",
      "fc layer 1 self.abs_max_out: 3655.0\n",
      "fc layer 1 self.abs_max_out: 3695.0\n",
      "lif layer 2 self.abs_max_v: 1678.0\n",
      "fc layer 1 self.abs_max_out: 3720.0\n",
      "fc layer 1 self.abs_max_out: 3836.0\n",
      "fc layer 1 self.abs_max_out: 4244.0\n",
      "epoch-5   lr=['0.0009766'], tr/val_loss:  2.155030/  2.218854, val:  47.08%, val_best:  49.17%, tr:  98.37%, tr_best:  98.37%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9418%\n",
      "layer   2  Sparsity: 90.5473%\n",
      "layer   3  Sparsity: 89.7773%\n",
      "total_backward_count 58740 real_backward_count 17295  29.443%\n",
      "fc layer 2 self.abs_max_out: 1534.0\n",
      "epoch-6   lr=['0.0009766'], tr/val_loss:  2.157043/  2.196918, val:  50.42%, val_best:  50.42%, tr:  99.18%, tr_best:  99.18%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9513%\n",
      "layer   2  Sparsity: 90.4142%\n",
      "layer   3  Sparsity: 89.3439%\n",
      "total_backward_count 68530 real_backward_count 19233  28.065%\n",
      "epoch-7   lr=['0.0009766'], tr/val_loss:  2.153237/  2.198041, val:  47.08%, val_best:  50.42%, tr:  98.88%, tr_best:  99.18%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9470%\n",
      "layer   2  Sparsity: 90.6571%\n",
      "layer   3  Sparsity: 89.6405%\n",
      "total_backward_count 78320 real_backward_count 20997  26.809%\n",
      "epoch-8   lr=['0.0009766'], tr/val_loss:  2.154962/  2.199771, val:  51.67%, val_best:  51.67%, tr:  98.26%, tr_best:  99.18%, epoch time: 72.06 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9392%\n",
      "layer   2  Sparsity: 90.8027%\n",
      "layer   3  Sparsity: 89.3730%\n",
      "total_backward_count 88110 real_backward_count 22859  25.944%\n",
      "fc layer 2 self.abs_max_out: 1554.0\n",
      "fc layer 2 self.abs_max_out: 1582.0\n",
      "fc layer 2 self.abs_max_out: 1618.0\n",
      "epoch-9   lr=['0.0009766'], tr/val_loss:  2.150719/  2.205377, val:  50.83%, val_best:  51.67%, tr:  99.18%, tr_best:  99.18%, epoch time: 72.36 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9295%\n",
      "layer   2  Sparsity: 90.5927%\n",
      "layer   3  Sparsity: 88.7551%\n",
      "total_backward_count 97900 real_backward_count 24614  25.142%\n",
      "epoch-10  lr=['0.0009766'], tr/val_loss:  2.140372/  2.190715, val:  47.92%, val_best:  51.67%, tr:  99.28%, tr_best:  99.28%, epoch time: 72.85 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9492%\n",
      "layer   2  Sparsity: 90.1840%\n",
      "layer   3  Sparsity: 88.4546%\n",
      "total_backward_count 107690 real_backward_count 26292  24.415%\n",
      "lif layer 1 self.abs_max_v: 6448.0\n",
      "lif layer 1 self.abs_max_v: 6518.5\n",
      "fc layer 2 self.abs_max_out: 1621.0\n",
      "fc layer 2 self.abs_max_out: 1632.0\n",
      "epoch-11  lr=['0.0009766'], tr/val_loss:  2.140279/  2.179721, val:  45.42%, val_best:  51.67%, tr:  99.08%, tr_best:  99.28%, epoch time: 72.49 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9332%\n",
      "layer   2  Sparsity: 89.7923%\n",
      "layer   3  Sparsity: 88.2401%\n",
      "total_backward_count 117480 real_backward_count 27953  23.794%\n",
      "fc layer 2 self.abs_max_out: 1670.0\n",
      "fc layer 2 self.abs_max_out: 1673.0\n",
      "fc layer 2 self.abs_max_out: 1691.0\n",
      "lif layer 2 self.abs_max_v: 1691.0\n",
      "lif layer 1 self.abs_max_v: 6553.5\n",
      "lif layer 1 self.abs_max_v: 6723.0\n",
      "lif layer 1 self.abs_max_v: 7171.5\n",
      "epoch-12  lr=['0.0009766'], tr/val_loss:  2.136605/  2.197854, val:  49.17%, val_best:  51.67%, tr:  99.28%, tr_best:  99.28%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9608%\n",
      "layer   2  Sparsity: 89.8251%\n",
      "layer   3  Sparsity: 88.6685%\n",
      "total_backward_count 127270 real_backward_count 29598  23.256%\n",
      "epoch-13  lr=['0.0009766'], tr/val_loss:  2.144725/  2.204434, val:  42.08%, val_best:  51.67%, tr:  99.08%, tr_best:  99.28%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9440%\n",
      "layer   2  Sparsity: 89.6685%\n",
      "layer   3  Sparsity: 88.6611%\n",
      "total_backward_count 137060 real_backward_count 31260  22.808%\n",
      "lif layer 2 self.abs_max_v: 1711.5\n",
      "epoch-14  lr=['0.0009766'], tr/val_loss:  2.136633/  2.190474, val:  52.50%, val_best:  52.50%, tr:  99.28%, tr_best:  99.28%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9546%\n",
      "layer   2  Sparsity: 89.6576%\n",
      "layer   3  Sparsity: 88.3932%\n",
      "total_backward_count 146850 real_backward_count 32844  22.366%\n",
      "epoch-15  lr=['0.0009766'], tr/val_loss:  2.146892/  2.207804, val:  51.25%, val_best:  52.50%, tr:  99.08%, tr_best:  99.28%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9446%\n",
      "layer   2  Sparsity: 90.3374%\n",
      "layer   3  Sparsity: 89.0223%\n",
      "total_backward_count 156640 real_backward_count 34532  22.045%\n",
      "lif layer 2 self.abs_max_v: 1776.0\n",
      "fc layer 2 self.abs_max_out: 1739.0\n",
      "fc layer 1 self.abs_max_out: 4262.0\n",
      "fc layer 2 self.abs_max_out: 1839.0\n",
      "lif layer 2 self.abs_max_v: 1839.0\n",
      "epoch-16  lr=['0.0009766'], tr/val_loss:  2.148491/  2.195123, val:  53.75%, val_best:  53.75%, tr:  99.28%, tr_best:  99.28%, epoch time: 73.00 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9466%\n",
      "layer   2  Sparsity: 90.7235%\n",
      "layer   3  Sparsity: 89.0156%\n",
      "total_backward_count 166430 real_backward_count 36142  21.716%\n",
      "epoch-17  lr=['0.0009766'], tr/val_loss:  2.138226/  2.190963, val:  52.08%, val_best:  53.75%, tr:  99.59%, tr_best:  99.59%, epoch time: 73.68 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9479%\n",
      "layer   2  Sparsity: 90.1678%\n",
      "layer   3  Sparsity: 88.4815%\n",
      "total_backward_count 176220 real_backward_count 37770  21.433%\n",
      "epoch-18  lr=['0.0009766'], tr/val_loss:  2.144109/  2.196313, val:  49.17%, val_best:  53.75%, tr:  99.28%, tr_best:  99.59%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9414%\n",
      "layer   2  Sparsity: 90.1267%\n",
      "layer   3  Sparsity: 88.8109%\n",
      "total_backward_count 186010 real_backward_count 39363  21.162%\n",
      "epoch-19  lr=['0.0009766'], tr/val_loss:  2.136380/  2.184897, val:  42.08%, val_best:  53.75%, tr:  99.49%, tr_best:  99.59%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9639%\n",
      "layer   2  Sparsity: 89.6203%\n",
      "layer   3  Sparsity: 88.2118%\n",
      "total_backward_count 195800 real_backward_count 40850  20.863%\n",
      "epoch-20  lr=['0.0009766'], tr/val_loss:  2.132167/  2.189900, val:  45.83%, val_best:  53.75%, tr:  99.49%, tr_best:  99.59%, epoch time: 72.51 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9509%\n",
      "layer   2  Sparsity: 89.5502%\n",
      "layer   3  Sparsity: 87.9639%\n",
      "total_backward_count 205590 real_backward_count 42351  20.600%\n",
      "lif layer 1 self.abs_max_v: 7263.0\n",
      "epoch-21  lr=['0.0009766'], tr/val_loss:  2.129293/  2.183606, val:  57.92%, val_best:  57.92%, tr:  99.69%, tr_best:  99.69%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9360%\n",
      "layer   2  Sparsity: 89.4582%\n",
      "layer   3  Sparsity: 88.1469%\n",
      "total_backward_count 215380 real_backward_count 43887  20.377%\n",
      "lif layer 1 self.abs_max_v: 7392.5\n",
      "lif layer 1 self.abs_max_v: 7408.0\n",
      "lif layer 1 self.abs_max_v: 7728.0\n",
      "epoch-22  lr=['0.0009766'], tr/val_loss:  2.137553/  2.187265, val:  56.67%, val_best:  57.92%, tr:  99.39%, tr_best:  99.69%, epoch time: 72.57 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9513%\n",
      "layer   2  Sparsity: 89.4412%\n",
      "layer   3  Sparsity: 88.4095%\n",
      "total_backward_count 225170 real_backward_count 45448  20.184%\n",
      "lif layer 1 self.abs_max_v: 7904.0\n",
      "epoch-23  lr=['0.0009766'], tr/val_loss:  2.144329/  2.186925, val:  58.75%, val_best:  58.75%, tr:  99.39%, tr_best:  99.69%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9635%\n",
      "layer   2  Sparsity: 90.1712%\n",
      "layer   3  Sparsity: 88.8732%\n",
      "total_backward_count 234960 real_backward_count 46967  19.989%\n",
      "fc layer 1 self.abs_max_out: 4399.0\n",
      "lif layer 1 self.abs_max_v: 7977.5\n",
      "fc layer 1 self.abs_max_out: 4473.0\n",
      "lif layer 1 self.abs_max_v: 8462.0\n",
      "fc layer 2 self.abs_max_out: 1846.0\n",
      "lif layer 2 self.abs_max_v: 1846.0\n",
      "epoch-24  lr=['0.0009766'], tr/val_loss:  2.131833/  2.179932, val:  54.58%, val_best:  58.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 72.39 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9468%\n",
      "layer   2  Sparsity: 89.7584%\n",
      "layer   3  Sparsity: 88.4539%\n",
      "total_backward_count 244750 real_backward_count 48468  19.803%\n",
      "fc layer 1 self.abs_max_out: 4530.0\n",
      "fc layer 1 self.abs_max_out: 4572.0\n",
      "epoch-25  lr=['0.0009766'], tr/val_loss:  2.131086/  2.185132, val:  57.50%, val_best:  58.75%, tr:  99.39%, tr_best:  99.69%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9459%\n",
      "layer   2  Sparsity: 89.5978%\n",
      "layer   3  Sparsity: 88.4560%\n",
      "total_backward_count 254540 real_backward_count 50002  19.644%\n",
      "lif layer 2 self.abs_max_v: 1852.0\n",
      "fc layer 2 self.abs_max_out: 1851.0\n",
      "fc layer 2 self.abs_max_out: 1854.0\n",
      "lif layer 2 self.abs_max_v: 1854.0\n",
      "epoch-26  lr=['0.0009766'], tr/val_loss:  2.130687/  2.187400, val:  50.42%, val_best:  58.75%, tr:  99.59%, tr_best:  99.69%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9483%\n",
      "layer   2  Sparsity: 88.8194%\n",
      "layer   3  Sparsity: 88.0202%\n",
      "total_backward_count 264330 real_backward_count 51496  19.482%\n",
      "lif layer 2 self.abs_max_v: 1897.0\n",
      "fc layer 1 self.abs_max_out: 4615.0\n",
      "fc layer 2 self.abs_max_out: 1878.0\n",
      "fc layer 2 self.abs_max_out: 1881.0\n",
      "fc layer 2 self.abs_max_out: 1882.0\n",
      "fc layer 2 self.abs_max_out: 1941.0\n",
      "lif layer 2 self.abs_max_v: 1941.0\n",
      "fc layer 2 self.abs_max_out: 1956.0\n",
      "lif layer 2 self.abs_max_v: 1956.0\n",
      "fc layer 2 self.abs_max_out: 1959.0\n",
      "lif layer 2 self.abs_max_v: 1959.0\n",
      "fc layer 2 self.abs_max_out: 1960.0\n",
      "lif layer 2 self.abs_max_v: 1960.0\n",
      "epoch-27  lr=['0.0009766'], tr/val_loss:  2.127312/  2.177064, val:  56.25%, val_best:  58.75%, tr:  99.49%, tr_best:  99.69%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9420%\n",
      "layer   2  Sparsity: 88.3569%\n",
      "layer   3  Sparsity: 87.4677%\n",
      "total_backward_count 274120 real_backward_count 52981  19.328%\n",
      "epoch-28  lr=['0.0009766'], tr/val_loss:  2.116448/  2.189994, val:  56.67%, val_best:  58.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.90 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9396%\n",
      "layer   2  Sparsity: 88.5840%\n",
      "layer   3  Sparsity: 87.4141%\n",
      "total_backward_count 283910 real_backward_count 54461  19.182%\n",
      "fc layer 2 self.abs_max_out: 2003.0\n",
      "lif layer 2 self.abs_max_v: 2003.0\n",
      "fc layer 2 self.abs_max_out: 2022.0\n",
      "lif layer 2 self.abs_max_v: 2022.0\n",
      "fc layer 2 self.abs_max_out: 2058.0\n",
      "lif layer 2 self.abs_max_v: 2058.0\n",
      "lif layer 1 self.abs_max_v: 8472.0\n",
      "fc layer 1 self.abs_max_out: 4619.0\n",
      "lif layer 1 self.abs_max_v: 8855.0\n",
      "epoch-29  lr=['0.0009766'], tr/val_loss:  2.124119/  2.176747, val:  56.67%, val_best:  58.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.85 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9508%\n",
      "layer   2  Sparsity: 89.0940%\n",
      "layer   3  Sparsity: 87.6372%\n",
      "total_backward_count 293700 real_backward_count 55935  19.045%\n",
      "epoch-30  lr=['0.0009766'], tr/val_loss:  2.125790/  2.181059, val:  60.00%, val_best:  60.00%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9397%\n",
      "layer   2  Sparsity: 89.2450%\n",
      "layer   3  Sparsity: 87.6953%\n",
      "total_backward_count 303490 real_backward_count 57378  18.906%\n",
      "epoch-31  lr=['0.0009766'], tr/val_loss:  2.120961/  2.180425, val:  52.08%, val_best:  60.00%, tr:  99.28%, tr_best:  99.80%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9500%\n",
      "layer   2  Sparsity: 89.2305%\n",
      "layer   3  Sparsity: 87.9862%\n",
      "total_backward_count 313280 real_backward_count 58829  18.778%\n",
      "epoch-32  lr=['0.0009766'], tr/val_loss:  2.120578/  2.196957, val:  55.83%, val_best:  60.00%, tr:  99.49%, tr_best:  99.80%, epoch time: 72.61 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9455%\n",
      "layer   2  Sparsity: 89.0651%\n",
      "layer   3  Sparsity: 87.3662%\n",
      "total_backward_count 323070 real_backward_count 60229  18.643%\n",
      "epoch-33  lr=['0.0009766'], tr/val_loss:  2.140679/  2.202343, val:  54.17%, val_best:  60.00%, tr:  99.28%, tr_best:  99.80%, epoch time: 71.80 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9305%\n",
      "layer   2  Sparsity: 89.6066%\n",
      "layer   3  Sparsity: 88.3734%\n",
      "total_backward_count 332860 real_backward_count 61674  18.529%\n",
      "epoch-34  lr=['0.0009766'], tr/val_loss:  2.125368/  2.179662, val:  55.42%, val_best:  60.00%, tr:  99.69%, tr_best:  99.80%, epoch time: 71.96 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9346%\n",
      "layer   2  Sparsity: 89.0436%\n",
      "layer   3  Sparsity: 87.4711%\n",
      "total_backward_count 342650 real_backward_count 63071  18.407%\n",
      "fc layer 1 self.abs_max_out: 4685.0\n",
      "epoch-35  lr=['0.0009766'], tr/val_loss:  2.121111/  2.192531, val:  55.83%, val_best:  60.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 72.02 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9463%\n",
      "layer   2  Sparsity: 88.5565%\n",
      "layer   3  Sparsity: 87.2621%\n",
      "total_backward_count 352440 real_backward_count 64475  18.294%\n",
      "fc layer 1 self.abs_max_out: 4789.0\n",
      "epoch-36  lr=['0.0009766'], tr/val_loss:  2.124565/  2.185000, val:  60.83%, val_best:  60.83%, tr:  99.69%, tr_best:  99.80%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9407%\n",
      "layer   2  Sparsity: 88.9966%\n",
      "layer   3  Sparsity: 87.5272%\n",
      "total_backward_count 362230 real_backward_count 65848  18.179%\n",
      "fc layer 1 self.abs_max_out: 4856.0\n",
      "fc layer 1 self.abs_max_out: 4919.0\n",
      "fc layer 1 self.abs_max_out: 4979.0\n",
      "epoch-37  lr=['0.0009766'], tr/val_loss:  2.120411/  2.185158, val:  58.33%, val_best:  60.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9420%\n",
      "layer   2  Sparsity: 88.4659%\n",
      "layer   3  Sparsity: 87.2657%\n",
      "total_backward_count 372020 real_backward_count 67191  18.061%\n",
      "epoch-38  lr=['0.0009766'], tr/val_loss:  2.115603/  2.171486, val:  57.08%, val_best:  60.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9417%\n",
      "layer   2  Sparsity: 88.1553%\n",
      "layer   3  Sparsity: 87.3575%\n",
      "total_backward_count 381810 real_backward_count 68640  17.978%\n",
      "lif layer 2 self.abs_max_v: 2109.5\n",
      "epoch-39  lr=['0.0009766'], tr/val_loss:  2.111401/  2.178097, val:  57.08%, val_best:  60.83%, tr:  99.39%, tr_best:  99.80%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9372%\n",
      "layer   2  Sparsity: 88.1286%\n",
      "layer   3  Sparsity: 87.1645%\n",
      "total_backward_count 391600 real_backward_count 69992  17.873%\n",
      "lif layer 2 self.abs_max_v: 2175.5\n",
      "epoch-40  lr=['0.0009766'], tr/val_loss:  2.108610/  2.175042, val:  58.75%, val_best:  60.83%, tr:  99.49%, tr_best:  99.80%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9450%\n",
      "layer   2  Sparsity: 88.1874%\n",
      "layer   3  Sparsity: 86.8673%\n",
      "total_backward_count 401390 real_backward_count 71395  17.787%\n",
      "epoch-41  lr=['0.0009766'], tr/val_loss:  2.114002/  2.170091, val:  65.42%, val_best:  65.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9496%\n",
      "layer   2  Sparsity: 88.4605%\n",
      "layer   3  Sparsity: 87.1401%\n",
      "total_backward_count 411180 real_backward_count 72784  17.701%\n",
      "fc layer 1 self.abs_max_out: 5035.0\n",
      "epoch-42  lr=['0.0009766'], tr/val_loss:  2.103990/  2.173989, val:  58.33%, val_best:  65.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 72.64 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9487%\n",
      "layer   2  Sparsity: 88.4248%\n",
      "layer   3  Sparsity: 87.0456%\n",
      "total_backward_count 420970 real_backward_count 74138  17.611%\n",
      "epoch-43  lr=['0.0009766'], tr/val_loss:  2.115877/  2.184831, val:  57.50%, val_best:  65.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9444%\n",
      "layer   2  Sparsity: 88.4383%\n",
      "layer   3  Sparsity: 87.0183%\n",
      "total_backward_count 430760 real_backward_count 75520  17.532%\n",
      "epoch-44  lr=['0.0009766'], tr/val_loss:  2.124501/  2.182488, val:  60.00%, val_best:  65.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 72.33 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9498%\n",
      "layer   2  Sparsity: 88.3144%\n",
      "layer   3  Sparsity: 87.2072%\n",
      "total_backward_count 440550 real_backward_count 76888  17.453%\n",
      "fc layer 1 self.abs_max_out: 5440.0\n",
      "lif layer 1 self.abs_max_v: 9177.5\n",
      "lif layer 1 self.abs_max_v: 9191.0\n",
      "epoch-45  lr=['0.0009766'], tr/val_loss:  2.114280/  2.175939, val:  60.00%, val_best:  65.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9584%\n",
      "layer   2  Sparsity: 88.0859%\n",
      "layer   3  Sparsity: 86.6038%\n",
      "total_backward_count 450340 real_backward_count 78247  17.375%\n",
      "lif layer 1 self.abs_max_v: 9345.0\n",
      "lif layer 1 self.abs_max_v: 9500.5\n",
      "epoch-46  lr=['0.0009766'], tr/val_loss:  2.105612/  2.169899, val:  57.50%, val_best:  65.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9277%\n",
      "layer   2  Sparsity: 87.8734%\n",
      "layer   3  Sparsity: 85.9406%\n",
      "total_backward_count 460130 real_backward_count 79636  17.307%\n",
      "epoch-47  lr=['0.0009766'], tr/val_loss:  2.094908/  2.169446, val:  47.08%, val_best:  65.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9457%\n",
      "layer   2  Sparsity: 88.0391%\n",
      "layer   3  Sparsity: 85.9196%\n",
      "total_backward_count 469920 real_backward_count 80999  17.237%\n",
      "epoch-48  lr=['0.0009766'], tr/val_loss:  2.100749/  2.160743, val:  62.50%, val_best:  65.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9415%\n",
      "layer   2  Sparsity: 88.2055%\n",
      "layer   3  Sparsity: 86.3989%\n",
      "total_backward_count 479710 real_backward_count 82327  17.162%\n",
      "epoch-49  lr=['0.0009766'], tr/val_loss:  2.097505/  2.165306, val:  55.42%, val_best:  65.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9507%\n",
      "layer   2  Sparsity: 87.8740%\n",
      "layer   3  Sparsity: 86.1267%\n",
      "total_backward_count 489500 real_backward_count 83704  17.100%\n",
      "epoch-50  lr=['0.0009766'], tr/val_loss:  2.098281/  2.161478, val:  47.92%, val_best:  65.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9367%\n",
      "layer   2  Sparsity: 87.9126%\n",
      "layer   3  Sparsity: 86.1898%\n",
      "total_backward_count 499290 real_backward_count 85074  17.039%\n",
      "lif layer 1 self.abs_max_v: 9604.0\n",
      "epoch-51  lr=['0.0009766'], tr/val_loss:  2.092194/  2.164986, val:  57.50%, val_best:  65.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9536%\n",
      "layer   2  Sparsity: 88.1306%\n",
      "layer   3  Sparsity: 86.2453%\n",
      "total_backward_count 509080 real_backward_count 86405  16.973%\n",
      "fc layer 1 self.abs_max_out: 5448.0\n",
      "lif layer 1 self.abs_max_v: 9677.5\n",
      "lif layer 2 self.abs_max_v: 2276.0\n",
      "epoch-52  lr=['0.0009766'], tr/val_loss:  2.098842/  2.167893, val:  60.00%, val_best:  65.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9506%\n",
      "layer   2  Sparsity: 87.8994%\n",
      "layer   3  Sparsity: 86.4739%\n",
      "total_backward_count 518870 real_backward_count 87782  16.918%\n",
      "lif layer 2 self.abs_max_v: 2342.0\n",
      "epoch-53  lr=['0.0009766'], tr/val_loss:  2.095418/  2.160690, val:  60.83%, val_best:  65.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9434%\n",
      "layer   2  Sparsity: 87.8240%\n",
      "layer   3  Sparsity: 86.4797%\n",
      "total_backward_count 528660 real_backward_count 89012  16.837%\n",
      "fc layer 1 self.abs_max_out: 5825.0\n",
      "lif layer 2 self.abs_max_v: 2352.5\n",
      "epoch-54  lr=['0.0009766'], tr/val_loss:  2.100296/  2.166116, val:  61.25%, val_best:  65.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9496%\n",
      "layer   2  Sparsity: 88.0501%\n",
      "layer   3  Sparsity: 86.7684%\n",
      "total_backward_count 538450 real_backward_count 90297  16.770%\n",
      "lif layer 1 self.abs_max_v: 10065.5\n",
      "epoch-55  lr=['0.0009766'], tr/val_loss:  2.099208/  2.160578, val:  59.58%, val_best:  65.42%, tr:  99.28%, tr_best:  99.90%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9486%\n",
      "layer   2  Sparsity: 87.7911%\n",
      "layer   3  Sparsity: 86.6471%\n",
      "total_backward_count 548240 real_backward_count 91640  16.715%\n",
      "fc layer 2 self.abs_max_out: 2118.0\n",
      "epoch-56  lr=['0.0009766'], tr/val_loss:  2.092744/  2.170758, val:  50.00%, val_best:  65.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.71 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9610%\n",
      "layer   2  Sparsity: 87.6748%\n",
      "layer   3  Sparsity: 86.7393%\n",
      "total_backward_count 558030 real_backward_count 93011  16.668%\n",
      "fc layer 2 self.abs_max_out: 2134.0\n",
      "epoch-57  lr=['0.0009766'], tr/val_loss:  2.095015/  2.159127, val:  57.92%, val_best:  65.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9443%\n",
      "layer   2  Sparsity: 87.9379%\n",
      "layer   3  Sparsity: 86.5813%\n",
      "total_backward_count 567820 real_backward_count 94332  16.613%\n",
      "fc layer 2 self.abs_max_out: 2233.0\n",
      "epoch-58  lr=['0.0009766'], tr/val_loss:  2.085224/  2.154398, val:  61.25%, val_best:  65.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.18 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9390%\n",
      "layer   2  Sparsity: 87.6251%\n",
      "layer   3  Sparsity: 86.2014%\n",
      "total_backward_count 577610 real_backward_count 95585  16.548%\n",
      "epoch-59  lr=['0.0009766'], tr/val_loss:  2.090524/  2.164649, val:  60.42%, val_best:  65.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9511%\n",
      "layer   2  Sparsity: 87.6893%\n",
      "layer   3  Sparsity: 86.4571%\n",
      "total_backward_count 587400 real_backward_count 96859  16.489%\n",
      "epoch-60  lr=['0.0009766'], tr/val_loss:  2.096380/  2.166195, val:  60.42%, val_best:  65.42%, tr:  99.49%, tr_best:  99.90%, epoch time: 73.78 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9397%\n",
      "layer   2  Sparsity: 87.4323%\n",
      "layer   3  Sparsity: 86.2007%\n",
      "total_backward_count 597190 real_backward_count 98173  16.439%\n",
      "lif layer 1 self.abs_max_v: 10133.0\n",
      "epoch-61  lr=['0.0009766'], tr/val_loss:  2.097239/  2.157879, val:  59.58%, val_best:  65.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.52 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9409%\n",
      "layer   2  Sparsity: 87.3550%\n",
      "layer   3  Sparsity: 86.2103%\n",
      "total_backward_count 606980 real_backward_count 99435  16.382%\n",
      "fc layer 1 self.abs_max_out: 5857.0\n",
      "lif layer 1 self.abs_max_v: 10197.5\n",
      "lif layer 2 self.abs_max_v: 2422.0\n",
      "fc layer 1 self.abs_max_out: 5968.0\n",
      "epoch-62  lr=['0.0009766'], tr/val_loss:  2.100468/  2.167966, val:  56.25%, val_best:  65.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9378%\n",
      "layer   2  Sparsity: 87.3482%\n",
      "layer   3  Sparsity: 86.1055%\n",
      "total_backward_count 616770 real_backward_count 100695  16.326%\n",
      "epoch-63  lr=['0.0009766'], tr/val_loss:  2.093743/  2.159849, val:  56.67%, val_best:  65.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 73.65 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9450%\n",
      "layer   2  Sparsity: 87.2969%\n",
      "layer   3  Sparsity: 85.7460%\n",
      "total_backward_count 626560 real_backward_count 101952  16.272%\n",
      "epoch-64  lr=['0.0009766'], tr/val_loss:  2.090506/  2.163012, val:  50.00%, val_best:  65.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9331%\n",
      "layer   2  Sparsity: 87.7084%\n",
      "layer   3  Sparsity: 85.8722%\n",
      "total_backward_count 636350 real_backward_count 103209  16.219%\n",
      "fc layer 1 self.abs_max_out: 5978.0\n",
      "fc layer 1 self.abs_max_out: 6303.0\n",
      "lif layer 1 self.abs_max_v: 10578.0\n",
      "epoch-65  lr=['0.0009766'], tr/val_loss:  2.086932/  2.157990, val:  63.33%, val_best:  65.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 72.18 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9440%\n",
      "layer   2  Sparsity: 87.4942%\n",
      "layer   3  Sparsity: 85.6908%\n",
      "total_backward_count 646140 real_backward_count 104473  16.169%\n",
      "fc layer 2 self.abs_max_out: 2257.0\n",
      "epoch-66  lr=['0.0009766'], tr/val_loss:  2.081247/  2.159336, val:  52.92%, val_best:  65.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 71.81 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9398%\n",
      "layer   2  Sparsity: 87.1582%\n",
      "layer   3  Sparsity: 85.6534%\n",
      "total_backward_count 655930 real_backward_count 105802  16.130%\n",
      "epoch-67  lr=['0.0009766'], tr/val_loss:  2.092993/  2.166208, val:  58.33%, val_best:  65.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.14 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9272%\n",
      "layer   2  Sparsity: 87.0371%\n",
      "layer   3  Sparsity: 86.3548%\n",
      "total_backward_count 665720 real_backward_count 107078  16.085%\n",
      "epoch-68  lr=['0.0009766'], tr/val_loss:  2.097737/  2.156278, val:  64.58%, val_best:  65.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9310%\n",
      "layer   2  Sparsity: 87.0526%\n",
      "layer   3  Sparsity: 86.2612%\n",
      "total_backward_count 675510 real_backward_count 108332  16.037%\n",
      "lif layer 2 self.abs_max_v: 2424.0\n",
      "epoch-69  lr=['0.0009766'], tr/val_loss:  2.087571/  2.164295, val:  62.50%, val_best:  65.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9608%\n",
      "layer   2  Sparsity: 86.8696%\n",
      "layer   3  Sparsity: 85.6384%\n",
      "total_backward_count 685300 real_backward_count 109502  15.979%\n",
      "lif layer 2 self.abs_max_v: 2492.5\n",
      "lif layer 2 self.abs_max_v: 2550.5\n",
      "epoch-70  lr=['0.0009766'], tr/val_loss:  2.081944/  2.147771, val:  62.50%, val_best:  65.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9429%\n",
      "layer   2  Sparsity: 86.6073%\n",
      "layer   3  Sparsity: 85.4320%\n",
      "total_backward_count 695090 real_backward_count 110784  15.938%\n",
      "epoch-71  lr=['0.0009766'], tr/val_loss:  2.078626/  2.162357, val:  57.08%, val_best:  65.42%, tr:  99.49%, tr_best:  99.90%, epoch time: 72.72 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9258%\n",
      "layer   2  Sparsity: 87.2243%\n",
      "layer   3  Sparsity: 85.6735%\n",
      "total_backward_count 704880 real_backward_count 111974  15.886%\n",
      "lif layer 2 self.abs_max_v: 2637.5\n",
      "epoch-72  lr=['0.0009766'], tr/val_loss:  2.080664/  2.151942, val:  52.50%, val_best:  65.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9442%\n",
      "layer   2  Sparsity: 86.9654%\n",
      "layer   3  Sparsity: 85.5317%\n",
      "total_backward_count 714670 real_backward_count 113196  15.839%\n",
      "lif layer 2 self.abs_max_v: 2648.5\n",
      "epoch-73  lr=['0.0009766'], tr/val_loss:  2.079719/  2.152403, val:  60.83%, val_best:  65.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9388%\n",
      "layer   2  Sparsity: 86.9774%\n",
      "layer   3  Sparsity: 85.8461%\n",
      "total_backward_count 724460 real_backward_count 114421  15.794%\n",
      "epoch-74  lr=['0.0009766'], tr/val_loss:  2.074660/  2.147167, val:  56.67%, val_best:  65.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9488%\n",
      "layer   2  Sparsity: 86.6658%\n",
      "layer   3  Sparsity: 85.3155%\n",
      "total_backward_count 734250 real_backward_count 115675  15.754%\n",
      "lif layer 2 self.abs_max_v: 2676.0\n",
      "epoch-75  lr=['0.0009766'], tr/val_loss:  2.064872/  2.134780, val:  60.42%, val_best:  65.42%, tr:  99.39%, tr_best:  99.90%, epoch time: 73.54 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9355%\n",
      "layer   2  Sparsity: 86.2788%\n",
      "layer   3  Sparsity: 84.8242%\n",
      "total_backward_count 744040 real_backward_count 116898  15.711%\n",
      "epoch-76  lr=['0.0009766'], tr/val_loss:  2.066540/  2.143234, val:  56.25%, val_best:  65.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 72.62 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9501%\n",
      "layer   2  Sparsity: 86.3100%\n",
      "layer   3  Sparsity: 84.7913%\n",
      "total_backward_count 753830 real_backward_count 118114  15.669%\n",
      "epoch-77  lr=['0.0009766'], tr/val_loss:  2.061310/  2.135481, val:  56.25%, val_best:  65.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9380%\n",
      "layer   2  Sparsity: 86.4944%\n",
      "layer   3  Sparsity: 84.7598%\n",
      "total_backward_count 763620 real_backward_count 119301  15.623%\n",
      "fc layer 3 self.abs_max_out: 341.0\n",
      "epoch-78  lr=['0.0009766'], tr/val_loss:  2.064844/  2.144019, val:  55.83%, val_best:  65.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 73.80 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9431%\n",
      "layer   2  Sparsity: 86.7114%\n",
      "layer   3  Sparsity: 85.1559%\n",
      "total_backward_count 773410 real_backward_count 120466  15.576%\n",
      "lif layer 1 self.abs_max_v: 10889.5\n",
      "epoch-79  lr=['0.0009766'], tr/val_loss:  2.068269/  2.147628, val:  59.17%, val_best:  65.42%, tr:  99.49%, tr_best:  99.90%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9421%\n",
      "layer   2  Sparsity: 87.0183%\n",
      "layer   3  Sparsity: 85.5709%\n",
      "total_backward_count 783200 real_backward_count 121682  15.537%\n",
      "epoch-80  lr=['0.0009766'], tr/val_loss:  2.073711/  2.141764, val:  65.42%, val_best:  65.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9591%\n",
      "layer   2  Sparsity: 86.9591%\n",
      "layer   3  Sparsity: 85.7287%\n",
      "total_backward_count 792990 real_backward_count 122913  15.500%\n",
      "epoch-81  lr=['0.0009766'], tr/val_loss:  2.070428/  2.142340, val:  51.25%, val_best:  65.42%, tr:  99.49%, tr_best:  99.90%, epoch time: 74.03 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9510%\n",
      "layer   2  Sparsity: 86.7798%\n",
      "layer   3  Sparsity: 85.7018%\n",
      "total_backward_count 802780 real_backward_count 124091  15.458%\n",
      "epoch-82  lr=['0.0009766'], tr/val_loss:  2.074682/  2.156941, val:  56.67%, val_best:  65.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9310%\n",
      "layer   2  Sparsity: 86.3520%\n",
      "layer   3  Sparsity: 85.2317%\n",
      "total_backward_count 812570 real_backward_count 125328  15.424%\n",
      "lif layer 2 self.abs_max_v: 2836.5\n",
      "epoch-83  lr=['0.0009766'], tr/val_loss:  2.073332/  2.154248, val:  50.42%, val_best:  65.42%, tr:  99.28%, tr_best:  99.90%, epoch time: 72.14 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9523%\n",
      "layer   2  Sparsity: 86.1729%\n",
      "layer   3  Sparsity: 85.1670%\n",
      "total_backward_count 822360 real_backward_count 126538  15.387%\n",
      "epoch-84  lr=['0.0009766'], tr/val_loss:  2.075763/  2.153914, val:  56.25%, val_best:  65.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 71.95 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9410%\n",
      "layer   2  Sparsity: 86.2670%\n",
      "layer   3  Sparsity: 85.1814%\n",
      "total_backward_count 832150 real_backward_count 127800  15.358%\n",
      "epoch-85  lr=['0.0009766'], tr/val_loss:  2.074080/  2.141507, val:  60.00%, val_best:  65.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9386%\n",
      "layer   2  Sparsity: 86.0336%\n",
      "layer   3  Sparsity: 85.0203%\n",
      "total_backward_count 841940 real_backward_count 128989  15.320%\n",
      "epoch-86  lr=['0.0009766'], tr/val_loss:  2.065443/  2.142934, val:  64.17%, val_best:  65.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9422%\n",
      "layer   2  Sparsity: 86.0111%\n",
      "layer   3  Sparsity: 84.6520%\n",
      "total_backward_count 851730 real_backward_count 130184  15.285%\n",
      "epoch-87  lr=['0.0009766'], tr/val_loss:  2.066510/  2.146876, val:  55.00%, val_best:  65.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9421%\n",
      "layer   2  Sparsity: 86.0952%\n",
      "layer   3  Sparsity: 84.9797%\n",
      "total_backward_count 861520 real_backward_count 131433  15.256%\n",
      "epoch-88  lr=['0.0009766'], tr/val_loss:  2.068439/  2.140570, val:  55.83%, val_best:  65.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9674%\n",
      "layer   2  Sparsity: 86.1872%\n",
      "layer   3  Sparsity: 85.0299%\n",
      "total_backward_count 871310 real_backward_count 132651  15.224%\n",
      "epoch-89  lr=['0.0009766'], tr/val_loss:  2.065932/  2.138069, val:  62.92%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9402%\n",
      "layer   2  Sparsity: 86.2494%\n",
      "layer   3  Sparsity: 84.9216%\n",
      "total_backward_count 881100 real_backward_count 133842  15.190%\n",
      "epoch-90  lr=['0.0009766'], tr/val_loss:  2.059505/  2.139800, val:  52.92%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9470%\n",
      "layer   2  Sparsity: 86.1815%\n",
      "layer   3  Sparsity: 84.5215%\n",
      "total_backward_count 890890 real_backward_count 135042  15.158%\n",
      "epoch-91  lr=['0.0009766'], tr/val_loss:  2.058024/  2.137012, val:  60.83%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9568%\n",
      "layer   2  Sparsity: 86.4330%\n",
      "layer   3  Sparsity: 84.4356%\n",
      "total_backward_count 900680 real_backward_count 136186  15.120%\n",
      "epoch-92  lr=['0.0009766'], tr/val_loss:  2.062174/  2.134425, val:  65.00%, val_best:  65.42%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9463%\n",
      "layer   2  Sparsity: 86.8010%\n",
      "layer   3  Sparsity: 84.8113%\n",
      "total_backward_count 910470 real_backward_count 137400  15.091%\n",
      "epoch-93  lr=['0.0009766'], tr/val_loss:  2.063812/  2.133623, val:  63.75%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.19 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9566%\n",
      "layer   2  Sparsity: 86.6955%\n",
      "layer   3  Sparsity: 84.8954%\n",
      "total_backward_count 920260 real_backward_count 138638  15.065%\n",
      "epoch-94  lr=['0.0009766'], tr/val_loss:  2.065510/  2.136515, val:  61.67%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9466%\n",
      "layer   2  Sparsity: 86.7792%\n",
      "layer   3  Sparsity: 85.0306%\n",
      "total_backward_count 930050 real_backward_count 139833  15.035%\n",
      "epoch-95  lr=['0.0009766'], tr/val_loss:  2.062431/  2.134257, val:  58.75%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9457%\n",
      "layer   2  Sparsity: 86.4909%\n",
      "layer   3  Sparsity: 84.6162%\n",
      "total_backward_count 939840 real_backward_count 140997  15.002%\n",
      "epoch-96  lr=['0.0009766'], tr/val_loss:  2.058950/  2.138383, val:  54.17%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9514%\n",
      "layer   2  Sparsity: 86.5713%\n",
      "layer   3  Sparsity: 84.7847%\n",
      "total_backward_count 949630 real_backward_count 142163  14.970%\n",
      "epoch-97  lr=['0.0009766'], tr/val_loss:  2.053035/  2.131558, val:  65.83%, val_best:  65.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 72.02 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9317%\n",
      "layer   2  Sparsity: 86.4777%\n",
      "layer   3  Sparsity: 84.8290%\n",
      "total_backward_count 959420 real_backward_count 143330  14.939%\n",
      "epoch-98  lr=['0.0009766'], tr/val_loss:  2.059499/  2.142525, val:  61.67%, val_best:  65.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 72.34 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9422%\n",
      "layer   2  Sparsity: 86.6021%\n",
      "layer   3  Sparsity: 84.8614%\n",
      "total_backward_count 969210 real_backward_count 144495  14.909%\n",
      "epoch-99  lr=['0.0009766'], tr/val_loss:  2.066279/  2.142432, val:  61.67%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.40 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9492%\n",
      "layer   2  Sparsity: 86.3476%\n",
      "layer   3  Sparsity: 84.9144%\n",
      "total_backward_count 979000 real_backward_count 145688  14.881%\n",
      "epoch-100 lr=['0.0009766'], tr/val_loss:  2.056567/  2.131432, val:  67.50%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9454%\n",
      "layer   2  Sparsity: 86.0088%\n",
      "layer   3  Sparsity: 84.7244%\n",
      "total_backward_count 988790 real_backward_count 146800  14.846%\n",
      "lif layer 1 self.abs_max_v: 11015.5\n",
      "epoch-101 lr=['0.0009766'], tr/val_loss:  2.063869/  2.131318, val:  61.67%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9420%\n",
      "layer   2  Sparsity: 86.1194%\n",
      "layer   3  Sparsity: 85.0498%\n",
      "total_backward_count 998580 real_backward_count 147991  14.820%\n",
      "epoch-102 lr=['0.0009766'], tr/val_loss:  2.062914/  2.134648, val:  62.08%, val_best:  67.50%, tr:  99.39%, tr_best: 100.00%, epoch time: 74.83 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 83.9336%\n",
      "layer   2  Sparsity: 85.9720%\n",
      "layer   3  Sparsity: 84.5411%\n",
      "total_backward_count 1008370 real_backward_count 149161  14.792%\n",
      "epoch-103 lr=['0.0009766'], tr/val_loss:  2.058904/  2.130754, val:  62.92%, val_best:  67.50%, tr:  99.39%, tr_best: 100.00%, epoch time: 74.56 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9445%\n",
      "layer   2  Sparsity: 86.0404%\n",
      "layer   3  Sparsity: 84.9203%\n",
      "total_backward_count 1018160 real_backward_count 150302  14.762%\n",
      "epoch-104 lr=['0.0009766'], tr/val_loss:  2.059061/  2.144737, val:  61.25%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.31 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9522%\n",
      "layer   2  Sparsity: 86.3897%\n",
      "layer   3  Sparsity: 84.8673%\n",
      "total_backward_count 1027950 real_backward_count 151442  14.732%\n",
      "lif layer 1 self.abs_max_v: 11166.5\n",
      "lif layer 1 self.abs_max_v: 11499.0\n",
      "epoch-105 lr=['0.0009766'], tr/val_loss:  2.064046/  2.139366, val:  62.50%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 83.9418%\n",
      "layer   2  Sparsity: 86.6911%\n",
      "layer   3  Sparsity: 85.0666%\n",
      "total_backward_count 1037740 real_backward_count 152603  14.705%\n",
      "fc layer 1 self.abs_max_out: 6468.0\n",
      "epoch-106 lr=['0.0009766'], tr/val_loss:  2.055549/  2.132825, val:  60.00%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.40 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9462%\n",
      "layer   2  Sparsity: 86.5459%\n",
      "layer   3  Sparsity: 84.7769%\n",
      "total_backward_count 1047530 real_backward_count 153745  14.677%\n",
      "lif layer 2 self.abs_max_v: 3011.0\n",
      "epoch-107 lr=['0.0009766'], tr/val_loss:  2.052638/  2.130775, val:  58.75%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.87 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9513%\n",
      "layer   2  Sparsity: 86.6672%\n",
      "layer   3  Sparsity: 84.8644%\n",
      "total_backward_count 1057320 real_backward_count 154960  14.656%\n",
      "epoch-108 lr=['0.0009766'], tr/val_loss:  2.055294/  2.131622, val:  60.83%, val_best:  67.50%, tr:  99.28%, tr_best: 100.00%, epoch time: 74.22 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9147%\n",
      "layer   2  Sparsity: 86.7737%\n",
      "layer   3  Sparsity: 84.6400%\n",
      "total_backward_count 1067110 real_backward_count 156154  14.633%\n",
      "lif layer 2 self.abs_max_v: 3019.5\n",
      "epoch-109 lr=['0.0009766'], tr/val_loss:  2.051497/  2.123997, val:  57.50%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.01 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9444%\n",
      "layer   2  Sparsity: 86.4516%\n",
      "layer   3  Sparsity: 84.2777%\n",
      "total_backward_count 1076900 real_backward_count 157281  14.605%\n",
      "epoch-110 lr=['0.0009766'], tr/val_loss:  2.042005/  2.133795, val:  64.58%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9346%\n",
      "layer   2  Sparsity: 86.6068%\n",
      "layer   3  Sparsity: 84.2931%\n",
      "total_backward_count 1086690 real_backward_count 158431  14.579%\n",
      "epoch-111 lr=['0.0009766'], tr/val_loss:  2.053735/  2.134941, val:  65.00%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9302%\n",
      "layer   2  Sparsity: 86.9205%\n",
      "layer   3  Sparsity: 84.8166%\n",
      "total_backward_count 1096480 real_backward_count 159591  14.555%\n",
      "epoch-112 lr=['0.0009766'], tr/val_loss:  2.056518/  2.133897, val:  60.42%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9495%\n",
      "layer   2  Sparsity: 86.8066%\n",
      "layer   3  Sparsity: 84.2918%\n",
      "total_backward_count 1106270 real_backward_count 160795  14.535%\n",
      "fc layer 1 self.abs_max_out: 6501.0\n",
      "lif layer 1 self.abs_max_v: 11668.0\n",
      "fc layer 1 self.abs_max_out: 6511.0\n",
      "epoch-113 lr=['0.0009766'], tr/val_loss:  2.058218/  2.134694, val:  62.92%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.50 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9435%\n",
      "layer   2  Sparsity: 86.5975%\n",
      "layer   3  Sparsity: 84.2799%\n",
      "total_backward_count 1116060 real_backward_count 161913  14.508%\n",
      "epoch-114 lr=['0.0009766'], tr/val_loss:  2.054855/  2.136563, val:  57.92%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.48 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9391%\n",
      "layer   2  Sparsity: 86.5313%\n",
      "layer   3  Sparsity: 84.2881%\n",
      "total_backward_count 1125850 real_backward_count 163032  14.481%\n",
      "lif layer 1 self.abs_max_v: 11797.5\n",
      "lif layer 1 self.abs_max_v: 11848.0\n",
      "fc layer 1 self.abs_max_out: 7044.0\n",
      "lif layer 1 self.abs_max_v: 12635.5\n",
      "epoch-115 lr=['0.0009766'], tr/val_loss:  2.061339/  2.146574, val:  53.75%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.17 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9339%\n",
      "layer   2  Sparsity: 87.0899%\n",
      "layer   3  Sparsity: 85.0458%\n",
      "total_backward_count 1135640 real_backward_count 164180  14.457%\n",
      "fc layer 1 self.abs_max_out: 7088.0\n",
      "lif layer 1 self.abs_max_v: 12637.5\n",
      "fc layer 1 self.abs_max_out: 7247.0\n",
      "epoch-116 lr=['0.0009766'], tr/val_loss:  2.061769/  2.135081, val:  55.83%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.99 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9462%\n",
      "layer   2  Sparsity: 87.0318%\n",
      "layer   3  Sparsity: 84.4651%\n",
      "total_backward_count 1145430 real_backward_count 165329  14.434%\n",
      "epoch-117 lr=['0.0009766'], tr/val_loss:  2.056638/  2.142285, val:  60.00%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.24 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9454%\n",
      "layer   2  Sparsity: 86.8984%\n",
      "layer   3  Sparsity: 84.3933%\n",
      "total_backward_count 1155220 real_backward_count 166511  14.414%\n",
      "epoch-118 lr=['0.0009766'], tr/val_loss:  2.057084/  2.134812, val:  59.58%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.13 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9530%\n",
      "layer   2  Sparsity: 86.4470%\n",
      "layer   3  Sparsity: 84.2962%\n",
      "total_backward_count 1165010 real_backward_count 167653  14.391%\n",
      "lif layer 1 self.abs_max_v: 12751.5\n",
      "epoch-119 lr=['0.0009766'], tr/val_loss:  2.051391/  2.127245, val:  62.92%, val_best:  67.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 74.34 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9278%\n",
      "layer   2  Sparsity: 86.2032%\n",
      "layer   3  Sparsity: 84.5616%\n",
      "total_backward_count 1174800 real_backward_count 168797  14.368%\n",
      "epoch-120 lr=['0.0009766'], tr/val_loss:  2.055639/  2.132746, val:  60.00%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 83.9286%\n",
      "layer   2  Sparsity: 86.5397%\n",
      "layer   3  Sparsity: 84.8177%\n",
      "total_backward_count 1184590 real_backward_count 169855  14.339%\n",
      "epoch-121 lr=['0.0009766'], tr/val_loss:  2.048582/  2.129556, val:  61.67%, val_best:  67.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 74.49 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9465%\n",
      "layer   2  Sparsity: 86.2979%\n",
      "layer   3  Sparsity: 84.3339%\n",
      "total_backward_count 1194380 real_backward_count 171024  14.319%\n",
      "epoch-122 lr=['0.0009766'], tr/val_loss:  2.052462/  2.134121, val:  59.17%, val_best:  67.50%, tr:  99.28%, tr_best: 100.00%, epoch time: 74.27 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9558%\n",
      "layer   2  Sparsity: 86.3157%\n",
      "layer   3  Sparsity: 84.4212%\n",
      "total_backward_count 1204170 real_backward_count 172148  14.296%\n",
      "epoch-123 lr=['0.0009766'], tr/val_loss:  2.047915/  2.121294, val:  65.00%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9458%\n",
      "layer   2  Sparsity: 86.4919%\n",
      "layer   3  Sparsity: 84.1109%\n",
      "total_backward_count 1213960 real_backward_count 173294  14.275%\n",
      "lif layer 2 self.abs_max_v: 3138.5\n",
      "lif layer 2 self.abs_max_v: 3206.5\n",
      "epoch-124 lr=['0.0009766'], tr/val_loss:  2.045852/  2.127814, val:  65.00%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 83.9398%\n",
      "layer   2  Sparsity: 86.3359%\n",
      "layer   3  Sparsity: 84.1595%\n",
      "total_backward_count 1223750 real_backward_count 174436  14.254%\n",
      "epoch-125 lr=['0.0009766'], tr/val_loss:  2.053729/  2.135944, val:  64.58%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.41 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9441%\n",
      "layer   2  Sparsity: 86.5525%\n",
      "layer   3  Sparsity: 84.2475%\n",
      "total_backward_count 1233540 real_backward_count 175597  14.235%\n",
      "epoch-126 lr=['0.0009766'], tr/val_loss:  2.054652/  2.140977, val:  56.67%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.39 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9436%\n",
      "layer   2  Sparsity: 86.8413%\n",
      "layer   3  Sparsity: 84.7549%\n",
      "total_backward_count 1243330 real_backward_count 176777  14.218%\n",
      "epoch-127 lr=['0.0009766'], tr/val_loss:  2.051422/  2.131315, val:  59.17%, val_best:  67.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 74.13 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9418%\n",
      "layer   2  Sparsity: 86.7418%\n",
      "layer   3  Sparsity: 85.0140%\n",
      "total_backward_count 1253120 real_backward_count 177904  14.197%\n",
      "epoch-128 lr=['0.0009766'], tr/val_loss:  2.060575/  2.136530, val:  65.42%, val_best:  67.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 74.63 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9367%\n",
      "layer   2  Sparsity: 87.1215%\n",
      "layer   3  Sparsity: 85.3476%\n",
      "total_backward_count 1262910 real_backward_count 179061  14.178%\n",
      "epoch-129 lr=['0.0009766'], tr/val_loss:  2.053518/  2.132448, val:  62.92%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.44 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9432%\n",
      "layer   2  Sparsity: 87.1720%\n",
      "layer   3  Sparsity: 84.6217%\n",
      "total_backward_count 1272700 real_backward_count 180116  14.152%\n",
      "epoch-130 lr=['0.0009766'], tr/val_loss:  2.042351/  2.119586, val:  59.58%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 83.9521%\n",
      "layer   2  Sparsity: 87.2373%\n",
      "layer   3  Sparsity: 84.3768%\n",
      "total_backward_count 1282490 real_backward_count 181192  14.128%\n",
      "epoch-131 lr=['0.0009766'], tr/val_loss:  2.042408/  2.130030, val:  63.33%, val_best:  67.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 83.9635%\n",
      "layer   2  Sparsity: 86.9739%\n",
      "layer   3  Sparsity: 84.1036%\n",
      "total_backward_count 1292280 real_backward_count 182393  14.114%\n",
      "lif layer 2 self.abs_max_v: 3254.0\n",
      "epoch-132 lr=['0.0009766'], tr/val_loss:  2.038667/  2.123115, val:  57.50%, val_best:  67.50%, tr:  99.18%, tr_best: 100.00%, epoch time: 74.08 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9530%\n",
      "layer   2  Sparsity: 86.6348%\n",
      "layer   3  Sparsity: 84.2054%\n",
      "total_backward_count 1302070 real_backward_count 183508  14.094%\n",
      "epoch-133 lr=['0.0009766'], tr/val_loss:  2.037819/  2.121316, val:  59.17%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9386%\n",
      "layer   2  Sparsity: 87.0231%\n",
      "layer   3  Sparsity: 84.5535%\n",
      "total_backward_count 1311860 real_backward_count 184657  14.076%\n",
      "epoch-134 lr=['0.0009766'], tr/val_loss:  2.045721/  2.130712, val:  57.92%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.27 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9514%\n",
      "layer   2  Sparsity: 87.0486%\n",
      "layer   3  Sparsity: 84.1836%\n",
      "total_backward_count 1321650 real_backward_count 185787  14.057%\n",
      "epoch-135 lr=['0.0009766'], tr/val_loss:  2.046420/  2.129043, val:  61.67%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 74.40 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9381%\n",
      "layer   2  Sparsity: 86.9153%\n",
      "layer   3  Sparsity: 84.2220%\n",
      "total_backward_count 1331440 real_backward_count 186906  14.038%\n",
      "epoch-136 lr=['0.0009766'], tr/val_loss:  2.055052/  2.135332, val:  67.50%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.59 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9471%\n",
      "layer   2  Sparsity: 87.0799%\n",
      "layer   3  Sparsity: 84.4783%\n",
      "total_backward_count 1341230 real_backward_count 188019  14.018%\n",
      "epoch-137 lr=['0.0009766'], tr/val_loss:  2.046650/  2.128463, val:  60.00%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.50 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9731%\n",
      "layer   2  Sparsity: 86.7609%\n",
      "layer   3  Sparsity: 84.0365%\n",
      "total_backward_count 1351020 real_backward_count 189095  13.996%\n",
      "epoch-138 lr=['0.0009766'], tr/val_loss:  2.045714/  2.128525, val:  58.33%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.99 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9351%\n",
      "layer   2  Sparsity: 86.9360%\n",
      "layer   3  Sparsity: 83.8304%\n",
      "total_backward_count 1360810 real_backward_count 190222  13.979%\n",
      "epoch-139 lr=['0.0009766'], tr/val_loss:  2.047164/  2.118151, val:  63.33%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.03 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9505%\n",
      "layer   2  Sparsity: 86.6788%\n",
      "layer   3  Sparsity: 83.8847%\n",
      "total_backward_count 1370600 real_backward_count 191314  13.958%\n",
      "epoch-140 lr=['0.0009766'], tr/val_loss:  2.037122/  2.116871, val:  58.75%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 83.9445%\n",
      "layer   2  Sparsity: 86.5277%\n",
      "layer   3  Sparsity: 83.8378%\n",
      "total_backward_count 1380390 real_backward_count 192426  13.940%\n",
      "epoch-141 lr=['0.0009766'], tr/val_loss:  2.031318/  2.125957, val:  62.92%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 83.9489%\n",
      "layer   2  Sparsity: 86.4795%\n",
      "layer   3  Sparsity: 83.5721%\n",
      "total_backward_count 1390180 real_backward_count 193525  13.921%\n",
      "fc layer 3 self.abs_max_out: 343.0\n",
      "epoch-142 lr=['0.0009766'], tr/val_loss:  2.035130/  2.115234, val:  60.00%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.93 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9612%\n",
      "layer   2  Sparsity: 86.7479%\n",
      "layer   3  Sparsity: 83.8641%\n",
      "total_backward_count 1399970 real_backward_count 194626  13.902%\n",
      "epoch-143 lr=['0.0009766'], tr/val_loss:  2.038558/  2.125721, val:  57.50%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9226%\n",
      "layer   2  Sparsity: 86.8024%\n",
      "layer   3  Sparsity: 83.8297%\n",
      "total_backward_count 1409760 real_backward_count 195710  13.883%\n",
      "epoch-144 lr=['0.0009766'], tr/val_loss:  2.042073/  2.126563, val:  55.83%, val_best:  67.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9436%\n",
      "layer   2  Sparsity: 86.6671%\n",
      "layer   3  Sparsity: 83.9109%\n",
      "total_backward_count 1419550 real_backward_count 196813  13.864%\n",
      "fc layer 1 self.abs_max_out: 7258.0\n",
      "epoch-145 lr=['0.0009766'], tr/val_loss:  2.032402/  2.111163, val:  61.67%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9347%\n",
      "layer   2  Sparsity: 86.6840%\n",
      "layer   3  Sparsity: 83.6176%\n",
      "total_backward_count 1429340 real_backward_count 197913  13.846%\n",
      "epoch-146 lr=['0.0009766'], tr/val_loss:  2.031620/  2.124474, val:  57.50%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9555%\n",
      "layer   2  Sparsity: 86.7609%\n",
      "layer   3  Sparsity: 83.7313%\n",
      "total_backward_count 1439130 real_backward_count 199014  13.829%\n",
      "epoch-147 lr=['0.0009766'], tr/val_loss:  2.030075/  2.118778, val:  64.17%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9457%\n",
      "layer   2  Sparsity: 86.8031%\n",
      "layer   3  Sparsity: 83.8126%\n",
      "total_backward_count 1448920 real_backward_count 200044  13.806%\n",
      "epoch-148 lr=['0.0009766'], tr/val_loss:  2.038033/  2.121425, val:  60.00%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9354%\n",
      "layer   2  Sparsity: 86.7289%\n",
      "layer   3  Sparsity: 83.5272%\n",
      "total_backward_count 1458710 real_backward_count 201176  13.791%\n",
      "epoch-149 lr=['0.0009766'], tr/val_loss:  2.030741/  2.113395, val:  62.50%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.10 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9416%\n",
      "layer   2  Sparsity: 86.5540%\n",
      "layer   3  Sparsity: 83.6155%\n",
      "total_backward_count 1468500 real_backward_count 202323  13.778%\n",
      "epoch-150 lr=['0.0009766'], tr/val_loss:  2.022967/  2.126375, val:  60.83%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9459%\n",
      "layer   2  Sparsity: 86.5066%\n",
      "layer   3  Sparsity: 83.6864%\n",
      "total_backward_count 1478290 real_backward_count 203394  13.759%\n",
      "fc layer 3 self.abs_max_out: 353.0\n",
      "epoch-151 lr=['0.0009766'], tr/val_loss:  2.036853/  2.115758, val:  60.42%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.50 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9569%\n",
      "layer   2  Sparsity: 86.7841%\n",
      "layer   3  Sparsity: 84.0623%\n",
      "total_backward_count 1488080 real_backward_count 204520  13.744%\n",
      "epoch-152 lr=['0.0009766'], tr/val_loss:  2.027257/  2.120964, val:  59.58%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9262%\n",
      "layer   2  Sparsity: 86.6089%\n",
      "layer   3  Sparsity: 84.0941%\n",
      "total_backward_count 1497870 real_backward_count 205573  13.724%\n",
      "epoch-153 lr=['0.0009766'], tr/val_loss:  2.035321/  2.110173, val:  68.33%, val_best:  68.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 74.06 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9495%\n",
      "layer   2  Sparsity: 86.7979%\n",
      "layer   3  Sparsity: 84.1799%\n",
      "total_backward_count 1507660 real_backward_count 206732  13.712%\n",
      "epoch-154 lr=['0.0009766'], tr/val_loss:  2.027330/  2.113933, val:  61.25%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9459%\n",
      "layer   2  Sparsity: 86.6473%\n",
      "layer   3  Sparsity: 83.6602%\n",
      "total_backward_count 1517450 real_backward_count 207814  13.695%\n",
      "epoch-155 lr=['0.0009766'], tr/val_loss:  2.034923/  2.117828, val:  66.67%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.68 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9415%\n",
      "layer   2  Sparsity: 86.9079%\n",
      "layer   3  Sparsity: 83.9879%\n",
      "total_backward_count 1527240 real_backward_count 208883  13.677%\n",
      "epoch-156 lr=['0.0009766'], tr/val_loss:  2.024380/  2.112766, val:  60.00%, val_best:  68.33%, tr:  99.18%, tr_best: 100.00%, epoch time: 73.83 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9372%\n",
      "layer   2  Sparsity: 86.7774%\n",
      "layer   3  Sparsity: 83.6162%\n",
      "total_backward_count 1537030 real_backward_count 209989  13.662%\n",
      "fc layer 1 self.abs_max_out: 7292.0\n",
      "lif layer 1 self.abs_max_v: 12805.0\n",
      "epoch-157 lr=['0.0009766'], tr/val_loss:  2.021886/  2.109134, val:  67.08%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9520%\n",
      "layer   2  Sparsity: 86.7376%\n",
      "layer   3  Sparsity: 83.5323%\n",
      "total_backward_count 1546820 real_backward_count 211038  13.643%\n",
      "epoch-158 lr=['0.0009766'], tr/val_loss:  2.025943/  2.111838, val:  62.92%, val_best:  68.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 71.59 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 83.9438%\n",
      "layer   2  Sparsity: 86.4874%\n",
      "layer   3  Sparsity: 83.3596%\n",
      "total_backward_count 1556610 real_backward_count 212144  13.629%\n",
      "epoch-159 lr=['0.0009766'], tr/val_loss:  2.017378/  2.102755, val:  63.75%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9389%\n",
      "layer   2  Sparsity: 86.3186%\n",
      "layer   3  Sparsity: 82.7442%\n",
      "total_backward_count 1566400 real_backward_count 213225  13.612%\n",
      "epoch-160 lr=['0.0009766'], tr/val_loss:  2.004043/  2.100750, val:  64.17%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.91 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9647%\n",
      "layer   2  Sparsity: 86.2124%\n",
      "layer   3  Sparsity: 82.7545%\n",
      "total_backward_count 1576190 real_backward_count 214286  13.595%\n",
      "lif layer 1 self.abs_max_v: 13013.5\n",
      "lif layer 2 self.abs_max_v: 3352.0\n",
      "epoch-161 lr=['0.0009766'], tr/val_loss:  2.016339/  2.105179, val:  63.75%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.85 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9367%\n",
      "layer   2  Sparsity: 86.4527%\n",
      "layer   3  Sparsity: 83.1169%\n",
      "total_backward_count 1585980 real_backward_count 215377  13.580%\n",
      "fc layer 1 self.abs_max_out: 7436.0\n",
      "epoch-162 lr=['0.0009766'], tr/val_loss:  2.018738/  2.105292, val:  57.08%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9567%\n",
      "layer   2  Sparsity: 86.7205%\n",
      "layer   3  Sparsity: 83.7582%\n",
      "total_backward_count 1595770 real_backward_count 216492  13.567%\n",
      "epoch-163 lr=['0.0009766'], tr/val_loss:  2.021090/  2.120436, val:  60.00%, val_best:  68.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9437%\n",
      "layer   2  Sparsity: 86.7151%\n",
      "layer   3  Sparsity: 83.5976%\n",
      "total_backward_count 1605560 real_backward_count 217544  13.549%\n",
      "epoch-164 lr=['0.0009766'], tr/val_loss:  2.023336/  2.111521, val:  63.33%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9547%\n",
      "layer   2  Sparsity: 86.5555%\n",
      "layer   3  Sparsity: 83.1513%\n",
      "total_backward_count 1615350 real_backward_count 218560  13.530%\n",
      "epoch-165 lr=['0.0009766'], tr/val_loss:  2.024863/  2.115275, val:  57.92%, val_best:  68.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 74.09 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9350%\n",
      "layer   2  Sparsity: 86.3539%\n",
      "layer   3  Sparsity: 83.2326%\n",
      "total_backward_count 1625140 real_backward_count 219645  13.515%\n",
      "epoch-166 lr=['0.0009766'], tr/val_loss:  2.024437/  2.119584, val:  61.25%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9626%\n",
      "layer   2  Sparsity: 86.4678%\n",
      "layer   3  Sparsity: 83.4434%\n",
      "total_backward_count 1634930 real_backward_count 220731  13.501%\n",
      "fc layer 3 self.abs_max_out: 356.0\n",
      "fc layer 3 self.abs_max_out: 372.0\n",
      "epoch-167 lr=['0.0009766'], tr/val_loss:  2.024124/  2.119609, val:  54.17%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9549%\n",
      "layer   2  Sparsity: 86.5597%\n",
      "layer   3  Sparsity: 83.2021%\n",
      "total_backward_count 1644720 real_backward_count 221863  13.489%\n",
      "epoch-168 lr=['0.0009766'], tr/val_loss:  2.015130/  2.108180, val:  61.25%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.49 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9384%\n",
      "layer   2  Sparsity: 86.7251%\n",
      "layer   3  Sparsity: 83.3007%\n",
      "total_backward_count 1654510 real_backward_count 222911  13.473%\n",
      "epoch-169 lr=['0.0009766'], tr/val_loss:  2.010176/  2.105844, val:  58.33%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.35 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9513%\n",
      "layer   2  Sparsity: 86.8751%\n",
      "layer   3  Sparsity: 83.0197%\n",
      "total_backward_count 1664300 real_backward_count 223934  13.455%\n",
      "lif layer 1 self.abs_max_v: 13104.5\n",
      "epoch-170 lr=['0.0009766'], tr/val_loss:  2.013880/  2.101541, val:  60.42%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9489%\n",
      "layer   2  Sparsity: 86.6807%\n",
      "layer   3  Sparsity: 82.8685%\n",
      "total_backward_count 1674090 real_backward_count 224955  13.437%\n",
      "epoch-171 lr=['0.0009766'], tr/val_loss:  2.007852/  2.103756, val:  54.17%, val_best:  68.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9433%\n",
      "layer   2  Sparsity: 86.8179%\n",
      "layer   3  Sparsity: 82.9388%\n",
      "total_backward_count 1683880 real_backward_count 226047  13.424%\n",
      "epoch-172 lr=['0.0009766'], tr/val_loss:  2.005211/  2.090533, val:  64.17%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.66 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9197%\n",
      "layer   2  Sparsity: 86.6600%\n",
      "layer   3  Sparsity: 82.7677%\n",
      "total_backward_count 1693670 real_backward_count 227145  13.411%\n",
      "epoch-173 lr=['0.0009766'], tr/val_loss:  1.998327/  2.107660, val:  56.25%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9406%\n",
      "layer   2  Sparsity: 86.5721%\n",
      "layer   3  Sparsity: 82.5525%\n",
      "total_backward_count 1703460 real_backward_count 228163  13.394%\n",
      "epoch-174 lr=['0.0009766'], tr/val_loss:  2.016073/  2.104822, val:  63.75%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9469%\n",
      "layer   2  Sparsity: 86.5759%\n",
      "layer   3  Sparsity: 82.7242%\n",
      "total_backward_count 1713250 real_backward_count 229228  13.380%\n",
      "epoch-175 lr=['0.0009766'], tr/val_loss:  2.008786/  2.095625, val:  68.33%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9388%\n",
      "layer   2  Sparsity: 86.7717%\n",
      "layer   3  Sparsity: 82.6847%\n",
      "total_backward_count 1723040 real_backward_count 230336  13.368%\n",
      "epoch-176 lr=['0.0009766'], tr/val_loss:  2.008240/  2.100211, val:  65.42%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.57 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9348%\n",
      "layer   2  Sparsity: 86.7680%\n",
      "layer   3  Sparsity: 82.8733%\n",
      "total_backward_count 1732830 real_backward_count 231393  13.353%\n",
      "epoch-177 lr=['0.0009766'], tr/val_loss:  2.012602/  2.098271, val:  60.42%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9319%\n",
      "layer   2  Sparsity: 86.6589%\n",
      "layer   3  Sparsity: 83.0375%\n",
      "total_backward_count 1742620 real_backward_count 232439  13.338%\n",
      "epoch-178 lr=['0.0009766'], tr/val_loss:  2.004488/  2.104586, val:  58.33%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9521%\n",
      "layer   2  Sparsity: 86.4247%\n",
      "layer   3  Sparsity: 82.1971%\n",
      "total_backward_count 1752410 real_backward_count 233490  13.324%\n",
      "epoch-179 lr=['0.0009766'], tr/val_loss:  1.993993/  2.098401, val:  57.92%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9535%\n",
      "layer   2  Sparsity: 86.4693%\n",
      "layer   3  Sparsity: 81.9516%\n",
      "total_backward_count 1762200 real_backward_count 234532  13.309%\n",
      "epoch-180 lr=['0.0009766'], tr/val_loss:  1.992168/  2.085964, val:  64.17%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9509%\n",
      "layer   2  Sparsity: 86.4856%\n",
      "layer   3  Sparsity: 82.1956%\n",
      "total_backward_count 1771990 real_backward_count 235535  13.292%\n",
      "epoch-181 lr=['0.0009766'], tr/val_loss:  2.004234/  2.107127, val:  62.92%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.39 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9360%\n",
      "layer   2  Sparsity: 86.9180%\n",
      "layer   3  Sparsity: 83.1407%\n",
      "total_backward_count 1781780 real_backward_count 236545  13.276%\n",
      "epoch-182 lr=['0.0009766'], tr/val_loss:  2.012428/  2.107104, val:  65.00%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9455%\n",
      "layer   2  Sparsity: 86.8178%\n",
      "layer   3  Sparsity: 82.9944%\n",
      "total_backward_count 1791570 real_backward_count 237630  13.264%\n",
      "epoch-183 lr=['0.0009766'], tr/val_loss:  2.008138/  2.098996, val:  61.67%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9516%\n",
      "layer   2  Sparsity: 86.7817%\n",
      "layer   3  Sparsity: 83.0326%\n",
      "total_backward_count 1801360 real_backward_count 238640  13.248%\n",
      "epoch-184 lr=['0.0009766'], tr/val_loss:  2.006758/  2.107305, val:  59.17%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 71.98 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9486%\n",
      "layer   2  Sparsity: 86.6666%\n",
      "layer   3  Sparsity: 83.2110%\n",
      "total_backward_count 1811150 real_backward_count 239662  13.233%\n",
      "epoch-185 lr=['0.0009766'], tr/val_loss:  2.007549/  2.098419, val:  67.92%, val_best:  68.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 71.69 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 83.9331%\n",
      "layer   2  Sparsity: 86.5732%\n",
      "layer   3  Sparsity: 82.8562%\n",
      "total_backward_count 1820940 real_backward_count 240737  13.220%\n",
      "epoch-186 lr=['0.0009766'], tr/val_loss:  2.004934/  2.099940, val:  65.83%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9655%\n",
      "layer   2  Sparsity: 86.3607%\n",
      "layer   3  Sparsity: 82.5381%\n",
      "total_backward_count 1830730 real_backward_count 241849  13.211%\n",
      "epoch-187 lr=['0.0009766'], tr/val_loss:  2.001602/  2.096581, val:  65.00%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.48 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9448%\n",
      "layer   2  Sparsity: 86.1179%\n",
      "layer   3  Sparsity: 82.7802%\n",
      "total_backward_count 1840520 real_backward_count 242918  13.198%\n",
      "epoch-188 lr=['0.0009766'], tr/val_loss:  2.006741/  2.103040, val:  60.00%, val_best:  68.33%, tr:  99.49%, tr_best: 100.00%, epoch time: 71.96 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9338%\n",
      "layer   2  Sparsity: 86.4659%\n",
      "layer   3  Sparsity: 83.1927%\n",
      "total_backward_count 1850310 real_backward_count 243956  13.185%\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "epoch-189 lr=['0.0009766'], tr/val_loss:  1.997203/  2.082361, val:  61.67%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9408%\n",
      "layer   2  Sparsity: 86.2215%\n",
      "layer   3  Sparsity: 82.8823%\n",
      "total_backward_count 1860100 real_backward_count 244978  13.170%\n",
      "epoch-190 lr=['0.0009766'], tr/val_loss:  2.000065/  2.089710, val:  67.92%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.16 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9377%\n",
      "layer   2  Sparsity: 86.5460%\n",
      "layer   3  Sparsity: 83.2628%\n",
      "total_backward_count 1869890 real_backward_count 245998  13.156%\n",
      "epoch-191 lr=['0.0009766'], tr/val_loss:  2.007119/  2.096538, val:  56.25%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9306%\n",
      "layer   2  Sparsity: 86.5446%\n",
      "layer   3  Sparsity: 82.7272%\n",
      "total_backward_count 1879680 real_backward_count 247032  13.142%\n",
      "epoch-192 lr=['0.0009766'], tr/val_loss:  2.002052/  2.098056, val:  64.17%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.28 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 83.9441%\n",
      "layer   2  Sparsity: 86.4826%\n",
      "layer   3  Sparsity: 82.7674%\n",
      "total_backward_count 1889470 real_backward_count 248087  13.130%\n",
      "epoch-193 lr=['0.0009766'], tr/val_loss:  2.000895/  2.106068, val:  57.08%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9494%\n",
      "layer   2  Sparsity: 86.4416%\n",
      "layer   3  Sparsity: 82.8488%\n",
      "total_backward_count 1899260 real_backward_count 249089  13.115%\n",
      "epoch-194 lr=['0.0009766'], tr/val_loss:  2.013101/  2.099555, val:  61.67%, val_best:  68.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.67 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 83.9393%\n",
      "layer   2  Sparsity: 86.3684%\n",
      "layer   3  Sparsity: 83.1251%\n",
      "total_backward_count 1909050 real_backward_count 250123  13.102%\n",
      "epoch-195 lr=['0.0009766'], tr/val_loss:  2.010029/  2.107104, val:  62.50%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9181%\n",
      "layer   2  Sparsity: 86.3171%\n",
      "layer   3  Sparsity: 83.0176%\n",
      "total_backward_count 1918840 real_backward_count 251151  13.089%\n",
      "epoch-196 lr=['0.0009766'], tr/val_loss:  2.011221/  2.110579, val:  60.42%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9359%\n",
      "layer   2  Sparsity: 86.5188%\n",
      "layer   3  Sparsity: 82.8713%\n",
      "total_backward_count 1928630 real_backward_count 252182  13.076%\n",
      "epoch-197 lr=['0.0009766'], tr/val_loss:  2.006403/  2.102193, val:  60.42%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9360%\n",
      "layer   2  Sparsity: 86.7242%\n",
      "layer   3  Sparsity: 82.8447%\n",
      "total_backward_count 1938420 real_backward_count 253218  13.063%\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "epoch-198 lr=['0.0009766'], tr/val_loss:  2.003675/  2.098393, val:  61.67%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 83.9516%\n",
      "layer   2  Sparsity: 86.8064%\n",
      "layer   3  Sparsity: 82.7561%\n",
      "total_backward_count 1948210 real_backward_count 254265  13.051%\n",
      "epoch-199 lr=['0.0009766'], tr/val_loss:  2.008621/  2.095358, val:  62.50%, val_best:  68.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 83.9559%\n",
      "layer   2  Sparsity: 86.7239%\n",
      "layer   3  Sparsity: 82.8856%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794996ef6c00479697165066da47717e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99694</td></tr><tr><td>tr_epoch_loss</td><td>2.00862</td></tr><tr><td>val_acc_best</td><td>0.68333</td></tr><tr><td>val_acc_now</td><td>0.625</td></tr><tr><td>val_loss</td><td>2.09536</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pretty-sweep-106</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9v7u2kug' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/9v7u2kug</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_014256-9v7u2kug/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a7elew9p with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_054739-a7elew9p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a7elew9p' target=\"_blank\">devout-sweep-110</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a7elew9p' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a7elew9p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251117_054747_947', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 8, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 464.0\n",
      "lif layer 1 self.abs_max_v: 464.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 512.0\n",
      "lif layer 2 self.abs_max_v: 512.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "lif layer 2 self.abs_max_v: 522.5\n",
      "fc layer 3 self.abs_max_out: 147.0\n",
      "fc layer 1 self.abs_max_out: 565.0\n",
      "lif layer 1 self.abs_max_v: 762.0\n",
      "fc layer 2 self.abs_max_out: 521.0\n",
      "lif layer 2 self.abs_max_v: 667.0\n",
      "fc layer 3 self.abs_max_out: 178.0\n",
      "fc layer 2 self.abs_max_out: 570.0\n",
      "lif layer 2 self.abs_max_v: 861.0\n",
      "fc layer 3 self.abs_max_out: 181.0\n",
      "fc layer 1 self.abs_max_out: 667.0\n",
      "fc layer 2 self.abs_max_out: 808.0\n",
      "lif layer 2 self.abs_max_v: 1054.0\n",
      "fc layer 3 self.abs_max_out: 214.0\n",
      "fc layer 1 self.abs_max_out: 800.0\n",
      "lif layer 1 self.abs_max_v: 800.0\n",
      "fc layer 3 self.abs_max_out: 218.0\n",
      "fc layer 1 self.abs_max_out: 995.0\n",
      "lif layer 1 self.abs_max_v: 995.0\n",
      "lif layer 2 self.abs_max_v: 1171.0\n",
      "fc layer 3 self.abs_max_out: 227.0\n",
      "fc layer 1 self.abs_max_out: 1101.0\n",
      "lif layer 1 self.abs_max_v: 1101.0\n",
      "fc layer 2 self.abs_max_out: 921.0\n",
      "fc layer 3 self.abs_max_out: 251.0\n",
      "lif layer 1 self.abs_max_v: 1131.0\n",
      "fc layer 3 self.abs_max_out: 392.0\n",
      "lif layer 1 self.abs_max_v: 1308.5\n",
      "lif layer 2 self.abs_max_v: 1313.5\n",
      "fc layer 1 self.abs_max_out: 1188.0\n",
      "lif layer 1 self.abs_max_v: 1329.5\n",
      "fc layer 2 self.abs_max_out: 1051.0\n",
      "fc layer 1 self.abs_max_out: 1311.0\n",
      "lif layer 1 self.abs_max_v: 1424.0\n",
      "fc layer 1 self.abs_max_out: 1433.0\n",
      "lif layer 1 self.abs_max_v: 1433.0\n",
      "lif layer 2 self.abs_max_v: 1389.5\n",
      "lif layer 1 self.abs_max_v: 1602.0\n",
      "fc layer 1 self.abs_max_out: 1465.0\n",
      "lif layer 1 self.abs_max_v: 1649.0\n",
      "fc layer 1 self.abs_max_out: 1735.0\n",
      "lif layer 1 self.abs_max_v: 1735.0\n",
      "fc layer 2 self.abs_max_out: 1104.0\n",
      "fc layer 3 self.abs_max_out: 425.0\n",
      "fc layer 2 self.abs_max_out: 1111.0\n",
      "lif layer 2 self.abs_max_v: 1601.5\n",
      "fc layer 1 self.abs_max_out: 1931.0\n",
      "lif layer 1 self.abs_max_v: 1931.0\n",
      "lif layer 2 self.abs_max_v: 1623.0\n",
      "lif layer 1 self.abs_max_v: 2021.0\n",
      "lif layer 2 self.abs_max_v: 1682.0\n",
      "fc layer 2 self.abs_max_out: 1116.0\n",
      "fc layer 2 self.abs_max_out: 1146.0\n",
      "fc layer 1 self.abs_max_out: 2122.0\n",
      "lif layer 1 self.abs_max_v: 2122.0\n",
      "fc layer 1 self.abs_max_out: 2146.0\n",
      "lif layer 1 self.abs_max_v: 2146.0\n",
      "fc layer 2 self.abs_max_out: 1376.0\n",
      "fc layer 2 self.abs_max_out: 1392.0\n",
      "fc layer 3 self.abs_max_out: 460.0\n",
      "fc layer 2 self.abs_max_out: 1467.0\n",
      "lif layer 2 self.abs_max_v: 1712.0\n",
      "fc layer 3 self.abs_max_out: 484.0\n",
      "lif layer 1 self.abs_max_v: 2178.5\n",
      "fc layer 2 self.abs_max_out: 1474.0\n",
      "fc layer 3 self.abs_max_out: 492.0\n",
      "lif layer 2 self.abs_max_v: 1746.5\n",
      "fc layer 2 self.abs_max_out: 1519.0\n",
      "fc layer 3 self.abs_max_out: 536.0\n",
      "fc layer 1 self.abs_max_out: 2459.0\n",
      "lif layer 1 self.abs_max_v: 2459.0\n",
      "fc layer 1 self.abs_max_out: 2567.0\n",
      "lif layer 1 self.abs_max_v: 2567.0\n",
      "lif layer 2 self.abs_max_v: 1987.5\n",
      "lif layer 2 self.abs_max_v: 2051.5\n",
      "lif layer 2 self.abs_max_v: 2171.0\n",
      "fc layer 2 self.abs_max_out: 1529.0\n",
      "lif layer 1 self.abs_max_v: 3041.0\n",
      "lif layer 2 self.abs_max_v: 2410.5\n",
      "lif layer 2 self.abs_max_v: 2504.5\n",
      "fc layer 1 self.abs_max_out: 2940.0\n",
      "lif layer 2 self.abs_max_v: 2562.5\n",
      "lif layer 1 self.abs_max_v: 3105.0\n",
      "fc layer 2 self.abs_max_out: 1757.0\n",
      "lif layer 2 self.abs_max_v: 2605.5\n",
      "lif layer 1 self.abs_max_v: 3475.0\n",
      "fc layer 1 self.abs_max_out: 3027.0\n",
      "lif layer 2 self.abs_max_v: 2666.5\n",
      "lif layer 2 self.abs_max_v: 2677.5\n",
      "fc layer 2 self.abs_max_out: 1972.0\n",
      "lif layer 2 self.abs_max_v: 2816.0\n",
      "lif layer 1 self.abs_max_v: 3856.5\n",
      "lif layer 1 self.abs_max_v: 4185.5\n",
      "lif layer 2 self.abs_max_v: 2891.0\n",
      "lif layer 2 self.abs_max_v: 2937.5\n",
      "lif layer 2 self.abs_max_v: 2947.5\n",
      "lif layer 2 self.abs_max_v: 2953.0\n",
      "lif layer 2 self.abs_max_v: 2965.0\n",
      "lif layer 2 self.abs_max_v: 3063.5\n",
      "fc layer 2 self.abs_max_out: 2011.0\n",
      "lif layer 2 self.abs_max_v: 3108.5\n",
      "lif layer 2 self.abs_max_v: 3145.5\n",
      "lif layer 2 self.abs_max_v: 3188.5\n",
      "lif layer 2 self.abs_max_v: 3241.5\n",
      "fc layer 3 self.abs_max_out: 573.0\n",
      "lif layer 2 self.abs_max_v: 3292.5\n",
      "lif layer 2 self.abs_max_v: 3331.0\n",
      "lif layer 2 self.abs_max_v: 3358.0\n",
      "lif layer 1 self.abs_max_v: 4273.5\n",
      "lif layer 1 self.abs_max_v: 4358.5\n",
      "lif layer 2 self.abs_max_v: 3420.0\n",
      "fc layer 3 self.abs_max_out: 577.0\n",
      "fc layer 2 self.abs_max_out: 2101.0\n",
      "lif layer 2 self.abs_max_v: 3426.5\n",
      "lif layer 1 self.abs_max_v: 4365.0\n",
      "fc layer 2 self.abs_max_out: 2191.0\n",
      "lif layer 2 self.abs_max_v: 3738.0\n",
      "lif layer 2 self.abs_max_v: 3918.0\n",
      "lif layer 1 self.abs_max_v: 4801.5\n",
      "fc layer 1 self.abs_max_out: 3407.0\n",
      "lif layer 1 self.abs_max_v: 5494.0\n",
      "lif layer 2 self.abs_max_v: 4049.5\n",
      "lif layer 1 self.abs_max_v: 6067.0\n",
      "fc layer 2 self.abs_max_out: 2205.0\n",
      "fc layer 2 self.abs_max_out: 2207.0\n",
      "fc layer 2 self.abs_max_out: 2224.0\n",
      "fc layer 2 self.abs_max_out: 2305.0\n",
      "lif layer 2 self.abs_max_v: 4106.0\n",
      "lif layer 2 self.abs_max_v: 4335.0\n",
      "fc layer 1 self.abs_max_out: 3491.0\n",
      "fc layer 2 self.abs_max_out: 2334.0\n",
      "fc layer 3 self.abs_max_out: 597.0\n",
      "lif layer 2 self.abs_max_v: 4401.0\n",
      "lif layer 2 self.abs_max_v: 4425.5\n",
      "fc layer 2 self.abs_max_out: 2353.0\n",
      "fc layer 2 self.abs_max_out: 2567.0\n",
      "fc layer 3 self.abs_max_out: 609.0\n",
      "lif layer 2 self.abs_max_v: 4518.5\n",
      "fc layer 3 self.abs_max_out: 617.0\n",
      "fc layer 1 self.abs_max_out: 4050.0\n",
      "lif layer 1 self.abs_max_v: 6157.5\n",
      "fc layer 1 self.abs_max_out: 4637.0\n",
      "fc layer 3 self.abs_max_out: 648.0\n",
      "fc layer 3 self.abs_max_out: 663.0\n",
      "lif layer 2 self.abs_max_v: 4519.0\n",
      "lif layer 2 self.abs_max_v: 4548.5\n",
      "lif layer 1 self.abs_max_v: 6195.5\n",
      "lif layer 1 self.abs_max_v: 6959.5\n",
      "lif layer 1 self.abs_max_v: 7428.0\n",
      "fc layer 2 self.abs_max_out: 2613.0\n",
      "fc layer 2 self.abs_max_out: 2667.0\n",
      "lif layer 1 self.abs_max_v: 7453.0\n",
      "lif layer 2 self.abs_max_v: 4562.0\n",
      "fc layer 3 self.abs_max_out: 672.0\n",
      "lif layer 2 self.abs_max_v: 4580.5\n",
      "lif layer 2 self.abs_max_v: 4586.5\n",
      "lif layer 2 self.abs_max_v: 4682.5\n",
      "lif layer 2 self.abs_max_v: 4861.0\n",
      "lif layer 2 self.abs_max_v: 5014.5\n",
      "fc layer 2 self.abs_max_out: 2706.0\n",
      "lif layer 2 self.abs_max_v: 5213.5\n",
      "fc layer 2 self.abs_max_out: 2749.0\n",
      "fc layer 3 self.abs_max_out: 695.0\n",
      "fc layer 2 self.abs_max_out: 2756.0\n",
      "fc layer 2 self.abs_max_out: 2839.0\n",
      "lif layer 2 self.abs_max_v: 5292.0\n",
      "lif layer 1 self.abs_max_v: 7798.0\n",
      "fc layer 2 self.abs_max_out: 2849.0\n",
      "fc layer 2 self.abs_max_out: 3131.0\n",
      "lif layer 2 self.abs_max_v: 5561.5\n",
      "lif layer 2 self.abs_max_v: 5593.5\n",
      "lif layer 2 self.abs_max_v: 5724.0\n",
      "fc layer 1 self.abs_max_out: 4857.0\n",
      "lif layer 1 self.abs_max_v: 8193.0\n",
      "lif layer 2 self.abs_max_v: 5747.0\n",
      "lif layer 2 self.abs_max_v: 5896.5\n",
      "lif layer 2 self.abs_max_v: 5924.0\n",
      "fc layer 2 self.abs_max_out: 3201.0\n",
      "fc layer 2 self.abs_max_out: 3405.0\n",
      "fc layer 2 self.abs_max_out: 3669.0\n",
      "lif layer 2 self.abs_max_v: 6455.5\n",
      "lif layer 2 self.abs_max_v: 6533.0\n",
      "lif layer 2 self.abs_max_v: 6786.0\n",
      "fc layer 3 self.abs_max_out: 746.0\n",
      "fc layer 3 self.abs_max_out: 890.0\n",
      "fc layer 1 self.abs_max_out: 5111.0\n",
      "fc layer 1 self.abs_max_out: 5301.0\n",
      "lif layer 1 self.abs_max_v: 8875.0\n",
      "lif layer 1 self.abs_max_v: 9048.5\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.743692/  1.934305, val:  40.42%, val_best:  40.42%, tr:  98.26%, tr_best:  98.26%, epoch time: 74.02 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6930%\n",
      "layer   2  Sparsity: 70.4431%\n",
      "layer   3  Sparsity: 70.4148%\n",
      "total_backward_count 9790 real_backward_count 1711  17.477%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 3710.0\n",
      "fc layer 2 self.abs_max_out: 3733.0\n",
      "lif layer 2 self.abs_max_v: 6945.5\n",
      "lif layer 1 self.abs_max_v: 9067.5\n",
      "fc layer 2 self.abs_max_out: 3937.0\n",
      "lif layer 2 self.abs_max_v: 7044.5\n",
      "lif layer 2 self.abs_max_v: 7169.5\n",
      "lif layer 2 self.abs_max_v: 7300.0\n",
      "fc layer 1 self.abs_max_out: 5547.0\n",
      "lif layer 1 self.abs_max_v: 9614.5\n",
      "lif layer 2 self.abs_max_v: 7396.0\n",
      "lif layer 2 self.abs_max_v: 7399.0\n",
      "lif layer 2 self.abs_max_v: 7435.5\n",
      "fc layer 1 self.abs_max_out: 5619.0\n",
      "fc layer 2 self.abs_max_out: 4006.0\n",
      "fc layer 1 self.abs_max_out: 5647.0\n",
      "fc layer 1 self.abs_max_out: 5742.0\n",
      "lif layer 1 self.abs_max_v: 9664.0\n",
      "fc layer 2 self.abs_max_out: 4054.0\n",
      "fc layer 1 self.abs_max_out: 6181.0\n",
      "lif layer 1 self.abs_max_v: 10453.0\n",
      "lif layer 1 self.abs_max_v: 10846.5\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.668297/  1.880922, val:  45.83%, val_best:  45.83%, tr:  98.98%, tr_best:  98.98%, epoch time: 74.49 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.7054%\n",
      "layer   2  Sparsity: 72.5465%\n",
      "layer   3  Sparsity: 71.5225%\n",
      "total_backward_count 19580 real_backward_count 3058  15.618%\n",
      "fc layer 2 self.abs_max_out: 4075.0\n",
      "lif layer 2 self.abs_max_v: 7484.5\n",
      "fc layer 2 self.abs_max_out: 4164.0\n",
      "lif layer 2 self.abs_max_v: 7492.5\n",
      "lif layer 2 self.abs_max_v: 7658.5\n",
      "lif layer 2 self.abs_max_v: 7686.5\n",
      "fc layer 3 self.abs_max_out: 917.0\n",
      "fc layer 2 self.abs_max_out: 4215.0\n",
      "fc layer 2 self.abs_max_out: 4357.0\n",
      "fc layer 3 self.abs_max_out: 943.0\n",
      "fc layer 3 self.abs_max_out: 952.0\n",
      "fc layer 1 self.abs_max_out: 6735.0\n",
      "fc layer 1 self.abs_max_out: 6861.0\n",
      "lif layer 1 self.abs_max_v: 11697.5\n",
      "lif layer 1 self.abs_max_v: 12094.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.633324/  1.880118, val:  44.58%, val_best:  45.83%, tr:  99.49%, tr_best:  99.49%, epoch time: 74.01 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7044%\n",
      "layer   2  Sparsity: 73.3452%\n",
      "layer   3  Sparsity: 71.4254%\n",
      "total_backward_count 29370 real_backward_count 4257  14.494%\n",
      "fc layer 1 self.abs_max_out: 7122.0\n",
      "lif layer 1 self.abs_max_v: 12520.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.625437/  1.863212, val:  42.50%, val_best:  45.83%, tr:  99.69%, tr_best:  99.69%, epoch time: 73.59 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6748%\n",
      "layer   2  Sparsity: 73.9114%\n",
      "layer   3  Sparsity: 71.2525%\n",
      "total_backward_count 39160 real_backward_count 5355  13.675%\n",
      "fc layer 3 self.abs_max_out: 953.0\n",
      "fc layer 1 self.abs_max_out: 7429.0\n",
      "lif layer 1 self.abs_max_v: 12521.5\n",
      "lif layer 1 self.abs_max_v: 13446.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.587236/  1.826196, val:  54.58%, val_best:  54.58%, tr:  99.59%, tr_best:  99.69%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6777%\n",
      "layer   2  Sparsity: 73.2004%\n",
      "layer   3  Sparsity: 71.5164%\n",
      "total_backward_count 48950 real_backward_count 6449  13.175%\n",
      "fc layer 1 self.abs_max_out: 7516.0\n",
      "fc layer 1 self.abs_max_out: 7885.0\n",
      "fc layer 1 self.abs_max_out: 8890.0\n",
      "lif layer 1 self.abs_max_v: 14545.5\n",
      "lif layer 1 self.abs_max_v: 15258.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.572161/  1.819820, val:  47.50%, val_best:  54.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.48 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6919%\n",
      "layer   2  Sparsity: 73.1024%\n",
      "layer   3  Sparsity: 70.5651%\n",
      "total_backward_count 58740 real_backward_count 7526  12.812%\n",
      "fc layer 3 self.abs_max_out: 954.0\n",
      "fc layer 3 self.abs_max_out: 958.0\n",
      "lif layer 2 self.abs_max_v: 7698.0\n",
      "lif layer 2 self.abs_max_v: 7861.0\n",
      "lif layer 2 self.abs_max_v: 7906.5\n",
      "lif layer 2 self.abs_max_v: 8076.5\n",
      "fc layer 2 self.abs_max_out: 4418.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.584695/  1.841837, val:  50.83%, val_best:  54.58%, tr:  99.69%, tr_best:  99.90%, epoch time: 74.23 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6820%\n",
      "layer   2  Sparsity: 73.6120%\n",
      "layer   3  Sparsity: 70.9958%\n",
      "total_backward_count 68530 real_backward_count 8594  12.540%\n",
      "fc layer 2 self.abs_max_out: 4484.0\n",
      "lif layer 2 self.abs_max_v: 8089.5\n",
      "lif layer 2 self.abs_max_v: 8332.0\n",
      "fc layer 2 self.abs_max_out: 4680.0\n",
      "lif layer 2 self.abs_max_v: 8600.5\n",
      "fc layer 1 self.abs_max_out: 8969.0\n",
      "lif layer 1 self.abs_max_v: 15496.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.562449/  1.814556, val:  52.50%, val_best:  54.58%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6900%\n",
      "layer   2  Sparsity: 72.6660%\n",
      "layer   3  Sparsity: 70.2709%\n",
      "total_backward_count 78320 real_backward_count 9576  12.227%\n",
      "fc layer 3 self.abs_max_out: 986.0\n",
      "fc layer 3 self.abs_max_out: 992.0\n",
      "fc layer 3 self.abs_max_out: 1055.0\n",
      "fc layer 1 self.abs_max_out: 9304.0\n",
      "lif layer 1 self.abs_max_v: 15895.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.541904/  1.783907, val:  58.33%, val_best:  58.33%, tr:  99.49%, tr_best:  99.90%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6853%\n",
      "layer   2  Sparsity: 73.7679%\n",
      "layer   3  Sparsity: 70.6941%\n",
      "total_backward_count 88110 real_backward_count 10619  12.052%\n",
      "fc layer 1 self.abs_max_out: 9341.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.497474/  1.797058, val:  46.25%, val_best:  58.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.67 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7107%\n",
      "layer   2  Sparsity: 73.2961%\n",
      "layer   3  Sparsity: 69.9043%\n",
      "total_backward_count 97900 real_backward_count 11595  11.844%\n",
      "fc layer 1 self.abs_max_out: 9771.0\n",
      "fc layer 1 self.abs_max_out: 9970.0\n",
      "lif layer 1 self.abs_max_v: 17024.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.488842/  1.747498, val:  52.92%, val_best:  58.33%, tr:  99.80%, tr_best:  99.90%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6907%\n",
      "layer   2  Sparsity: 72.7079%\n",
      "layer   3  Sparsity: 70.1191%\n",
      "total_backward_count 107690 real_backward_count 12600  11.700%\n",
      "lif layer 2 self.abs_max_v: 8609.0\n",
      "lif layer 2 self.abs_max_v: 8812.5\n",
      "lif layer 2 self.abs_max_v: 8854.5\n",
      "lif layer 2 self.abs_max_v: 8925.5\n",
      "lif layer 2 self.abs_max_v: 8990.0\n",
      "fc layer 2 self.abs_max_out: 4702.0\n",
      "fc layer 2 self.abs_max_out: 4717.0\n",
      "fc layer 2 self.abs_max_out: 4883.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.463521/  1.741125, val:  57.50%, val_best:  58.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6830%\n",
      "layer   2  Sparsity: 72.2120%\n",
      "layer   3  Sparsity: 70.2614%\n",
      "total_backward_count 117480 real_backward_count 13652  11.621%\n",
      "fc layer 1 self.abs_max_out: 10073.0\n",
      "lif layer 1 self.abs_max_v: 17368.5\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.457525/  1.759404, val:  57.08%, val_best:  58.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6834%\n",
      "layer   2  Sparsity: 72.0883%\n",
      "layer   3  Sparsity: 70.5748%\n",
      "total_backward_count 127270 real_backward_count 14616  11.484%\n",
      "fc layer 2 self.abs_max_out: 5216.0\n",
      "fc layer 2 self.abs_max_out: 5296.0\n",
      "lif layer 2 self.abs_max_v: 9150.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.460950/  1.780539, val:  50.00%, val_best:  58.33%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6995%\n",
      "layer   2  Sparsity: 71.7426%\n",
      "layer   3  Sparsity: 70.7039%\n",
      "total_backward_count 137060 real_backward_count 15596  11.379%\n",
      "lif layer 2 self.abs_max_v: 9183.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.476875/  1.731519, val:  58.75%, val_best:  58.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6985%\n",
      "layer   2  Sparsity: 72.1594%\n",
      "layer   3  Sparsity: 71.1235%\n",
      "total_backward_count 146850 real_backward_count 16530  11.256%\n",
      "fc layer 1 self.abs_max_out: 10320.0\n",
      "lif layer 1 self.abs_max_v: 17904.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.457864/  1.759498, val:  54.17%, val_best:  58.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6997%\n",
      "layer   2  Sparsity: 71.9742%\n",
      "layer   3  Sparsity: 71.2825%\n",
      "total_backward_count 156640 real_backward_count 17484  11.162%\n",
      "fc layer 1 self.abs_max_out: 10810.0\n",
      "lif layer 1 self.abs_max_v: 18056.5\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.448027/  1.709175, val:  60.83%, val_best:  60.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6972%\n",
      "layer   2  Sparsity: 71.4857%\n",
      "layer   3  Sparsity: 70.9938%\n",
      "total_backward_count 166430 real_backward_count 18422  11.069%\n",
      "fc layer 3 self.abs_max_out: 1075.0\n",
      "fc layer 1 self.abs_max_out: 10850.0\n",
      "lif layer 1 self.abs_max_v: 18751.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.433928/  1.687706, val:  65.00%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6929%\n",
      "layer   2  Sparsity: 71.4936%\n",
      "layer   3  Sparsity: 70.5444%\n",
      "total_backward_count 176220 real_backward_count 19344  10.977%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.418438/  1.696570, val:  52.92%, val_best:  65.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6702%\n",
      "layer   2  Sparsity: 71.0910%\n",
      "layer   3  Sparsity: 70.5611%\n",
      "total_backward_count 186010 real_backward_count 20258  10.891%\n",
      "lif layer 1 self.abs_max_v: 18844.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.414564/  1.771545, val:  47.92%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6995%\n",
      "layer   2  Sparsity: 71.0178%\n",
      "layer   3  Sparsity: 69.7241%\n",
      "total_backward_count 195800 real_backward_count 21163  10.808%\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.427207/  1.710763, val:  59.58%, val_best:  65.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.07 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6923%\n",
      "layer   2  Sparsity: 70.8723%\n",
      "layer   3  Sparsity: 70.4088%\n",
      "total_backward_count 205590 real_backward_count 22022  10.712%\n",
      "fc layer 1 self.abs_max_out: 11191.0\n",
      "lif layer 1 self.abs_max_v: 19599.5\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.439918/  1.727193, val:  65.83%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.05 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6990%\n",
      "layer   2  Sparsity: 70.6882%\n",
      "layer   3  Sparsity: 70.1536%\n",
      "total_backward_count 215380 real_backward_count 22995  10.676%\n",
      "lif layer 2 self.abs_max_v: 9390.0\n",
      "fc layer 1 self.abs_max_out: 11194.0\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.446884/  1.693234, val:  63.33%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.38 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6818%\n",
      "layer   2  Sparsity: 70.2458%\n",
      "layer   3  Sparsity: 69.8841%\n",
      "total_backward_count 225170 real_backward_count 23869  10.600%\n",
      "fc layer 3 self.abs_max_out: 1099.0\n",
      "lif layer 2 self.abs_max_v: 9508.5\n",
      "lif layer 2 self.abs_max_v: 9817.5\n",
      "lif layer 2 self.abs_max_v: 10128.5\n",
      "lif layer 2 self.abs_max_v: 10130.0\n",
      "fc layer 3 self.abs_max_out: 1123.0\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.423401/  1.710299, val:  56.67%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6926%\n",
      "layer   2  Sparsity: 70.2448%\n",
      "layer   3  Sparsity: 70.3459%\n",
      "total_backward_count 234960 real_backward_count 24753  10.535%\n",
      "fc layer 1 self.abs_max_out: 11797.0\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.434646/  1.735534, val:  63.33%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.28 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6939%\n",
      "layer   2  Sparsity: 70.4133%\n",
      "layer   3  Sparsity: 70.9220%\n",
      "total_backward_count 244750 real_backward_count 25620  10.468%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.448933/  1.688882, val:  72.08%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.14 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6770%\n",
      "layer   2  Sparsity: 69.9801%\n",
      "layer   3  Sparsity: 71.5201%\n",
      "total_backward_count 254540 real_backward_count 26548  10.430%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.438186/  1.707880, val:  65.42%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6921%\n",
      "layer   2  Sparsity: 70.5997%\n",
      "layer   3  Sparsity: 72.2660%\n",
      "total_backward_count 264330 real_backward_count 27354  10.348%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.439195/  1.720175, val:  71.25%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6957%\n",
      "layer   2  Sparsity: 70.4311%\n",
      "layer   3  Sparsity: 72.0192%\n",
      "total_backward_count 274120 real_backward_count 28208  10.290%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.455269/  1.703625, val:  62.50%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.63 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6835%\n",
      "layer   2  Sparsity: 69.9323%\n",
      "layer   3  Sparsity: 71.6974%\n",
      "total_backward_count 283910 real_backward_count 29043  10.230%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.449552/  1.715847, val:  55.83%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6936%\n",
      "layer   2  Sparsity: 70.2962%\n",
      "layer   3  Sparsity: 72.9965%\n",
      "total_backward_count 293700 real_backward_count 29871  10.171%\n",
      "fc layer 1 self.abs_max_out: 12078.0\n",
      "fc layer 2 self.abs_max_out: 5772.0\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.443879/  1.739529, val:  65.83%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.02 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6946%\n",
      "layer   2  Sparsity: 70.1184%\n",
      "layer   3  Sparsity: 72.7046%\n",
      "total_backward_count 303490 real_backward_count 30700  10.116%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.466241/  1.714858, val:  59.17%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7033%\n",
      "layer   2  Sparsity: 70.3011%\n",
      "layer   3  Sparsity: 72.4779%\n",
      "total_backward_count 313280 real_backward_count 31498  10.054%\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.445582/  1.740884, val:  56.67%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6875%\n",
      "layer   2  Sparsity: 69.8421%\n",
      "layer   3  Sparsity: 71.3550%\n",
      "total_backward_count 323070 real_backward_count 32291   9.995%\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.481119/  1.717128, val:  61.67%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.02 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6954%\n",
      "layer   2  Sparsity: 69.4879%\n",
      "layer   3  Sparsity: 73.3017%\n",
      "total_backward_count 332860 real_backward_count 33125   9.952%\n",
      "lif layer 1 self.abs_max_v: 19779.5\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.483016/  1.758464, val:  60.00%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6787%\n",
      "layer   2  Sparsity: 69.6776%\n",
      "layer   3  Sparsity: 73.4341%\n",
      "total_backward_count 342650 real_backward_count 33953   9.909%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.467024/  1.715703, val:  62.50%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6980%\n",
      "layer   2  Sparsity: 69.4259%\n",
      "layer   3  Sparsity: 73.7867%\n",
      "total_backward_count 352440 real_backward_count 34732   9.855%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.466242/  1.683998, val:  69.17%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6944%\n",
      "layer   2  Sparsity: 69.7995%\n",
      "layer   3  Sparsity: 73.7132%\n",
      "total_backward_count 362230 real_backward_count 35511   9.803%\n",
      "lif layer 1 self.abs_max_v: 19790.0\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.424986/  1.703355, val:  59.17%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6825%\n",
      "layer   2  Sparsity: 69.4860%\n",
      "layer   3  Sparsity: 71.9341%\n",
      "total_backward_count 372020 real_backward_count 36271   9.750%\n",
      "lif layer 1 self.abs_max_v: 20070.0\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.423194/  1.682682, val:  68.75%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.7050%\n",
      "layer   2  Sparsity: 69.1799%\n",
      "layer   3  Sparsity: 72.3991%\n",
      "total_backward_count 381810 real_backward_count 37041   9.701%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.398512/  1.662665, val:  70.83%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.81 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6767%\n",
      "layer   2  Sparsity: 69.5906%\n",
      "layer   3  Sparsity: 73.0388%\n",
      "total_backward_count 391600 real_backward_count 37809   9.655%\n",
      "lif layer 1 self.abs_max_v: 20851.0\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.419593/  1.691645, val:  57.50%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.22 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6900%\n",
      "layer   2  Sparsity: 69.5086%\n",
      "layer   3  Sparsity: 74.6460%\n",
      "total_backward_count 401390 real_backward_count 38636   9.626%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.462772/  1.700138, val:  69.58%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.81 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6958%\n",
      "layer   2  Sparsity: 68.8660%\n",
      "layer   3  Sparsity: 74.4482%\n",
      "total_backward_count 411180 real_backward_count 39419   9.587%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.434697/  1.670970, val:  62.50%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6953%\n",
      "layer   2  Sparsity: 68.7509%\n",
      "layer   3  Sparsity: 72.7717%\n",
      "total_backward_count 420970 real_backward_count 40207   9.551%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.464366/  1.685993, val:  72.08%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6903%\n",
      "layer   2  Sparsity: 68.3812%\n",
      "layer   3  Sparsity: 73.7245%\n",
      "total_backward_count 430760 real_backward_count 40962   9.509%\n",
      "fc layer 1 self.abs_max_out: 12250.0\n",
      "fc layer 1 self.abs_max_out: 12253.0\n",
      "lif layer 1 self.abs_max_v: 21141.0\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.444465/  1.698373, val:  63.33%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6935%\n",
      "layer   2  Sparsity: 68.6333%\n",
      "layer   3  Sparsity: 73.5534%\n",
      "total_backward_count 440550 real_backward_count 41704   9.466%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.452325/  1.675564, val:  64.17%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.34 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.7040%\n",
      "layer   2  Sparsity: 68.1582%\n",
      "layer   3  Sparsity: 72.8207%\n",
      "total_backward_count 450340 real_backward_count 42494   9.436%\n",
      "fc layer 1 self.abs_max_out: 12339.0\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.446749/  1.692683, val:  63.33%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.28 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6901%\n",
      "layer   2  Sparsity: 67.7233%\n",
      "layer   3  Sparsity: 73.5146%\n",
      "total_backward_count 460130 real_backward_count 43206   9.390%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.435184/  1.709024, val:  59.58%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.39 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.7016%\n",
      "layer   2  Sparsity: 67.6463%\n",
      "layer   3  Sparsity: 74.0394%\n",
      "total_backward_count 469920 real_backward_count 43937   9.350%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.440695/  1.662827, val:  66.67%, val_best:  72.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7048%\n",
      "layer   2  Sparsity: 67.5683%\n",
      "layer   3  Sparsity: 73.7489%\n",
      "total_backward_count 479710 real_backward_count 44664   9.311%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.439698/  1.688523, val:  64.58%, val_best:  72.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.07 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6866%\n",
      "layer   2  Sparsity: 67.7527%\n",
      "layer   3  Sparsity: 73.4829%\n",
      "total_backward_count 489500 real_backward_count 45375   9.270%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.430451/  1.727045, val:  58.33%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6985%\n",
      "layer   2  Sparsity: 67.4061%\n",
      "layer   3  Sparsity: 72.5096%\n",
      "total_backward_count 499290 real_backward_count 46153   9.244%\n",
      "fc layer 1 self.abs_max_out: 12790.0\n",
      "lif layer 1 self.abs_max_v: 21793.0\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.421642/  1.684565, val:  69.58%, val_best:  72.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6818%\n",
      "layer   2  Sparsity: 67.9275%\n",
      "layer   3  Sparsity: 73.5887%\n",
      "total_backward_count 509080 real_backward_count 46871   9.207%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.429617/  1.657697, val:  78.75%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.54 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6952%\n",
      "layer   2  Sparsity: 68.1133%\n",
      "layer   3  Sparsity: 73.1574%\n",
      "total_backward_count 518870 real_backward_count 47591   9.172%\n",
      "fc layer 1 self.abs_max_out: 12795.0\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.406816/  1.665536, val:  62.92%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.80 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6849%\n",
      "layer   2  Sparsity: 68.4779%\n",
      "layer   3  Sparsity: 73.0198%\n",
      "total_backward_count 528660 real_backward_count 48287   9.134%\n",
      "fc layer 3 self.abs_max_out: 1134.0\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.377278/  1.636693, val:  68.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.80 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6902%\n",
      "layer   2  Sparsity: 67.9066%\n",
      "layer   3  Sparsity: 71.4864%\n",
      "total_backward_count 538450 real_backward_count 49004   9.101%\n",
      "fc layer 1 self.abs_max_out: 12796.0\n",
      "lif layer 1 self.abs_max_v: 21905.5\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.382718/  1.652279, val:  70.00%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.85 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6891%\n",
      "layer   2  Sparsity: 67.8932%\n",
      "layer   3  Sparsity: 71.6864%\n",
      "total_backward_count 548240 real_backward_count 49707   9.067%\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.383356/  1.653893, val:  60.83%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6974%\n",
      "layer   2  Sparsity: 67.9197%\n",
      "layer   3  Sparsity: 72.3103%\n",
      "total_backward_count 558030 real_backward_count 50425   9.036%\n",
      "fc layer 3 self.abs_max_out: 1147.0\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.368999/  1.644888, val:  67.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6737%\n",
      "layer   2  Sparsity: 67.6352%\n",
      "layer   3  Sparsity: 73.0226%\n",
      "total_backward_count 567820 real_backward_count 51108   9.001%\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.365788/  1.628396, val:  71.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.34 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6809%\n",
      "layer   2  Sparsity: 67.4896%\n",
      "layer   3  Sparsity: 71.9030%\n",
      "total_backward_count 577610 real_backward_count 51793   8.967%\n",
      "fc layer 1 self.abs_max_out: 13277.0\n",
      "lif layer 1 self.abs_max_v: 22801.5\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.367550/  1.637776, val:  68.75%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.92 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7045%\n",
      "layer   2  Sparsity: 68.1280%\n",
      "layer   3  Sparsity: 71.3968%\n",
      "total_backward_count 587400 real_backward_count 52518   8.941%\n",
      "fc layer 3 self.abs_max_out: 1167.0\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.371327/  1.668934, val:  57.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.37 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6860%\n",
      "layer   2  Sparsity: 67.7321%\n",
      "layer   3  Sparsity: 72.8070%\n",
      "total_backward_count 597190 real_backward_count 53220   8.912%\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.374324/  1.627513, val:  70.42%, val_best:  78.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6895%\n",
      "layer   2  Sparsity: 67.8759%\n",
      "layer   3  Sparsity: 72.5224%\n",
      "total_backward_count 606980 real_backward_count 53911   8.882%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.380021/  1.661096, val:  64.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6934%\n",
      "layer   2  Sparsity: 67.8469%\n",
      "layer   3  Sparsity: 73.1838%\n",
      "total_backward_count 616770 real_backward_count 54606   8.854%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.409641/  1.649314, val:  67.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6951%\n",
      "layer   2  Sparsity: 67.5747%\n",
      "layer   3  Sparsity: 73.7715%\n",
      "total_backward_count 626560 real_backward_count 55286   8.824%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.394012/  1.646663, val:  69.58%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6867%\n",
      "layer   2  Sparsity: 67.6407%\n",
      "layer   3  Sparsity: 73.5015%\n",
      "total_backward_count 636350 real_backward_count 55977   8.797%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.360760/  1.638800, val:  72.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6880%\n",
      "layer   2  Sparsity: 67.2541%\n",
      "layer   3  Sparsity: 72.6244%\n",
      "total_backward_count 646140 real_backward_count 56634   8.765%\n",
      "fc layer 1 self.abs_max_out: 13413.0\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.365433/  1.660761, val:  65.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6873%\n",
      "layer   2  Sparsity: 66.9062%\n",
      "layer   3  Sparsity: 72.9123%\n",
      "total_backward_count 655930 real_backward_count 57258   8.729%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.389410/  1.669410, val:  53.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7114%\n",
      "layer   2  Sparsity: 67.1762%\n",
      "layer   3  Sparsity: 73.2792%\n",
      "total_backward_count 665720 real_backward_count 57964   8.707%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.376591/  1.638096, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.31 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6774%\n",
      "layer   2  Sparsity: 67.2177%\n",
      "layer   3  Sparsity: 73.1379%\n",
      "total_backward_count 675510 real_backward_count 58626   8.679%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.369656/  1.648921, val:  64.17%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.50 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6998%\n",
      "layer   2  Sparsity: 67.5607%\n",
      "layer   3  Sparsity: 74.0190%\n",
      "total_backward_count 685300 real_backward_count 59297   8.653%\n",
      "fc layer 1 self.abs_max_out: 13550.0\n",
      "lif layer 1 self.abs_max_v: 22813.0\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.372877/  1.622619, val:  68.33%, val_best:  78.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.87 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6881%\n",
      "layer   2  Sparsity: 67.5347%\n",
      "layer   3  Sparsity: 74.4227%\n",
      "total_backward_count 695090 real_backward_count 59998   8.632%\n",
      "fc layer 1 self.abs_max_out: 13998.0\n",
      "lif layer 1 self.abs_max_v: 23661.5\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.357410/  1.630843, val:  63.33%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7142%\n",
      "layer   2  Sparsity: 67.2954%\n",
      "layer   3  Sparsity: 73.4170%\n",
      "total_backward_count 704880 real_backward_count 60659   8.606%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.362555/  1.592416, val:  76.67%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.45 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6837%\n",
      "layer   2  Sparsity: 67.2202%\n",
      "layer   3  Sparsity: 72.6596%\n",
      "total_backward_count 714670 real_backward_count 61384   8.589%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.316892/  1.628668, val:  63.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6894%\n",
      "layer   2  Sparsity: 67.3817%\n",
      "layer   3  Sparsity: 72.4347%\n",
      "total_backward_count 724460 real_backward_count 62010   8.559%\n",
      "fc layer 3 self.abs_max_out: 1185.0\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.325900/  1.606964, val:  61.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6867%\n",
      "layer   2  Sparsity: 67.3870%\n",
      "layer   3  Sparsity: 72.3881%\n",
      "total_backward_count 734250 real_backward_count 62710   8.541%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.313260/  1.588047, val:  69.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.86 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6697%\n",
      "layer   2  Sparsity: 66.9742%\n",
      "layer   3  Sparsity: 72.5785%\n",
      "total_backward_count 744040 real_backward_count 63370   8.517%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.307259/  1.586578, val:  66.25%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7138%\n",
      "layer   2  Sparsity: 67.1630%\n",
      "layer   3  Sparsity: 72.5871%\n",
      "total_backward_count 753830 real_backward_count 63981   8.487%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.313796/  1.609259, val:  62.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7002%\n",
      "layer   2  Sparsity: 66.8604%\n",
      "layer   3  Sparsity: 72.2948%\n",
      "total_backward_count 763620 real_backward_count 64616   8.462%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.309884/  1.595499, val:  67.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6975%\n",
      "layer   2  Sparsity: 67.2354%\n",
      "layer   3  Sparsity: 71.8541%\n",
      "total_backward_count 773410 real_backward_count 65220   8.433%\n",
      "fc layer 3 self.abs_max_out: 1203.0\n",
      "lif layer 1 self.abs_max_v: 23911.0\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.316376/  1.603621, val:  61.25%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.83 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6873%\n",
      "layer   2  Sparsity: 67.3602%\n",
      "layer   3  Sparsity: 71.8503%\n",
      "total_backward_count 783200 real_backward_count 65857   8.409%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.320302/  1.596091, val:  70.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.18 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6951%\n",
      "layer   2  Sparsity: 66.8427%\n",
      "layer   3  Sparsity: 72.0648%\n",
      "total_backward_count 792990 real_backward_count 66467   8.382%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.346586/  1.627172, val:  62.50%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7025%\n",
      "layer   2  Sparsity: 66.5155%\n",
      "layer   3  Sparsity: 72.0913%\n",
      "total_backward_count 802780 real_backward_count 67138   8.363%\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.334971/  1.628672, val:  70.00%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6824%\n",
      "layer   2  Sparsity: 66.6800%\n",
      "layer   3  Sparsity: 71.9632%\n",
      "total_backward_count 812570 real_backward_count 67809   8.345%\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.314531/  1.606984, val:  68.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6980%\n",
      "layer   2  Sparsity: 66.9591%\n",
      "layer   3  Sparsity: 72.1934%\n",
      "total_backward_count 822360 real_backward_count 68435   8.322%\n",
      "lif layer 2 self.abs_max_v: 10334.0\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.312293/  1.593307, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6932%\n",
      "layer   2  Sparsity: 66.8680%\n",
      "layer   3  Sparsity: 72.2422%\n",
      "total_backward_count 832150 real_backward_count 69094   8.303%\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.321380/  1.630835, val:  64.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7034%\n",
      "layer   2  Sparsity: 66.7132%\n",
      "layer   3  Sparsity: 72.3189%\n",
      "total_backward_count 841940 real_backward_count 69732   8.282%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.321542/  1.585384, val:  70.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6828%\n",
      "layer   2  Sparsity: 66.7953%\n",
      "layer   3  Sparsity: 72.6591%\n",
      "total_backward_count 851730 real_backward_count 70350   8.260%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.307860/  1.599349, val:  59.17%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6963%\n",
      "layer   2  Sparsity: 66.8328%\n",
      "layer   3  Sparsity: 72.7887%\n",
      "total_backward_count 861520 real_backward_count 70984   8.239%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.299510/  1.574754, val:  65.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6915%\n",
      "layer   2  Sparsity: 65.9532%\n",
      "layer   3  Sparsity: 70.9190%\n",
      "total_backward_count 871310 real_backward_count 71654   8.224%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.295000/  1.619925, val:  71.67%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.05 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6950%\n",
      "layer   2  Sparsity: 66.6122%\n",
      "layer   3  Sparsity: 71.1561%\n",
      "total_backward_count 881100 real_backward_count 72316   8.207%\n",
      "fc layer 1 self.abs_max_out: 14022.0\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.318288/  1.587219, val:  73.75%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.82 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6934%\n",
      "layer   2  Sparsity: 66.9380%\n",
      "layer   3  Sparsity: 72.2253%\n",
      "total_backward_count 890890 real_backward_count 72991   8.193%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.306541/  1.566832, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7000%\n",
      "layer   2  Sparsity: 66.2358%\n",
      "layer   3  Sparsity: 71.9026%\n",
      "total_backward_count 900680 real_backward_count 73608   8.172%\n",
      "fc layer 1 self.abs_max_out: 14488.0\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.298398/  1.572103, val:  71.25%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6828%\n",
      "layer   2  Sparsity: 66.7436%\n",
      "layer   3  Sparsity: 71.9790%\n",
      "total_backward_count 910470 real_backward_count 74230   8.153%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.310659/  1.565485, val:  75.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6996%\n",
      "layer   2  Sparsity: 66.5274%\n",
      "layer   3  Sparsity: 70.6321%\n",
      "total_backward_count 920260 real_backward_count 74871   8.136%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.295242/  1.575523, val:  68.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6987%\n",
      "layer   2  Sparsity: 66.6486%\n",
      "layer   3  Sparsity: 69.6015%\n",
      "total_backward_count 930050 real_backward_count 75485   8.116%\n",
      "lif layer 1 self.abs_max_v: 23969.5\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.290505/  1.620572, val:  65.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.64 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6853%\n",
      "layer   2  Sparsity: 66.2171%\n",
      "layer   3  Sparsity: 71.4011%\n",
      "total_backward_count 939840 real_backward_count 76136   8.101%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.286037/  1.589915, val:  72.50%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7055%\n",
      "layer   2  Sparsity: 66.3438%\n",
      "layer   3  Sparsity: 70.1577%\n",
      "total_backward_count 949630 real_backward_count 76794   8.087%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.262354/  1.573927, val:  69.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.34 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6899%\n",
      "layer   2  Sparsity: 66.5417%\n",
      "layer   3  Sparsity: 70.5777%\n",
      "total_backward_count 959420 real_backward_count 77396   8.067%\n",
      "lif layer 2 self.abs_max_v: 10568.0\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.262771/  1.603595, val:  65.83%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6731%\n",
      "layer   2  Sparsity: 65.9018%\n",
      "layer   3  Sparsity: 71.4317%\n",
      "total_backward_count 969210 real_backward_count 77984   8.046%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.285133/  1.588143, val:  68.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.68 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7011%\n",
      "layer   2  Sparsity: 66.7541%\n",
      "layer   3  Sparsity: 71.8825%\n",
      "total_backward_count 979000 real_backward_count 78662   8.035%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.280699/  1.573225, val:  76.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.84 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6967%\n",
      "layer   2  Sparsity: 66.5319%\n",
      "layer   3  Sparsity: 71.7968%\n",
      "total_backward_count 988790 real_backward_count 79245   8.014%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.297003/  1.574000, val:  71.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6866%\n",
      "layer   2  Sparsity: 66.4078%\n",
      "layer   3  Sparsity: 71.0495%\n",
      "total_backward_count 998580 real_backward_count 79888   8.000%\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.294034/  1.566938, val:  72.92%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6847%\n",
      "layer   2  Sparsity: 66.8390%\n",
      "layer   3  Sparsity: 70.9107%\n",
      "total_backward_count 1008370 real_backward_count 80517   7.985%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.265659/  1.566670, val:  72.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6861%\n",
      "layer   2  Sparsity: 66.2895%\n",
      "layer   3  Sparsity: 70.1244%\n",
      "total_backward_count 1018160 real_backward_count 81116   7.967%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.283201/  1.582412, val:  67.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.7124%\n",
      "layer   2  Sparsity: 65.9627%\n",
      "layer   3  Sparsity: 69.8477%\n",
      "total_backward_count 1027950 real_backward_count 81750   7.953%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.286352/  1.558804, val:  70.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.66 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6865%\n",
      "layer   2  Sparsity: 66.4183%\n",
      "layer   3  Sparsity: 70.1639%\n",
      "total_backward_count 1037740 real_backward_count 82347   7.935%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.254420/  1.595417, val:  56.67%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6992%\n",
      "layer   2  Sparsity: 66.8151%\n",
      "layer   3  Sparsity: 69.9255%\n",
      "total_backward_count 1047530 real_backward_count 82953   7.919%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.261001/  1.588664, val:  64.17%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.29 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6824%\n",
      "layer   2  Sparsity: 66.1876%\n",
      "layer   3  Sparsity: 69.8563%\n",
      "total_backward_count 1057320 real_backward_count 83577   7.905%\n",
      "fc layer 3 self.abs_max_out: 1240.0\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.241849/  1.525006, val:  72.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7151%\n",
      "layer   2  Sparsity: 66.1468%\n",
      "layer   3  Sparsity: 69.2260%\n",
      "total_backward_count 1067110 real_backward_count 84193   7.890%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.221719/  1.521603, val:  73.75%, val_best:  80.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.17 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6849%\n",
      "layer   2  Sparsity: 66.3169%\n",
      "layer   3  Sparsity: 70.7528%\n",
      "total_backward_count 1076900 real_backward_count 84778   7.872%\n",
      "fc layer 3 self.abs_max_out: 1244.0\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.228546/  1.525030, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6908%\n",
      "layer   2  Sparsity: 66.3982%\n",
      "layer   3  Sparsity: 71.1401%\n",
      "total_backward_count 1086690 real_backward_count 85375   7.856%\n",
      "fc layer 3 self.abs_max_out: 1306.0\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.230673/  1.519074, val:  69.17%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6959%\n",
      "layer   2  Sparsity: 66.4440%\n",
      "layer   3  Sparsity: 70.7586%\n",
      "total_backward_count 1096480 real_backward_count 86007   7.844%\n",
      "lif layer 1 self.abs_max_v: 23982.5\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.247236/  1.534221, val:  75.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.25 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6857%\n",
      "layer   2  Sparsity: 66.2388%\n",
      "layer   3  Sparsity: 70.2534%\n",
      "total_backward_count 1106270 real_backward_count 86605   7.829%\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.247892/  1.533931, val:  68.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6929%\n",
      "layer   2  Sparsity: 66.1104%\n",
      "layer   3  Sparsity: 69.5546%\n",
      "total_backward_count 1116060 real_backward_count 87189   7.812%\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.225975/  1.555728, val:  70.42%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.17 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6995%\n",
      "layer   2  Sparsity: 66.2409%\n",
      "layer   3  Sparsity: 69.2133%\n",
      "total_backward_count 1125850 real_backward_count 87772   7.796%\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.250425/  1.546834, val:  77.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6882%\n",
      "layer   2  Sparsity: 66.3950%\n",
      "layer   3  Sparsity: 70.2900%\n",
      "total_backward_count 1135640 real_backward_count 88335   7.778%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.241909/  1.538718, val:  57.50%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6740%\n",
      "layer   2  Sparsity: 66.1731%\n",
      "layer   3  Sparsity: 70.3199%\n",
      "total_backward_count 1145430 real_backward_count 89010   7.771%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.224169/  1.518171, val:  66.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6877%\n",
      "layer   2  Sparsity: 66.3972%\n",
      "layer   3  Sparsity: 70.6978%\n",
      "total_backward_count 1155220 real_backward_count 89621   7.758%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.241665/  1.537075, val:  73.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6940%\n",
      "layer   2  Sparsity: 66.2323%\n",
      "layer   3  Sparsity: 71.2654%\n",
      "total_backward_count 1165010 real_backward_count 90216   7.744%\n",
      "fc layer 3 self.abs_max_out: 1315.0\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.228426/  1.490644, val:  73.33%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7044%\n",
      "layer   2  Sparsity: 66.0858%\n",
      "layer   3  Sparsity: 70.8303%\n",
      "total_backward_count 1174800 real_backward_count 90764   7.726%\n",
      "fc layer 1 self.abs_max_out: 14840.0\n",
      "lif layer 1 self.abs_max_v: 24252.0\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.199974/  1.572040, val:  65.42%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6945%\n",
      "layer   2  Sparsity: 66.5488%\n",
      "layer   3  Sparsity: 70.3312%\n",
      "total_backward_count 1184590 real_backward_count 91370   7.713%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.204712/  1.502009, val:  72.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6721%\n",
      "layer   2  Sparsity: 66.7532%\n",
      "layer   3  Sparsity: 69.7831%\n",
      "total_backward_count 1194380 real_backward_count 91965   7.700%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.242467/  1.578979, val:  71.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7023%\n",
      "layer   2  Sparsity: 66.1029%\n",
      "layer   3  Sparsity: 70.1521%\n",
      "total_backward_count 1204170 real_backward_count 92554   7.686%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.233261/  1.522929, val:  75.42%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6972%\n",
      "layer   2  Sparsity: 65.5262%\n",
      "layer   3  Sparsity: 68.7287%\n",
      "total_backward_count 1213960 real_backward_count 93158   7.674%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.227141/  1.528531, val:  70.42%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.87 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6951%\n",
      "layer   2  Sparsity: 66.0535%\n",
      "layer   3  Sparsity: 69.7262%\n",
      "total_backward_count 1223750 real_backward_count 93803   7.665%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.220901/  1.505201, val:  71.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6980%\n",
      "layer   2  Sparsity: 66.7978%\n",
      "layer   3  Sparsity: 70.2179%\n",
      "total_backward_count 1233540 real_backward_count 94371   7.650%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.216851/  1.556157, val:  69.17%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6977%\n",
      "layer   2  Sparsity: 66.2799%\n",
      "layer   3  Sparsity: 70.0274%\n",
      "total_backward_count 1243330 real_backward_count 94985   7.640%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.206259/  1.483794, val:  71.25%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6908%\n",
      "layer   2  Sparsity: 66.6805%\n",
      "layer   3  Sparsity: 69.4706%\n",
      "total_backward_count 1253120 real_backward_count 95579   7.627%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.168641/  1.555108, val:  71.25%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6784%\n",
      "layer   2  Sparsity: 66.7381%\n",
      "layer   3  Sparsity: 69.5096%\n",
      "total_backward_count 1262910 real_backward_count 96169   7.615%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.207123/  1.506476, val:  72.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.07 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6979%\n",
      "layer   2  Sparsity: 66.1168%\n",
      "layer   3  Sparsity: 69.4544%\n",
      "total_backward_count 1272700 real_backward_count 96783   7.605%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.209954/  1.516545, val:  67.92%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6951%\n",
      "layer   2  Sparsity: 65.9970%\n",
      "layer   3  Sparsity: 71.2149%\n",
      "total_backward_count 1282490 real_backward_count 97394   7.594%\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.210816/  1.526939, val:  72.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6965%\n",
      "layer   2  Sparsity: 66.2586%\n",
      "layer   3  Sparsity: 70.3757%\n",
      "total_backward_count 1292280 real_backward_count 97996   7.583%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.196381/  1.500614, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.08 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6878%\n",
      "layer   2  Sparsity: 66.3493%\n",
      "layer   3  Sparsity: 69.7403%\n",
      "total_backward_count 1302070 real_backward_count 98566   7.570%\n",
      "fc layer 1 self.abs_max_out: 14922.0\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.190350/  1.526754, val:  72.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.54 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7021%\n",
      "layer   2  Sparsity: 66.5801%\n",
      "layer   3  Sparsity: 70.2544%\n",
      "total_backward_count 1311860 real_backward_count 99127   7.556%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.194617/  1.498306, val:  75.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6975%\n",
      "layer   2  Sparsity: 66.1322%\n",
      "layer   3  Sparsity: 69.9728%\n",
      "total_backward_count 1321650 real_backward_count 99724   7.545%\n",
      "fc layer 3 self.abs_max_out: 1316.0\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.209668/  1.516291, val:  65.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6869%\n",
      "layer   2  Sparsity: 66.3847%\n",
      "layer   3  Sparsity: 70.8499%\n",
      "total_backward_count 1331440 real_backward_count 100364   7.538%\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.180825/  1.470957, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6938%\n",
      "layer   2  Sparsity: 66.3740%\n",
      "layer   3  Sparsity: 70.0858%\n",
      "total_backward_count 1341230 real_backward_count 100903   7.523%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.167444/  1.494394, val:  67.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.23 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6935%\n",
      "layer   2  Sparsity: 66.0884%\n",
      "layer   3  Sparsity: 69.5331%\n",
      "total_backward_count 1351020 real_backward_count 101474   7.511%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.173425/  1.515029, val:  62.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.95 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6965%\n",
      "layer   2  Sparsity: 66.0872%\n",
      "layer   3  Sparsity: 68.8880%\n",
      "total_backward_count 1360810 real_backward_count 102105   7.503%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.200978/  1.545618, val:  68.75%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.92 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6957%\n",
      "layer   2  Sparsity: 65.7158%\n",
      "layer   3  Sparsity: 69.2466%\n",
      "total_backward_count 1370600 real_backward_count 102687   7.492%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.198018/  1.517539, val:  63.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6903%\n",
      "layer   2  Sparsity: 65.9312%\n",
      "layer   3  Sparsity: 68.7491%\n",
      "total_backward_count 1380390 real_backward_count 103275   7.482%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.186663/  1.480626, val:  70.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6798%\n",
      "layer   2  Sparsity: 66.1213%\n",
      "layer   3  Sparsity: 69.5040%\n",
      "total_backward_count 1390180 real_backward_count 103843   7.470%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.177634/  1.506146, val:  67.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.24 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6987%\n",
      "layer   2  Sparsity: 66.3138%\n",
      "layer   3  Sparsity: 70.0006%\n",
      "total_backward_count 1399970 real_backward_count 104430   7.459%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.164305/  1.515613, val:  61.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.7062%\n",
      "layer   2  Sparsity: 66.1730%\n",
      "layer   3  Sparsity: 70.5087%\n",
      "total_backward_count 1409760 real_backward_count 104992   7.448%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.166353/  1.479530, val:  71.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6764%\n",
      "layer   2  Sparsity: 65.6849%\n",
      "layer   3  Sparsity: 69.1771%\n",
      "total_backward_count 1419550 real_backward_count 105547   7.435%\n",
      "fc layer 3 self.abs_max_out: 1346.0\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.169459/  1.517423, val:  64.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.27 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6982%\n",
      "layer   2  Sparsity: 65.9298%\n",
      "layer   3  Sparsity: 68.5701%\n",
      "total_backward_count 1429340 real_backward_count 106109   7.424%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.157156/  1.495857, val:  62.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.91 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6997%\n",
      "layer   2  Sparsity: 66.2856%\n",
      "layer   3  Sparsity: 68.4370%\n",
      "total_backward_count 1439130 real_backward_count 106727   7.416%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.126569/  1.497514, val:  64.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6893%\n",
      "layer   2  Sparsity: 65.8555%\n",
      "layer   3  Sparsity: 68.7815%\n",
      "total_backward_count 1448920 real_backward_count 107323   7.407%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.145307/  1.481632, val:  69.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.7011%\n",
      "layer   2  Sparsity: 65.8840%\n",
      "layer   3  Sparsity: 68.4431%\n",
      "total_backward_count 1458710 real_backward_count 107888   7.396%\n",
      "fc layer 3 self.abs_max_out: 1350.0\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.160616/  1.480858, val:  67.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 89.6884%\n",
      "layer   2  Sparsity: 65.9875%\n",
      "layer   3  Sparsity: 68.4015%\n",
      "total_backward_count 1468500 real_backward_count 108428   7.384%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.148302/  1.499397, val:  76.67%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 89.6878%\n",
      "layer   2  Sparsity: 66.1242%\n",
      "layer   3  Sparsity: 69.3694%\n",
      "total_backward_count 1478290 real_backward_count 109006   7.374%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.192060/  1.538837, val:  62.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6955%\n",
      "layer   2  Sparsity: 66.0488%\n",
      "layer   3  Sparsity: 70.2684%\n",
      "total_backward_count 1488080 real_backward_count 109574   7.363%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.195825/  1.488399, val:  72.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6838%\n",
      "layer   2  Sparsity: 66.0753%\n",
      "layer   3  Sparsity: 70.0007%\n",
      "total_backward_count 1497870 real_backward_count 110134   7.353%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.177629/  1.460058, val:  75.42%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.90 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6878%\n",
      "layer   2  Sparsity: 66.3279%\n",
      "layer   3  Sparsity: 69.7888%\n",
      "total_backward_count 1507660 real_backward_count 110742   7.345%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.118871/  1.443752, val:  71.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.00 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6914%\n",
      "layer   2  Sparsity: 66.2869%\n",
      "layer   3  Sparsity: 68.8440%\n",
      "total_backward_count 1517450 real_backward_count 111290   7.334%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.146378/  1.512141, val:  65.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.62 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6957%\n",
      "layer   2  Sparsity: 66.2271%\n",
      "layer   3  Sparsity: 68.6043%\n",
      "total_backward_count 1527240 real_backward_count 111815   7.321%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.183357/  1.495580, val:  71.67%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6882%\n",
      "layer   2  Sparsity: 65.6399%\n",
      "layer   3  Sparsity: 69.4228%\n",
      "total_backward_count 1537030 real_backward_count 112404   7.313%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.185193/  1.520200, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.23 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7024%\n",
      "layer   2  Sparsity: 65.6376%\n",
      "layer   3  Sparsity: 70.6675%\n",
      "total_backward_count 1546820 real_backward_count 112963   7.303%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.199089/  1.514854, val:  66.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.45 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6899%\n",
      "layer   2  Sparsity: 65.8215%\n",
      "layer   3  Sparsity: 70.7137%\n",
      "total_backward_count 1556610 real_backward_count 113560   7.295%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.192774/  1.512918, val:  72.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.62 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6883%\n",
      "layer   2  Sparsity: 65.6553%\n",
      "layer   3  Sparsity: 70.5320%\n",
      "total_backward_count 1566400 real_backward_count 114159   7.288%\n",
      "fc layer 3 self.abs_max_out: 1379.0\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.195735/  1.489653, val:  68.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6929%\n",
      "layer   2  Sparsity: 65.5551%\n",
      "layer   3  Sparsity: 70.9276%\n",
      "total_backward_count 1576190 real_backward_count 114681   7.276%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.201083/  1.479293, val:  76.25%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.12 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6872%\n",
      "layer   2  Sparsity: 65.5287%\n",
      "layer   3  Sparsity: 69.3998%\n",
      "total_backward_count 1585980 real_backward_count 115273   7.268%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.180422/  1.496182, val:  65.00%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.59 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.7106%\n",
      "layer   2  Sparsity: 65.7588%\n",
      "layer   3  Sparsity: 69.0736%\n",
      "total_backward_count 1595770 real_backward_count 115865   7.261%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.189091/  1.515187, val:  70.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.31 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.7005%\n",
      "layer   2  Sparsity: 65.8173%\n",
      "layer   3  Sparsity: 69.4041%\n",
      "total_backward_count 1605560 real_backward_count 116411   7.250%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.152225/  1.443493, val:  77.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.74 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6865%\n",
      "layer   2  Sparsity: 65.9708%\n",
      "layer   3  Sparsity: 68.7047%\n",
      "total_backward_count 1615350 real_backward_count 116989   7.242%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.165214/  1.485292, val:  62.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6952%\n",
      "layer   2  Sparsity: 66.1256%\n",
      "layer   3  Sparsity: 69.2398%\n",
      "total_backward_count 1625140 real_backward_count 117572   7.235%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.154896/  1.494193, val:  70.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.90 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7042%\n",
      "layer   2  Sparsity: 66.1119%\n",
      "layer   3  Sparsity: 69.3294%\n",
      "total_backward_count 1634930 real_backward_count 118111   7.224%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.174263/  1.514048, val:  62.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6964%\n",
      "layer   2  Sparsity: 65.6130%\n",
      "layer   3  Sparsity: 69.9055%\n",
      "total_backward_count 1644720 real_backward_count 118663   7.215%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.191350/  1.474769, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6751%\n",
      "layer   2  Sparsity: 65.4094%\n",
      "layer   3  Sparsity: 69.9885%\n",
      "total_backward_count 1654510 real_backward_count 119220   7.206%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.161952/  1.457819, val:  66.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.22 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6907%\n",
      "layer   2  Sparsity: 65.3578%\n",
      "layer   3  Sparsity: 69.4613%\n",
      "total_backward_count 1664300 real_backward_count 119765   7.196%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.128372/  1.483657, val:  69.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.31 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6931%\n",
      "layer   2  Sparsity: 66.4219%\n",
      "layer   3  Sparsity: 68.5128%\n",
      "total_backward_count 1674090 real_backward_count 120299   7.186%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.162412/  1.513811, val:  65.83%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6924%\n",
      "layer   2  Sparsity: 66.2409%\n",
      "layer   3  Sparsity: 68.4311%\n",
      "total_backward_count 1683880 real_backward_count 120867   7.178%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.143748/  1.461123, val:  62.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6926%\n",
      "layer   2  Sparsity: 65.6135%\n",
      "layer   3  Sparsity: 67.4779%\n",
      "total_backward_count 1693670 real_backward_count 121440   7.170%\n",
      "lif layer 1 self.abs_max_v: 24467.0\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.137563/  1.451401, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.50 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6814%\n",
      "layer   2  Sparsity: 65.8779%\n",
      "layer   3  Sparsity: 69.6272%\n",
      "total_backward_count 1703460 real_backward_count 122034   7.164%\n",
      "lif layer 1 self.abs_max_v: 24744.0\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.128982/  1.465190, val:  70.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.44 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6997%\n",
      "layer   2  Sparsity: 65.5211%\n",
      "layer   3  Sparsity: 68.7469%\n",
      "total_backward_count 1713250 real_backward_count 122611   7.157%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.128141/  1.417253, val:  77.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.00 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6707%\n",
      "layer   2  Sparsity: 65.7319%\n",
      "layer   3  Sparsity: 68.3522%\n",
      "total_backward_count 1723040 real_backward_count 123234   7.152%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.106715/  1.473923, val:  74.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.7088%\n",
      "layer   2  Sparsity: 65.4318%\n",
      "layer   3  Sparsity: 68.5669%\n",
      "total_backward_count 1732830 real_backward_count 123770   7.143%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.140643/  1.469607, val:  67.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.20 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6907%\n",
      "layer   2  Sparsity: 65.7635%\n",
      "layer   3  Sparsity: 69.7950%\n",
      "total_backward_count 1742620 real_backward_count 124316   7.134%\n",
      "fc layer 1 self.abs_max_out: 14927.0\n",
      "lif layer 1 self.abs_max_v: 25183.0\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.114671/  1.469868, val:  65.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6888%\n",
      "layer   2  Sparsity: 64.9387%\n",
      "layer   3  Sparsity: 68.3913%\n",
      "total_backward_count 1752410 real_backward_count 124913   7.128%\n",
      "fc layer 1 self.abs_max_out: 15025.0\n",
      "lif layer 1 self.abs_max_v: 25667.5\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.132211/  1.484674, val:  68.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.7033%\n",
      "layer   2  Sparsity: 64.6176%\n",
      "layer   3  Sparsity: 69.8322%\n",
      "total_backward_count 1762200 real_backward_count 125531   7.124%\n",
      "fc layer 3 self.abs_max_out: 1381.0\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.105557/  1.419328, val:  73.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.59 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.7096%\n",
      "layer   2  Sparsity: 65.2723%\n",
      "layer   3  Sparsity: 69.7586%\n",
      "total_backward_count 1771990 real_backward_count 126070   7.115%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.119791/  1.451039, val:  70.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.40 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6935%\n",
      "layer   2  Sparsity: 65.9131%\n",
      "layer   3  Sparsity: 70.1436%\n",
      "total_backward_count 1781780 real_backward_count 126628   7.107%\n",
      "fc layer 3 self.abs_max_out: 1431.0\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.118486/  1.441215, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.29 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6930%\n",
      "layer   2  Sparsity: 65.7080%\n",
      "layer   3  Sparsity: 69.3278%\n",
      "total_backward_count 1791570 real_backward_count 127171   7.098%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.128888/  1.459454, val:  62.50%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6878%\n",
      "layer   2  Sparsity: 65.3311%\n",
      "layer   3  Sparsity: 67.9135%\n",
      "total_backward_count 1801360 real_backward_count 127746   7.092%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.107904/  1.496474, val:  68.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6890%\n",
      "layer   2  Sparsity: 65.6048%\n",
      "layer   3  Sparsity: 67.6455%\n",
      "total_backward_count 1811150 real_backward_count 128297   7.084%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.126476/  1.442878, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.49 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.7013%\n",
      "layer   2  Sparsity: 65.5339%\n",
      "layer   3  Sparsity: 68.9503%\n",
      "total_backward_count 1820940 real_backward_count 128846   7.076%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.116499/  1.467142, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6976%\n",
      "layer   2  Sparsity: 65.3628%\n",
      "layer   3  Sparsity: 69.3670%\n",
      "total_backward_count 1830730 real_backward_count 129357   7.066%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.120289/  1.452170, val:  75.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6917%\n",
      "layer   2  Sparsity: 65.3023%\n",
      "layer   3  Sparsity: 69.4062%\n",
      "total_backward_count 1840520 real_backward_count 129897   7.058%\n",
      "fc layer 1 self.abs_max_out: 15243.0\n",
      "lif layer 1 self.abs_max_v: 26294.0\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.107821/  1.427502, val:  77.08%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.49 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6830%\n",
      "layer   2  Sparsity: 65.7463%\n",
      "layer   3  Sparsity: 69.3840%\n",
      "total_backward_count 1850310 real_backward_count 130470   7.051%\n",
      "fc layer 3 self.abs_max_out: 1440.0\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.114878/  1.452101, val:  68.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.75 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6943%\n",
      "layer   2  Sparsity: 65.6035%\n",
      "layer   3  Sparsity: 69.0554%\n",
      "total_backward_count 1860100 real_backward_count 131018   7.044%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.105243/  1.447012, val:  74.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7027%\n",
      "layer   2  Sparsity: 65.4849%\n",
      "layer   3  Sparsity: 67.8091%\n",
      "total_backward_count 1869890 real_backward_count 131569   7.036%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.125065/  1.510570, val:  59.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6940%\n",
      "layer   2  Sparsity: 64.9375%\n",
      "layer   3  Sparsity: 69.2958%\n",
      "total_backward_count 1879680 real_backward_count 132128   7.029%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.111472/  1.460503, val:  74.17%, val_best:  80.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.45 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6765%\n",
      "layer   2  Sparsity: 65.1767%\n",
      "layer   3  Sparsity: 68.6537%\n",
      "total_backward_count 1889470 real_backward_count 132691   7.023%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.092449/  1.436539, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.7063%\n",
      "layer   2  Sparsity: 65.4870%\n",
      "layer   3  Sparsity: 69.1096%\n",
      "total_backward_count 1899260 real_backward_count 133245   7.016%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.118007/  1.439542, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.36 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.7003%\n",
      "layer   2  Sparsity: 65.3322%\n",
      "layer   3  Sparsity: 68.6949%\n",
      "total_backward_count 1909050 real_backward_count 133791   7.008%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.120791/  1.505641, val:  56.25%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.24 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6998%\n",
      "layer   2  Sparsity: 65.5508%\n",
      "layer   3  Sparsity: 69.1789%\n",
      "total_backward_count 1918840 real_backward_count 134355   7.002%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.140903/  1.498395, val:  61.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6852%\n",
      "layer   2  Sparsity: 65.1744%\n",
      "layer   3  Sparsity: 68.1581%\n",
      "total_backward_count 1928630 real_backward_count 134879   6.994%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.127733/  1.455879, val:  76.67%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6912%\n",
      "layer   2  Sparsity: 64.7413%\n",
      "layer   3  Sparsity: 68.1561%\n",
      "total_backward_count 1938420 real_backward_count 135437   6.987%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.104253/  1.467084, val:  65.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 89.6897%\n",
      "layer   2  Sparsity: 65.5388%\n",
      "layer   3  Sparsity: 66.8267%\n",
      "total_backward_count 1948210 real_backward_count 136003   6.981%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.118303/  1.510708, val:  67.92%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.7003%\n",
      "layer   2  Sparsity: 64.9742%\n",
      "layer   3  Sparsity: 68.8976%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936cd8bfe53546bb8c544682747be76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñá‚ñà‚ñÖ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñá‚ñà‚ñÖ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.1183</td></tr><tr><td>val_acc_best</td><td>0.80833</td></tr><tr><td>val_acc_now</td><td>0.67917</td></tr><tr><td>val_loss</td><td>1.51071</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devout-sweep-110</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a7elew9p' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/a7elew9p</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_054739-a7elew9p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7omq4uvm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00390625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_095500-7omq4uvm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7omq4uvm' target=\"_blank\">vivid-sweep-114</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7omq4uvm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7omq4uvm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251117_095509_703', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.00390625, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]], 'random_select_ratio': 8, 'leaky_temporal_filter': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 640.0\n",
      "lif layer 1 self.abs_max_v: 640.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 909.0\n",
      "lif layer 2 self.abs_max_v: 909.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "fc layer 1 self.abs_max_out: 645.0\n",
      "lif layer 1 self.abs_max_v: 905.5\n",
      "lif layer 2 self.abs_max_v: 1143.0\n",
      "fc layer 1 self.abs_max_out: 652.0\n",
      "lif layer 1 self.abs_max_v: 946.5\n",
      "fc layer 2 self.abs_max_out: 1014.0\n",
      "lif layer 2 self.abs_max_v: 1417.5\n",
      "fc layer 3 self.abs_max_out: 453.0\n",
      "fc layer 1 self.abs_max_out: 852.0\n",
      "lif layer 1 self.abs_max_v: 1109.0\n",
      "fc layer 2 self.abs_max_out: 1036.0\n",
      "lif layer 2 self.abs_max_v: 1657.5\n",
      "fc layer 1 self.abs_max_out: 930.0\n",
      "lif layer 1 self.abs_max_v: 1283.5\n",
      "fc layer 2 self.abs_max_out: 1069.0\n",
      "lif layer 2 self.abs_max_v: 1898.0\n",
      "fc layer 1 self.abs_max_out: 1095.0\n",
      "lif layer 1 self.abs_max_v: 1584.0\n",
      "fc layer 1 self.abs_max_out: 1096.0\n",
      "fc layer 2 self.abs_max_out: 1202.0\n",
      "lif layer 2 self.abs_max_v: 1989.5\n",
      "fc layer 1 self.abs_max_out: 1103.0\n",
      "fc layer 2 self.abs_max_out: 1243.0\n",
      "lif layer 2 self.abs_max_v: 2189.5\n",
      "fc layer 1 self.abs_max_out: 1207.0\n",
      "lif layer 1 self.abs_max_v: 1641.5\n",
      "fc layer 1 self.abs_max_out: 1946.0\n",
      "lif layer 1 self.abs_max_v: 1946.0\n",
      "fc layer 2 self.abs_max_out: 1349.0\n",
      "lif layer 1 self.abs_max_v: 2198.0\n",
      "lif layer 1 self.abs_max_v: 2484.0\n",
      "fc layer 2 self.abs_max_out: 1375.0\n",
      "fc layer 1 self.abs_max_out: 1984.0\n",
      "lif layer 1 self.abs_max_v: 2578.5\n",
      "fc layer 1 self.abs_max_out: 2099.0\n",
      "fc layer 3 self.abs_max_out: 458.0\n",
      "lif layer 1 self.abs_max_v: 2767.5\n",
      "fc layer 2 self.abs_max_out: 1383.0\n",
      "fc layer 3 self.abs_max_out: 470.0\n",
      "fc layer 1 self.abs_max_out: 2947.0\n",
      "lif layer 1 self.abs_max_v: 2947.0\n",
      "fc layer 1 self.abs_max_out: 3446.0\n",
      "lif layer 1 self.abs_max_v: 3654.0\n",
      "fc layer 1 self.abs_max_out: 3623.0\n",
      "lif layer 1 self.abs_max_v: 3994.0\n",
      "fc layer 1 self.abs_max_out: 3635.0\n",
      "lif layer 1 self.abs_max_v: 4369.0\n",
      "lif layer 1 self.abs_max_v: 4727.5\n",
      "fc layer 3 self.abs_max_out: 510.0\n",
      "fc layer 2 self.abs_max_out: 1465.0\n",
      "fc layer 2 self.abs_max_out: 1516.0\n",
      "fc layer 3 self.abs_max_out: 511.0\n",
      "fc layer 2 self.abs_max_out: 1632.0\n",
      "fc layer 1 self.abs_max_out: 4073.0\n",
      "lif layer 2 self.abs_max_v: 2290.0\n",
      "fc layer 1 self.abs_max_out: 4252.0\n",
      "lif layer 1 self.abs_max_v: 5389.0\n",
      "lif layer 2 self.abs_max_v: 2386.5\n",
      "fc layer 1 self.abs_max_out: 4295.0\n",
      "lif layer 1 self.abs_max_v: 6005.5\n",
      "fc layer 2 self.abs_max_out: 1688.0\n",
      "lif layer 2 self.abs_max_v: 2541.5\n",
      "lif layer 1 self.abs_max_v: 6137.0\n",
      "lif layer 2 self.abs_max_v: 2574.5\n",
      "fc layer 2 self.abs_max_out: 1732.0\n",
      "fc layer 2 self.abs_max_out: 1835.0\n",
      "fc layer 2 self.abs_max_out: 2173.0\n",
      "lif layer 2 self.abs_max_v: 2627.0\n",
      "lif layer 2 self.abs_max_v: 2638.5\n",
      "lif layer 2 self.abs_max_v: 2693.5\n",
      "fc layer 3 self.abs_max_out: 554.0\n",
      "lif layer 2 self.abs_max_v: 2994.0\n",
      "lif layer 1 self.abs_max_v: 6144.0\n",
      "lif layer 1 self.abs_max_v: 6258.5\n",
      "lif layer 1 self.abs_max_v: 6561.5\n",
      "lif layer 1 self.abs_max_v: 6631.0\n",
      "fc layer 3 self.abs_max_out: 591.0\n",
      "lif layer 2 self.abs_max_v: 3028.5\n",
      "lif layer 2 self.abs_max_v: 3091.5\n",
      "lif layer 2 self.abs_max_v: 3109.5\n",
      "lif layer 2 self.abs_max_v: 3139.0\n",
      "lif layer 2 self.abs_max_v: 3185.5\n",
      "lif layer 2 self.abs_max_v: 3191.0\n",
      "lif layer 1 self.abs_max_v: 7015.0\n",
      "lif layer 1 self.abs_max_v: 7041.5\n",
      "lif layer 2 self.abs_max_v: 3237.0\n",
      "lif layer 2 self.abs_max_v: 3383.0\n",
      "lif layer 2 self.abs_max_v: 3474.5\n",
      "lif layer 2 self.abs_max_v: 3581.0\n",
      "lif layer 2 self.abs_max_v: 3610.5\n",
      "fc layer 3 self.abs_max_out: 596.0\n",
      "lif layer 2 self.abs_max_v: 3640.0\n",
      "lif layer 2 self.abs_max_v: 3737.0\n",
      "lif layer 2 self.abs_max_v: 3811.5\n",
      "lif layer 2 self.abs_max_v: 3819.0\n",
      "lif layer 2 self.abs_max_v: 3852.0\n",
      "fc layer 2 self.abs_max_out: 2262.0\n",
      "lif layer 2 self.abs_max_v: 3997.0\n",
      "lif layer 2 self.abs_max_v: 4125.5\n",
      "lif layer 2 self.abs_max_v: 4143.0\n",
      "lif layer 2 self.abs_max_v: 4176.5\n",
      "fc layer 2 self.abs_max_out: 2272.0\n",
      "lif layer 2 self.abs_max_v: 4222.5\n",
      "fc layer 3 self.abs_max_out: 615.0\n",
      "fc layer 2 self.abs_max_out: 2294.0\n",
      "fc layer 3 self.abs_max_out: 686.0\n",
      "fc layer 3 self.abs_max_out: 704.0\n",
      "lif layer 2 self.abs_max_v: 4314.5\n",
      "lif layer 2 self.abs_max_v: 4326.5\n",
      "fc layer 2 self.abs_max_out: 2436.0\n",
      "lif layer 2 self.abs_max_v: 4451.5\n",
      "lif layer 2 self.abs_max_v: 4565.0\n",
      "lif layer 2 self.abs_max_v: 4574.5\n",
      "lif layer 1 self.abs_max_v: 7225.0\n",
      "lif layer 1 self.abs_max_v: 7620.5\n",
      "lif layer 1 self.abs_max_v: 7627.5\n",
      "lif layer 2 self.abs_max_v: 4609.0\n",
      "lif layer 2 self.abs_max_v: 4665.5\n",
      "lif layer 2 self.abs_max_v: 4694.0\n",
      "fc layer 2 self.abs_max_out: 2510.0\n",
      "lif layer 1 self.abs_max_v: 7644.5\n",
      "fc layer 1 self.abs_max_out: 4518.0\n",
      "lif layer 1 self.abs_max_v: 8264.5\n",
      "lif layer 2 self.abs_max_v: 4694.5\n",
      "fc layer 1 self.abs_max_out: 4568.0\n",
      "fc layer 1 self.abs_max_out: 4934.0\n",
      "lif layer 1 self.abs_max_v: 8745.5\n",
      "fc layer 1 self.abs_max_out: 4945.0\n",
      "lif layer 1 self.abs_max_v: 8762.5\n",
      "fc layer 3 self.abs_max_out: 761.0\n",
      "fc layer 2 self.abs_max_out: 2516.0\n",
      "lif layer 1 self.abs_max_v: 8954.0\n",
      "lif layer 1 self.abs_max_v: 8963.0\n",
      "fc layer 3 self.abs_max_out: 774.0\n",
      "fc layer 2 self.abs_max_out: 2565.0\n",
      "fc layer 3 self.abs_max_out: 786.0\n",
      "fc layer 3 self.abs_max_out: 803.0\n",
      "fc layer 1 self.abs_max_out: 5088.0\n",
      "fc layer 1 self.abs_max_out: 5175.0\n",
      "lif layer 1 self.abs_max_v: 8984.5\n",
      "lif layer 1 self.abs_max_v: 9170.5\n",
      "lif layer 1 self.abs_max_v: 9408.0\n",
      "fc layer 1 self.abs_max_out: 5221.0\n",
      "fc layer 1 self.abs_max_out: 5286.0\n",
      "fc layer 2 self.abs_max_out: 2595.0\n",
      "fc layer 1 self.abs_max_out: 5628.0\n",
      "lif layer 1 self.abs_max_v: 10205.5\n",
      "lif layer 1 self.abs_max_v: 10513.0\n",
      "lif layer 1 self.abs_max_v: 10599.5\n",
      "fc layer 3 self.abs_max_out: 822.0\n",
      "fc layer 3 self.abs_max_out: 847.0\n",
      "fc layer 2 self.abs_max_out: 2669.0\n",
      "fc layer 2 self.abs_max_out: 2754.0\n",
      "fc layer 1 self.abs_max_out: 5909.0\n",
      "lif layer 1 self.abs_max_v: 10713.5\n",
      "lif layer 1 self.abs_max_v: 10900.0\n",
      "lif layer 2 self.abs_max_v: 4817.5\n",
      "fc layer 2 self.abs_max_out: 2771.0\n",
      "fc layer 2 self.abs_max_out: 2918.0\n",
      "fc layer 3 self.abs_max_out: 850.0\n",
      "fc layer 1 self.abs_max_out: 6002.0\n",
      "lif layer 1 self.abs_max_v: 10956.0\n",
      "lif layer 1 self.abs_max_v: 11191.0\n",
      "fc layer 1 self.abs_max_out: 6027.0\n",
      "fc layer 1 self.abs_max_out: 6509.0\n",
      "lif layer 1 self.abs_max_v: 11386.0\n",
      "lif layer 1 self.abs_max_v: 11686.0\n",
      "fc layer 3 self.abs_max_out: 875.0\n",
      "fc layer 3 self.abs_max_out: 886.0\n",
      "fc layer 3 self.abs_max_out: 901.0\n",
      "fc layer 3 self.abs_max_out: 929.0\n",
      "fc layer 3 self.abs_max_out: 952.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.670410/  1.939680, val:  38.75%, val_best:  38.75%, tr:  98.88%, tr_best:  98.88%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8761%\n",
      "layer   2  Sparsity: 66.6979%\n",
      "layer   3  Sparsity: 55.9691%\n",
      "total_backward_count 9790 real_backward_count 1390  14.198%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 6557.0\n",
      "fc layer 2 self.abs_max_out: 2974.0\n",
      "fc layer 2 self.abs_max_out: 2992.0\n",
      "fc layer 1 self.abs_max_out: 6726.0\n",
      "lif layer 1 self.abs_max_v: 12019.0\n",
      "lif layer 1 self.abs_max_v: 12074.5\n",
      "lif layer 1 self.abs_max_v: 12199.5\n",
      "fc layer 3 self.abs_max_out: 1020.0\n",
      "fc layer 3 self.abs_max_out: 1046.0\n",
      "fc layer 2 self.abs_max_out: 2999.0\n",
      "fc layer 2 self.abs_max_out: 3016.0\n",
      "fc layer 1 self.abs_max_out: 6814.0\n",
      "fc layer 1 self.abs_max_out: 7031.0\n",
      "fc layer 2 self.abs_max_out: 3065.0\n",
      "fc layer 2 self.abs_max_out: 3151.0\n",
      "fc layer 2 self.abs_max_out: 3178.0\n",
      "fc layer 2 self.abs_max_out: 3218.0\n",
      "fc layer 2 self.abs_max_out: 3227.0\n",
      "fc layer 2 self.abs_max_out: 3228.0\n",
      "lif layer 2 self.abs_max_v: 4825.0\n",
      "lif layer 2 self.abs_max_v: 4892.5\n",
      "lif layer 2 self.abs_max_v: 4997.5\n",
      "fc layer 2 self.abs_max_out: 3321.0\n",
      "fc layer 2 self.abs_max_out: 3335.0\n",
      "lif layer 2 self.abs_max_v: 5023.0\n",
      "lif layer 2 self.abs_max_v: 5065.5\n",
      "lif layer 2 self.abs_max_v: 5114.0\n",
      "lif layer 2 self.abs_max_v: 5436.0\n",
      "lif layer 2 self.abs_max_v: 5655.0\n",
      "lif layer 2 self.abs_max_v: 5809.0\n",
      "lif layer 2 self.abs_max_v: 6041.5\n",
      "lif layer 2 self.abs_max_v: 6060.0\n",
      "lif layer 2 self.abs_max_v: 6167.0\n",
      "lif layer 1 self.abs_max_v: 12471.5\n",
      "lif layer 1 self.abs_max_v: 13108.5\n",
      "fc layer 2 self.abs_max_out: 3367.0\n",
      "fc layer 1 self.abs_max_out: 7513.0\n",
      "fc layer 3 self.abs_max_out: 1072.0\n",
      "fc layer 3 self.abs_max_out: 1079.0\n",
      "lif layer 1 self.abs_max_v: 13192.5\n",
      "fc layer 2 self.abs_max_out: 3407.0\n",
      "fc layer 3 self.abs_max_out: 1089.0\n",
      "lif layer 1 self.abs_max_v: 13351.0\n",
      "fc layer 1 self.abs_max_out: 7663.0\n",
      "fc layer 3 self.abs_max_out: 1117.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.515076/  1.843319, val:  40.83%, val_best:  40.83%, tr:  99.59%, tr_best:  99.59%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 81.8877%\n",
      "layer   2  Sparsity: 74.6519%\n",
      "layer   3  Sparsity: 61.4944%\n",
      "total_backward_count 19580 real_backward_count 2614  13.350%\n",
      "fc layer 2 self.abs_max_out: 3492.0\n",
      "fc layer 2 self.abs_max_out: 3522.0\n",
      "fc layer 2 self.abs_max_out: 3704.0\n",
      "fc layer 3 self.abs_max_out: 1119.0\n",
      "fc layer 3 self.abs_max_out: 1138.0\n",
      "fc layer 3 self.abs_max_out: 1218.0\n",
      "fc layer 3 self.abs_max_out: 1236.0\n",
      "fc layer 1 self.abs_max_out: 8064.0\n",
      "lif layer 1 self.abs_max_v: 14096.5\n",
      "fc layer 2 self.abs_max_out: 3712.0\n",
      "fc layer 2 self.abs_max_out: 3753.0\n",
      "fc layer 2 self.abs_max_out: 3754.0\n",
      "fc layer 1 self.abs_max_out: 8354.0\n",
      "fc layer 2 self.abs_max_out: 3837.0\n",
      "lif layer 1 self.abs_max_v: 14715.0\n",
      "fc layer 1 self.abs_max_out: 8362.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.499795/  1.853055, val:  40.42%, val_best:  40.83%, tr:  99.49%, tr_best:  99.59%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8792%\n",
      "layer   2  Sparsity: 75.4382%\n",
      "layer   3  Sparsity: 62.2615%\n",
      "total_backward_count 29370 real_backward_count 3821  13.010%\n",
      "lif layer 1 self.abs_max_v: 14914.5\n",
      "fc layer 2 self.abs_max_out: 3875.0\n",
      "lif layer 1 self.abs_max_v: 15130.0\n",
      "fc layer 1 self.abs_max_out: 8978.0\n",
      "lif layer 1 self.abs_max_v: 15248.0\n",
      "lif layer 1 self.abs_max_v: 16513.0\n",
      "fc layer 1 self.abs_max_out: 9167.0\n",
      "lif layer 1 self.abs_max_v: 17423.5\n",
      "fc layer 1 self.abs_max_out: 9582.0\n",
      "fc layer 1 self.abs_max_out: 9771.0\n",
      "fc layer 2 self.abs_max_out: 3933.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.485997/  1.772547, val:  50.83%, val_best:  50.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 72.56 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8364%\n",
      "layer   2  Sparsity: 76.7596%\n",
      "layer   3  Sparsity: 62.1112%\n",
      "total_backward_count 39160 real_backward_count 4941  12.617%\n",
      "fc layer 3 self.abs_max_out: 1363.0\n",
      "lif layer 2 self.abs_max_v: 6843.5\n",
      "lif layer 2 self.abs_max_v: 6950.0\n",
      "lif layer 2 self.abs_max_v: 7116.0\n",
      "fc layer 2 self.abs_max_out: 3934.0\n",
      "fc layer 2 self.abs_max_out: 3935.0\n",
      "fc layer 2 self.abs_max_out: 3994.0\n",
      "fc layer 2 self.abs_max_out: 4163.0\n",
      "fc layer 1 self.abs_max_out: 9794.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.453965/  1.808262, val:  52.08%, val_best:  52.08%, tr:  99.49%, tr_best:  99.90%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8515%\n",
      "layer   2  Sparsity: 75.9447%\n",
      "layer   3  Sparsity: 63.0756%\n",
      "total_backward_count 48950 real_backward_count 6000  12.257%\n",
      "lif layer 2 self.abs_max_v: 7170.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.466080/  1.794402, val:  43.33%, val_best:  52.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 72.10 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8607%\n",
      "layer   2  Sparsity: 78.8125%\n",
      "layer   3  Sparsity: 63.2186%\n",
      "total_backward_count 58740 real_backward_count 7037  11.980%\n",
      "fc layer 3 self.abs_max_out: 1369.0\n",
      "fc layer 1 self.abs_max_out: 9796.0\n",
      "fc layer 1 self.abs_max_out: 10378.0\n",
      "lif layer 1 self.abs_max_v: 17459.5\n",
      "lif layer 1 self.abs_max_v: 17974.5\n",
      "lif layer 1 self.abs_max_v: 18035.0\n",
      "fc layer 1 self.abs_max_out: 10443.0\n",
      "lif layer 1 self.abs_max_v: 18681.0\n",
      "fc layer 1 self.abs_max_out: 10956.0\n",
      "lif layer 1 self.abs_max_v: 19205.0\n",
      "lif layer 1 self.abs_max_v: 19725.5\n",
      "fc layer 1 self.abs_max_out: 11214.0\n",
      "fc layer 1 self.abs_max_out: 11722.0\n",
      "lif layer 1 self.abs_max_v: 19731.0\n",
      "lif layer 1 self.abs_max_v: 20152.5\n",
      "lif layer 1 self.abs_max_v: 20583.5\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.453751/  1.780470, val:  50.83%, val_best:  52.08%, tr:  99.39%, tr_best:  99.90%, epoch time: 71.65 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 81.8613%\n",
      "layer   2  Sparsity: 78.6906%\n",
      "layer   3  Sparsity: 62.2556%\n",
      "total_backward_count 68530 real_backward_count 8127  11.859%\n",
      "lif layer 1 self.abs_max_v: 21334.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.418549/  1.737806, val:  55.83%, val_best:  55.83%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.22 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8607%\n",
      "layer   2  Sparsity: 79.4416%\n",
      "layer   3  Sparsity: 61.4881%\n",
      "total_backward_count 78320 real_backward_count 9123  11.648%\n",
      "fc layer 3 self.abs_max_out: 1448.0\n",
      "fc layer 1 self.abs_max_out: 12181.0\n",
      "fc layer 2 self.abs_max_out: 4307.0\n",
      "fc layer 3 self.abs_max_out: 1487.0\n",
      "lif layer 2 self.abs_max_v: 7317.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.416448/  1.728892, val:  49.58%, val_best:  55.83%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.16 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8534%\n",
      "layer   2  Sparsity: 79.4659%\n",
      "layer   3  Sparsity: 61.5214%\n",
      "total_backward_count 88110 real_backward_count 10184  11.558%\n",
      "lif layer 2 self.abs_max_v: 7390.5\n",
      "lif layer 2 self.abs_max_v: 7591.5\n",
      "lif layer 2 self.abs_max_v: 7687.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.417033/  1.748984, val:  54.58%, val_best:  55.83%, tr:  99.39%, tr_best:  99.90%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8852%\n",
      "layer   2  Sparsity: 78.2441%\n",
      "layer   3  Sparsity: 61.4570%\n",
      "total_backward_count 97900 real_backward_count 11146  11.385%\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.405597/  1.717495, val:  56.25%, val_best:  56.25%, tr:  99.49%, tr_best:  99.90%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8665%\n",
      "layer   2  Sparsity: 77.6803%\n",
      "layer   3  Sparsity: 62.8096%\n",
      "total_backward_count 107690 real_backward_count 12082  11.219%\n",
      "lif layer 1 self.abs_max_v: 21461.0\n",
      "fc layer 1 self.abs_max_out: 12241.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.364706/  1.702228, val:  48.75%, val_best:  56.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8630%\n",
      "layer   2  Sparsity: 76.1210%\n",
      "layer   3  Sparsity: 62.6034%\n",
      "total_backward_count 117480 real_backward_count 13086  11.139%\n",
      "fc layer 1 self.abs_max_out: 12598.0\n",
      "fc layer 2 self.abs_max_out: 4364.0\n",
      "lif layer 2 self.abs_max_v: 7971.0\n",
      "lif layer 2 self.abs_max_v: 8055.5\n",
      "lif layer 2 self.abs_max_v: 8117.0\n",
      "lif layer 1 self.abs_max_v: 21513.5\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.360339/  1.696699, val:  47.50%, val_best:  56.25%, tr:  99.59%, tr_best:  99.90%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8504%\n",
      "layer   2  Sparsity: 76.4334%\n",
      "layer   3  Sparsity: 62.6016%\n",
      "total_backward_count 127270 real_backward_count 14028  11.022%\n",
      "lif layer 1 self.abs_max_v: 21693.0\n",
      "lif layer 2 self.abs_max_v: 8161.0\n",
      "lif layer 2 self.abs_max_v: 8240.5\n",
      "lif layer 1 self.abs_max_v: 22022.5\n",
      "fc layer 1 self.abs_max_out: 12700.0\n",
      "lif layer 1 self.abs_max_v: 23624.5\n",
      "fc layer 1 self.abs_max_out: 13174.0\n",
      "fc layer 1 self.abs_max_out: 13529.0\n",
      "lif layer 1 self.abs_max_v: 23993.5\n",
      "fc layer 2 self.abs_max_out: 4579.0\n",
      "lif layer 2 self.abs_max_v: 8384.0\n",
      "fc layer 2 self.abs_max_out: 4588.0\n",
      "fc layer 2 self.abs_max_out: 4664.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.350551/  1.703659, val:  52.08%, val_best:  56.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8969%\n",
      "layer   2  Sparsity: 75.6174%\n",
      "layer   3  Sparsity: 62.1505%\n",
      "total_backward_count 137060 real_backward_count 15008  10.950%\n",
      "lif layer 1 self.abs_max_v: 24198.0\n",
      "fc layer 1 self.abs_max_out: 14012.0\n",
      "lif layer 1 self.abs_max_v: 25438.0\n",
      "lif layer 1 self.abs_max_v: 25824.0\n",
      "fc layer 1 self.abs_max_out: 14052.0\n",
      "fc layer 1 self.abs_max_out: 14488.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.356061/  1.664957, val:  53.33%, val_best:  56.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 73.48 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8791%\n",
      "layer   2  Sparsity: 75.9031%\n",
      "layer   3  Sparsity: 62.7361%\n",
      "total_backward_count 146850 real_backward_count 15889  10.820%\n",
      "fc layer 1 self.abs_max_out: 14652.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.356443/  1.719870, val:  56.25%, val_best:  56.25%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8779%\n",
      "layer   2  Sparsity: 76.7681%\n",
      "layer   3  Sparsity: 62.2256%\n",
      "total_backward_count 156640 real_backward_count 16869  10.769%\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.350361/  1.679440, val:  61.67%, val_best:  61.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 81.8743%\n",
      "layer   2  Sparsity: 75.6422%\n",
      "layer   3  Sparsity: 62.8487%\n",
      "total_backward_count 166430 real_backward_count 17813  10.703%\n",
      "fc layer 2 self.abs_max_out: 4691.0\n",
      "lif layer 2 self.abs_max_v: 8805.5\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.370287/  1.641770, val:  61.67%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8709%\n",
      "layer   2  Sparsity: 75.8207%\n",
      "layer   3  Sparsity: 63.7200%\n",
      "total_backward_count 176220 real_backward_count 18726  10.626%\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.382882/  1.659539, val:  57.08%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8365%\n",
      "layer   2  Sparsity: 74.6202%\n",
      "layer   3  Sparsity: 63.0423%\n",
      "total_backward_count 186010 real_backward_count 19655  10.567%\n",
      "fc layer 2 self.abs_max_out: 4701.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.375865/  1.707556, val:  52.50%, val_best:  61.67%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.62 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8826%\n",
      "layer   2  Sparsity: 75.1289%\n",
      "layer   3  Sparsity: 63.7465%\n",
      "total_backward_count 195800 real_backward_count 20555  10.498%\n",
      "fc layer 2 self.abs_max_out: 4936.0\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.372750/  1.723697, val:  47.08%, val_best:  61.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 72.78 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8674%\n",
      "layer   2  Sparsity: 75.4492%\n",
      "layer   3  Sparsity: 64.4278%\n",
      "total_backward_count 205590 real_backward_count 21419  10.418%\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.370945/  1.667593, val:  62.50%, val_best:  62.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8742%\n",
      "layer   2  Sparsity: 75.2648%\n",
      "layer   3  Sparsity: 64.0091%\n",
      "total_backward_count 215380 real_backward_count 22333  10.369%\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.369909/  1.677316, val:  51.67%, val_best:  62.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8421%\n",
      "layer   2  Sparsity: 74.1443%\n",
      "layer   3  Sparsity: 63.5110%\n",
      "total_backward_count 225170 real_backward_count 23215  10.310%\n",
      "fc layer 1 self.abs_max_out: 15916.0\n",
      "lif layer 1 self.abs_max_v: 28570.0\n",
      "fc layer 1 self.abs_max_out: 16146.0\n",
      "lif layer 1 self.abs_max_v: 28977.0\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.334799/  1.652462, val:  60.42%, val_best:  62.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8717%\n",
      "layer   2  Sparsity: 74.2037%\n",
      "layer   3  Sparsity: 63.6251%\n",
      "total_backward_count 234960 real_backward_count 24084  10.250%\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.348943/  1.692988, val:  57.50%, val_best:  62.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8680%\n",
      "layer   2  Sparsity: 74.2937%\n",
      "layer   3  Sparsity: 65.0111%\n",
      "total_backward_count 244750 real_backward_count 24980  10.206%\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.325599/  1.583633, val:  62.50%, val_best:  62.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 72.48 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8568%\n",
      "layer   2  Sparsity: 74.2269%\n",
      "layer   3  Sparsity: 64.6116%\n",
      "total_backward_count 254540 real_backward_count 25870  10.163%\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.305332/  1.652387, val:  62.50%, val_best:  62.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8638%\n",
      "layer   2  Sparsity: 73.8060%\n",
      "layer   3  Sparsity: 63.4561%\n",
      "total_backward_count 264330 real_backward_count 26723  10.110%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.307311/  1.643344, val:  55.83%, val_best:  62.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8598%\n",
      "layer   2  Sparsity: 73.2330%\n",
      "layer   3  Sparsity: 63.7474%\n",
      "total_backward_count 274120 real_backward_count 27616  10.074%\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.280285/  1.639679, val:  60.00%, val_best:  62.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 71.91 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8544%\n",
      "layer   2  Sparsity: 74.8114%\n",
      "layer   3  Sparsity: 63.9264%\n",
      "total_backward_count 283910 real_backward_count 28535  10.051%\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.301888/  1.649353, val:  50.42%, val_best:  62.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8814%\n",
      "layer   2  Sparsity: 74.5540%\n",
      "layer   3  Sparsity: 64.7940%\n",
      "total_backward_count 293700 real_backward_count 29427  10.019%\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.320299/  1.604591, val:  64.58%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.98 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 81.8835%\n",
      "layer   2  Sparsity: 74.1368%\n",
      "layer   3  Sparsity: 66.4007%\n",
      "total_backward_count 303490 real_backward_count 30324   9.992%\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.339435/  1.645730, val:  59.17%, val_best:  64.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.17 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8839%\n",
      "layer   2  Sparsity: 74.5293%\n",
      "layer   3  Sparsity: 66.6306%\n",
      "total_backward_count 313280 real_backward_count 31163   9.947%\n",
      "fc layer 1 self.abs_max_out: 16315.0\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.322163/  1.671302, val:  55.83%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.15 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8674%\n",
      "layer   2  Sparsity: 73.3809%\n",
      "layer   3  Sparsity: 66.4155%\n",
      "total_backward_count 323070 real_backward_count 32013   9.909%\n",
      "fc layer 1 self.abs_max_out: 16932.0\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.317017/  1.618804, val:  59.58%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8708%\n",
      "layer   2  Sparsity: 73.6336%\n",
      "layer   3  Sparsity: 65.7818%\n",
      "total_backward_count 332860 real_backward_count 32885   9.880%\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.292716/  1.632760, val:  50.83%, val_best:  64.58%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.21 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8692%\n",
      "layer   2  Sparsity: 73.0148%\n",
      "layer   3  Sparsity: 66.6509%\n",
      "total_backward_count 342650 real_backward_count 33743   9.848%\n",
      "fc layer 1 self.abs_max_out: 17157.0\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.293573/  1.653613, val:  51.25%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8663%\n",
      "layer   2  Sparsity: 73.9337%\n",
      "layer   3  Sparsity: 66.3649%\n",
      "total_backward_count 352440 real_backward_count 34566   9.808%\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.289128/  1.599630, val:  57.92%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8714%\n",
      "layer   2  Sparsity: 73.0810%\n",
      "layer   3  Sparsity: 66.0781%\n",
      "total_backward_count 362230 real_backward_count 35403   9.774%\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.277717/  1.626997, val:  60.00%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 71.75 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8858%\n",
      "layer   2  Sparsity: 74.7866%\n",
      "layer   3  Sparsity: 67.2603%\n",
      "total_backward_count 372020 real_backward_count 36227   9.738%\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.315132/  1.611220, val:  62.50%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.15 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8801%\n",
      "layer   2  Sparsity: 74.3959%\n",
      "layer   3  Sparsity: 68.3940%\n",
      "total_backward_count 381810 real_backward_count 37083   9.712%\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.309757/  1.637431, val:  60.83%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.30 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8483%\n",
      "layer   2  Sparsity: 73.5871%\n",
      "layer   3  Sparsity: 67.9521%\n",
      "total_backward_count 391600 real_backward_count 37924   9.684%\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.313343/  1.609270, val:  64.17%, val_best:  64.58%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8621%\n",
      "layer   2  Sparsity: 74.4178%\n",
      "layer   3  Sparsity: 68.5193%\n",
      "total_backward_count 401390 real_backward_count 38736   9.650%\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.339702/  1.655548, val:  61.25%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8697%\n",
      "layer   2  Sparsity: 74.7128%\n",
      "layer   3  Sparsity: 68.3743%\n",
      "total_backward_count 411180 real_backward_count 39573   9.624%\n",
      "lif layer 1 self.abs_max_v: 29579.5\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.352387/  1.654610, val:  60.42%, val_best:  64.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8926%\n",
      "layer   2  Sparsity: 75.6064%\n",
      "layer   3  Sparsity: 68.3036%\n",
      "total_backward_count 420970 real_backward_count 40409   9.599%\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.334590/  1.650431, val:  64.17%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8577%\n",
      "layer   2  Sparsity: 74.5232%\n",
      "layer   3  Sparsity: 67.9167%\n",
      "total_backward_count 430760 real_backward_count 41215   9.568%\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.352393/  1.631410, val:  62.08%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.46 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8622%\n",
      "layer   2  Sparsity: 74.6937%\n",
      "layer   3  Sparsity: 68.9998%\n",
      "total_backward_count 440550 real_backward_count 42059   9.547%\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.340883/  1.630074, val:  57.50%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.00 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8863%\n",
      "layer   2  Sparsity: 73.3754%\n",
      "layer   3  Sparsity: 67.1020%\n",
      "total_backward_count 450340 real_backward_count 42913   9.529%\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.323455/  1.653212, val:  57.08%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8689%\n",
      "layer   2  Sparsity: 73.2222%\n",
      "layer   3  Sparsity: 67.2443%\n",
      "total_backward_count 460130 real_backward_count 43785   9.516%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.320849/  1.623594, val:  62.92%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8762%\n",
      "layer   2  Sparsity: 73.4257%\n",
      "layer   3  Sparsity: 67.4921%\n",
      "total_backward_count 469920 real_backward_count 44580   9.487%\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.325410/  1.595062, val:  65.42%, val_best:  65.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.60 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8711%\n",
      "layer   2  Sparsity: 73.6037%\n",
      "layer   3  Sparsity: 68.4072%\n",
      "total_backward_count 479710 real_backward_count 45392   9.462%\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.315431/  1.606124, val:  57.08%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.80 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 81.8823%\n",
      "layer   2  Sparsity: 74.0877%\n",
      "layer   3  Sparsity: 67.7212%\n",
      "total_backward_count 489500 real_backward_count 46252   9.449%\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.334722/  1.685694, val:  55.00%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.08 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8441%\n",
      "layer   2  Sparsity: 72.9034%\n",
      "layer   3  Sparsity: 68.2772%\n",
      "total_backward_count 499290 real_backward_count 47096   9.433%\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.316855/  1.609477, val:  61.67%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.51 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8664%\n",
      "layer   2  Sparsity: 72.6089%\n",
      "layer   3  Sparsity: 68.5114%\n",
      "total_backward_count 509080 real_backward_count 47935   9.416%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.304439/  1.626998, val:  60.00%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.31 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8672%\n",
      "layer   2  Sparsity: 72.6264%\n",
      "layer   3  Sparsity: 68.5584%\n",
      "total_backward_count 518870 real_backward_count 48778   9.401%\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.310992/  1.581466, val:  61.25%, val_best:  65.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.68 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8480%\n",
      "layer   2  Sparsity: 72.9763%\n",
      "layer   3  Sparsity: 69.5158%\n",
      "total_backward_count 528660 real_backward_count 49682   9.398%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.332470/  1.617190, val:  62.50%, val_best:  65.42%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.50 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8694%\n",
      "layer   2  Sparsity: 73.3286%\n",
      "layer   3  Sparsity: 69.7810%\n",
      "total_backward_count 538450 real_backward_count 50516   9.382%\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.296065/  1.580700, val:  65.83%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8513%\n",
      "layer   2  Sparsity: 73.0029%\n",
      "layer   3  Sparsity: 67.9941%\n",
      "total_backward_count 548240 real_backward_count 51322   9.361%\n",
      "lif layer 1 self.abs_max_v: 29832.0\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.288787/  1.648699, val:  54.58%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.93 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8715%\n",
      "layer   2  Sparsity: 73.3482%\n",
      "layer   3  Sparsity: 68.2231%\n",
      "total_backward_count 558030 real_backward_count 52132   9.342%\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.287910/  1.581411, val:  57.50%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 71.84 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8501%\n",
      "layer   2  Sparsity: 72.7283%\n",
      "layer   3  Sparsity: 67.7443%\n",
      "total_backward_count 567820 real_backward_count 52939   9.323%\n",
      "lif layer 1 self.abs_max_v: 30019.0\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.283298/  1.600596, val:  63.75%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.94 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8443%\n",
      "layer   2  Sparsity: 72.3195%\n",
      "layer   3  Sparsity: 67.3258%\n",
      "total_backward_count 577610 real_backward_count 53731   9.302%\n",
      "lif layer 2 self.abs_max_v: 8989.0\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.307603/  1.652986, val:  59.58%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 71.80 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8846%\n",
      "layer   2  Sparsity: 72.6272%\n",
      "layer   3  Sparsity: 68.2626%\n",
      "total_backward_count 587400 real_backward_count 54542   9.285%\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.289582/  1.588381, val:  60.42%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.68 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8492%\n",
      "layer   2  Sparsity: 72.4287%\n",
      "layer   3  Sparsity: 67.1610%\n",
      "total_backward_count 597190 real_backward_count 55345   9.268%\n",
      "fc layer 1 self.abs_max_out: 17429.0\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.274871/  1.605934, val:  63.33%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8855%\n",
      "layer   2  Sparsity: 72.0826%\n",
      "layer   3  Sparsity: 67.0178%\n",
      "total_backward_count 606980 real_backward_count 56137   9.249%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.314013/  1.626672, val:  61.67%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8806%\n",
      "layer   2  Sparsity: 72.5592%\n",
      "layer   3  Sparsity: 67.3121%\n",
      "total_backward_count 616770 real_backward_count 56934   9.231%\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.320003/  1.593919, val:  61.67%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8763%\n",
      "layer   2  Sparsity: 72.4531%\n",
      "layer   3  Sparsity: 67.9388%\n",
      "total_backward_count 626560 real_backward_count 57712   9.211%\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.301651/  1.641165, val:  61.25%, val_best:  65.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8566%\n",
      "layer   2  Sparsity: 72.2288%\n",
      "layer   3  Sparsity: 67.3030%\n",
      "total_backward_count 636350 real_backward_count 58542   9.200%\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.280463/  1.622931, val:  61.25%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8528%\n",
      "layer   2  Sparsity: 72.3787%\n",
      "layer   3  Sparsity: 66.8579%\n",
      "total_backward_count 646140 real_backward_count 59322   9.181%\n",
      "lif layer 2 self.abs_max_v: 9041.5\n",
      "lif layer 2 self.abs_max_v: 9405.0\n",
      "fc layer 2 self.abs_max_out: 4957.0\n",
      "lif layer 2 self.abs_max_v: 9659.5\n",
      "fc layer 2 self.abs_max_out: 5160.0\n",
      "lif layer 2 self.abs_max_v: 9990.0\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.286906/  1.622707, val:  62.08%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8608%\n",
      "layer   2  Sparsity: 71.9496%\n",
      "layer   3  Sparsity: 68.0207%\n",
      "total_backward_count 655930 real_backward_count 60152   9.170%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.271806/  1.579497, val:  61.67%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.71 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8973%\n",
      "layer   2  Sparsity: 71.8840%\n",
      "layer   3  Sparsity: 68.4122%\n",
      "total_backward_count 665720 real_backward_count 60963   9.157%\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.261953/  1.589480, val:  64.17%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.61 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8547%\n",
      "layer   2  Sparsity: 71.6629%\n",
      "layer   3  Sparsity: 66.9388%\n",
      "total_backward_count 675510 real_backward_count 61758   9.142%\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.254326/  1.563449, val:  65.83%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.35 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8727%\n",
      "layer   2  Sparsity: 71.7736%\n",
      "layer   3  Sparsity: 67.6473%\n",
      "total_backward_count 685300 real_backward_count 62546   9.127%\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.274534/  1.601572, val:  62.92%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.19 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8616%\n",
      "layer   2  Sparsity: 71.8424%\n",
      "layer   3  Sparsity: 67.9876%\n",
      "total_backward_count 695090 real_backward_count 63383   9.119%\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.323739/  1.684773, val:  52.08%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.40 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8811%\n",
      "layer   2  Sparsity: 72.5340%\n",
      "layer   3  Sparsity: 70.2132%\n",
      "total_backward_count 704880 real_backward_count 64222   9.111%\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.330633/  1.619235, val:  60.42%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8538%\n",
      "layer   2  Sparsity: 71.7304%\n",
      "layer   3  Sparsity: 68.5220%\n",
      "total_backward_count 714670 real_backward_count 65043   9.101%\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.303847/  1.593489, val:  61.25%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.96 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 81.8518%\n",
      "layer   2  Sparsity: 71.5646%\n",
      "layer   3  Sparsity: 67.9291%\n",
      "total_backward_count 724460 real_backward_count 65799   9.082%\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.291801/  1.582691, val:  60.42%, val_best:  65.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8887%\n",
      "layer   2  Sparsity: 71.6454%\n",
      "layer   3  Sparsity: 67.6542%\n",
      "total_backward_count 734250 real_backward_count 66582   9.068%\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.276878/  1.580009, val:  62.50%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.19 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8422%\n",
      "layer   2  Sparsity: 70.5691%\n",
      "layer   3  Sparsity: 67.6881%\n",
      "total_backward_count 744040 real_backward_count 67344   9.051%\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.302419/  1.591497, val:  61.67%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.89 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8802%\n",
      "layer   2  Sparsity: 72.4327%\n",
      "layer   3  Sparsity: 69.3105%\n",
      "total_backward_count 753830 real_backward_count 68226   9.051%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.300819/  1.625747, val:  52.50%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.97 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8715%\n",
      "layer   2  Sparsity: 72.2012%\n",
      "layer   3  Sparsity: 70.0407%\n",
      "total_backward_count 763620 real_backward_count 68975   9.033%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.319389/  1.638976, val:  63.75%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.06 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8709%\n",
      "layer   2  Sparsity: 72.1734%\n",
      "layer   3  Sparsity: 69.0369%\n",
      "total_backward_count 773410 real_backward_count 69749   9.018%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.297114/  1.659643, val:  60.00%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8533%\n",
      "layer   2  Sparsity: 70.3570%\n",
      "layer   3  Sparsity: 68.0298%\n",
      "total_backward_count 783200 real_backward_count 70521   9.004%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.292506/  1.616054, val:  69.17%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.18 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8589%\n",
      "layer   2  Sparsity: 70.3531%\n",
      "layer   3  Sparsity: 68.4148%\n",
      "total_backward_count 792990 real_backward_count 71300   8.991%\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.309804/  1.605191, val:  56.67%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.36 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8949%\n",
      "layer   2  Sparsity: 71.1163%\n",
      "layer   3  Sparsity: 69.0824%\n",
      "total_backward_count 802780 real_backward_count 72058   8.976%\n",
      "fc layer 1 self.abs_max_out: 17741.0\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.301556/  1.620998, val:  60.83%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8596%\n",
      "layer   2  Sparsity: 71.1366%\n",
      "layer   3  Sparsity: 69.0984%\n",
      "total_backward_count 812570 real_backward_count 72868   8.968%\n",
      "lif layer 1 self.abs_max_v: 30551.5\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.302569/  1.649104, val:  60.00%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8776%\n",
      "layer   2  Sparsity: 71.3045%\n",
      "layer   3  Sparsity: 68.8237%\n",
      "total_backward_count 822360 real_backward_count 73669   8.958%\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.322643/  1.642632, val:  54.58%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.52 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8664%\n",
      "layer   2  Sparsity: 71.3939%\n",
      "layer   3  Sparsity: 69.4929%\n",
      "total_backward_count 832150 real_backward_count 74511   8.954%\n",
      "lif layer 1 self.abs_max_v: 30976.0\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.315772/  1.626984, val:  57.08%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.61 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8785%\n",
      "layer   2  Sparsity: 70.8042%\n",
      "layer   3  Sparsity: 69.8139%\n",
      "total_backward_count 841940 real_backward_count 75292   8.943%\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.328201/  1.646966, val:  60.00%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8502%\n",
      "layer   2  Sparsity: 71.3058%\n",
      "layer   3  Sparsity: 68.7088%\n",
      "total_backward_count 851730 real_backward_count 76091   8.934%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.307442/  1.648359, val:  58.75%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8754%\n",
      "layer   2  Sparsity: 71.3020%\n",
      "layer   3  Sparsity: 67.8807%\n",
      "total_backward_count 861520 real_backward_count 76860   8.921%\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.304154/  1.586594, val:  61.67%, val_best:  69.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.54 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8541%\n",
      "layer   2  Sparsity: 71.3891%\n",
      "layer   3  Sparsity: 67.8150%\n",
      "total_backward_count 871310 real_backward_count 77660   8.913%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.317777/  1.645420, val:  60.42%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8725%\n",
      "layer   2  Sparsity: 71.2941%\n",
      "layer   3  Sparsity: 69.3581%\n",
      "total_backward_count 881100 real_backward_count 78501   8.909%\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.333397/  1.610371, val:  68.75%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8749%\n",
      "layer   2  Sparsity: 70.6790%\n",
      "layer   3  Sparsity: 69.9853%\n",
      "total_backward_count 890890 real_backward_count 79322   8.904%\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.325532/  1.589464, val:  70.83%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.99 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 81.8897%\n",
      "layer   2  Sparsity: 71.0930%\n",
      "layer   3  Sparsity: 70.2569%\n",
      "total_backward_count 900680 real_backward_count 80051   8.888%\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.324849/  1.668272, val:  59.17%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8674%\n",
      "layer   2  Sparsity: 69.5264%\n",
      "layer   3  Sparsity: 69.3258%\n",
      "total_backward_count 910470 real_backward_count 80826   8.877%\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.325652/  1.595617, val:  66.25%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8793%\n",
      "layer   2  Sparsity: 70.5120%\n",
      "layer   3  Sparsity: 68.6326%\n",
      "total_backward_count 920260 real_backward_count 81694   8.877%\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.294678/  1.623864, val:  59.17%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8797%\n",
      "layer   2  Sparsity: 70.5401%\n",
      "layer   3  Sparsity: 67.7977%\n",
      "total_backward_count 930050 real_backward_count 82534   8.874%\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.299325/  1.575873, val:  67.50%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.47 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8405%\n",
      "layer   2  Sparsity: 70.2818%\n",
      "layer   3  Sparsity: 67.4689%\n",
      "total_backward_count 939840 real_backward_count 83318   8.865%\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.289805/  1.656264, val:  53.75%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8898%\n",
      "layer   2  Sparsity: 71.2599%\n",
      "layer   3  Sparsity: 68.3336%\n",
      "total_backward_count 949630 real_backward_count 84119   8.858%\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.295571/  1.592115, val:  67.92%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.51 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 81.8668%\n",
      "layer   2  Sparsity: 71.4534%\n",
      "layer   3  Sparsity: 69.3083%\n",
      "total_backward_count 959420 real_backward_count 84921   8.851%\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.314800/  1.687227, val:  53.75%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8645%\n",
      "layer   2  Sparsity: 70.2566%\n",
      "layer   3  Sparsity: 69.3436%\n",
      "total_backward_count 969210 real_backward_count 85684   8.841%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.320665/  1.590342, val:  63.75%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8637%\n",
      "layer   2  Sparsity: 70.4808%\n",
      "layer   3  Sparsity: 70.5150%\n",
      "total_backward_count 979000 real_backward_count 86472   8.833%\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.308607/  1.634708, val:  62.50%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 71.73 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8652%\n",
      "layer   2  Sparsity: 70.5797%\n",
      "layer   3  Sparsity: 69.8932%\n",
      "total_backward_count 988790 real_backward_count 87230   8.822%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.301064/  1.638272, val:  59.58%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.07 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8517%\n",
      "layer   2  Sparsity: 70.4785%\n",
      "layer   3  Sparsity: 69.5410%\n",
      "total_backward_count 998580 real_backward_count 88012   8.814%\n",
      "lif layer 1 self.abs_max_v: 31038.0\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.304283/  1.598972, val:  64.17%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.36 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8721%\n",
      "layer   2  Sparsity: 70.4502%\n",
      "layer   3  Sparsity: 70.2224%\n",
      "total_backward_count 1008370 real_backward_count 88827   8.809%\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.297998/  1.601975, val:  58.33%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.03 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 81.8540%\n",
      "layer   2  Sparsity: 69.7664%\n",
      "layer   3  Sparsity: 69.0144%\n",
      "total_backward_count 1018160 real_backward_count 89588   8.799%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.291925/  1.565701, val:  68.75%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8873%\n",
      "layer   2  Sparsity: 71.1046%\n",
      "layer   3  Sparsity: 68.5828%\n",
      "total_backward_count 1027950 real_backward_count 90342   8.789%\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.293644/  1.587617, val:  64.17%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8987%\n",
      "layer   2  Sparsity: 70.8594%\n",
      "layer   3  Sparsity: 68.7740%\n",
      "total_backward_count 1037740 real_backward_count 91145   8.783%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.287376/  1.621324, val:  57.08%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.95 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 81.8608%\n",
      "layer   2  Sparsity: 71.2683%\n",
      "layer   3  Sparsity: 68.9226%\n",
      "total_backward_count 1047530 real_backward_count 91889   8.772%\n",
      "fc layer 1 self.abs_max_out: 17988.0\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.301917/  1.623587, val:  56.25%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.22 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8571%\n",
      "layer   2  Sparsity: 70.3622%\n",
      "layer   3  Sparsity: 69.6352%\n",
      "total_backward_count 1057320 real_backward_count 92708   8.768%\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.306514/  1.618683, val:  58.75%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.65 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8835%\n",
      "layer   2  Sparsity: 70.4308%\n",
      "layer   3  Sparsity: 70.4925%\n",
      "total_backward_count 1067110 real_backward_count 93465   8.759%\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.308074/  1.599726, val:  59.58%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.29 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8465%\n",
      "layer   2  Sparsity: 70.5030%\n",
      "layer   3  Sparsity: 70.3388%\n",
      "total_backward_count 1076900 real_backward_count 94213   8.749%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.322741/  1.611342, val:  65.00%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8613%\n",
      "layer   2  Sparsity: 71.2089%\n",
      "layer   3  Sparsity: 70.8425%\n",
      "total_backward_count 1086690 real_backward_count 94975   8.740%\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.350161/  1.623358, val:  62.08%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8606%\n",
      "layer   2  Sparsity: 70.4685%\n",
      "layer   3  Sparsity: 71.6648%\n",
      "total_backward_count 1096480 real_backward_count 95780   8.735%\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.335538/  1.655747, val:  62.08%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.53 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8515%\n",
      "layer   2  Sparsity: 70.4861%\n",
      "layer   3  Sparsity: 70.6590%\n",
      "total_backward_count 1106270 real_backward_count 96583   8.731%\n",
      "fc layer 1 self.abs_max_out: 18450.0\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.339666/  1.614568, val:  68.33%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.55 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 81.8753%\n",
      "layer   2  Sparsity: 71.5622%\n",
      "layer   3  Sparsity: 70.7511%\n",
      "total_backward_count 1116060 real_backward_count 97386   8.726%\n",
      "lif layer 1 self.abs_max_v: 31840.5\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.361943/  1.615872, val:  65.83%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8576%\n",
      "layer   2  Sparsity: 70.9066%\n",
      "layer   3  Sparsity: 72.2552%\n",
      "total_backward_count 1125850 real_backward_count 98175   8.720%\n",
      "lif layer 1 self.abs_max_v: 32041.5\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.363017/  1.638847, val:  56.67%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8659%\n",
      "layer   2  Sparsity: 70.8686%\n",
      "layer   3  Sparsity: 70.8605%\n",
      "total_backward_count 1135640 real_backward_count 98950   8.713%\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.366114/  1.632489, val:  65.83%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.51 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8515%\n",
      "layer   2  Sparsity: 72.1179%\n",
      "layer   3  Sparsity: 71.7246%\n",
      "total_backward_count 1145430 real_backward_count 99742   8.708%\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.339744/  1.605919, val:  67.08%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8566%\n",
      "layer   2  Sparsity: 72.1583%\n",
      "layer   3  Sparsity: 71.7643%\n",
      "total_backward_count 1155220 real_backward_count 100546   8.704%\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.344240/  1.620409, val:  62.50%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.66 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8614%\n",
      "layer   2  Sparsity: 72.0196%\n",
      "layer   3  Sparsity: 71.0230%\n",
      "total_backward_count 1165010 real_backward_count 101318   8.697%\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.314276/  1.595646, val:  65.83%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8642%\n",
      "layer   2  Sparsity: 71.8576%\n",
      "layer   3  Sparsity: 70.8589%\n",
      "total_backward_count 1174800 real_backward_count 102100   8.691%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.343442/  1.682811, val:  60.42%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8676%\n",
      "layer   2  Sparsity: 71.5822%\n",
      "layer   3  Sparsity: 71.0232%\n",
      "total_backward_count 1184590 real_backward_count 102844   8.682%\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.330345/  1.615811, val:  59.17%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8509%\n",
      "layer   2  Sparsity: 71.2307%\n",
      "layer   3  Sparsity: 70.3468%\n",
      "total_backward_count 1194380 real_backward_count 103623   8.676%\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.323714/  1.623484, val:  60.83%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.83 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 81.8701%\n",
      "layer   2  Sparsity: 71.4463%\n",
      "layer   3  Sparsity: 70.5006%\n",
      "total_backward_count 1204170 real_backward_count 104421   8.672%\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.313189/  1.592526, val:  61.67%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8609%\n",
      "layer   2  Sparsity: 71.6243%\n",
      "layer   3  Sparsity: 70.3190%\n",
      "total_backward_count 1213960 real_backward_count 105203   8.666%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.309139/  1.592605, val:  65.83%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.46 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8757%\n",
      "layer   2  Sparsity: 71.4521%\n",
      "layer   3  Sparsity: 70.7781%\n",
      "total_backward_count 1223750 real_backward_count 105990   8.661%\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.329078/  1.630330, val:  55.00%, val_best:  70.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8833%\n",
      "layer   2  Sparsity: 71.2459%\n",
      "layer   3  Sparsity: 70.6245%\n",
      "total_backward_count 1233540 real_backward_count 106758   8.655%\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.289107/  1.614702, val:  59.58%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8768%\n",
      "layer   2  Sparsity: 71.6055%\n",
      "layer   3  Sparsity: 70.2595%\n",
      "total_backward_count 1243330 real_backward_count 107551   8.650%\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.314721/  1.588588, val:  61.25%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.60 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8470%\n",
      "layer   2  Sparsity: 71.9461%\n",
      "layer   3  Sparsity: 71.3280%\n",
      "total_backward_count 1253120 real_backward_count 108245   8.638%\n",
      "fc layer 1 self.abs_max_out: 18768.0\n",
      "lif layer 1 self.abs_max_v: 32595.0\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.315701/  1.572893, val:  64.58%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.03 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8706%\n",
      "layer   2  Sparsity: 71.2404%\n",
      "layer   3  Sparsity: 70.0577%\n",
      "total_backward_count 1262910 real_backward_count 109029   8.633%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.276860/  1.553161, val:  66.25%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8702%\n",
      "layer   2  Sparsity: 71.3463%\n",
      "layer   3  Sparsity: 69.9836%\n",
      "total_backward_count 1272700 real_backward_count 109829   8.630%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.249997/  1.576250, val:  60.42%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 71.79 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8715%\n",
      "layer   2  Sparsity: 71.9576%\n",
      "layer   3  Sparsity: 68.8512%\n",
      "total_backward_count 1282490 real_backward_count 110545   8.620%\n",
      "lif layer 1 self.abs_max_v: 32662.0\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.276107/  1.592894, val:  60.83%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8716%\n",
      "layer   2  Sparsity: 71.8176%\n",
      "layer   3  Sparsity: 69.6633%\n",
      "total_backward_count 1292280 real_backward_count 111326   8.615%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.284844/  1.605609, val:  63.75%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.07 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8643%\n",
      "layer   2  Sparsity: 70.9268%\n",
      "layer   3  Sparsity: 69.7454%\n",
      "total_backward_count 1302070 real_backward_count 112113   8.610%\n",
      "lif layer 1 self.abs_max_v: 32956.5\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.294769/  1.561296, val:  59.17%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.19 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8885%\n",
      "layer   2  Sparsity: 71.3595%\n",
      "layer   3  Sparsity: 69.0038%\n",
      "total_backward_count 1311860 real_backward_count 112886   8.605%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.282461/  1.634636, val:  58.33%, val_best:  70.83%, tr:  99.39%, tr_best: 100.00%, epoch time: 72.33 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8824%\n",
      "layer   2  Sparsity: 71.8822%\n",
      "layer   3  Sparsity: 69.0810%\n",
      "total_backward_count 1321650 real_backward_count 113631   8.598%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.318064/  1.599918, val:  60.42%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.34 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8635%\n",
      "layer   2  Sparsity: 71.5813%\n",
      "layer   3  Sparsity: 69.7919%\n",
      "total_backward_count 1331440 real_backward_count 114390   8.591%\n",
      "fc layer 1 self.abs_max_out: 19758.0\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.315894/  1.610362, val:  59.17%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.23 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8625%\n",
      "layer   2  Sparsity: 71.2975%\n",
      "layer   3  Sparsity: 70.2411%\n",
      "total_backward_count 1341230 real_backward_count 115155   8.586%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.328463/  1.632360, val:  50.42%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.47 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8515%\n",
      "layer   2  Sparsity: 70.3588%\n",
      "layer   3  Sparsity: 70.8851%\n",
      "total_backward_count 1351020 real_backward_count 115901   8.579%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.289549/  1.577354, val:  63.33%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8543%\n",
      "layer   2  Sparsity: 70.5879%\n",
      "layer   3  Sparsity: 69.5934%\n",
      "total_backward_count 1360810 real_backward_count 116700   8.576%\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.286366/  1.576330, val:  66.25%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.75 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8734%\n",
      "layer   2  Sparsity: 71.2993%\n",
      "layer   3  Sparsity: 70.3461%\n",
      "total_backward_count 1370600 real_backward_count 117449   8.569%\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.274364/  1.592592, val:  58.75%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8673%\n",
      "layer   2  Sparsity: 71.2776%\n",
      "layer   3  Sparsity: 69.3630%\n",
      "total_backward_count 1380390 real_backward_count 118227   8.565%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.295646/  1.589537, val:  61.25%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.35 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8371%\n",
      "layer   2  Sparsity: 70.1329%\n",
      "layer   3  Sparsity: 68.8382%\n",
      "total_backward_count 1390180 real_backward_count 118964   8.557%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.292122/  1.598183, val:  62.92%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.07 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8811%\n",
      "layer   2  Sparsity: 70.2820%\n",
      "layer   3  Sparsity: 67.7463%\n",
      "total_backward_count 1399970 real_backward_count 119768   8.555%\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.270047/  1.561862, val:  59.58%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8883%\n",
      "layer   2  Sparsity: 69.7438%\n",
      "layer   3  Sparsity: 68.4290%\n",
      "total_backward_count 1409760 real_backward_count 120538   8.550%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.276418/  1.584575, val:  61.25%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8404%\n",
      "layer   2  Sparsity: 70.1263%\n",
      "layer   3  Sparsity: 68.4752%\n",
      "total_backward_count 1419550 real_backward_count 121291   8.544%\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.288698/  1.582994, val:  61.25%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.93 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8765%\n",
      "layer   2  Sparsity: 70.4639%\n",
      "layer   3  Sparsity: 68.7922%\n",
      "total_backward_count 1429340 real_backward_count 122049   8.539%\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.283490/  1.603471, val:  60.00%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8665%\n",
      "layer   2  Sparsity: 70.7067%\n",
      "layer   3  Sparsity: 69.4400%\n",
      "total_backward_count 1439130 real_backward_count 122840   8.536%\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.308814/  1.635716, val:  57.50%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8649%\n",
      "layer   2  Sparsity: 70.3063%\n",
      "layer   3  Sparsity: 70.6052%\n",
      "total_backward_count 1448920 real_backward_count 123561   8.528%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.326931/  1.632131, val:  55.00%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.57 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8862%\n",
      "layer   2  Sparsity: 70.5545%\n",
      "layer   3  Sparsity: 69.9770%\n",
      "total_backward_count 1458710 real_backward_count 124331   8.523%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.301218/  1.652516, val:  60.42%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8625%\n",
      "layer   2  Sparsity: 70.3452%\n",
      "layer   3  Sparsity: 70.5788%\n",
      "total_backward_count 1468500 real_backward_count 125131   8.521%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.296054/  1.610216, val:  63.75%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8408%\n",
      "layer   2  Sparsity: 70.8628%\n",
      "layer   3  Sparsity: 70.3176%\n",
      "total_backward_count 1478290 real_backward_count 125876   8.515%\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.294771/  1.608914, val:  59.58%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.93 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 81.8659%\n",
      "layer   2  Sparsity: 70.7492%\n",
      "layer   3  Sparsity: 69.0920%\n",
      "total_backward_count 1488080 real_backward_count 126632   8.510%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.274543/  1.646340, val:  58.33%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.17 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8425%\n",
      "layer   2  Sparsity: 70.4523%\n",
      "layer   3  Sparsity: 69.2153%\n",
      "total_backward_count 1497870 real_backward_count 127362   8.503%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.288065/  1.583945, val:  63.33%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.87 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8576%\n",
      "layer   2  Sparsity: 70.9655%\n",
      "layer   3  Sparsity: 70.5347%\n",
      "total_backward_count 1507660 real_backward_count 128200   8.503%\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.308583/  1.612327, val:  58.33%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8709%\n",
      "layer   2  Sparsity: 70.4877%\n",
      "layer   3  Sparsity: 70.9126%\n",
      "total_backward_count 1517450 real_backward_count 128987   8.500%\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.298202/  1.647437, val:  53.33%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8754%\n",
      "layer   2  Sparsity: 70.5171%\n",
      "layer   3  Sparsity: 69.7856%\n",
      "total_backward_count 1527240 real_backward_count 129745   8.495%\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.305275/  1.628772, val:  62.08%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8684%\n",
      "layer   2  Sparsity: 69.7399%\n",
      "layer   3  Sparsity: 70.5292%\n",
      "total_backward_count 1537030 real_backward_count 130466   8.488%\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.310605/  1.597033, val:  60.42%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.22 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8700%\n",
      "layer   2  Sparsity: 70.2645%\n",
      "layer   3  Sparsity: 69.5256%\n",
      "total_backward_count 1546820 real_backward_count 131215   8.483%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.315705/  1.646843, val:  52.92%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8702%\n",
      "layer   2  Sparsity: 69.7931%\n",
      "layer   3  Sparsity: 70.2054%\n",
      "total_backward_count 1556610 real_backward_count 132034   8.482%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.294267/  1.614220, val:  60.83%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.52 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8611%\n",
      "layer   2  Sparsity: 70.0874%\n",
      "layer   3  Sparsity: 71.0028%\n",
      "total_backward_count 1566400 real_backward_count 132764   8.476%\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.323987/  1.605199, val:  60.42%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8546%\n",
      "layer   2  Sparsity: 70.4399%\n",
      "layer   3  Sparsity: 71.7466%\n",
      "total_backward_count 1576190 real_backward_count 133549   8.473%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.323897/  1.619668, val:  62.08%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8641%\n",
      "layer   2  Sparsity: 69.4584%\n",
      "layer   3  Sparsity: 71.5707%\n",
      "total_backward_count 1585980 real_backward_count 134412   8.475%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.315144/  1.675457, val:  55.83%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 71.60 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 81.8971%\n",
      "layer   2  Sparsity: 70.4623%\n",
      "layer   3  Sparsity: 72.1140%\n",
      "total_backward_count 1595770 real_backward_count 135233   8.474%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.337005/  1.620506, val:  63.33%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.85 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8727%\n",
      "layer   2  Sparsity: 69.9254%\n",
      "layer   3  Sparsity: 72.5543%\n",
      "total_backward_count 1605560 real_backward_count 136021   8.472%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.337078/  1.652716, val:  60.83%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.48 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8628%\n",
      "layer   2  Sparsity: 70.2495%\n",
      "layer   3  Sparsity: 71.9772%\n",
      "total_backward_count 1615350 real_backward_count 136795   8.468%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.325169/  1.605193, val:  65.00%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 71.24 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 81.8793%\n",
      "layer   2  Sparsity: 70.4236%\n",
      "layer   3  Sparsity: 70.8947%\n",
      "total_backward_count 1625140 real_backward_count 137578   8.466%\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.310366/  1.657180, val:  58.33%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.02 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8912%\n",
      "layer   2  Sparsity: 70.0221%\n",
      "layer   3  Sparsity: 70.7889%\n",
      "total_backward_count 1634930 real_backward_count 138310   8.460%\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.315312/  1.606976, val:  59.58%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.71 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8673%\n",
      "layer   2  Sparsity: 69.5209%\n",
      "layer   3  Sparsity: 71.2869%\n",
      "total_backward_count 1644720 real_backward_count 139066   8.455%\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.312397/  1.635061, val:  58.33%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.58 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 81.8593%\n",
      "layer   2  Sparsity: 69.7352%\n",
      "layer   3  Sparsity: 71.8883%\n",
      "total_backward_count 1654510 real_backward_count 139850   8.453%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.300359/  1.642854, val:  60.00%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 74.22 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 81.8627%\n",
      "layer   2  Sparsity: 69.8466%\n",
      "layer   3  Sparsity: 71.7602%\n",
      "total_backward_count 1664300 real_backward_count 140588   8.447%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.319115/  1.643119, val:  57.08%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8561%\n",
      "layer   2  Sparsity: 69.5224%\n",
      "layer   3  Sparsity: 71.2732%\n",
      "total_backward_count 1674090 real_backward_count 141347   8.443%\n",
      "lif layer 1 self.abs_max_v: 33014.0\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.372436/  1.693706, val:  57.92%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.41 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8712%\n",
      "layer   2  Sparsity: 69.6557%\n",
      "layer   3  Sparsity: 72.6743%\n",
      "total_backward_count 1683880 real_backward_count 142145   8.442%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.359460/  1.647494, val:  56.67%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8712%\n",
      "layer   2  Sparsity: 69.1864%\n",
      "layer   3  Sparsity: 70.4275%\n",
      "total_backward_count 1693670 real_backward_count 142944   8.440%\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.332415/  1.617748, val:  63.75%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8431%\n",
      "layer   2  Sparsity: 69.3954%\n",
      "layer   3  Sparsity: 70.1930%\n",
      "total_backward_count 1703460 real_backward_count 143688   8.435%\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.296646/  1.606153, val:  62.08%, val_best:  70.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8939%\n",
      "layer   2  Sparsity: 69.8907%\n",
      "layer   3  Sparsity: 69.6128%\n",
      "total_backward_count 1713250 real_backward_count 144476   8.433%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.309908/  1.583624, val:  71.67%, val_best:  71.67%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8639%\n",
      "layer   2  Sparsity: 69.1038%\n",
      "layer   3  Sparsity: 70.5755%\n",
      "total_backward_count 1723040 real_backward_count 145238   8.429%\n",
      "fc layer 2 self.abs_max_out: 5313.0\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.304069/  1.648308, val:  60.00%, val_best:  71.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8971%\n",
      "layer   2  Sparsity: 69.5297%\n",
      "layer   3  Sparsity: 71.4308%\n",
      "total_backward_count 1732830 real_backward_count 146007   8.426%\n",
      "lif layer 1 self.abs_max_v: 33214.0\n",
      "lif layer 1 self.abs_max_v: 33293.5\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.347851/  1.622627, val:  58.75%, val_best:  71.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8604%\n",
      "layer   2  Sparsity: 69.3326%\n",
      "layer   3  Sparsity: 71.8531%\n",
      "total_backward_count 1742620 real_backward_count 146762   8.422%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.342485/  1.652623, val:  58.75%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8693%\n",
      "layer   2  Sparsity: 69.7619%\n",
      "layer   3  Sparsity: 71.5111%\n",
      "total_backward_count 1752410 real_backward_count 147479   8.416%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.330979/  1.612541, val:  60.00%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8779%\n",
      "layer   2  Sparsity: 69.8479%\n",
      "layer   3  Sparsity: 72.0140%\n",
      "total_backward_count 1762200 real_backward_count 148248   8.413%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.349492/  1.622242, val:  65.42%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.17 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8937%\n",
      "layer   2  Sparsity: 70.1511%\n",
      "layer   3  Sparsity: 72.4617%\n",
      "total_backward_count 1771990 real_backward_count 148993   8.408%\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.347139/  1.651141, val:  64.58%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8654%\n",
      "layer   2  Sparsity: 70.1746%\n",
      "layer   3  Sparsity: 71.9510%\n",
      "total_backward_count 1781780 real_backward_count 149714   8.402%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.370681/  1.625137, val:  63.75%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.88 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8720%\n",
      "layer   2  Sparsity: 69.7018%\n",
      "layer   3  Sparsity: 72.2330%\n",
      "total_backward_count 1791570 real_backward_count 150499   8.400%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.378298/  1.659881, val:  61.25%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8569%\n",
      "layer   2  Sparsity: 69.7398%\n",
      "layer   3  Sparsity: 71.6772%\n",
      "total_backward_count 1801360 real_backward_count 151260   8.397%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.347329/  1.637096, val:  58.33%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8303%\n",
      "layer   2  Sparsity: 69.8228%\n",
      "layer   3  Sparsity: 70.4509%\n",
      "total_backward_count 1811150 real_backward_count 152011   8.393%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.296637/  1.627403, val:  63.75%, val_best:  71.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.33 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8964%\n",
      "layer   2  Sparsity: 69.7257%\n",
      "layer   3  Sparsity: 70.5395%\n",
      "total_backward_count 1820940 real_backward_count 152747   8.388%\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.352207/  1.652340, val:  61.67%, val_best:  71.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8732%\n",
      "layer   2  Sparsity: 69.5048%\n",
      "layer   3  Sparsity: 71.2509%\n",
      "total_backward_count 1830730 real_backward_count 153540   8.387%\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.347796/  1.667197, val:  55.00%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.10 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8557%\n",
      "layer   2  Sparsity: 70.1664%\n",
      "layer   3  Sparsity: 71.9450%\n",
      "total_backward_count 1840520 real_backward_count 154288   8.383%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.361938/  1.674317, val:  65.42%, val_best:  71.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 81.8658%\n",
      "layer   2  Sparsity: 68.8973%\n",
      "layer   3  Sparsity: 71.5913%\n",
      "total_backward_count 1850310 real_backward_count 155077   8.381%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.350925/  1.657716, val:  60.83%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.65 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 81.8600%\n",
      "layer   2  Sparsity: 69.8504%\n",
      "layer   3  Sparsity: 71.5894%\n",
      "total_backward_count 1860100 real_backward_count 155828   8.377%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.330201/  1.631728, val:  61.67%, val_best:  71.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.27 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8642%\n",
      "layer   2  Sparsity: 70.0784%\n",
      "layer   3  Sparsity: 71.5898%\n",
      "total_backward_count 1869890 real_backward_count 156581   8.374%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.358101/  1.703635, val:  45.83%, val_best:  71.67%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8704%\n",
      "layer   2  Sparsity: 69.7058%\n",
      "layer   3  Sparsity: 71.9976%\n",
      "total_backward_count 1879680 real_backward_count 157367   8.372%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.334946/  1.675155, val:  55.83%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.11 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8573%\n",
      "layer   2  Sparsity: 68.9989%\n",
      "layer   3  Sparsity: 71.3530%\n",
      "total_backward_count 1889470 real_backward_count 158174   8.371%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.323719/  1.591312, val:  69.58%, val_best:  71.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8824%\n",
      "layer   2  Sparsity: 68.8777%\n",
      "layer   3  Sparsity: 69.3062%\n",
      "total_backward_count 1899260 real_backward_count 158880   8.365%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.345884/  1.657336, val:  62.08%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.78 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8786%\n",
      "layer   2  Sparsity: 69.6269%\n",
      "layer   3  Sparsity: 72.0733%\n",
      "total_backward_count 1909050 real_backward_count 159615   8.361%\n",
      "lif layer 1 self.abs_max_v: 33587.0\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.354165/  1.617473, val:  61.67%, val_best:  71.67%, tr:  99.59%, tr_best: 100.00%, epoch time: 71.79 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8746%\n",
      "layer   2  Sparsity: 69.2848%\n",
      "layer   3  Sparsity: 72.5527%\n",
      "total_backward_count 1918840 real_backward_count 160394   8.359%\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.352685/  1.631472, val:  57.92%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.17 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8654%\n",
      "layer   2  Sparsity: 69.1375%\n",
      "layer   3  Sparsity: 71.8956%\n",
      "total_backward_count 1928630 real_backward_count 161161   8.356%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.318588/  1.594245, val:  60.42%, val_best:  71.67%, tr:  99.39%, tr_best: 100.00%, epoch time: 72.78 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8586%\n",
      "layer   2  Sparsity: 69.5720%\n",
      "layer   3  Sparsity: 70.9347%\n",
      "total_backward_count 1938420 real_backward_count 161927   8.354%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.313779/  1.641382, val:  57.50%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.42 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 81.8497%\n",
      "layer   2  Sparsity: 69.8998%\n",
      "layer   3  Sparsity: 72.0425%\n",
      "total_backward_count 1948210 real_backward_count 162707   8.352%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.337450/  1.644732, val:  63.75%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.06 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 81.8950%\n",
      "layer   2  Sparsity: 70.0431%\n",
      "layer   3  Sparsity: 73.1114%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c9c1f6a1164f519f2c6ccfd21a7eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñà‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñà‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñÅ‚ñÖ</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñà‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.99796</td></tr><tr><td>tr_epoch_loss</td><td>1.33745</td></tr><tr><td>val_acc_best</td><td>0.71667</td></tr><tr><td>val_acc_now</td><td>0.6375</td></tr><tr><td>val_loss</td><td>1.64473</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-sweep-114</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7omq4uvm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/7omq4uvm</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_095500-7omq4uvm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3wfhju6n with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_135809-3wfhju6n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3wfhju6n' target=\"_blank\">laced-sweep-118</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3wfhju6n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3wfhju6n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251117_135817_329', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 15, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'random_select_ratio': 8, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 237.0\n",
      "lif layer 1 self.abs_max_v: 237.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 276.5\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 36.0\n",
      "fc layer 1 self.abs_max_out: 283.0\n",
      "lif layer 1 self.abs_max_v: 385.5\n",
      "fc layer 2 self.abs_max_out: 129.0\n",
      "lif layer 2 self.abs_max_v: 140.5\n",
      "fc layer 1 self.abs_max_out: 300.0\n",
      "lif layer 2 self.abs_max_v: 144.5\n",
      "fc layer 1 self.abs_max_out: 365.0\n",
      "fc layer 2 self.abs_max_out: 131.0\n",
      "lif layer 2 self.abs_max_v: 194.0\n",
      "fc layer 1 self.abs_max_out: 461.0\n",
      "lif layer 1 self.abs_max_v: 461.0\n",
      "fc layer 2 self.abs_max_out: 163.0\n",
      "lif layer 2 self.abs_max_v: 223.5\n",
      "fc layer 2 self.abs_max_out: 244.0\n",
      "lif layer 2 self.abs_max_v: 273.0\n",
      "fc layer 1 self.abs_max_out: 549.0\n",
      "lif layer 1 self.abs_max_v: 549.0\n",
      "lif layer 2 self.abs_max_v: 282.0\n",
      "fc layer 3 self.abs_max_out: 30.0\n",
      "fc layer 1 self.abs_max_out: 606.0\n",
      "lif layer 1 self.abs_max_v: 606.0\n",
      "lif layer 2 self.abs_max_v: 326.5\n",
      "fc layer 1 self.abs_max_out: 652.0\n",
      "lif layer 1 self.abs_max_v: 652.0\n",
      "lif layer 2 self.abs_max_v: 388.5\n",
      "fc layer 3 self.abs_max_out: 51.0\n",
      "fc layer 2 self.abs_max_out: 247.0\n",
      "lif layer 1 self.abs_max_v: 755.0\n",
      "fc layer 1 self.abs_max_out: 772.0\n",
      "lif layer 1 self.abs_max_v: 772.0\n",
      "fc layer 1 self.abs_max_out: 790.0\n",
      "lif layer 1 self.abs_max_v: 790.0\n",
      "fc layer 2 self.abs_max_out: 262.0\n",
      "fc layer 2 self.abs_max_out: 284.0\n",
      "lif layer 2 self.abs_max_v: 400.0\n",
      "lif layer 2 self.abs_max_v: 419.0\n",
      "fc layer 1 self.abs_max_out: 831.0\n",
      "lif layer 1 self.abs_max_v: 909.5\n",
      "fc layer 2 self.abs_max_out: 310.0\n",
      "lif layer 2 self.abs_max_v: 439.5\n",
      "lif layer 2 self.abs_max_v: 452.0\n",
      "lif layer 2 self.abs_max_v: 453.5\n",
      "lif layer 2 self.abs_max_v: 495.0\n",
      "fc layer 2 self.abs_max_out: 331.0\n",
      "lif layer 2 self.abs_max_v: 578.5\n",
      "fc layer 1 self.abs_max_out: 903.0\n",
      "fc layer 3 self.abs_max_out: 60.0\n",
      "fc layer 2 self.abs_max_out: 337.0\n",
      "fc layer 2 self.abs_max_out: 359.0\n",
      "fc layer 2 self.abs_max_out: 416.0\n",
      "fc layer 3 self.abs_max_out: 77.0\n",
      "fc layer 2 self.abs_max_out: 442.0\n",
      "fc layer 3 self.abs_max_out: 80.0\n",
      "lif layer 2 self.abs_max_v: 610.5\n",
      "fc layer 3 self.abs_max_out: 91.0\n",
      "fc layer 3 self.abs_max_out: 96.0\n",
      "fc layer 1 self.abs_max_out: 1013.0\n",
      "lif layer 1 self.abs_max_v: 1013.0\n",
      "fc layer 1 self.abs_max_out: 1198.0\n",
      "lif layer 1 self.abs_max_v: 1198.0\n",
      "lif layer 2 self.abs_max_v: 639.0\n",
      "fc layer 1 self.abs_max_out: 1380.0\n",
      "lif layer 1 self.abs_max_v: 1380.0\n",
      "fc layer 2 self.abs_max_out: 451.0\n",
      "lif layer 2 self.abs_max_v: 663.5\n",
      "fc layer 3 self.abs_max_out: 105.0\n",
      "fc layer 2 self.abs_max_out: 457.0\n",
      "fc layer 2 self.abs_max_out: 487.0\n",
      "fc layer 2 self.abs_max_out: 496.0\n",
      "fc layer 2 self.abs_max_out: 500.0\n",
      "fc layer 2 self.abs_max_out: 508.0\n",
      "fc layer 2 self.abs_max_out: 518.0\n",
      "fc layer 3 self.abs_max_out: 111.0\n",
      "fc layer 2 self.abs_max_out: 521.0\n",
      "lif layer 2 self.abs_max_v: 670.0\n",
      "lif layer 2 self.abs_max_v: 682.5\n",
      "fc layer 2 self.abs_max_out: 532.0\n",
      "fc layer 2 self.abs_max_out: 559.0\n",
      "lif layer 2 self.abs_max_v: 716.5\n",
      "fc layer 3 self.abs_max_out: 116.0\n",
      "fc layer 3 self.abs_max_out: 118.0\n",
      "fc layer 2 self.abs_max_out: 586.0\n",
      "fc layer 3 self.abs_max_out: 121.0\n",
      "fc layer 2 self.abs_max_out: 588.0\n",
      "lif layer 2 self.abs_max_v: 750.0\n",
      "fc layer 2 self.abs_max_out: 589.0\n",
      "fc layer 2 self.abs_max_out: 603.0\n",
      "fc layer 2 self.abs_max_out: 612.0\n",
      "lif layer 2 self.abs_max_v: 790.5\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "lif layer 2 self.abs_max_v: 832.0\n",
      "fc layer 2 self.abs_max_out: 627.0\n",
      "fc layer 2 self.abs_max_out: 632.0\n",
      "fc layer 1 self.abs_max_out: 1428.0\n",
      "lif layer 1 self.abs_max_v: 1428.0\n",
      "fc layer 2 self.abs_max_out: 652.0\n",
      "fc layer 3 self.abs_max_out: 139.0\n",
      "fc layer 2 self.abs_max_out: 755.0\n",
      "fc layer 2 self.abs_max_out: 791.0\n",
      "fc layer 3 self.abs_max_out: 144.0\n",
      "fc layer 2 self.abs_max_out: 802.0\n",
      "fc layer 3 self.abs_max_out: 167.0\n",
      "fc layer 1 self.abs_max_out: 1456.0\n",
      "lif layer 1 self.abs_max_v: 1456.0\n",
      "lif layer 1 self.abs_max_v: 1464.0\n",
      "fc layer 1 self.abs_max_out: 1472.0\n",
      "lif layer 1 self.abs_max_v: 1472.0\n",
      "fc layer 1 self.abs_max_out: 1476.0\n",
      "lif layer 1 self.abs_max_v: 1476.0\n",
      "fc layer 1 self.abs_max_out: 1499.0\n",
      "lif layer 1 self.abs_max_v: 1499.0\n",
      "lif layer 1 self.abs_max_v: 1505.0\n",
      "lif layer 1 self.abs_max_v: 1526.0\n",
      "lif layer 1 self.abs_max_v: 1542.0\n",
      "fc layer 1 self.abs_max_out: 1513.0\n",
      "fc layer 1 self.abs_max_out: 1551.0\n",
      "lif layer 1 self.abs_max_v: 1551.0\n",
      "fc layer 1 self.abs_max_out: 1657.0\n",
      "lif layer 1 self.abs_max_v: 1657.0\n",
      "fc layer 1 self.abs_max_out: 1741.0\n",
      "lif layer 1 self.abs_max_v: 1741.0\n",
      "fc layer 1 self.abs_max_out: 1743.0\n",
      "lif layer 1 self.abs_max_v: 1743.0\n",
      "fc layer 1 self.abs_max_out: 1769.0\n",
      "lif layer 1 self.abs_max_v: 1769.0\n",
      "fc layer 2 self.abs_max_out: 842.0\n",
      "lif layer 2 self.abs_max_v: 842.0\n",
      "fc layer 1 self.abs_max_out: 1802.0\n",
      "lif layer 1 self.abs_max_v: 1802.0\n",
      "fc layer 1 self.abs_max_out: 1836.0\n",
      "lif layer 1 self.abs_max_v: 1836.0\n",
      "lif layer 1 self.abs_max_v: 1886.5\n",
      "lif layer 1 self.abs_max_v: 1914.5\n",
      "fc layer 1 self.abs_max_out: 1841.0\n",
      "lif layer 1 self.abs_max_v: 1977.5\n",
      "fc layer 1 self.abs_max_out: 1867.0\n",
      "fc layer 1 self.abs_max_out: 1875.0\n",
      "fc layer 1 self.abs_max_out: 1988.0\n",
      "lif layer 1 self.abs_max_v: 1988.0\n",
      "fc layer 1 self.abs_max_out: 2064.0\n",
      "lif layer 1 self.abs_max_v: 2064.0\n",
      "fc layer 1 self.abs_max_out: 2167.0\n",
      "lif layer 1 self.abs_max_v: 2167.0\n",
      "fc layer 2 self.abs_max_out: 849.0\n",
      "lif layer 2 self.abs_max_v: 849.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.133867/  2.172448, val:  40.83%, val_best:  40.83%, tr:  85.80%, tr_best:  85.80%, epoch time: 74.17 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.6930%\n",
      "layer   2  Sparsity: 88.6284%\n",
      "layer   3  Sparsity: 93.8185%\n",
      "total_backward_count 9790 real_backward_count 3438  35.117%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 1 self.abs_max_v: 2207.5\n",
      "lif layer 1 self.abs_max_v: 2264.0\n",
      "lif layer 1 self.abs_max_v: 2321.0\n",
      "fc layer 1 self.abs_max_out: 2177.0\n",
      "fc layer 1 self.abs_max_out: 2212.0\n",
      "lif layer 2 self.abs_max_v: 883.5\n",
      "fc layer 1 self.abs_max_out: 2302.0\n",
      "fc layer 2 self.abs_max_out: 877.0\n",
      "lif layer 1 self.abs_max_v: 2353.5\n",
      "lif layer 2 self.abs_max_v: 901.5\n",
      "fc layer 2 self.abs_max_out: 894.0\n",
      "lif layer 2 self.abs_max_v: 940.0\n",
      "fc layer 2 self.abs_max_out: 896.0\n",
      "lif layer 1 self.abs_max_v: 2380.5\n",
      "fc layer 1 self.abs_max_out: 2342.0\n",
      "fc layer 1 self.abs_max_out: 2394.0\n",
      "lif layer 1 self.abs_max_v: 2394.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.092101/  2.153187, val:  45.00%, val_best:  45.00%, tr:  97.96%, tr_best:  97.96%, epoch time: 73.63 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7054%\n",
      "layer   2  Sparsity: 88.1128%\n",
      "layer   3  Sparsity: 91.9911%\n",
      "total_backward_count 19580 real_backward_count 5387  27.513%\n",
      "fc layer 2 self.abs_max_out: 902.0\n",
      "lif layer 2 self.abs_max_v: 956.5\n",
      "fc layer 3 self.abs_max_out: 174.0\n",
      "fc layer 1 self.abs_max_out: 2498.0\n",
      "lif layer 1 self.abs_max_v: 2498.0\n",
      "fc layer 1 self.abs_max_out: 2516.0\n",
      "lif layer 1 self.abs_max_v: 2516.0\n",
      "lif layer 1 self.abs_max_v: 2808.0\n",
      "fc layer 1 self.abs_max_out: 2589.0\n",
      "fc layer 1 self.abs_max_out: 2667.0\n",
      "fc layer 2 self.abs_max_out: 908.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.068902/  2.148742, val:  44.58%, val_best:  45.00%, tr:  98.88%, tr_best:  98.88%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7044%\n",
      "layer   2  Sparsity: 87.7176%\n",
      "layer   3  Sparsity: 90.9376%\n",
      "total_backward_count 29370 real_backward_count 7189  24.477%\n",
      "fc layer 2 self.abs_max_out: 916.0\n",
      "lif layer 2 self.abs_max_v: 978.5\n",
      "fc layer 2 self.abs_max_out: 1018.0\n",
      "lif layer 2 self.abs_max_v: 1018.0\n",
      "lif layer 2 self.abs_max_v: 1054.0\n",
      "fc layer 1 self.abs_max_out: 2668.0\n",
      "lif layer 1 self.abs_max_v: 3076.5\n",
      "fc layer 1 self.abs_max_out: 2746.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.062054/  2.153154, val:  42.92%, val_best:  45.00%, tr:  99.18%, tr_best:  99.18%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6748%\n",
      "layer   2  Sparsity: 87.7477%\n",
      "layer   3  Sparsity: 90.4876%\n",
      "total_backward_count 39160 real_backward_count 8822  22.528%\n",
      "fc layer 1 self.abs_max_out: 2791.0\n",
      "lif layer 1 self.abs_max_v: 3555.5\n",
      "fc layer 1 self.abs_max_out: 2845.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.071614/  2.144899, val:  51.67%, val_best:  51.67%, tr:  99.28%, tr_best:  99.28%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6777%\n",
      "layer   2  Sparsity: 87.8758%\n",
      "layer   3  Sparsity: 91.0674%\n",
      "total_backward_count 48950 real_backward_count 10399  21.244%\n",
      "fc layer 1 self.abs_max_out: 2849.0\n",
      "fc layer 1 self.abs_max_out: 2869.0\n",
      "fc layer 1 self.abs_max_out: 2938.0\n",
      "lif layer 2 self.abs_max_v: 1070.5\n",
      "lif layer 2 self.abs_max_v: 1086.5\n",
      "lif layer 2 self.abs_max_v: 1132.5\n",
      "lif layer 2 self.abs_max_v: 1135.0\n",
      "lif layer 2 self.abs_max_v: 1174.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.052631/  2.135945, val:  46.67%, val_best:  51.67%, tr:  98.88%, tr_best:  99.28%, epoch time: 72.98 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6919%\n",
      "layer   2  Sparsity: 88.2152%\n",
      "layer   3  Sparsity: 90.5536%\n",
      "total_backward_count 58740 real_backward_count 12010  20.446%\n",
      "fc layer 2 self.abs_max_out: 1043.0\n",
      "lif layer 2 self.abs_max_v: 1233.0\n",
      "lif layer 2 self.abs_max_v: 1281.5\n",
      "lif layer 1 self.abs_max_v: 3625.5\n",
      "fc layer 2 self.abs_max_out: 1071.0\n",
      "lif layer 1 self.abs_max_v: 3732.0\n",
      "lif layer 1 self.abs_max_v: 4049.0\n",
      "fc layer 1 self.abs_max_out: 3014.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.052150/  2.136661, val:  57.08%, val_best:  57.08%, tr:  99.59%, tr_best:  99.59%, epoch time: 72.21 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6820%\n",
      "layer   2  Sparsity: 88.5626%\n",
      "layer   3  Sparsity: 90.3539%\n",
      "total_backward_count 68530 real_backward_count 13581  19.818%\n",
      "fc layer 2 self.abs_max_out: 1087.0\n",
      "fc layer 2 self.abs_max_out: 1093.0\n",
      "fc layer 3 self.abs_max_out: 181.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  2.049164/  2.126728, val:  52.92%, val_best:  57.08%, tr:  99.69%, tr_best:  99.69%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6900%\n",
      "layer   2  Sparsity: 88.3190%\n",
      "layer   3  Sparsity: 90.0787%\n",
      "total_backward_count 78320 real_backward_count 15002  19.155%\n",
      "fc layer 2 self.abs_max_out: 1099.0\n",
      "fc layer 1 self.abs_max_out: 3019.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  2.054574/  2.117192, val:  59.58%, val_best:  59.58%, tr:  99.49%, tr_best:  99.69%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6853%\n",
      "layer   2  Sparsity: 88.3654%\n",
      "layer   3  Sparsity: 90.2908%\n",
      "total_backward_count 88110 real_backward_count 16460  18.681%\n",
      "fc layer 1 self.abs_max_out: 3129.0\n",
      "fc layer 2 self.abs_max_out: 1150.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  2.039362/  2.122041, val:  47.92%, val_best:  59.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 71.86 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.7107%\n",
      "layer   2  Sparsity: 88.2988%\n",
      "layer   3  Sparsity: 90.2647%\n",
      "total_backward_count 97900 real_backward_count 17823  18.205%\n",
      "fc layer 3 self.abs_max_out: 182.0\n",
      "fc layer 2 self.abs_max_out: 1196.0\n",
      "fc layer 1 self.abs_max_out: 3139.0\n",
      "fc layer 1 self.abs_max_out: 3183.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  2.044064/  2.147468, val:  50.42%, val_best:  59.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6907%\n",
      "layer   2  Sparsity: 88.0494%\n",
      "layer   3  Sparsity: 90.6903%\n",
      "total_backward_count 107690 real_backward_count 19236  17.862%\n",
      "fc layer 2 self.abs_max_out: 1241.0\n",
      "lif layer 2 self.abs_max_v: 1288.5\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  2.055285/  2.132998, val:  59.58%, val_best:  59.58%, tr:  99.59%, tr_best:  99.80%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6830%\n",
      "layer   2  Sparsity: 87.7430%\n",
      "layer   3  Sparsity: 90.7786%\n",
      "total_backward_count 117480 real_backward_count 20667  17.592%\n",
      "fc layer 1 self.abs_max_out: 3199.0\n",
      "fc layer 2 self.abs_max_out: 1243.0\n",
      "fc layer 2 self.abs_max_out: 1253.0\n",
      "lif layer 1 self.abs_max_v: 4209.5\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  2.048542/  2.126707, val:  54.58%, val_best:  59.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 71.93 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6834%\n",
      "layer   2  Sparsity: 87.7836%\n",
      "layer   3  Sparsity: 90.6902%\n",
      "total_backward_count 127270 real_backward_count 22042  17.319%\n",
      "fc layer 1 self.abs_max_out: 3238.0\n",
      "lif layer 1 self.abs_max_v: 4213.5\n",
      "fc layer 1 self.abs_max_out: 3277.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  2.047693/  2.127475, val:  55.00%, val_best:  59.58%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.38 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6995%\n",
      "layer   2  Sparsity: 87.6449%\n",
      "layer   3  Sparsity: 90.8404%\n",
      "total_backward_count 137060 real_backward_count 23387  17.063%\n",
      "fc layer 3 self.abs_max_out: 183.0\n",
      "fc layer 1 self.abs_max_out: 3317.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  2.033933/  2.114142, val:  53.75%, val_best:  59.58%, tr:  99.49%, tr_best:  99.80%, epoch time: 72.47 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6985%\n",
      "layer   2  Sparsity: 87.3364%\n",
      "layer   3  Sparsity: 90.8345%\n",
      "total_backward_count 146850 real_backward_count 24669  16.799%\n",
      "fc layer 3 self.abs_max_out: 190.0\n",
      "fc layer 3 self.abs_max_out: 191.0\n",
      "fc layer 1 self.abs_max_out: 3322.0\n",
      "lif layer 2 self.abs_max_v: 1309.5\n",
      "fc layer 2 self.abs_max_out: 1379.0\n",
      "lif layer 2 self.abs_max_v: 1379.0\n",
      "lif layer 1 self.abs_max_v: 4595.5\n",
      "lif layer 1 self.abs_max_v: 4647.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  2.021786/  2.123856, val:  54.58%, val_best:  59.58%, tr:  99.69%, tr_best:  99.80%, epoch time: 72.51 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6997%\n",
      "layer   2  Sparsity: 87.0415%\n",
      "layer   3  Sparsity: 90.2725%\n",
      "total_backward_count 156640 real_backward_count 26018  16.610%\n",
      "fc layer 3 self.abs_max_out: 192.0\n",
      "fc layer 1 self.abs_max_out: 3329.0\n",
      "fc layer 1 self.abs_max_out: 3360.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  2.020483/  2.115134, val:  57.92%, val_best:  59.58%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6972%\n",
      "layer   2  Sparsity: 87.2425%\n",
      "layer   3  Sparsity: 90.1977%\n",
      "total_backward_count 166430 real_backward_count 27336  16.425%\n",
      "lif layer 2 self.abs_max_v: 1386.0\n",
      "lif layer 1 self.abs_max_v: 4792.0\n",
      "lif layer 1 self.abs_max_v: 4847.0\n",
      "fc layer 1 self.abs_max_out: 3396.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  2.039650/  2.123887, val:  63.75%, val_best:  63.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6929%\n",
      "layer   2  Sparsity: 87.4004%\n",
      "layer   3  Sparsity: 90.7250%\n",
      "total_backward_count 176220 real_backward_count 28601  16.230%\n",
      "fc layer 1 self.abs_max_out: 3407.0\n",
      "lif layer 2 self.abs_max_v: 1394.0\n",
      "lif layer 2 self.abs_max_v: 1425.0\n",
      "lif layer 1 self.abs_max_v: 4982.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  2.040982/  2.126582, val:  45.83%, val_best:  63.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 71.92 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6702%\n",
      "layer   2  Sparsity: 87.1647%\n",
      "layer   3  Sparsity: 90.6446%\n",
      "total_backward_count 186010 real_backward_count 29907  16.078%\n",
      "fc layer 3 self.abs_max_out: 196.0\n",
      "fc layer 3 self.abs_max_out: 201.0\n",
      "fc layer 3 self.abs_max_out: 204.0\n",
      "fc layer 1 self.abs_max_out: 3464.0\n",
      "fc layer 3 self.abs_max_out: 205.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  2.027116/  2.119502, val:  47.08%, val_best:  63.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6995%\n",
      "layer   2  Sparsity: 87.3799%\n",
      "layer   3  Sparsity: 90.6877%\n",
      "total_backward_count 195800 real_backward_count 31163  15.916%\n",
      "fc layer 1 self.abs_max_out: 3593.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  2.020466/  2.112944, val:  50.42%, val_best:  63.75%, tr:  99.39%, tr_best:  99.80%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6923%\n",
      "layer   2  Sparsity: 87.6015%\n",
      "layer   3  Sparsity: 90.8233%\n",
      "total_backward_count 205590 real_backward_count 32460  15.789%\n",
      "lif layer 2 self.abs_max_v: 1461.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  2.033022/  2.119719, val:  63.75%, val_best:  63.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 72.82 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6990%\n",
      "layer   2  Sparsity: 87.4702%\n",
      "layer   3  Sparsity: 91.0940%\n",
      "total_backward_count 215380 real_backward_count 33767  15.678%\n",
      "fc layer 1 self.abs_max_out: 3610.0\n",
      "lif layer 1 self.abs_max_v: 4997.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  2.031168/  2.099684, val:  59.17%, val_best:  63.75%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.44 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6818%\n",
      "layer   2  Sparsity: 87.3185%\n",
      "layer   3  Sparsity: 90.7186%\n",
      "total_backward_count 225170 real_backward_count 35071  15.575%\n",
      "lif layer 2 self.abs_max_v: 1526.0\n",
      "lif layer 1 self.abs_max_v: 5037.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  2.017048/  2.100666, val:  60.00%, val_best:  63.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6926%\n",
      "layer   2  Sparsity: 87.3487%\n",
      "layer   3  Sparsity: 90.4443%\n",
      "total_backward_count 234960 real_backward_count 36308  15.453%\n",
      "lif layer 2 self.abs_max_v: 1563.0\n",
      "fc layer 2 self.abs_max_out: 1381.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  2.018550/  2.098205, val:  60.42%, val_best:  63.75%, tr:  99.59%, tr_best:  99.80%, epoch time: 72.56 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6939%\n",
      "layer   2  Sparsity: 87.5339%\n",
      "layer   3  Sparsity: 90.4870%\n",
      "total_backward_count 244750 real_backward_count 37577  15.353%\n",
      "fc layer 1 self.abs_max_out: 3846.0\n",
      "lif layer 2 self.abs_max_v: 1612.5\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  2.015866/  2.096084, val:  53.33%, val_best:  63.75%, tr:  99.69%, tr_best:  99.80%, epoch time: 72.83 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6770%\n",
      "layer   2  Sparsity: 87.0884%\n",
      "layer   3  Sparsity: 90.4300%\n",
      "total_backward_count 254540 real_backward_count 38855  15.265%\n",
      "fc layer 3 self.abs_max_out: 214.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  2.005186/  2.082780, val:  65.42%, val_best:  65.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 72.09 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6921%\n",
      "layer   2  Sparsity: 87.3381%\n",
      "layer   3  Sparsity: 90.6094%\n",
      "total_backward_count 264330 real_backward_count 40091  15.167%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  2.003883/  2.078958, val:  64.17%, val_best:  65.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 71.77 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6957%\n",
      "layer   2  Sparsity: 87.1414%\n",
      "layer   3  Sparsity: 90.4117%\n",
      "total_backward_count 274120 real_backward_count 41339  15.081%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  2.004084/  2.098367, val:  62.92%, val_best:  65.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 72.03 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6835%\n",
      "layer   2  Sparsity: 87.4165%\n",
      "layer   3  Sparsity: 90.4524%\n",
      "total_backward_count 283910 real_backward_count 42555  14.989%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  2.012301/  2.092088, val:  57.50%, val_best:  65.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.82 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6936%\n",
      "layer   2  Sparsity: 87.3866%\n",
      "layer   3  Sparsity: 90.4226%\n",
      "total_backward_count 293700 real_backward_count 43767  14.902%\n",
      "lif layer 1 self.abs_max_v: 5131.5\n",
      "lif layer 1 self.abs_max_v: 5514.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  2.015185/  2.097045, val:  65.00%, val_best:  65.42%, tr:  99.59%, tr_best:  99.80%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6946%\n",
      "layer   2  Sparsity: 87.2467%\n",
      "layer   3  Sparsity: 90.5061%\n",
      "total_backward_count 303490 real_backward_count 44965  14.816%\n",
      "lif layer 2 self.abs_max_v: 1656.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  2.004568/  2.094625, val:  60.42%, val_best:  65.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7033%\n",
      "layer   2  Sparsity: 87.1144%\n",
      "layer   3  Sparsity: 90.5270%\n",
      "total_backward_count 313280 real_backward_count 46201  14.748%\n",
      "fc layer 1 self.abs_max_out: 3959.0\n",
      "fc layer 2 self.abs_max_out: 1402.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  2.011100/  2.100903, val:  58.75%, val_best:  65.42%, tr:  99.39%, tr_best:  99.80%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6875%\n",
      "layer   2  Sparsity: 87.4747%\n",
      "layer   3  Sparsity: 90.5426%\n",
      "total_backward_count 323070 real_backward_count 47382  14.666%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  2.009396/  2.097605, val:  57.92%, val_best:  65.42%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.39 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6954%\n",
      "layer   2  Sparsity: 87.1571%\n",
      "layer   3  Sparsity: 90.6076%\n",
      "total_backward_count 332860 real_backward_count 48576  14.594%\n",
      "fc layer 2 self.abs_max_out: 1433.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  2.000803/  2.075670, val:  62.92%, val_best:  65.42%, tr:  99.28%, tr_best:  99.80%, epoch time: 73.19 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6787%\n",
      "layer   2  Sparsity: 87.2075%\n",
      "layer   3  Sparsity: 90.6357%\n",
      "total_backward_count 342650 real_backward_count 49764  14.523%\n",
      "fc layer 2 self.abs_max_out: 1443.0\n",
      "fc layer 2 self.abs_max_out: 1470.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.997957/  2.087020, val:  67.08%, val_best:  67.08%, tr:  99.69%, tr_best:  99.80%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6980%\n",
      "layer   2  Sparsity: 87.4122%\n",
      "layer   3  Sparsity: 90.5231%\n",
      "total_backward_count 352440 real_backward_count 50951  14.457%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.994364/  2.078795, val:  69.17%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6944%\n",
      "layer   2  Sparsity: 87.4220%\n",
      "layer   3  Sparsity: 90.3251%\n",
      "total_backward_count 362230 real_backward_count 52103  14.384%\n",
      "lif layer 2 self.abs_max_v: 1682.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.981865/  2.074875, val:  61.67%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.14 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6825%\n",
      "layer   2  Sparsity: 87.4848%\n",
      "layer   3  Sparsity: 89.8391%\n",
      "total_backward_count 372020 real_backward_count 53258  14.316%\n",
      "fc layer 3 self.abs_max_out: 215.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.982942/  2.068231, val:  71.25%, val_best:  71.25%, tr:  99.39%, tr_best: 100.00%, epoch time: 72.25 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.7050%\n",
      "layer   2  Sparsity: 87.5055%\n",
      "layer   3  Sparsity: 90.0080%\n",
      "total_backward_count 381810 real_backward_count 54445  14.260%\n",
      "fc layer 1 self.abs_max_out: 3993.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.989675/  2.078678, val:  56.67%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.35 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6767%\n",
      "layer   2  Sparsity: 87.5605%\n",
      "layer   3  Sparsity: 90.1217%\n",
      "total_backward_count 391600 real_backward_count 55582  14.194%\n",
      "fc layer 2 self.abs_max_out: 1504.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.980092/  2.055411, val:  61.67%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6900%\n",
      "layer   2  Sparsity: 87.0936%\n",
      "layer   3  Sparsity: 89.6547%\n",
      "total_backward_count 401390 real_backward_count 56738  14.135%\n",
      "fc layer 3 self.abs_max_out: 218.0\n",
      "fc layer 2 self.abs_max_out: 1531.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.965999/  2.079364, val:  66.25%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6958%\n",
      "layer   2  Sparsity: 87.1483%\n",
      "layer   3  Sparsity: 89.7696%\n",
      "total_backward_count 411180 real_backward_count 57833  14.065%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.974947/  2.068410, val:  67.50%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6953%\n",
      "layer   2  Sparsity: 86.8773%\n",
      "layer   3  Sparsity: 89.9079%\n",
      "total_backward_count 420970 real_backward_count 58969  14.008%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.978424/  2.080114, val:  62.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6903%\n",
      "layer   2  Sparsity: 86.8286%\n",
      "layer   3  Sparsity: 89.9582%\n",
      "total_backward_count 430760 real_backward_count 60071  13.945%\n",
      "lif layer 2 self.abs_max_v: 1692.5\n",
      "lif layer 2 self.abs_max_v: 1720.0\n",
      "lif layer 2 self.abs_max_v: 1798.5\n",
      "lif layer 2 self.abs_max_v: 1816.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.970254/  2.052134, val:  65.00%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.37 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6935%\n",
      "layer   2  Sparsity: 86.5447%\n",
      "layer   3  Sparsity: 89.6221%\n",
      "total_backward_count 440550 real_backward_count 61211  13.894%\n",
      "fc layer 3 self.abs_max_out: 225.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.959590/  2.037893, val:  72.92%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.70 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.7040%\n",
      "layer   2  Sparsity: 86.3068%\n",
      "layer   3  Sparsity: 89.9105%\n",
      "total_backward_count 450340 real_backward_count 62297  13.833%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.960619/  2.056177, val:  66.67%, val_best:  72.92%, tr:  99.49%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6901%\n",
      "layer   2  Sparsity: 86.2857%\n",
      "layer   3  Sparsity: 89.9911%\n",
      "total_backward_count 460130 real_backward_count 63362  13.770%\n",
      "fc layer 2 self.abs_max_out: 1606.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.957133/  2.061729, val:  54.58%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.46 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.7016%\n",
      "layer   2  Sparsity: 86.4908%\n",
      "layer   3  Sparsity: 89.8324%\n",
      "total_backward_count 469920 real_backward_count 64460  13.717%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.968449/  2.058403, val:  65.00%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.57 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.7048%\n",
      "layer   2  Sparsity: 86.4270%\n",
      "layer   3  Sparsity: 89.6817%\n",
      "total_backward_count 479710 real_backward_count 65544  13.663%\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.955687/  2.047252, val:  65.83%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6866%\n",
      "layer   2  Sparsity: 86.0254%\n",
      "layer   3  Sparsity: 89.3911%\n",
      "total_backward_count 489500 real_backward_count 66606  13.607%\n",
      "fc layer 1 self.abs_max_out: 4001.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.951445/  2.058211, val:  65.83%, val_best:  72.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6985%\n",
      "layer   2  Sparsity: 86.1735%\n",
      "layer   3  Sparsity: 89.3190%\n",
      "total_backward_count 499290 real_backward_count 67746  13.568%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.950379/  2.063744, val:  75.00%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6818%\n",
      "layer   2  Sparsity: 86.0171%\n",
      "layer   3  Sparsity: 89.2867%\n",
      "total_backward_count 509080 real_backward_count 68861  13.527%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.950001/  2.025735, val:  62.08%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6952%\n",
      "layer   2  Sparsity: 86.1751%\n",
      "layer   3  Sparsity: 89.0853%\n",
      "total_backward_count 518870 real_backward_count 69970  13.485%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.934719/  2.024977, val:  64.17%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.12 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6849%\n",
      "layer   2  Sparsity: 86.2073%\n",
      "layer   3  Sparsity: 89.2381%\n",
      "total_backward_count 528660 real_backward_count 71023  13.435%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.940339/  2.037681, val:  67.92%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6902%\n",
      "layer   2  Sparsity: 86.3636%\n",
      "layer   3  Sparsity: 89.4763%\n",
      "total_backward_count 538450 real_backward_count 72090  13.388%\n",
      "lif layer 1 self.abs_max_v: 5525.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.954059/  2.049091, val:  65.42%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6891%\n",
      "layer   2  Sparsity: 86.3134%\n",
      "layer   3  Sparsity: 89.6565%\n",
      "total_backward_count 548240 real_backward_count 73162  13.345%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.963967/  2.052588, val:  73.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6974%\n",
      "layer   2  Sparsity: 86.1424%\n",
      "layer   3  Sparsity: 89.6599%\n",
      "total_backward_count 558030 real_backward_count 74190  13.295%\n",
      "fc layer 1 self.abs_max_out: 4102.0\n",
      "fc layer 1 self.abs_max_out: 4196.0\n",
      "lif layer 1 self.abs_max_v: 5684.5\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.955192/  2.044498, val:  70.42%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.55 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6737%\n",
      "layer   2  Sparsity: 86.1389%\n",
      "layer   3  Sparsity: 89.4172%\n",
      "total_backward_count 567820 real_backward_count 75232  13.249%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.946373/  2.053738, val:  60.00%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 71.92 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6809%\n",
      "layer   2  Sparsity: 86.1725%\n",
      "layer   3  Sparsity: 89.5378%\n",
      "total_backward_count 577610 real_backward_count 76264  13.203%\n",
      "fc layer 1 self.abs_max_out: 4223.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.951305/  2.050347, val:  60.83%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.29 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.7045%\n",
      "layer   2  Sparsity: 85.9656%\n",
      "layer   3  Sparsity: 89.4138%\n",
      "total_backward_count 587400 real_backward_count 77298  13.159%\n",
      "lif layer 2 self.abs_max_v: 1852.5\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.936162/  2.031648, val:  60.83%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.02 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6860%\n",
      "layer   2  Sparsity: 85.7954%\n",
      "layer   3  Sparsity: 88.9675%\n",
      "total_backward_count 597190 real_backward_count 78335  13.117%\n",
      "lif layer 1 self.abs_max_v: 5856.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.930518/  2.013043, val:  72.50%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.42 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6895%\n",
      "layer   2  Sparsity: 85.8431%\n",
      "layer   3  Sparsity: 89.1649%\n",
      "total_backward_count 606980 real_backward_count 79349  13.073%\n",
      "fc layer 2 self.abs_max_out: 1625.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.930992/  2.035064, val:  67.92%, val_best:  75.00%, tr:  99.39%, tr_best: 100.00%, epoch time: 72.28 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6934%\n",
      "layer   2  Sparsity: 86.3022%\n",
      "layer   3  Sparsity: 89.3326%\n",
      "total_backward_count 616770 real_backward_count 80428  13.040%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.936491/  2.040496, val:  62.92%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.63 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6951%\n",
      "layer   2  Sparsity: 86.0617%\n",
      "layer   3  Sparsity: 89.4785%\n",
      "total_backward_count 626560 real_backward_count 81404  12.992%\n",
      "lif layer 2 self.abs_max_v: 1857.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.941103/  2.037694, val:  67.92%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.49 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6867%\n",
      "layer   2  Sparsity: 85.8254%\n",
      "layer   3  Sparsity: 89.2869%\n",
      "total_backward_count 636350 real_backward_count 82407  12.950%\n",
      "fc layer 1 self.abs_max_out: 4263.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.941131/  2.054098, val:  67.50%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.20 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6880%\n",
      "layer   2  Sparsity: 85.6538%\n",
      "layer   3  Sparsity: 89.3679%\n",
      "total_backward_count 646140 real_backward_count 83421  12.911%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.932934/  2.026205, val:  72.08%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6873%\n",
      "layer   2  Sparsity: 85.5322%\n",
      "layer   3  Sparsity: 89.2216%\n",
      "total_backward_count 655930 real_backward_count 84464  12.877%\n",
      "fc layer 2 self.abs_max_out: 1639.0\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.923115/  2.025798, val:  66.67%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.07 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.7114%\n",
      "layer   2  Sparsity: 85.5232%\n",
      "layer   3  Sparsity: 88.9465%\n",
      "total_backward_count 665720 real_backward_count 85458  12.837%\n",
      "fc layer 1 self.abs_max_out: 4330.0\n",
      "lif layer 2 self.abs_max_v: 1867.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.931229/  2.029055, val:  67.92%, val_best:  75.00%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6774%\n",
      "layer   2  Sparsity: 85.8898%\n",
      "layer   3  Sparsity: 89.0654%\n",
      "total_backward_count 675510 real_backward_count 86446  12.797%\n",
      "fc layer 2 self.abs_max_out: 1695.0\n",
      "lif layer 2 self.abs_max_v: 1908.5\n",
      "lif layer 2 self.abs_max_v: 1915.5\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.925378/  2.031946, val:  59.17%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6998%\n",
      "layer   2  Sparsity: 85.7820%\n",
      "layer   3  Sparsity: 88.9530%\n",
      "total_backward_count 685300 real_backward_count 87420  12.756%\n",
      "lif layer 2 self.abs_max_v: 1916.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.926003/  2.022931, val:  71.25%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6881%\n",
      "layer   2  Sparsity: 86.0400%\n",
      "layer   3  Sparsity: 89.0940%\n",
      "total_backward_count 695090 real_backward_count 88453  12.725%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.925758/  2.037204, val:  57.50%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.7142%\n",
      "layer   2  Sparsity: 86.1219%\n",
      "layer   3  Sparsity: 89.2716%\n",
      "total_backward_count 704880 real_backward_count 89471  12.693%\n",
      "lif layer 1 self.abs_max_v: 6018.5\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.929848/  2.032318, val:  67.50%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6837%\n",
      "layer   2  Sparsity: 85.8390%\n",
      "layer   3  Sparsity: 89.3540%\n",
      "total_backward_count 714670 real_backward_count 90485  12.661%\n",
      "lif layer 2 self.abs_max_v: 2012.5\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.914562/  2.027651, val:  68.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6894%\n",
      "layer   2  Sparsity: 85.7731%\n",
      "layer   3  Sparsity: 89.1521%\n",
      "total_backward_count 724460 real_backward_count 91481  12.627%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.921269/  2.033108, val:  67.50%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.66 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6867%\n",
      "layer   2  Sparsity: 85.5987%\n",
      "layer   3  Sparsity: 89.1298%\n",
      "total_backward_count 734250 real_backward_count 92525  12.601%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.916607/  1.998989, val:  72.92%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6697%\n",
      "layer   2  Sparsity: 85.7649%\n",
      "layer   3  Sparsity: 88.8632%\n",
      "total_backward_count 744040 real_backward_count 93527  12.570%\n",
      "fc layer 3 self.abs_max_out: 257.0\n",
      "lif layer 1 self.abs_max_v: 6136.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.914210/  2.032531, val:  57.08%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.88 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.7138%\n",
      "layer   2  Sparsity: 85.9204%\n",
      "layer   3  Sparsity: 89.1676%\n",
      "total_backward_count 753830 real_backward_count 94495  12.535%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.918280/  2.021500, val:  64.58%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7002%\n",
      "layer   2  Sparsity: 85.7741%\n",
      "layer   3  Sparsity: 89.1656%\n",
      "total_backward_count 763620 real_backward_count 95490  12.505%\n",
      "fc layer 2 self.abs_max_out: 1705.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.913396/  2.021876, val:  66.67%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6975%\n",
      "layer   2  Sparsity: 85.6027%\n",
      "layer   3  Sparsity: 89.1411%\n",
      "total_backward_count 773410 real_backward_count 96454  12.471%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.914309/  2.027074, val:  66.25%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6873%\n",
      "layer   2  Sparsity: 85.7334%\n",
      "layer   3  Sparsity: 89.0880%\n",
      "total_backward_count 783200 real_backward_count 97429  12.440%\n",
      "fc layer 1 self.abs_max_out: 4439.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.920255/  2.014492, val:  70.42%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.45 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6951%\n",
      "layer   2  Sparsity: 85.7190%\n",
      "layer   3  Sparsity: 89.1473%\n",
      "total_backward_count 792990 real_backward_count 98354  12.403%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.904650/  2.023077, val:  66.25%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.40 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.7025%\n",
      "layer   2  Sparsity: 85.5009%\n",
      "layer   3  Sparsity: 88.8380%\n",
      "total_backward_count 802780 real_backward_count 99293  12.369%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.905782/  2.015144, val:  64.58%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.30 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6824%\n",
      "layer   2  Sparsity: 85.8643%\n",
      "layer   3  Sparsity: 88.6258%\n",
      "total_backward_count 812570 real_backward_count 100266  12.339%\n",
      "fc layer 1 self.abs_max_out: 4462.0\n",
      "lif layer 1 self.abs_max_v: 6432.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.900610/  2.011479, val:  69.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6980%\n",
      "layer   2  Sparsity: 85.9377%\n",
      "layer   3  Sparsity: 88.6728%\n",
      "total_backward_count 822360 real_backward_count 101258  12.313%\n",
      "lif layer 1 self.abs_max_v: 6471.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.898364/  2.004713, val:  71.67%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.64 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 89.6932%\n",
      "layer   2  Sparsity: 86.0628%\n",
      "layer   3  Sparsity: 88.7895%\n",
      "total_backward_count 832150 real_backward_count 102198  12.281%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.905077/  2.021723, val:  62.92%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 71.62 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 89.7034%\n",
      "layer   2  Sparsity: 86.1135%\n",
      "layer   3  Sparsity: 88.9609%\n",
      "total_backward_count 841940 real_backward_count 103169  12.254%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.895953/  1.995899, val:  66.25%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 89.6828%\n",
      "layer   2  Sparsity: 85.9847%\n",
      "layer   3  Sparsity: 88.7891%\n",
      "total_backward_count 851730 real_backward_count 104156  12.229%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.901152/  2.007072, val:  67.08%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.74 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6963%\n",
      "layer   2  Sparsity: 85.8337%\n",
      "layer   3  Sparsity: 88.9403%\n",
      "total_backward_count 861520 real_backward_count 105116  12.201%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.893803/  2.003867, val:  59.17%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.63 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6915%\n",
      "layer   2  Sparsity: 85.7429%\n",
      "layer   3  Sparsity: 88.5855%\n",
      "total_backward_count 871310 real_backward_count 106049  12.171%\n",
      "lif layer 1 self.abs_max_v: 6578.5\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.899593/  1.995313, val:  71.25%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 71.81 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6950%\n",
      "layer   2  Sparsity: 85.7484%\n",
      "layer   3  Sparsity: 88.6587%\n",
      "total_backward_count 881100 real_backward_count 106981  12.142%\n",
      "fc layer 2 self.abs_max_out: 1718.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.896198/  2.016350, val:  65.83%, val_best:  75.00%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.06 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6934%\n",
      "layer   2  Sparsity: 85.8188%\n",
      "layer   3  Sparsity: 88.5452%\n",
      "total_backward_count 890890 real_backward_count 107970  12.119%\n",
      "fc layer 2 self.abs_max_out: 1778.0\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.901885/  1.993415, val:  76.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.7000%\n",
      "layer   2  Sparsity: 85.6991%\n",
      "layer   3  Sparsity: 88.6550%\n",
      "total_backward_count 900680 real_backward_count 108916  12.093%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.896207/  2.002279, val:  75.00%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.52 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6828%\n",
      "layer   2  Sparsity: 85.4226%\n",
      "layer   3  Sparsity: 88.4240%\n",
      "total_backward_count 910470 real_backward_count 109858  12.066%\n",
      "fc layer 2 self.abs_max_out: 1800.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.913517/  2.022714, val:  67.92%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.83 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6996%\n",
      "layer   2  Sparsity: 85.6822%\n",
      "layer   3  Sparsity: 88.7758%\n",
      "total_backward_count 920260 real_backward_count 110834  12.044%\n",
      "fc layer 1 self.abs_max_out: 4611.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.921721/  2.033618, val:  59.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6987%\n",
      "layer   2  Sparsity: 85.8226%\n",
      "layer   3  Sparsity: 89.0242%\n",
      "total_backward_count 930050 real_backward_count 111788  12.020%\n",
      "fc layer 2 self.abs_max_out: 1815.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.918423/  2.024457, val:  67.08%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6853%\n",
      "layer   2  Sparsity: 85.7428%\n",
      "layer   3  Sparsity: 89.0531%\n",
      "total_backward_count 939840 real_backward_count 112731  11.995%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.920026/  2.021013, val:  64.58%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7055%\n",
      "layer   2  Sparsity: 85.7181%\n",
      "layer   3  Sparsity: 88.8156%\n",
      "total_backward_count 949630 real_backward_count 113625  11.965%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.924602/  2.024982, val:  65.42%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.08 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6899%\n",
      "layer   2  Sparsity: 86.0077%\n",
      "layer   3  Sparsity: 88.7676%\n",
      "total_backward_count 959420 real_backward_count 114538  11.938%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.921400/  2.029458, val:  61.67%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6731%\n",
      "layer   2  Sparsity: 86.0242%\n",
      "layer   3  Sparsity: 88.9926%\n",
      "total_backward_count 969210 real_backward_count 115459  11.913%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.912857/  2.018276, val:  67.92%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7011%\n",
      "layer   2  Sparsity: 85.8503%\n",
      "layer   3  Sparsity: 89.0250%\n",
      "total_backward_count 979000 real_backward_count 116356  11.885%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.917541/  2.017981, val:  71.67%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.61 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6967%\n",
      "layer   2  Sparsity: 85.7355%\n",
      "layer   3  Sparsity: 89.3273%\n",
      "total_backward_count 988790 real_backward_count 117266  11.860%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.928830/  2.017408, val:  70.83%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6866%\n",
      "layer   2  Sparsity: 85.8170%\n",
      "layer   3  Sparsity: 89.4236%\n",
      "total_backward_count 998580 real_backward_count 118222  11.839%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.922697/  2.020589, val:  72.92%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.83 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6847%\n",
      "layer   2  Sparsity: 85.7238%\n",
      "layer   3  Sparsity: 89.0612%\n",
      "total_backward_count 1008370 real_backward_count 119141  11.815%\n",
      "fc layer 1 self.abs_max_out: 4682.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.917661/  2.009351, val:  67.08%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6861%\n",
      "layer   2  Sparsity: 85.9126%\n",
      "layer   3  Sparsity: 89.0372%\n",
      "total_backward_count 1018160 real_backward_count 120038  11.790%\n",
      "fc layer 1 self.abs_max_out: 4685.0\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.908839/  2.014277, val:  70.42%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.80 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7124%\n",
      "layer   2  Sparsity: 86.0196%\n",
      "layer   3  Sparsity: 89.2292%\n",
      "total_backward_count 1027950 real_backward_count 120925  11.764%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.911570/  2.000637, val:  67.50%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6865%\n",
      "layer   2  Sparsity: 85.9406%\n",
      "layer   3  Sparsity: 88.8041%\n",
      "total_backward_count 1037740 real_backward_count 121794  11.736%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.899948/  2.011164, val:  74.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.61 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6992%\n",
      "layer   2  Sparsity: 85.8039%\n",
      "layer   3  Sparsity: 88.7685%\n",
      "total_backward_count 1047530 real_backward_count 122694  11.713%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.896951/  2.000428, val:  65.83%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6824%\n",
      "layer   2  Sparsity: 85.6225%\n",
      "layer   3  Sparsity: 88.7468%\n",
      "total_backward_count 1057320 real_backward_count 123639  11.694%\n",
      "lif layer 1 self.abs_max_v: 6610.5\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.888147/  1.991713, val:  71.67%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.48 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.7151%\n",
      "layer   2  Sparsity: 85.6142%\n",
      "layer   3  Sparsity: 88.6568%\n",
      "total_backward_count 1067110 real_backward_count 124519  11.669%\n",
      "fc layer 1 self.abs_max_out: 4835.0\n",
      "lif layer 1 self.abs_max_v: 6644.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.889922/  2.000496, val:  79.58%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.82 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6849%\n",
      "layer   2  Sparsity: 85.5108%\n",
      "layer   3  Sparsity: 88.6949%\n",
      "total_backward_count 1076900 real_backward_count 125427  11.647%\n",
      "lif layer 1 self.abs_max_v: 6779.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.894834/  1.988399, val:  63.75%, val_best:  79.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6908%\n",
      "layer   2  Sparsity: 85.4627%\n",
      "layer   3  Sparsity: 88.5949%\n",
      "total_backward_count 1086690 real_backward_count 126344  11.626%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.897027/  2.007695, val:  68.75%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6959%\n",
      "layer   2  Sparsity: 85.6862%\n",
      "layer   3  Sparsity: 88.7285%\n",
      "total_backward_count 1096480 real_backward_count 127250  11.605%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.900915/  2.000524, val:  69.17%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.68 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6857%\n",
      "layer   2  Sparsity: 86.0648%\n",
      "layer   3  Sparsity: 88.8792%\n",
      "total_backward_count 1106270 real_backward_count 128129  11.582%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.895793/  2.013981, val:  71.67%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.21 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6929%\n",
      "layer   2  Sparsity: 85.9923%\n",
      "layer   3  Sparsity: 88.7741%\n",
      "total_backward_count 1116060 real_backward_count 129036  11.562%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.898101/  2.011526, val:  59.58%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.07 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6995%\n",
      "layer   2  Sparsity: 85.7342%\n",
      "layer   3  Sparsity: 88.4827%\n",
      "total_backward_count 1125850 real_backward_count 129896  11.538%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.904188/  2.020570, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.64 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6882%\n",
      "layer   2  Sparsity: 85.4820%\n",
      "layer   3  Sparsity: 88.6936%\n",
      "total_backward_count 1135640 real_backward_count 130764  11.515%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.907279/  2.002727, val:  63.75%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.94 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6740%\n",
      "layer   2  Sparsity: 85.5433%\n",
      "layer   3  Sparsity: 88.5701%\n",
      "total_backward_count 1145430 real_backward_count 131641  11.493%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.903536/  2.010209, val:  67.50%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.30 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6877%\n",
      "layer   2  Sparsity: 85.5667%\n",
      "layer   3  Sparsity: 88.5368%\n",
      "total_backward_count 1155220 real_backward_count 132538  11.473%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.898493/  2.002187, val:  67.92%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6940%\n",
      "layer   2  Sparsity: 85.4509%\n",
      "layer   3  Sparsity: 88.5118%\n",
      "total_backward_count 1165010 real_backward_count 133434  11.453%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.902814/  1.991899, val:  72.92%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.7044%\n",
      "layer   2  Sparsity: 85.3501%\n",
      "layer   3  Sparsity: 88.5945%\n",
      "total_backward_count 1174800 real_backward_count 134328  11.434%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.891281/  2.015360, val:  68.75%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6945%\n",
      "layer   2  Sparsity: 85.6247%\n",
      "layer   3  Sparsity: 88.3872%\n",
      "total_backward_count 1184590 real_backward_count 135212  11.414%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.896408/  2.022884, val:  64.17%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.76 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6721%\n",
      "layer   2  Sparsity: 85.4370%\n",
      "layer   3  Sparsity: 88.3905%\n",
      "total_backward_count 1194380 real_backward_count 136066  11.392%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.912160/  2.019513, val:  66.67%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.19 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.7023%\n",
      "layer   2  Sparsity: 85.6657%\n",
      "layer   3  Sparsity: 88.6933%\n",
      "total_backward_count 1204170 real_backward_count 136977  11.375%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.902435/  2.007210, val:  75.42%, val_best:  79.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.14 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6972%\n",
      "layer   2  Sparsity: 85.6530%\n",
      "layer   3  Sparsity: 88.6834%\n",
      "total_backward_count 1213960 real_backward_count 137855  11.356%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.886555/  2.001011, val:  69.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6951%\n",
      "layer   2  Sparsity: 85.6721%\n",
      "layer   3  Sparsity: 88.4071%\n",
      "total_backward_count 1223750 real_backward_count 138714  11.335%\n",
      "lif layer 2 self.abs_max_v: 2082.0\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.900279/  2.009381, val:  69.58%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.86 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6980%\n",
      "layer   2  Sparsity: 85.5044%\n",
      "layer   3  Sparsity: 88.4316%\n",
      "total_backward_count 1233540 real_backward_count 139574  11.315%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.893216/  1.996816, val:  66.25%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.34 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6977%\n",
      "layer   2  Sparsity: 85.5434%\n",
      "layer   3  Sparsity: 88.2630%\n",
      "total_backward_count 1243330 real_backward_count 140375  11.290%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.879882/  1.998423, val:  75.42%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6908%\n",
      "layer   2  Sparsity: 85.7746%\n",
      "layer   3  Sparsity: 88.2511%\n",
      "total_backward_count 1253120 real_backward_count 141244  11.271%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.884553/  1.994407, val:  72.92%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6784%\n",
      "layer   2  Sparsity: 85.5480%\n",
      "layer   3  Sparsity: 88.0541%\n",
      "total_backward_count 1262910 real_backward_count 142082  11.250%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.881721/  1.976404, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.43 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6979%\n",
      "layer   2  Sparsity: 85.4694%\n",
      "layer   3  Sparsity: 88.1105%\n",
      "total_backward_count 1272700 real_backward_count 142907  11.229%\n",
      "lif layer 2 self.abs_max_v: 2137.5\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.868071/  1.976884, val:  63.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6951%\n",
      "layer   2  Sparsity: 85.4811%\n",
      "layer   3  Sparsity: 87.9234%\n",
      "total_backward_count 1282490 real_backward_count 143734  11.207%\n",
      "fc layer 3 self.abs_max_out: 262.0\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.870080/  1.983835, val:  69.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6965%\n",
      "layer   2  Sparsity: 85.4505%\n",
      "layer   3  Sparsity: 87.9941%\n",
      "total_backward_count 1292280 real_backward_count 144616  11.191%\n",
      "fc layer 3 self.abs_max_out: 263.0\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.872794/  1.982700, val:  69.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6878%\n",
      "layer   2  Sparsity: 85.4516%\n",
      "layer   3  Sparsity: 88.1134%\n",
      "total_backward_count 1302070 real_backward_count 145417  11.168%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.875791/  1.981981, val:  69.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7021%\n",
      "layer   2  Sparsity: 85.6938%\n",
      "layer   3  Sparsity: 88.1863%\n",
      "total_backward_count 1311860 real_backward_count 146259  11.149%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.879471/  1.993906, val:  69.58%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.51 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6975%\n",
      "layer   2  Sparsity: 85.7454%\n",
      "layer   3  Sparsity: 88.3509%\n",
      "total_backward_count 1321650 real_backward_count 147108  11.131%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.871561/  1.978787, val:  68.33%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.26 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6869%\n",
      "layer   2  Sparsity: 85.5219%\n",
      "layer   3  Sparsity: 88.4130%\n",
      "total_backward_count 1331440 real_backward_count 147989  11.115%\n",
      "lif layer 1 self.abs_max_v: 6888.5\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.876909/  1.983241, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.46 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6938%\n",
      "layer   2  Sparsity: 85.6041%\n",
      "layer   3  Sparsity: 88.2824%\n",
      "total_backward_count 1341230 real_backward_count 148788  11.093%\n",
      "fc layer 1 self.abs_max_out: 4875.0\n",
      "lif layer 1 self.abs_max_v: 6965.5\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.886576/  1.998896, val:  69.58%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.03 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6935%\n",
      "layer   2  Sparsity: 85.6962%\n",
      "layer   3  Sparsity: 88.5251%\n",
      "total_backward_count 1351020 real_backward_count 149611  11.074%\n",
      "lif layer 2 self.abs_max_v: 2213.5\n",
      "lif layer 1 self.abs_max_v: 7047.5\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.887142/  1.987406, val:  71.67%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.45 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6965%\n",
      "layer   2  Sparsity: 85.7642%\n",
      "layer   3  Sparsity: 88.4637%\n",
      "total_backward_count 1360810 real_backward_count 150457  11.056%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.879729/  1.986164, val:  65.00%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.25 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6957%\n",
      "layer   2  Sparsity: 85.5162%\n",
      "layer   3  Sparsity: 88.2925%\n",
      "total_backward_count 1370600 real_backward_count 151301  11.039%\n",
      "fc layer 3 self.abs_max_out: 276.0\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.873929/  1.983600, val:  66.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.08 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6903%\n",
      "layer   2  Sparsity: 85.4225%\n",
      "layer   3  Sparsity: 88.1972%\n",
      "total_backward_count 1380390 real_backward_count 152138  11.021%\n",
      "lif layer 1 self.abs_max_v: 7101.5\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.884500/  1.999762, val:  70.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.68 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6798%\n",
      "layer   2  Sparsity: 85.4848%\n",
      "layer   3  Sparsity: 88.3673%\n",
      "total_backward_count 1390180 real_backward_count 152971  11.004%\n",
      "lif layer 1 self.abs_max_v: 7196.0\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.896773/  1.998743, val:  63.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.87 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6987%\n",
      "layer   2  Sparsity: 85.4799%\n",
      "layer   3  Sparsity: 88.1885%\n",
      "total_backward_count 1399970 real_backward_count 153823  10.988%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.898881/  1.996162, val:  70.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.54 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7062%\n",
      "layer   2  Sparsity: 85.3423%\n",
      "layer   3  Sparsity: 88.4598%\n",
      "total_backward_count 1409760 real_backward_count 154653  10.970%\n",
      "lif layer 1 self.abs_max_v: 7216.0\n",
      "fc layer 1 self.abs_max_out: 5003.0\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.885880/  1.979773, val:  74.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6764%\n",
      "layer   2  Sparsity: 85.3715%\n",
      "layer   3  Sparsity: 88.2469%\n",
      "total_backward_count 1419550 real_backward_count 155481  10.953%\n",
      "lif layer 1 self.abs_max_v: 7353.5\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.887511/  1.987055, val:  66.25%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.28 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6982%\n",
      "layer   2  Sparsity: 85.6765%\n",
      "layer   3  Sparsity: 88.3687%\n",
      "total_backward_count 1429340 real_backward_count 156275  10.933%\n",
      "fc layer 1 self.abs_max_out: 5093.0\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.882829/  2.000339, val:  62.50%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.40 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6997%\n",
      "layer   2  Sparsity: 85.6932%\n",
      "layer   3  Sparsity: 88.1374%\n",
      "total_backward_count 1439130 real_backward_count 157117  10.917%\n",
      "lif layer 1 self.abs_max_v: 7391.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.871747/  1.993805, val:  69.17%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6893%\n",
      "layer   2  Sparsity: 85.7589%\n",
      "layer   3  Sparsity: 88.5908%\n",
      "total_backward_count 1448920 real_backward_count 157945  10.901%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.888331/  1.990297, val:  65.42%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.18 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.7011%\n",
      "layer   2  Sparsity: 85.8086%\n",
      "layer   3  Sparsity: 88.6758%\n",
      "total_backward_count 1458710 real_backward_count 158777  10.885%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.881184/  2.000961, val:  72.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.49 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6884%\n",
      "layer   2  Sparsity: 85.5603%\n",
      "layer   3  Sparsity: 88.3310%\n",
      "total_backward_count 1468500 real_backward_count 159659  10.872%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.869730/  1.987251, val:  69.17%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.53 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6878%\n",
      "layer   2  Sparsity: 85.5045%\n",
      "layer   3  Sparsity: 88.0024%\n",
      "total_backward_count 1478290 real_backward_count 160495  10.857%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.870071/  1.991206, val:  62.92%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.35 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6955%\n",
      "layer   2  Sparsity: 85.4087%\n",
      "layer   3  Sparsity: 88.0445%\n",
      "total_backward_count 1488080 real_backward_count 161351  10.843%\n",
      "fc layer 1 self.abs_max_out: 5231.0\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.872839/  1.994963, val:  73.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.58 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6838%\n",
      "layer   2  Sparsity: 85.5577%\n",
      "layer   3  Sparsity: 88.2917%\n",
      "total_backward_count 1497870 real_backward_count 162161  10.826%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.881432/  1.985220, val:  76.67%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.24 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6878%\n",
      "layer   2  Sparsity: 85.5803%\n",
      "layer   3  Sparsity: 88.1798%\n",
      "total_backward_count 1507660 real_backward_count 163022  10.813%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.871457/  1.990181, val:  73.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.43 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6914%\n",
      "layer   2  Sparsity: 85.5959%\n",
      "layer   3  Sparsity: 87.9436%\n",
      "total_backward_count 1517450 real_backward_count 163855  10.798%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.871529/  1.994354, val:  68.75%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.77 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6957%\n",
      "layer   2  Sparsity: 85.5460%\n",
      "layer   3  Sparsity: 88.0532%\n",
      "total_backward_count 1527240 real_backward_count 164654  10.781%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.868488/  1.976799, val:  74.17%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6882%\n",
      "layer   2  Sparsity: 85.5154%\n",
      "layer   3  Sparsity: 88.1026%\n",
      "total_backward_count 1537030 real_backward_count 165464  10.765%\n",
      "lif layer 1 self.abs_max_v: 7561.5\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.861583/  1.977719, val:  75.00%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.24 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.7024%\n",
      "layer   2  Sparsity: 85.7378%\n",
      "layer   3  Sparsity: 88.0002%\n",
      "total_backward_count 1546820 real_backward_count 166317  10.752%\n",
      "lif layer 1 self.abs_max_v: 7642.5\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.862640/  1.975694, val:  76.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.85 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6899%\n",
      "layer   2  Sparsity: 85.8233%\n",
      "layer   3  Sparsity: 87.7494%\n",
      "total_backward_count 1556610 real_backward_count 167184  10.740%\n",
      "lif layer 1 self.abs_max_v: 7687.0\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.860398/  1.975680, val:  76.25%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6883%\n",
      "layer   2  Sparsity: 85.6525%\n",
      "layer   3  Sparsity: 87.6962%\n",
      "total_backward_count 1566400 real_backward_count 167998  10.725%\n",
      "fc layer 3 self.abs_max_out: 285.0\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.851772/  1.963215, val:  71.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.35 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6929%\n",
      "layer   2  Sparsity: 85.5455%\n",
      "layer   3  Sparsity: 87.2242%\n",
      "total_backward_count 1576190 real_backward_count 168819  10.711%\n",
      "fc layer 3 self.abs_max_out: 291.0\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.856084/  1.968137, val:  73.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6872%\n",
      "layer   2  Sparsity: 85.4573%\n",
      "layer   3  Sparsity: 87.3807%\n",
      "total_backward_count 1585980 real_backward_count 169668  10.698%\n",
      "lif layer 2 self.abs_max_v: 2218.0\n",
      "fc layer 3 self.abs_max_out: 298.0\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.860305/  1.979948, val:  67.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.14 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.7106%\n",
      "layer   2  Sparsity: 85.4949%\n",
      "layer   3  Sparsity: 87.6928%\n",
      "total_backward_count 1595770 real_backward_count 170485  10.684%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.856898/  1.977899, val:  73.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7005%\n",
      "layer   2  Sparsity: 85.6773%\n",
      "layer   3  Sparsity: 87.6509%\n",
      "total_backward_count 1605560 real_backward_count 171301  10.669%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.859327/  1.981546, val:  65.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6865%\n",
      "layer   2  Sparsity: 85.4922%\n",
      "layer   3  Sparsity: 87.4958%\n",
      "total_backward_count 1615350 real_backward_count 172156  10.658%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.854890/  1.976474, val:  69.58%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.59 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6952%\n",
      "layer   2  Sparsity: 85.3512%\n",
      "layer   3  Sparsity: 87.5612%\n",
      "total_backward_count 1625140 real_backward_count 172963  10.643%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.849912/  1.974535, val:  67.08%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.07 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.7042%\n",
      "layer   2  Sparsity: 85.3726%\n",
      "layer   3  Sparsity: 87.4986%\n",
      "total_backward_count 1634930 real_backward_count 173775  10.629%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.830362/  1.959960, val:  63.33%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.34 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6964%\n",
      "layer   2  Sparsity: 85.2159%\n",
      "layer   3  Sparsity: 87.3717%\n",
      "total_backward_count 1644720 real_backward_count 174551  10.613%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.829687/  1.940734, val:  79.58%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.10 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6751%\n",
      "layer   2  Sparsity: 85.3096%\n",
      "layer   3  Sparsity: 87.1794%\n",
      "total_backward_count 1654510 real_backward_count 175356  10.599%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.819504/  1.941301, val:  65.83%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6907%\n",
      "layer   2  Sparsity: 85.5772%\n",
      "layer   3  Sparsity: 87.3005%\n",
      "total_backward_count 1664300 real_backward_count 176167  10.585%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.840611/  1.956691, val:  70.83%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.78 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6931%\n",
      "layer   2  Sparsity: 85.4833%\n",
      "layer   3  Sparsity: 87.8173%\n",
      "total_backward_count 1674090 real_backward_count 176946  10.570%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.840982/  1.955932, val:  68.75%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.18 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6924%\n",
      "layer   2  Sparsity: 85.3430%\n",
      "layer   3  Sparsity: 87.6256%\n",
      "total_backward_count 1683880 real_backward_count 177748  10.556%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.840686/  1.959920, val:  73.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.89 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6926%\n",
      "layer   2  Sparsity: 85.2614%\n",
      "layer   3  Sparsity: 87.7096%\n",
      "total_backward_count 1693670 real_backward_count 178571  10.543%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.840783/  1.956035, val:  76.67%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.69 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6814%\n",
      "layer   2  Sparsity: 84.9897%\n",
      "layer   3  Sparsity: 87.3944%\n",
      "total_backward_count 1703460 real_backward_count 179423  10.533%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.837168/  1.969212, val:  73.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6997%\n",
      "layer   2  Sparsity: 84.9972%\n",
      "layer   3  Sparsity: 87.0943%\n",
      "total_backward_count 1713250 real_backward_count 180226  10.520%\n",
      "lif layer 1 self.abs_max_v: 7693.5\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.842773/  1.954683, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.47 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6707%\n",
      "layer   2  Sparsity: 84.9275%\n",
      "layer   3  Sparsity: 86.9565%\n",
      "total_backward_count 1723040 real_backward_count 181032  10.507%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.833285/  1.945054, val:  74.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.96 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7088%\n",
      "layer   2  Sparsity: 85.3064%\n",
      "layer   3  Sparsity: 87.0644%\n",
      "total_backward_count 1732830 real_backward_count 181844  10.494%\n",
      "lif layer 2 self.abs_max_v: 2238.0\n",
      "lif layer 1 self.abs_max_v: 7722.5\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.836578/  1.968057, val:  73.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.32 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6907%\n",
      "layer   2  Sparsity: 85.1981%\n",
      "layer   3  Sparsity: 87.4275%\n",
      "total_backward_count 1742620 real_backward_count 182599  10.478%\n",
      "lif layer 1 self.abs_max_v: 7839.0\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.852883/  1.968065, val:  70.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6888%\n",
      "layer   2  Sparsity: 85.0438%\n",
      "layer   3  Sparsity: 87.1773%\n",
      "total_backward_count 1752410 real_backward_count 183398  10.465%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.853341/  1.980873, val:  75.00%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.13 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 89.7033%\n",
      "layer   2  Sparsity: 85.3978%\n",
      "layer   3  Sparsity: 87.4522%\n",
      "total_backward_count 1762200 real_backward_count 184186  10.452%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.854738/  1.963438, val:  71.67%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.00 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7096%\n",
      "layer   2  Sparsity: 85.4284%\n",
      "layer   3  Sparsity: 87.3679%\n",
      "total_backward_count 1771990 real_backward_count 184964  10.438%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.853707/  1.960416, val:  75.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 89.6935%\n",
      "layer   2  Sparsity: 85.3843%\n",
      "layer   3  Sparsity: 87.3898%\n",
      "total_backward_count 1781780 real_backward_count 185759  10.425%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.841044/  1.963517, val:  70.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6930%\n",
      "layer   2  Sparsity: 85.3144%\n",
      "layer   3  Sparsity: 87.3529%\n",
      "total_backward_count 1791570 real_backward_count 186613  10.416%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.854616/  1.956778, val:  70.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6878%\n",
      "layer   2  Sparsity: 85.3286%\n",
      "layer   3  Sparsity: 87.4010%\n",
      "total_backward_count 1801360 real_backward_count 187401  10.403%\n",
      "lif layer 2 self.abs_max_v: 2331.0\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.842721/  1.961715, val:  67.92%, val_best:  80.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 71.87 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6890%\n",
      "layer   2  Sparsity: 85.1746%\n",
      "layer   3  Sparsity: 87.1543%\n",
      "total_backward_count 1811150 real_backward_count 188188  10.391%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.833713/  1.953416, val:  67.08%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.10 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.7013%\n",
      "layer   2  Sparsity: 85.1258%\n",
      "layer   3  Sparsity: 87.2424%\n",
      "total_backward_count 1820940 real_backward_count 188976  10.378%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.831603/  1.955820, val:  71.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.27 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 89.6976%\n",
      "layer   2  Sparsity: 85.0743%\n",
      "layer   3  Sparsity: 87.1037%\n",
      "total_backward_count 1830730 real_backward_count 189738  10.364%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.822503/  1.961409, val:  68.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.40 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6917%\n",
      "layer   2  Sparsity: 85.1311%\n",
      "layer   3  Sparsity: 87.0727%\n",
      "total_backward_count 1840520 real_backward_count 190527  10.352%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.836456/  1.941363, val:  78.75%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.90 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6830%\n",
      "layer   2  Sparsity: 85.1432%\n",
      "layer   3  Sparsity: 87.2924%\n",
      "total_backward_count 1850310 real_backward_count 191330  10.340%\n",
      "fc layer 3 self.abs_max_out: 308.0\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.834824/  1.957164, val:  63.33%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6943%\n",
      "layer   2  Sparsity: 85.1130%\n",
      "layer   3  Sparsity: 87.0582%\n",
      "total_backward_count 1860100 real_backward_count 192093  10.327%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.833138/  1.948518, val:  75.83%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.05 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7027%\n",
      "layer   2  Sparsity: 84.8403%\n",
      "layer   3  Sparsity: 87.1004%\n",
      "total_backward_count 1869890 real_backward_count 192858  10.314%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.840094/  1.964635, val:  62.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6940%\n",
      "layer   2  Sparsity: 84.7774%\n",
      "layer   3  Sparsity: 87.3516%\n",
      "total_backward_count 1879680 real_backward_count 193672  10.303%\n",
      "fc layer 1 self.abs_max_out: 5505.0\n",
      "fc layer 1 self.abs_max_out: 5556.0\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.842629/  1.959897, val:  76.25%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6765%\n",
      "layer   2  Sparsity: 84.8042%\n",
      "layer   3  Sparsity: 87.5406%\n",
      "total_backward_count 1889470 real_backward_count 194418  10.290%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.840212/  1.951657, val:  72.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.84 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.7063%\n",
      "layer   2  Sparsity: 84.7324%\n",
      "layer   3  Sparsity: 87.5101%\n",
      "total_backward_count 1899260 real_backward_count 195210  10.278%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.839113/  1.952113, val:  67.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.39 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7003%\n",
      "layer   2  Sparsity: 84.7979%\n",
      "layer   3  Sparsity: 87.7461%\n",
      "total_backward_count 1909050 real_backward_count 195993  10.267%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.834806/  1.953639, val:  57.08%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.07 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6998%\n",
      "layer   2  Sparsity: 85.1278%\n",
      "layer   3  Sparsity: 87.6481%\n",
      "total_backward_count 1918840 real_backward_count 196777  10.255%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.844692/  1.954641, val:  71.25%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.16 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.6852%\n",
      "layer   2  Sparsity: 85.2733%\n",
      "layer   3  Sparsity: 87.3881%\n",
      "total_backward_count 1928630 real_backward_count 197568  10.244%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.836455/  1.952028, val:  63.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6912%\n",
      "layer   2  Sparsity: 85.0114%\n",
      "layer   3  Sparsity: 87.4546%\n",
      "total_backward_count 1938420 real_backward_count 198356  10.233%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.844747/  1.966432, val:  72.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.76 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 89.6897%\n",
      "layer   2  Sparsity: 85.0950%\n",
      "layer   3  Sparsity: 87.6103%\n",
      "total_backward_count 1948210 real_backward_count 199149  10.222%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.842002/  1.956989, val:  69.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.27 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 89.7003%\n",
      "layer   2  Sparsity: 84.9772%\n",
      "layer   3  Sparsity: 87.4579%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5d2954b0fe400fb0bd535428b746cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñÖ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñÖ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.842</td></tr><tr><td>val_acc_best</td><td>0.80417</td></tr><tr><td>val_acc_now</td><td>0.69583</td></tr><tr><td>val_loss</td><td>1.95699</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">laced-sweep-118</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3wfhju6n' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3wfhju6n</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251117_135809-3wfhju6n/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8xjb0oej with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tleaky_temporal_filter: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0078125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_select_ratio: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251117_180134-8xjb0oej</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8xjb0oej' target=\"_blank\">splendid-sweep-121</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/xdbl2gfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8xjb0oej' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/8xjb0oej</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'random_select_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'leaky_temporal_filter' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251117_180143_145', 'my_seed': 42, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.125, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.0078125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 25, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': True, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-11, -11], [-11, -11], [-10, -10]], 'random_select_ratio': 6, 'leaky_temporal_filter': 0.25} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -11 -11\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -11\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.125, v_reset=10000, sg_width=6, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-11, -11], [-11, -11], [-10, -10]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.0078125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 828.0\n",
      "lif layer 1 self.abs_max_v: 828.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1480.0\n",
      "lif layer 2 self.abs_max_v: 1480.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 368.0\n",
      "lif layer 2 self.abs_max_v: 1757.0\n",
      "fc layer 3 self.abs_max_out: 684.0\n",
      "fc layer 1 self.abs_max_out: 1039.0\n",
      "lif layer 1 self.abs_max_v: 1039.0\n",
      "lif layer 2 self.abs_max_v: 2093.5\n",
      "lif layer 1 self.abs_max_v: 1118.5\n",
      "fc layer 2 self.abs_max_out: 1506.0\n",
      "lif layer 2 self.abs_max_v: 2504.5\n",
      "lif layer 1 self.abs_max_v: 1166.0\n",
      "fc layer 1 self.abs_max_out: 1259.0\n",
      "lif layer 1 self.abs_max_v: 1259.0\n",
      "fc layer 2 self.abs_max_out: 1507.0\n",
      "lif layer 2 self.abs_max_v: 2680.5\n",
      "fc layer 1 self.abs_max_out: 1265.0\n",
      "lif layer 1 self.abs_max_v: 1265.0\n",
      "fc layer 2 self.abs_max_out: 1603.0\n",
      "fc layer 3 self.abs_max_out: 739.0\n",
      "lif layer 1 self.abs_max_v: 1336.0\n",
      "fc layer 2 self.abs_max_out: 1736.0\n",
      "fc layer 3 self.abs_max_out: 905.0\n",
      "fc layer 2 self.abs_max_out: 2158.0\n",
      "lif layer 2 self.abs_max_v: 2756.5\n",
      "fc layer 1 self.abs_max_out: 1370.0\n",
      "lif layer 1 self.abs_max_v: 2030.5\n",
      "fc layer 3 self.abs_max_out: 1041.0\n",
      "fc layer 1 self.abs_max_out: 1497.0\n",
      "lif layer 1 self.abs_max_v: 2310.5\n",
      "lif layer 2 self.abs_max_v: 2975.0\n",
      "fc layer 1 self.abs_max_out: 1651.0\n",
      "lif layer 1 self.abs_max_v: 2470.5\n",
      "lif layer 2 self.abs_max_v: 3263.5\n",
      "fc layer 1 self.abs_max_out: 1871.0\n",
      "lif layer 1 self.abs_max_v: 2641.5\n",
      "lif layer 2 self.abs_max_v: 3296.0\n",
      "lif layer 1 self.abs_max_v: 2790.0\n",
      "fc layer 3 self.abs_max_out: 1050.0\n",
      "fc layer 3 self.abs_max_out: 1125.0\n",
      "fc layer 1 self.abs_max_out: 1916.0\n",
      "fc layer 1 self.abs_max_out: 2004.0\n",
      "fc layer 2 self.abs_max_out: 2223.0\n",
      "fc layer 1 self.abs_max_out: 2089.0\n",
      "fc layer 1 self.abs_max_out: 2474.0\n",
      "fc layer 3 self.abs_max_out: 1180.0\n",
      "lif layer 2 self.abs_max_v: 3544.0\n",
      "fc layer 1 self.abs_max_out: 2539.0\n",
      "fc layer 2 self.abs_max_out: 2297.0\n",
      "lif layer 2 self.abs_max_v: 3994.0\n",
      "fc layer 1 self.abs_max_out: 2637.0\n",
      "fc layer 2 self.abs_max_out: 2363.0\n",
      "lif layer 2 self.abs_max_v: 4360.0\n",
      "lif layer 1 self.abs_max_v: 2937.5\n",
      "fc layer 2 self.abs_max_out: 2369.0\n",
      "fc layer 2 self.abs_max_out: 2416.0\n",
      "lif layer 1 self.abs_max_v: 3055.5\n",
      "fc layer 2 self.abs_max_out: 2536.0\n",
      "lif layer 1 self.abs_max_v: 3060.5\n",
      "fc layer 3 self.abs_max_out: 1286.0\n",
      "fc layer 1 self.abs_max_out: 2800.0\n",
      "fc layer 1 self.abs_max_out: 2849.0\n",
      "fc layer 1 self.abs_max_out: 3395.0\n",
      "lif layer 1 self.abs_max_v: 3395.0\n",
      "lif layer 1 self.abs_max_v: 3570.0\n",
      "lif layer 1 self.abs_max_v: 3659.5\n",
      "fc layer 2 self.abs_max_out: 2711.0\n",
      "fc layer 2 self.abs_max_out: 2780.0\n",
      "fc layer 3 self.abs_max_out: 1293.0\n",
      "fc layer 3 self.abs_max_out: 1307.0\n",
      "fc layer 3 self.abs_max_out: 1332.0\n",
      "lif layer 2 self.abs_max_v: 4444.5\n",
      "lif layer 2 self.abs_max_v: 4492.0\n",
      "fc layer 3 self.abs_max_out: 1348.0\n",
      "lif layer 2 self.abs_max_v: 4796.5\n",
      "fc layer 2 self.abs_max_out: 2782.0\n",
      "fc layer 2 self.abs_max_out: 2825.0\n",
      "fc layer 1 self.abs_max_out: 3706.0\n",
      "lif layer 1 self.abs_max_v: 3706.0\n",
      "fc layer 2 self.abs_max_out: 3199.0\n",
      "lif layer 1 self.abs_max_v: 4237.0\n",
      "fc layer 1 self.abs_max_out: 4440.0\n",
      "lif layer 1 self.abs_max_v: 4828.5\n",
      "fc layer 1 self.abs_max_out: 4503.0\n",
      "lif layer 1 self.abs_max_v: 5124.0\n",
      "lif layer 1 self.abs_max_v: 5629.0\n",
      "lif layer 2 self.abs_max_v: 4990.5\n",
      "fc layer 3 self.abs_max_out: 1423.0\n",
      "fc layer 3 self.abs_max_out: 1580.0\n",
      "lif layer 2 self.abs_max_v: 5244.0\n",
      "lif layer 2 self.abs_max_v: 5468.5\n",
      "lif layer 2 self.abs_max_v: 5471.5\n",
      "lif layer 2 self.abs_max_v: 5543.0\n",
      "lif layer 2 self.abs_max_v: 5852.5\n",
      "fc layer 2 self.abs_max_out: 3248.0\n",
      "lif layer 2 self.abs_max_v: 5883.5\n",
      "lif layer 2 self.abs_max_v: 5907.0\n",
      "fc layer 3 self.abs_max_out: 1637.0\n",
      "lif layer 2 self.abs_max_v: 6078.5\n",
      "lif layer 2 self.abs_max_v: 6228.0\n",
      "fc layer 2 self.abs_max_out: 3371.0\n",
      "fc layer 3 self.abs_max_out: 1812.0\n",
      "lif layer 1 self.abs_max_v: 5724.0\n",
      "fc layer 2 self.abs_max_out: 3379.0\n",
      "fc layer 2 self.abs_max_out: 3432.0\n",
      "fc layer 2 self.abs_max_out: 3843.0\n",
      "lif layer 2 self.abs_max_v: 6233.5\n",
      "lif layer 1 self.abs_max_v: 5736.0\n",
      "lif layer 1 self.abs_max_v: 5901.0\n",
      "lif layer 2 self.abs_max_v: 6236.5\n",
      "lif layer 1 self.abs_max_v: 6353.5\n",
      "lif layer 2 self.abs_max_v: 6274.5\n",
      "lif layer 1 self.abs_max_v: 6579.0\n",
      "lif layer 2 self.abs_max_v: 6366.5\n",
      "fc layer 3 self.abs_max_out: 1859.0\n",
      "fc layer 3 self.abs_max_out: 1874.0\n",
      "fc layer 3 self.abs_max_out: 1909.0\n",
      "fc layer 3 self.abs_max_out: 1951.0\n",
      "fc layer 3 self.abs_max_out: 1990.0\n",
      "lif layer 2 self.abs_max_v: 6367.5\n",
      "lif layer 2 self.abs_max_v: 6449.5\n",
      "lif layer 2 self.abs_max_v: 6687.5\n",
      "lif layer 2 self.abs_max_v: 6761.0\n",
      "fc layer 1 self.abs_max_out: 4554.0\n",
      "lif layer 1 self.abs_max_v: 6715.0\n",
      "fc layer 1 self.abs_max_out: 4952.0\n",
      "fc layer 1 self.abs_max_out: 5302.0\n",
      "lif layer 1 self.abs_max_v: 7018.5\n",
      "lif layer 1 self.abs_max_v: 7092.5\n",
      "fc layer 2 self.abs_max_out: 4079.0\n",
      "fc layer 3 self.abs_max_out: 2132.0\n",
      "fc layer 3 self.abs_max_out: 2260.0\n",
      "fc layer 2 self.abs_max_out: 4154.0\n",
      "fc layer 3 self.abs_max_out: 2331.0\n",
      "fc layer 3 self.abs_max_out: 2372.0\n",
      "fc layer 1 self.abs_max_out: 5551.0\n",
      "lif layer 1 self.abs_max_v: 7343.0\n",
      "lif layer 2 self.abs_max_v: 6961.5\n",
      "lif layer 2 self.abs_max_v: 6998.0\n",
      "lif layer 2 self.abs_max_v: 7226.0\n",
      "lif layer 1 self.abs_max_v: 7660.5\n",
      "lif layer 1 self.abs_max_v: 8007.5\n",
      "lif layer 1 self.abs_max_v: 8202.0\n",
      "lif layer 1 self.abs_max_v: 8700.0\n",
      "lif layer 1 self.abs_max_v: 9036.0\n",
      "lif layer 2 self.abs_max_v: 7393.5\n",
      "lif layer 1 self.abs_max_v: 9298.5\n",
      "fc layer 1 self.abs_max_out: 6292.0\n",
      "lif layer 1 self.abs_max_v: 9405.5\n",
      "fc layer 2 self.abs_max_out: 4178.0\n",
      "lif layer 2 self.abs_max_v: 7836.0\n",
      "lif layer 2 self.abs_max_v: 7901.5\n",
      "fc layer 3 self.abs_max_out: 2500.0\n",
      "fc layer 3 self.abs_max_out: 2514.0\n",
      "fc layer 2 self.abs_max_out: 4759.0\n",
      "fc layer 3 self.abs_max_out: 2599.0\n",
      "lif layer 1 self.abs_max_v: 9552.0\n",
      "lif layer 1 self.abs_max_v: 10086.0\n",
      "lif layer 1 self.abs_max_v: 10418.5\n",
      "fc layer 1 self.abs_max_out: 7035.0\n",
      "fc layer 1 self.abs_max_out: 7209.0\n",
      "lif layer 1 self.abs_max_v: 11180.5\n",
      "fc layer 1 self.abs_max_out: 7392.0\n",
      "lif layer 1 self.abs_max_v: 11299.5\n",
      "lif layer 1 self.abs_max_v: 11456.5\n",
      "lif layer 1 self.abs_max_v: 12634.5\n",
      "lif layer 1 self.abs_max_v: 13197.5\n",
      "lif layer 1 self.abs_max_v: 13343.0\n",
      "epoch-0   lr=['0.0078125'], tr/val_loss:  1.429134/  1.855459, val:  37.50%, val_best:  37.50%, tr:  99.49%, tr_best:  99.49%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8272%\n",
      "layer   2  Sparsity: 73.8054%\n",
      "layer   3  Sparsity: 67.2356%\n",
      "total_backward_count 9790 real_backward_count 1213  12.390%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 7462.0\n",
      "fc layer 3 self.abs_max_out: 2656.0\n",
      "fc layer 2 self.abs_max_out: 4773.0\n",
      "fc layer 3 self.abs_max_out: 2734.0\n",
      "fc layer 3 self.abs_max_out: 2754.0\n",
      "fc layer 3 self.abs_max_out: 2805.0\n",
      "fc layer 3 self.abs_max_out: 2806.0\n",
      "fc layer 3 self.abs_max_out: 2884.0\n",
      "fc layer 3 self.abs_max_out: 2912.0\n",
      "fc layer 3 self.abs_max_out: 2929.0\n",
      "lif layer 2 self.abs_max_v: 8151.5\n",
      "fc layer 1 self.abs_max_out: 8163.0\n",
      "lif layer 1 self.abs_max_v: 13799.5\n",
      "lif layer 1 self.abs_max_v: 14875.0\n",
      "fc layer 1 self.abs_max_out: 8528.0\n",
      "lif layer 1 self.abs_max_v: 15033.0\n",
      "fc layer 1 self.abs_max_out: 9140.0\n",
      "lif layer 1 self.abs_max_v: 16656.5\n",
      "epoch-1   lr=['0.0078125'], tr/val_loss:  1.248054/  1.717508, val:  43.33%, val_best:  43.33%, tr:  99.39%, tr_best:  99.49%, epoch time: 73.04 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8240%\n",
      "layer   2  Sparsity: 77.8781%\n",
      "layer   3  Sparsity: 68.6625%\n",
      "total_backward_count 19580 real_backward_count 2288  11.685%\n",
      "fc layer 3 self.abs_max_out: 3011.0\n",
      "lif layer 2 self.abs_max_v: 8281.0\n",
      "lif layer 2 self.abs_max_v: 8460.5\n",
      "lif layer 2 self.abs_max_v: 8549.5\n",
      "lif layer 2 self.abs_max_v: 8591.0\n",
      "fc layer 1 self.abs_max_out: 9506.0\n",
      "lif layer 1 self.abs_max_v: 17457.5\n",
      "fc layer 1 self.abs_max_out: 9736.0\n",
      "lif layer 1 self.abs_max_v: 18465.0\n",
      "fc layer 1 self.abs_max_out: 10096.0\n",
      "lif layer 1 self.abs_max_v: 19317.5\n",
      "lif layer 1 self.abs_max_v: 19429.5\n",
      "epoch-2   lr=['0.0078125'], tr/val_loss:  1.241939/  1.734576, val:  40.42%, val_best:  43.33%, tr:  99.80%, tr_best:  99.80%, epoch time: 72.92 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8292%\n",
      "layer   2  Sparsity: 79.4690%\n",
      "layer   3  Sparsity: 70.4788%\n",
      "total_backward_count 29370 real_backward_count 3316  11.290%\n",
      "fc layer 3 self.abs_max_out: 3029.0\n",
      "fc layer 3 self.abs_max_out: 3048.0\n",
      "fc layer 3 self.abs_max_out: 3109.0\n",
      "fc layer 3 self.abs_max_out: 3146.0\n",
      "fc layer 3 self.abs_max_out: 3201.0\n",
      "fc layer 3 self.abs_max_out: 3263.0\n",
      "fc layer 3 self.abs_max_out: 3479.0\n",
      "fc layer 2 self.abs_max_out: 4862.0\n",
      "fc layer 1 self.abs_max_out: 10866.0\n",
      "lif layer 1 self.abs_max_v: 20011.0\n",
      "lif layer 1 self.abs_max_v: 20399.5\n",
      "fc layer 1 self.abs_max_out: 11766.0\n",
      "lif layer 1 self.abs_max_v: 21749.0\n",
      "lif layer 1 self.abs_max_v: 21917.0\n",
      "epoch-3   lr=['0.0078125'], tr/val_loss:  1.227303/  1.821668, val:  40.00%, val_best:  43.33%, tr:  99.69%, tr_best:  99.80%, epoch time: 73.29 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8267%\n",
      "layer   2  Sparsity: 79.4281%\n",
      "layer   3  Sparsity: 70.6085%\n",
      "total_backward_count 39160 real_backward_count 4340  11.083%\n",
      "fc layer 1 self.abs_max_out: 12009.0\n",
      "lif layer 1 self.abs_max_v: 22653.5\n",
      "lif layer 1 self.abs_max_v: 22822.0\n",
      "epoch-4   lr=['0.0078125'], tr/val_loss:  1.275507/  1.675147, val:  46.67%, val_best:  46.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 73.34 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8291%\n",
      "layer   2  Sparsity: 78.6971%\n",
      "layer   3  Sparsity: 72.0740%\n",
      "total_backward_count 48950 real_backward_count 5367  10.964%\n",
      "fc layer 1 self.abs_max_out: 12491.0\n",
      "lif layer 1 self.abs_max_v: 23593.5\n",
      "lif layer 1 self.abs_max_v: 23770.0\n",
      "epoch-5   lr=['0.0078125'], tr/val_loss:  1.221876/  1.658869, val:  46.67%, val_best:  46.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8243%\n",
      "layer   2  Sparsity: 78.5405%\n",
      "layer   3  Sparsity: 71.2376%\n",
      "total_backward_count 58740 real_backward_count 6365  10.836%\n",
      "fc layer 1 self.abs_max_out: 12615.0\n",
      "lif layer 1 self.abs_max_v: 23834.0\n",
      "lif layer 1 self.abs_max_v: 24022.0\n",
      "epoch-6   lr=['0.0078125'], tr/val_loss:  1.263581/  1.717516, val:  49.17%, val_best:  49.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 73.25 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8298%\n",
      "layer   2  Sparsity: 78.3670%\n",
      "layer   3  Sparsity: 74.1715%\n",
      "total_backward_count 68530 real_backward_count 7448  10.868%\n",
      "fc layer 1 self.abs_max_out: 12897.0\n",
      "lif layer 1 self.abs_max_v: 24383.5\n",
      "lif layer 1 self.abs_max_v: 24580.5\n",
      "epoch-7   lr=['0.0078125'], tr/val_loss:  1.272763/  1.711096, val:  50.83%, val_best:  50.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8285%\n",
      "layer   2  Sparsity: 78.3231%\n",
      "layer   3  Sparsity: 75.7546%\n",
      "total_backward_count 78320 real_backward_count 8524  10.884%\n",
      "fc layer 2 self.abs_max_out: 5054.0\n",
      "lif layer 2 self.abs_max_v: 8998.5\n",
      "fc layer 1 self.abs_max_out: 13197.0\n",
      "lif layer 1 self.abs_max_v: 24963.0\n",
      "lif layer 1 self.abs_max_v: 25168.5\n",
      "epoch-8   lr=['0.0078125'], tr/val_loss:  1.314566/  1.669902, val:  52.50%, val_best:  52.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.89 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8134%\n",
      "layer   2  Sparsity: 78.7608%\n",
      "layer   3  Sparsity: 75.6922%\n",
      "total_backward_count 88110 real_backward_count 9664  10.968%\n",
      "fc layer 1 self.abs_max_out: 13267.0\n",
      "lif layer 1 self.abs_max_v: 25305.5\n",
      "epoch-9   lr=['0.0078125'], tr/val_loss:  1.267214/  1.789989, val:  45.83%, val_best:  52.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.80 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 92.8375%\n",
      "layer   2  Sparsity: 77.5803%\n",
      "layer   3  Sparsity: 74.3253%\n",
      "total_backward_count 97900 real_backward_count 10696  10.925%\n",
      "fc layer 2 self.abs_max_out: 5206.0\n",
      "lif layer 2 self.abs_max_v: 9217.0\n",
      "fc layer 1 self.abs_max_out: 13540.0\n",
      "lif layer 1 self.abs_max_v: 25639.0\n",
      "lif layer 1 self.abs_max_v: 25849.0\n",
      "fc layer 2 self.abs_max_out: 5516.0\n",
      "epoch-10  lr=['0.0078125'], tr/val_loss:  1.283908/  1.724956, val:  54.17%, val_best:  54.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.66 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 92.8290%\n",
      "layer   2  Sparsity: 78.2689%\n",
      "layer   3  Sparsity: 75.7128%\n",
      "total_backward_count 107690 real_backward_count 11719  10.882%\n",
      "lif layer 2 self.abs_max_v: 9232.0\n",
      "lif layer 2 self.abs_max_v: 9532.0\n",
      "fc layer 1 self.abs_max_out: 13700.0\n",
      "lif layer 1 self.abs_max_v: 25954.5\n",
      "lif layer 1 self.abs_max_v: 26158.0\n",
      "lif layer 2 self.abs_max_v: 9630.5\n",
      "epoch-11  lr=['0.0078125'], tr/val_loss:  1.291414/  1.693219, val:  52.08%, val_best:  54.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.88 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 92.8264%\n",
      "layer   2  Sparsity: 77.3893%\n",
      "layer   3  Sparsity: 75.8025%\n",
      "total_backward_count 117480 real_backward_count 12802  10.897%\n",
      "lif layer 2 self.abs_max_v: 9859.5\n",
      "lif layer 2 self.abs_max_v: 10069.0\n",
      "fc layer 2 self.abs_max_out: 5722.0\n",
      "fc layer 1 self.abs_max_out: 13821.0\n",
      "lif layer 1 self.abs_max_v: 26187.0\n",
      "lif layer 1 self.abs_max_v: 26393.5\n",
      "epoch-12  lr=['0.0078125'], tr/val_loss:  1.249273/  1.732362, val:  41.25%, val_best:  54.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.24 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8294%\n",
      "layer   2  Sparsity: 78.2764%\n",
      "layer   3  Sparsity: 75.5782%\n",
      "total_backward_count 127270 real_backward_count 13855  10.886%\n",
      "fc layer 2 self.abs_max_out: 5729.0\n",
      "lif layer 2 self.abs_max_v: 10115.5\n",
      "lif layer 2 self.abs_max_v: 10354.0\n",
      "lif layer 2 self.abs_max_v: 10380.0\n",
      "fc layer 1 self.abs_max_out: 13917.0\n",
      "lif layer 1 self.abs_max_v: 26588.0\n",
      "epoch-13  lr=['0.0078125'], tr/val_loss:  1.259859/  1.755579, val:  44.17%, val_best:  54.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8198%\n",
      "layer   2  Sparsity: 78.4495%\n",
      "layer   3  Sparsity: 76.3390%\n",
      "total_backward_count 137060 real_backward_count 14950  10.908%\n",
      "lif layer 2 self.abs_max_v: 10448.0\n",
      "fc layer 1 self.abs_max_out: 14047.0\n",
      "lif layer 1 self.abs_max_v: 26633.0\n",
      "lif layer 1 self.abs_max_v: 26848.0\n",
      "epoch-14  lr=['0.0078125'], tr/val_loss:  1.259668/  1.669582, val:  50.83%, val_best:  54.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.66 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 92.8277%\n",
      "layer   2  Sparsity: 79.0683%\n",
      "layer   3  Sparsity: 75.5753%\n",
      "total_backward_count 146850 real_backward_count 15951  10.862%\n",
      "epoch-15  lr=['0.0078125'], tr/val_loss:  1.257228/  1.697133, val:  51.25%, val_best:  54.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.96 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 92.8250%\n",
      "layer   2  Sparsity: 78.4551%\n",
      "layer   3  Sparsity: 76.3873%\n",
      "total_backward_count 156640 real_backward_count 16947  10.819%\n",
      "fc layer 1 self.abs_max_out: 14099.0\n",
      "lif layer 1 self.abs_max_v: 26948.5\n",
      "epoch-16  lr=['0.0078125'], tr/val_loss:  1.243850/  1.677306, val:  52.92%, val_best:  54.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.57 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 92.8200%\n",
      "layer   2  Sparsity: 78.4806%\n",
      "layer   3  Sparsity: 77.1194%\n",
      "total_backward_count 166430 real_backward_count 17917  10.765%\n",
      "fc layer 1 self.abs_max_out: 14181.0\n",
      "lif layer 1 self.abs_max_v: 27110.5\n",
      "epoch-17  lr=['0.0078125'], tr/val_loss:  1.266311/  1.731778, val:  46.67%, val_best:  54.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.81 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 92.8372%\n",
      "layer   2  Sparsity: 78.3295%\n",
      "layer   3  Sparsity: 76.0085%\n",
      "total_backward_count 176220 real_backward_count 18951  10.754%\n",
      "fc layer 1 self.abs_max_out: 14231.0\n",
      "lif layer 1 self.abs_max_v: 27213.5\n",
      "epoch-18  lr=['0.0078125'], tr/val_loss:  1.266788/  1.693107, val:  45.42%, val_best:  54.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.08 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8276%\n",
      "layer   2  Sparsity: 78.2925%\n",
      "layer   3  Sparsity: 76.7804%\n",
      "total_backward_count 186010 real_backward_count 19988  10.746%\n",
      "fc layer 1 self.abs_max_out: 14315.0\n",
      "lif layer 1 self.abs_max_v: 27374.5\n",
      "fc layer 2 self.abs_max_out: 5929.0\n",
      "epoch-19  lr=['0.0078125'], tr/val_loss:  1.238345/  1.692754, val:  41.67%, val_best:  54.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.45 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8265%\n",
      "layer   2  Sparsity: 77.8329%\n",
      "layer   3  Sparsity: 75.7242%\n",
      "total_backward_count 195800 real_backward_count 20930  10.689%\n",
      "fc layer 1 self.abs_max_out: 14627.0\n",
      "fc layer 1 self.abs_max_out: 14984.0\n",
      "lif layer 1 self.abs_max_v: 28354.0\n",
      "lif layer 1 self.abs_max_v: 29050.0\n",
      "lif layer 1 self.abs_max_v: 29445.0\n",
      "epoch-20  lr=['0.0078125'], tr/val_loss:  1.317892/  1.704594, val:  44.58%, val_best:  54.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8170%\n",
      "layer   2  Sparsity: 78.6169%\n",
      "layer   3  Sparsity: 78.7557%\n",
      "total_backward_count 205590 real_backward_count 21939  10.671%\n",
      "epoch-21  lr=['0.0078125'], tr/val_loss:  1.326433/  1.789696, val:  47.08%, val_best:  54.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.55 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8222%\n",
      "layer   2  Sparsity: 78.9534%\n",
      "layer   3  Sparsity: 79.2570%\n",
      "total_backward_count 215380 real_backward_count 22970  10.665%\n",
      "lif layer 2 self.abs_max_v: 10697.0\n",
      "epoch-22  lr=['0.0078125'], tr/val_loss:  1.345954/  1.686097, val:  51.67%, val_best:  54.17%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8246%\n",
      "layer   2  Sparsity: 78.0984%\n",
      "layer   3  Sparsity: 79.9666%\n",
      "total_backward_count 225170 real_backward_count 24012  10.664%\n",
      "lif layer 2 self.abs_max_v: 10750.0\n",
      "lif layer 2 self.abs_max_v: 11180.0\n",
      "epoch-23  lr=['0.0078125'], tr/val_loss:  1.298052/  1.674029, val:  54.58%, val_best:  54.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.17 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8368%\n",
      "layer   2  Sparsity: 78.0243%\n",
      "layer   3  Sparsity: 77.9648%\n",
      "total_backward_count 234960 real_backward_count 25041  10.658%\n",
      "epoch-24  lr=['0.0078125'], tr/val_loss:  1.332149/  1.657120, val:  60.83%, val_best:  60.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.87 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8205%\n",
      "layer   2  Sparsity: 78.3486%\n",
      "layer   3  Sparsity: 78.0091%\n",
      "total_backward_count 244750 real_backward_count 26054  10.645%\n",
      "epoch-25  lr=['0.0078125'], tr/val_loss:  1.311608/  1.662314, val:  62.08%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.53 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8319%\n",
      "layer   2  Sparsity: 78.5592%\n",
      "layer   3  Sparsity: 78.1457%\n",
      "total_backward_count 254540 real_backward_count 27122  10.655%\n",
      "fc layer 1 self.abs_max_out: 15116.0\n",
      "epoch-26  lr=['0.0078125'], tr/val_loss:  1.235478/  1.596207, val:  52.92%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8249%\n",
      "layer   2  Sparsity: 78.1530%\n",
      "layer   3  Sparsity: 76.6165%\n",
      "total_backward_count 264330 real_backward_count 28123  10.639%\n",
      "fc layer 1 self.abs_max_out: 15318.0\n",
      "epoch-27  lr=['0.0078125'], tr/val_loss:  1.269502/  1.685177, val:  49.17%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.56 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8304%\n",
      "layer   2  Sparsity: 78.3886%\n",
      "layer   3  Sparsity: 78.1907%\n",
      "total_backward_count 274120 real_backward_count 29170  10.641%\n",
      "fc layer 2 self.abs_max_out: 6058.0\n",
      "fc layer 1 self.abs_max_out: 15838.0\n",
      "epoch-28  lr=['0.0078125'], tr/val_loss:  1.230649/  1.665335, val:  59.17%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.09 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8223%\n",
      "layer   2  Sparsity: 77.4476%\n",
      "layer   3  Sparsity: 76.8993%\n",
      "total_backward_count 283910 real_backward_count 30136  10.615%\n",
      "fc layer 1 self.abs_max_out: 15878.0\n",
      "epoch-29  lr=['0.0078125'], tr/val_loss:  1.232552/  1.706598, val:  47.08%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.72 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8215%\n",
      "layer   2  Sparsity: 78.1061%\n",
      "layer   3  Sparsity: 77.0390%\n",
      "total_backward_count 293700 real_backward_count 31106  10.591%\n",
      "fc layer 2 self.abs_max_out: 6073.0\n",
      "fc layer 1 self.abs_max_out: 16028.0\n",
      "lif layer 1 self.abs_max_v: 29649.0\n",
      "epoch-30  lr=['0.0078125'], tr/val_loss:  1.262070/  1.686273, val:  57.08%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8283%\n",
      "layer   2  Sparsity: 77.5818%\n",
      "layer   3  Sparsity: 77.6584%\n",
      "total_backward_count 303490 real_backward_count 32113  10.581%\n",
      "lif layer 2 self.abs_max_v: 11292.0\n",
      "fc layer 2 self.abs_max_out: 6601.0\n",
      "lif layer 2 self.abs_max_v: 11483.0\n",
      "lif layer 2 self.abs_max_v: 11638.5\n",
      "lif layer 2 self.abs_max_v: 11663.5\n",
      "lif layer 2 self.abs_max_v: 11822.0\n",
      "epoch-31  lr=['0.0078125'], tr/val_loss:  1.267063/  1.661657, val:  55.42%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.23 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8358%\n",
      "layer   2  Sparsity: 78.2873%\n",
      "layer   3  Sparsity: 77.6433%\n",
      "total_backward_count 313280 real_backward_count 33080  10.559%\n",
      "fc layer 1 self.abs_max_out: 16236.0\n",
      "lif layer 1 self.abs_max_v: 30037.5\n",
      "epoch-32  lr=['0.0078125'], tr/val_loss:  1.302151/  1.674656, val:  56.25%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8290%\n",
      "layer   2  Sparsity: 78.2136%\n",
      "layer   3  Sparsity: 77.9894%\n",
      "total_backward_count 323070 real_backward_count 34032  10.534%\n",
      "epoch-33  lr=['0.0078125'], tr/val_loss:  1.265942/  1.605866, val:  57.50%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8226%\n",
      "layer   2  Sparsity: 78.4441%\n",
      "layer   3  Sparsity: 76.7500%\n",
      "total_backward_count 332860 real_backward_count 34976  10.508%\n",
      "epoch-34  lr=['0.0078125'], tr/val_loss:  1.255115/  1.679127, val:  52.50%, val_best:  62.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.33 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 92.8269%\n",
      "layer   2  Sparsity: 78.4308%\n",
      "layer   3  Sparsity: 77.3478%\n",
      "total_backward_count 342650 real_backward_count 35887  10.473%\n",
      "fc layer 1 self.abs_max_out: 16503.0\n",
      "lif layer 1 self.abs_max_v: 30557.5\n",
      "epoch-35  lr=['0.0078125'], tr/val_loss:  1.311994/  1.692035, val:  57.50%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.18 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 92.8329%\n",
      "layer   2  Sparsity: 78.6603%\n",
      "layer   3  Sparsity: 78.3407%\n",
      "total_backward_count 352440 real_backward_count 36869  10.461%\n",
      "lif layer 2 self.abs_max_v: 11872.5\n",
      "lif layer 2 self.abs_max_v: 12096.5\n",
      "fc layer 2 self.abs_max_out: 6603.0\n",
      "lif layer 2 self.abs_max_v: 12367.0\n",
      "fc layer 1 self.abs_max_out: 16575.0\n",
      "lif layer 1 self.abs_max_v: 30699.5\n",
      "epoch-36  lr=['0.0078125'], tr/val_loss:  1.299789/  1.702818, val:  59.17%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.56 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 92.8231%\n",
      "layer   2  Sparsity: 78.5499%\n",
      "layer   3  Sparsity: 78.3062%\n",
      "total_backward_count 362230 real_backward_count 37831  10.444%\n",
      "fc layer 1 self.abs_max_out: 16697.0\n",
      "lif layer 1 self.abs_max_v: 30937.0\n",
      "epoch-37  lr=['0.0078125'], tr/val_loss:  1.264069/  1.633379, val:  54.17%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.74 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8236%\n",
      "layer   2  Sparsity: 78.8491%\n",
      "layer   3  Sparsity: 78.4609%\n",
      "total_backward_count 372020 real_backward_count 38736  10.412%\n",
      "fc layer 1 self.abs_max_out: 16841.0\n",
      "lif layer 1 self.abs_max_v: 31221.5\n",
      "epoch-38  lr=['0.0078125'], tr/val_loss:  1.304242/  1.686708, val:  57.50%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.73 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8174%\n",
      "layer   2  Sparsity: 78.6836%\n",
      "layer   3  Sparsity: 79.3352%\n",
      "total_backward_count 381810 real_backward_count 39774  10.417%\n",
      "fc layer 1 self.abs_max_out: 16917.0\n",
      "lif layer 1 self.abs_max_v: 31370.5\n",
      "epoch-39  lr=['0.0078125'], tr/val_loss:  1.302798/  1.714388, val:  57.08%, val_best:  62.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 74.14 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8301%\n",
      "layer   2  Sparsity: 79.1404%\n",
      "layer   3  Sparsity: 79.3437%\n",
      "total_backward_count 391600 real_backward_count 40724  10.399%\n",
      "epoch-40  lr=['0.0078125'], tr/val_loss:  1.365010/  1.689450, val:  55.00%, val_best:  62.08%, tr:  99.59%, tr_best: 100.00%, epoch time: 72.79 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 92.8217%\n",
      "layer   2  Sparsity: 79.0726%\n",
      "layer   3  Sparsity: 80.0255%\n",
      "total_backward_count 401390 real_backward_count 41750  10.401%\n",
      "fc layer 1 self.abs_max_out: 16969.0\n",
      "lif layer 1 self.abs_max_v: 31473.5\n",
      "epoch-41  lr=['0.0078125'], tr/val_loss:  1.354614/  1.711514, val:  59.58%, val_best:  62.08%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.15 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8172%\n",
      "layer   2  Sparsity: 78.8580%\n",
      "layer   3  Sparsity: 79.3678%\n",
      "total_backward_count 411180 real_backward_count 42768  10.401%\n",
      "fc layer 1 self.abs_max_out: 17057.0\n",
      "lif layer 1 self.abs_max_v: 31649.0\n",
      "epoch-42  lr=['0.0078125'], tr/val_loss:  1.335017/  1.724424, val:  55.00%, val_best:  62.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.44 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8351%\n",
      "layer   2  Sparsity: 78.6238%\n",
      "layer   3  Sparsity: 79.5474%\n",
      "total_backward_count 420970 real_backward_count 43776  10.399%\n",
      "fc layer 1 self.abs_max_out: 17205.0\n",
      "lif layer 1 self.abs_max_v: 31934.5\n",
      "fc layer 2 self.abs_max_out: 6682.0\n",
      "epoch-43  lr=['0.0078125'], tr/val_loss:  1.314598/  1.692897, val:  53.33%, val_best:  62.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.13 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8317%\n",
      "layer   2  Sparsity: 78.9164%\n",
      "layer   3  Sparsity: 78.8639%\n",
      "total_backward_count 430760 real_backward_count 44770  10.393%\n",
      "fc layer 1 self.abs_max_out: 17231.0\n",
      "lif layer 1 self.abs_max_v: 31987.0\n",
      "epoch-44  lr=['0.0078125'], tr/val_loss:  1.340139/  1.614523, val:  62.92%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.36 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8299%\n",
      "layer   2  Sparsity: 79.2399%\n",
      "layer   3  Sparsity: 79.4866%\n",
      "total_backward_count 440550 real_backward_count 45793  10.395%\n",
      "fc layer 1 self.abs_max_out: 17253.0\n",
      "lif layer 1 self.abs_max_v: 32027.5\n",
      "epoch-45  lr=['0.0078125'], tr/val_loss:  1.307947/  1.693652, val:  48.33%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.11 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8289%\n",
      "layer   2  Sparsity: 78.4078%\n",
      "layer   3  Sparsity: 79.1306%\n",
      "total_backward_count 450340 real_backward_count 46823  10.397%\n",
      "epoch-46  lr=['0.0078125'], tr/val_loss:  1.305468/  1.688418, val:  57.08%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.84 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8158%\n",
      "layer   2  Sparsity: 78.8866%\n",
      "layer   3  Sparsity: 78.5350%\n",
      "total_backward_count 460130 real_backward_count 47818  10.392%\n",
      "fc layer 1 self.abs_max_out: 17325.0\n",
      "lif layer 1 self.abs_max_v: 32159.0\n",
      "epoch-47  lr=['0.0078125'], tr/val_loss:  1.266585/  1.693936, val:  45.83%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 72.53 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 92.8387%\n",
      "layer   2  Sparsity: 78.9021%\n",
      "layer   3  Sparsity: 77.9850%\n",
      "total_backward_count 469920 real_backward_count 48736  10.371%\n",
      "fc layer 1 self.abs_max_out: 17343.0\n",
      "lif layer 1 self.abs_max_v: 32195.0\n",
      "epoch-48  lr=['0.0078125'], tr/val_loss:  1.282115/  1.704648, val:  50.83%, val_best:  62.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.13 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8275%\n",
      "layer   2  Sparsity: 78.9677%\n",
      "layer   3  Sparsity: 79.2013%\n",
      "total_backward_count 479710 real_backward_count 49670  10.354%\n",
      "fc layer 1 self.abs_max_out: 17433.0\n",
      "lif layer 1 self.abs_max_v: 32372.0\n",
      "epoch-49  lr=['0.0078125'], tr/val_loss:  1.252562/  1.661638, val:  57.08%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.35 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8192%\n",
      "layer   2  Sparsity: 78.5966%\n",
      "layer   3  Sparsity: 76.5623%\n",
      "total_backward_count 489500 real_backward_count 50634  10.344%\n",
      "epoch-50  lr=['0.0078125'], tr/val_loss:  1.276812/  1.636319, val:  52.08%, val_best:  62.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.64 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8288%\n",
      "layer   2  Sparsity: 78.3902%\n",
      "layer   3  Sparsity: 77.3042%\n",
      "total_backward_count 499290 real_backward_count 51589  10.332%\n",
      "fc layer 1 self.abs_max_out: 17469.0\n",
      "lif layer 1 self.abs_max_v: 32445.5\n",
      "epoch-51  lr=['0.0078125'], tr/val_loss:  1.339867/  1.685646, val:  53.75%, val_best:  62.92%, tr:  99.59%, tr_best: 100.00%, epoch time: 74.25 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8142%\n",
      "layer   2  Sparsity: 78.8446%\n",
      "layer   3  Sparsity: 80.2610%\n",
      "total_backward_count 509080 real_backward_count 52638  10.340%\n",
      "fc layer 1 self.abs_max_out: 17505.0\n",
      "lif layer 1 self.abs_max_v: 32516.5\n",
      "epoch-52  lr=['0.0078125'], tr/val_loss:  1.278255/  1.688438, val:  65.83%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.53 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8293%\n",
      "layer   2  Sparsity: 78.3487%\n",
      "layer   3  Sparsity: 79.4114%\n",
      "total_backward_count 518870 real_backward_count 53644  10.339%\n",
      "epoch-53  lr=['0.0078125'], tr/val_loss:  1.320895/  1.673452, val:  51.67%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.16 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8231%\n",
      "layer   2  Sparsity: 78.5968%\n",
      "layer   3  Sparsity: 79.3582%\n",
      "total_backward_count 528660 real_backward_count 54619  10.332%\n",
      "epoch-54  lr=['0.0078125'], tr/val_loss:  1.308241/  1.684576, val:  55.00%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.31 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8085%\n",
      "layer   2  Sparsity: 78.1103%\n",
      "layer   3  Sparsity: 79.4776%\n",
      "total_backward_count 538450 real_backward_count 55637  10.333%\n",
      "fc layer 1 self.abs_max_out: 17569.0\n",
      "lif layer 1 self.abs_max_v: 32641.5\n",
      "epoch-55  lr=['0.0078125'], tr/val_loss:  1.283440/  1.702249, val:  52.92%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.76 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8351%\n",
      "layer   2  Sparsity: 79.4488%\n",
      "layer   3  Sparsity: 78.5331%\n",
      "total_backward_count 548240 real_backward_count 56641  10.331%\n",
      "fc layer 1 self.abs_max_out: 17607.0\n",
      "lif layer 1 self.abs_max_v: 32718.5\n",
      "epoch-56  lr=['0.0078125'], tr/val_loss:  1.328497/  1.736554, val:  50.83%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.62 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8371%\n",
      "layer   2  Sparsity: 78.7081%\n",
      "layer   3  Sparsity: 80.1047%\n",
      "total_backward_count 558030 real_backward_count 57706  10.341%\n",
      "fc layer 1 self.abs_max_out: 17693.0\n",
      "lif layer 1 self.abs_max_v: 32893.0\n",
      "epoch-57  lr=['0.0078125'], tr/val_loss:  1.361849/  1.735268, val:  55.42%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.84 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8208%\n",
      "layer   2  Sparsity: 79.2141%\n",
      "layer   3  Sparsity: 81.3059%\n",
      "total_backward_count 567820 real_backward_count 58726  10.342%\n",
      "fc layer 1 self.abs_max_out: 17832.0\n",
      "lif layer 1 self.abs_max_v: 33155.0\n",
      "epoch-58  lr=['0.0078125'], tr/val_loss:  1.331600/  1.759295, val:  52.08%, val_best:  65.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.88 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8272%\n",
      "layer   2  Sparsity: 79.2302%\n",
      "layer   3  Sparsity: 78.4993%\n",
      "total_backward_count 577610 real_backward_count 59685  10.333%\n",
      "fc layer 1 self.abs_max_out: 17864.0\n",
      "lif layer 1 self.abs_max_v: 33218.5\n",
      "epoch-59  lr=['0.0078125'], tr/val_loss:  1.258406/  1.651121, val:  54.17%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.50 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8246%\n",
      "layer   2  Sparsity: 78.7075%\n",
      "layer   3  Sparsity: 75.7859%\n",
      "total_backward_count 587400 real_backward_count 60639  10.323%\n",
      "epoch-60  lr=['0.0078125'], tr/val_loss:  1.267610/  1.697013, val:  50.42%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.21 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8297%\n",
      "layer   2  Sparsity: 79.1117%\n",
      "layer   3  Sparsity: 76.9178%\n",
      "total_backward_count 597190 real_backward_count 61616  10.318%\n",
      "epoch-61  lr=['0.0078125'], tr/val_loss:  1.277302/  1.683337, val:  55.42%, val_best:  65.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8214%\n",
      "layer   2  Sparsity: 80.3970%\n",
      "layer   3  Sparsity: 77.9354%\n",
      "total_backward_count 606980 real_backward_count 62579  10.310%\n",
      "fc layer 1 self.abs_max_out: 17880.0\n",
      "lif layer 1 self.abs_max_v: 33250.0\n",
      "epoch-62  lr=['0.0078125'], tr/val_loss:  1.327173/  1.745625, val:  47.50%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.20 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8224%\n",
      "layer   2  Sparsity: 79.6872%\n",
      "layer   3  Sparsity: 79.0374%\n",
      "total_backward_count 616770 real_backward_count 63520  10.299%\n",
      "epoch-63  lr=['0.0078125'], tr/val_loss:  1.317533/  1.702101, val:  51.67%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.98 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8262%\n",
      "layer   2  Sparsity: 79.1793%\n",
      "layer   3  Sparsity: 78.3631%\n",
      "total_backward_count 626560 real_backward_count 64470  10.290%\n",
      "fc layer 1 self.abs_max_out: 17884.0\n",
      "lif layer 1 self.abs_max_v: 33258.0\n",
      "epoch-64  lr=['0.0078125'], tr/val_loss:  1.276804/  1.630859, val:  53.33%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.97 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8293%\n",
      "layer   2  Sparsity: 79.7019%\n",
      "layer   3  Sparsity: 76.9805%\n",
      "total_backward_count 636350 real_backward_count 65442  10.284%\n",
      "fc layer 1 self.abs_max_out: 17942.0\n",
      "lif layer 1 self.abs_max_v: 33359.5\n",
      "epoch-65  lr=['0.0078125'], tr/val_loss:  1.227674/  1.668663, val:  62.50%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.36 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8208%\n",
      "layer   2  Sparsity: 79.8221%\n",
      "layer   3  Sparsity: 76.5299%\n",
      "total_backward_count 646140 real_backward_count 66369  10.272%\n",
      "fc layer 1 self.abs_max_out: 17955.0\n",
      "lif layer 1 self.abs_max_v: 33385.0\n",
      "epoch-66  lr=['0.0078125'], tr/val_loss:  1.242332/  1.620490, val:  54.58%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.57 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 92.8259%\n",
      "layer   2  Sparsity: 79.6108%\n",
      "layer   3  Sparsity: 77.5717%\n",
      "total_backward_count 655930 real_backward_count 67280  10.257%\n",
      "fc layer 1 self.abs_max_out: 17981.0\n",
      "lif layer 1 self.abs_max_v: 33436.5\n",
      "epoch-67  lr=['0.0078125'], tr/val_loss:  1.236295/  1.611238, val:  56.25%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.32 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 92.8281%\n",
      "layer   2  Sparsity: 79.8439%\n",
      "layer   3  Sparsity: 76.4684%\n",
      "total_backward_count 665720 real_backward_count 68245  10.251%\n",
      "fc layer 1 self.abs_max_out: 18029.0\n",
      "lif layer 1 self.abs_max_v: 33530.5\n",
      "epoch-68  lr=['0.0078125'], tr/val_loss:  1.260197/  1.644770, val:  59.17%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.57 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8290%\n",
      "layer   2  Sparsity: 79.6437%\n",
      "layer   3  Sparsity: 78.1734%\n",
      "total_backward_count 675510 real_backward_count 69185  10.242%\n",
      "fc layer 1 self.abs_max_out: 18043.0\n",
      "lif layer 1 self.abs_max_v: 33558.5\n",
      "epoch-69  lr=['0.0078125'], tr/val_loss:  1.256167/  1.736887, val:  42.92%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.79 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8109%\n",
      "layer   2  Sparsity: 79.7902%\n",
      "layer   3  Sparsity: 78.5703%\n",
      "total_backward_count 685300 real_backward_count 70176  10.240%\n",
      "fc layer 1 self.abs_max_out: 18063.0\n",
      "lif layer 1 self.abs_max_v: 33598.0\n",
      "epoch-70  lr=['0.0078125'], tr/val_loss:  1.254768/  1.693787, val:  51.67%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8294%\n",
      "layer   2  Sparsity: 79.6266%\n",
      "layer   3  Sparsity: 77.6917%\n",
      "total_backward_count 695090 real_backward_count 71149  10.236%\n",
      "fc layer 1 self.abs_max_out: 18089.0\n",
      "lif layer 1 self.abs_max_v: 33649.5\n",
      "epoch-71  lr=['0.0078125'], tr/val_loss:  1.271477/  1.725229, val:  46.67%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.09 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8179%\n",
      "layer   2  Sparsity: 79.5639%\n",
      "layer   3  Sparsity: 77.9776%\n",
      "total_backward_count 704880 real_backward_count 72172  10.239%\n",
      "epoch-72  lr=['0.0078125'], tr/val_loss:  1.321660/  1.685729, val:  59.17%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.17 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8336%\n",
      "layer   2  Sparsity: 79.3425%\n",
      "layer   3  Sparsity: 80.1672%\n",
      "total_backward_count 714670 real_backward_count 73167  10.238%\n",
      "epoch-73  lr=['0.0078125'], tr/val_loss:  1.255727/  1.701070, val:  54.17%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.62 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8268%\n",
      "layer   2  Sparsity: 79.8726%\n",
      "layer   3  Sparsity: 78.9276%\n",
      "total_backward_count 724460 real_backward_count 74139  10.234%\n",
      "fc layer 1 self.abs_max_out: 18113.0\n",
      "lif layer 1 self.abs_max_v: 33697.0\n",
      "epoch-74  lr=['0.0078125'], tr/val_loss:  1.272060/  1.728536, val:  54.58%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.11 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8123%\n",
      "layer   2  Sparsity: 79.6500%\n",
      "layer   3  Sparsity: 78.5210%\n",
      "total_backward_count 734250 real_backward_count 75119  10.231%\n",
      "epoch-75  lr=['0.0078125'], tr/val_loss:  1.271857/  1.647117, val:  65.42%, val_best:  65.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 73.87 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8234%\n",
      "layer   2  Sparsity: 79.9056%\n",
      "layer   3  Sparsity: 78.1922%\n",
      "total_backward_count 744040 real_backward_count 76109  10.229%\n",
      "epoch-76  lr=['0.0078125'], tr/val_loss:  1.329445/  1.660273, val:  60.83%, val_best:  65.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.71 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8247%\n",
      "layer   2  Sparsity: 80.5326%\n",
      "layer   3  Sparsity: 79.8254%\n",
      "total_backward_count 753830 real_backward_count 77134  10.232%\n",
      "fc layer 1 self.abs_max_out: 18147.0\n",
      "lif layer 1 self.abs_max_v: 33764.5\n",
      "epoch-77  lr=['0.0078125'], tr/val_loss:  1.338474/  1.793482, val:  52.50%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.02 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8379%\n",
      "layer   2  Sparsity: 80.1995%\n",
      "layer   3  Sparsity: 81.3561%\n",
      "total_backward_count 763620 real_backward_count 78132  10.232%\n",
      "epoch-78  lr=['0.0078125'], tr/val_loss:  1.346621/  1.739029, val:  59.58%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.99 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8220%\n",
      "layer   2  Sparsity: 80.0947%\n",
      "layer   3  Sparsity: 79.9376%\n",
      "total_backward_count 773410 real_backward_count 79137  10.232%\n",
      "fc layer 1 self.abs_max_out: 18167.0\n",
      "lif layer 1 self.abs_max_v: 33800.5\n",
      "epoch-79  lr=['0.0078125'], tr/val_loss:  1.398341/  1.694091, val:  58.33%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.12 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 92.8225%\n",
      "layer   2  Sparsity: 80.0191%\n",
      "layer   3  Sparsity: 80.4263%\n",
      "total_backward_count 783200 real_backward_count 80181  10.238%\n",
      "epoch-80  lr=['0.0078125'], tr/val_loss:  1.364345/  1.717282, val:  55.00%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 92.8258%\n",
      "layer   2  Sparsity: 80.7926%\n",
      "layer   3  Sparsity: 80.5814%\n",
      "total_backward_count 792990 real_backward_count 81115  10.229%\n",
      "fc layer 1 self.abs_max_out: 18181.0\n",
      "lif layer 1 self.abs_max_v: 33828.0\n",
      "epoch-81  lr=['0.0078125'], tr/val_loss:  1.425536/  1.778949, val:  51.67%, val_best:  65.83%, tr:  99.28%, tr_best: 100.00%, epoch time: 73.38 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8272%\n",
      "layer   2  Sparsity: 80.3240%\n",
      "layer   3  Sparsity: 81.8500%\n",
      "total_backward_count 802780 real_backward_count 82161  10.235%\n",
      "fc layer 1 self.abs_max_out: 18221.0\n",
      "lif layer 1 self.abs_max_v: 33907.0\n",
      "epoch-82  lr=['0.0078125'], tr/val_loss:  1.460430/  1.749502, val:  52.08%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.46 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8119%\n",
      "layer   2  Sparsity: 80.4721%\n",
      "layer   3  Sparsity: 81.6001%\n",
      "total_backward_count 812570 real_backward_count 83193  10.238%\n",
      "fc layer 1 self.abs_max_out: 18265.0\n",
      "lif layer 1 self.abs_max_v: 33994.0\n",
      "epoch-83  lr=['0.0078125'], tr/val_loss:  1.371169/  1.813006, val:  45.42%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.95 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8296%\n",
      "layer   2  Sparsity: 79.9146%\n",
      "layer   3  Sparsity: 79.4176%\n",
      "total_backward_count 822360 real_backward_count 84232  10.243%\n",
      "fc layer 1 self.abs_max_out: 18343.0\n",
      "lif layer 1 self.abs_max_v: 34130.0\n",
      "epoch-84  lr=['0.0078125'], tr/val_loss:  1.388714/  1.806884, val:  51.67%, val_best:  65.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 74.41 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 92.8308%\n",
      "layer   2  Sparsity: 79.8655%\n",
      "layer   3  Sparsity: 80.2569%\n",
      "total_backward_count 832150 real_backward_count 85313  10.252%\n",
      "epoch-85  lr=['0.0078125'], tr/val_loss:  1.488174/  1.758143, val:  56.67%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.69 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 92.8136%\n",
      "layer   2  Sparsity: 79.9049%\n",
      "layer   3  Sparsity: 82.1386%\n",
      "total_backward_count 841940 real_backward_count 86405  10.263%\n",
      "epoch-86  lr=['0.0078125'], tr/val_loss:  1.432167/  1.763926, val:  52.92%, val_best:  65.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 73.41 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8296%\n",
      "layer   2  Sparsity: 79.6215%\n",
      "layer   3  Sparsity: 81.5258%\n",
      "total_backward_count 851730 real_backward_count 87421  10.264%\n",
      "epoch-87  lr=['0.0078125'], tr/val_loss:  1.457332/  1.723951, val:  52.50%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.90 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8068%\n",
      "layer   2  Sparsity: 79.9276%\n",
      "layer   3  Sparsity: 81.6905%\n",
      "total_backward_count 861520 real_backward_count 88470  10.269%\n",
      "fc layer 1 self.abs_max_out: 18435.0\n",
      "lif layer 1 self.abs_max_v: 34307.5\n",
      "epoch-88  lr=['0.0078125'], tr/val_loss:  1.423291/  1.764806, val:  48.75%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 73.99 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8277%\n",
      "layer   2  Sparsity: 80.1297%\n",
      "layer   3  Sparsity: 80.0623%\n",
      "total_backward_count 871310 real_backward_count 89479  10.269%\n",
      "fc layer 1 self.abs_max_out: 18449.0\n",
      "lif layer 1 self.abs_max_v: 34335.0\n",
      "epoch-89  lr=['0.0078125'], tr/val_loss:  1.381431/  1.676752, val:  65.00%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.68 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8324%\n",
      "layer   2  Sparsity: 80.2253%\n",
      "layer   3  Sparsity: 79.5941%\n",
      "total_backward_count 881100 real_backward_count 90464  10.267%\n",
      "fc layer 1 self.abs_max_out: 18471.0\n",
      "lif layer 1 self.abs_max_v: 34378.5\n",
      "epoch-90  lr=['0.0078125'], tr/val_loss:  1.321652/  1.656733, val:  58.33%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.08 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8094%\n",
      "layer   2  Sparsity: 79.8124%\n",
      "layer   3  Sparsity: 77.3800%\n",
      "total_backward_count 890890 real_backward_count 91433  10.263%\n",
      "fc layer 1 self.abs_max_out: 18520.0\n",
      "lif layer 1 self.abs_max_v: 34473.0\n",
      "epoch-91  lr=['0.0078125'], tr/val_loss:  1.345750/  1.702688, val:  59.58%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 73.26 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8244%\n",
      "layer   2  Sparsity: 79.7231%\n",
      "layer   3  Sparsity: 79.6540%\n",
      "total_backward_count 900680 real_backward_count 92445  10.264%\n",
      "fc layer 1 self.abs_max_out: 18566.0\n",
      "lif layer 1 self.abs_max_v: 34560.5\n",
      "epoch-92  lr=['0.0078125'], tr/val_loss:  1.360451/  1.684783, val:  54.58%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.52 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 92.8285%\n",
      "layer   2  Sparsity: 79.4363%\n",
      "layer   3  Sparsity: 79.0037%\n",
      "total_backward_count 910470 real_backward_count 93422  10.261%\n",
      "epoch-93  lr=['0.0078125'], tr/val_loss:  1.368842/  1.687350, val:  54.17%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.48 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.8268%\n",
      "layer   2  Sparsity: 80.0382%\n",
      "layer   3  Sparsity: 79.4005%\n",
      "total_backward_count 920260 real_backward_count 94402  10.258%\n",
      "epoch-94  lr=['0.0078125'], tr/val_loss:  1.325683/  1.688734, val:  57.50%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.23 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 92.8249%\n",
      "layer   2  Sparsity: 79.9486%\n",
      "layer   3  Sparsity: 78.6857%\n",
      "total_backward_count 930050 real_backward_count 95408  10.258%\n",
      "epoch-95  lr=['0.0078125'], tr/val_loss:  1.327713/  1.637830, val:  59.17%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 92.8276%\n",
      "layer   2  Sparsity: 80.0650%\n",
      "layer   3  Sparsity: 79.5514%\n",
      "total_backward_count 939840 real_backward_count 96414  10.259%\n",
      "fc layer 1 self.abs_max_out: 18594.0\n",
      "lif layer 1 self.abs_max_v: 34620.0\n",
      "epoch-96  lr=['0.0078125'], tr/val_loss:  1.367015/  1.735869, val:  51.25%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 80.40 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 92.8193%\n",
      "layer   2  Sparsity: 79.9294%\n",
      "layer   3  Sparsity: 80.2161%\n",
      "total_backward_count 949630 real_backward_count 97380  10.255%\n",
      "epoch-97  lr=['0.0078125'], tr/val_loss:  1.326476/  1.738102, val:  49.17%, val_best:  65.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 80.31 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 92.8350%\n",
      "layer   2  Sparsity: 79.8428%\n",
      "layer   3  Sparsity: 79.1582%\n",
      "total_backward_count 959420 real_backward_count 98360  10.252%\n",
      "fc layer 1 self.abs_max_out: 18604.0\n",
      "lif layer 1 self.abs_max_v: 34638.0\n",
      "epoch-98  lr=['0.0078125'], tr/val_loss:  1.366547/  1.768560, val:  43.75%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.17 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.8303%\n",
      "layer   2  Sparsity: 80.4166%\n",
      "layer   3  Sparsity: 80.0534%\n",
      "total_backward_count 969210 real_backward_count 99336  10.249%\n",
      "epoch-99  lr=['0.0078125'], tr/val_loss:  1.362147/  1.669557, val:  62.92%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.69 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.8217%\n",
      "layer   2  Sparsity: 80.7514%\n",
      "layer   3  Sparsity: 79.6111%\n",
      "total_backward_count 979000 real_backward_count 100239  10.239%\n",
      "epoch-100 lr=['0.0078125'], tr/val_loss:  1.429021/  1.716694, val:  60.42%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.18 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 92.8317%\n",
      "layer   2  Sparsity: 80.2150%\n",
      "layer   3  Sparsity: 81.2039%\n",
      "total_backward_count 988790 real_backward_count 101214  10.236%\n",
      "fc layer 1 self.abs_max_out: 18624.0\n",
      "lif layer 1 self.abs_max_v: 34678.0\n",
      "epoch-101 lr=['0.0078125'], tr/val_loss:  1.340908/  1.692634, val:  61.25%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.8295%\n",
      "layer   2  Sparsity: 80.1482%\n",
      "layer   3  Sparsity: 79.2135%\n",
      "total_backward_count 998580 real_backward_count 102192  10.234%\n",
      "fc layer 1 self.abs_max_out: 18666.0\n",
      "lif layer 1 self.abs_max_v: 34752.5\n",
      "epoch-102 lr=['0.0078125'], tr/val_loss:  1.380291/  1.734635, val:  55.00%, val_best:  65.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 92.8249%\n",
      "layer   2  Sparsity: 80.4142%\n",
      "layer   3  Sparsity: 79.9067%\n",
      "total_backward_count 1008370 real_backward_count 103166  10.231%\n",
      "epoch-103 lr=['0.0078125'], tr/val_loss:  1.383343/  1.762053, val:  45.00%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.99 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.8155%\n",
      "layer   2  Sparsity: 80.0693%\n",
      "layer   3  Sparsity: 79.7249%\n",
      "total_backward_count 1018160 real_backward_count 104088  10.223%\n",
      "epoch-104 lr=['0.0078125'], tr/val_loss:  1.388950/  1.683035, val:  65.00%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.8249%\n",
      "layer   2  Sparsity: 80.3211%\n",
      "layer   3  Sparsity: 80.8315%\n",
      "total_backward_count 1027950 real_backward_count 105122  10.226%\n",
      "fc layer 1 self.abs_max_out: 18702.0\n",
      "lif layer 1 self.abs_max_v: 34821.0\n",
      "epoch-105 lr=['0.0078125'], tr/val_loss:  1.355790/  1.735324, val:  57.08%, val_best:  65.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 79.89 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.8282%\n",
      "layer   2  Sparsity: 80.4356%\n",
      "layer   3  Sparsity: 79.1381%\n",
      "total_backward_count 1037740 real_backward_count 106126  10.227%\n",
      "fc layer 1 self.abs_max_out: 18738.0\n",
      "lif layer 1 self.abs_max_v: 34939.5\n",
      "epoch-106 lr=['0.0078125'], tr/val_loss:  1.365309/  1.757404, val:  36.67%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 79.93 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 92.8358%\n",
      "layer   2  Sparsity: 80.5129%\n",
      "layer   3  Sparsity: 79.9411%\n",
      "total_backward_count 1047530 real_backward_count 107103  10.224%\n",
      "epoch-107 lr=['0.0078125'], tr/val_loss:  1.336532/  1.769960, val:  59.17%, val_best:  65.83%, tr:  99.49%, tr_best: 100.00%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 92.8263%\n",
      "layer   2  Sparsity: 80.0487%\n",
      "layer   3  Sparsity: 79.3767%\n",
      "total_backward_count 1057320 real_backward_count 108149  10.229%\n",
      "epoch-108 lr=['0.0078125'], tr/val_loss:  1.377672/  1.732792, val:  60.00%, val_best:  65.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 72.91 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 92.8346%\n",
      "layer   2  Sparsity: 79.7116%\n",
      "layer   3  Sparsity: 81.5573%\n",
      "total_backward_count 1067110 real_backward_count 109187  10.232%\n",
      "fc layer 1 self.abs_max_out: 18770.0\n",
      "lif layer 1 self.abs_max_v: 34998.0\n",
      "epoch-109 lr=['0.0078125'], tr/val_loss:  1.419622/  1.724228, val:  55.00%, val_best:  65.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 72.73 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 92.8197%\n",
      "layer   2  Sparsity: 80.1214%\n",
      "layer   3  Sparsity: 82.3250%\n",
      "total_backward_count 1076900 real_backward_count 110179  10.231%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        # \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.5, 0.25, 0.125, 0.0625]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [2, 4, 6, 8, 10]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/128, 1/256, 1/512, 1/1024]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [10,15, 20, 25, 30]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [True]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-11,-10,-9]},\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "        \n",
    "        \"random_select_ratio\": {\"values\": [2,4,6,8]},\n",
    "        \"leaky_temporal_filter\": {\"values\": [0.0, 0.25, 0.5, 1.0]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "        random_select_ratio  =  wandb.config.random_select_ratio,\n",
    "        leaky_temporal_filter  =  wandb.config.leaky_temporal_filter,\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'xdbl2gfg'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
