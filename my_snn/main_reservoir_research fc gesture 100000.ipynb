{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    " \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                  FC_RESERVOIR=False):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        if self.FC_RESERVOIR == True:\n",
    "            self.reservoir = nn.Linear(in_features=self.in_channel*self.in_spike_size*self.in_spike_size, out_features=self.in_channel*self.in_spike_size*self.in_spike_size, bias=True)\n",
    "        else:\n",
    "            self.reservoir = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                            kernel_size=self.receptive_size, \n",
    "                                            stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.reservoir.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.reservoir.bias, 0)\n",
    "\n",
    "        # membrane potential 초기화\n",
    "        self.v = torch.full((self.in_channel, self.in_spike_size, self.in_spike_size), fill_value=self.v_init, requires_grad=False)\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # depthwise conv reservoir: pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            # fc conv reservoir: pre_spike[t] [batch_size, in_channel*in_spike_size*in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "                \n",
    "            recurrent_current = self.reservoir(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*v + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1,\n",
    "                 no_reservoir = False, FC_RESERVOIR=False):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.no_reservoir = no_reservoir\n",
    "        self.FC_RESERVOIR = FC_RESERVOIR\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight,\n",
    "                                       FC_RESERVOIR=FC_RESERVOIR)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.TIME_STEP == x.size(1), f\"Time step mismatch: {x.size(1)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        if (self.FC_RESERVOIR == True):\n",
    "            x = x.reshape(x.size(0), x.size(1), -1)\n",
    "\n",
    "        if self.no_reservoir == False:\n",
    "            with torch.no_grad():\n",
    "                x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE, TIME, dvs_duration, dvs_clipping):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    elif (which_data == 'DVS_GESTURE'):\n",
    "        data_dir = data_path + '/gesture'\n",
    "        transform = None\n",
    "\n",
    "        # # spikingjelly.datasets.dvs128_gesture.DVS128Gesture(root: str, train: bool, use_frame=True, frames_num=10, split_by='number', normalization='max')\n",
    "       \n",
    "        #https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based_en/neuromorphic_datasets.html\n",
    "        # 10ms마다 1개의 timestep하고 싶으면 위의 주소 참고. 근데 timestep이 각각 좀 다를 거임.\n",
    "\n",
    "        \n",
    "        if dvs_duration > 0:\n",
    "            resize_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(\n",
    "                data_dir, train=False, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "\n",
    "        else:\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(data_dir, train=False,\n",
    "                                            data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        \n",
    "        exclude_class = 10\n",
    "        train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "        test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "    \n",
    "        # SubsetRandomSampler 생성\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "        # ([B, T, 2, 128, 128]) \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, sampler=train_sampler, collate_fn=pad_sequence_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, sampler=test_sampler, collate_fn=pad_sequence_collate)\n",
    "        synapse_conv_in_channels = 2\n",
    "        CLASS_NUM = 10\n",
    "        # mapping = { 0 :'Hand Clapping'  1 :'Right Hand Wave'2 :'Left Hand Wave' 3 :'Right Arm CW'   4 :'Right Arm CCW'  5 :'Left Arm CW'    6 :'Left Arm CCW'   7 :'Arm Roll'       8 :'Air Drums'      9 :'Air Guitar'     10:'Other'}\n",
    "\n",
    "\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterator = enumerate(train_loader, 0)\n",
    "    for i, data in iterator:\n",
    "    # for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if len(data) == 2:\n",
    "            inputs, labels = data\n",
    "            # 처리 로직 작성\n",
    "        elif len(data) == 3:\n",
    "            inputs, labels, x_len = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # if rate_coding == True:\n",
    "        #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        # else:\n",
    "        #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        \n",
    "\n",
    "        ###########################################################################################################################        \n",
    "        if (which_data == 'n_tidigits'):\n",
    "            inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "            labels = labels[:, 0, :]\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        elif (which_data == 'heidelberg'):\n",
    "            inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "            print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "        # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "        # print(labels)\n",
    "            \n",
    "        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        elif rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "        ####################################################################################################################### \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        # if i % 100 == 99:\n",
    "        # print(f\"[{i+1}] loss: {running_loss / 100:.3f}\")\n",
    "        # running_loss = 0.0\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "    tr_accuracy = 100 * correct / total         \n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    iterator = enumerate(test_loader, 0)\n",
    "    with torch.no_grad():\n",
    "        for i, data in iterator:\n",
    "        # for inputs, labels in test_loader:\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # if rate_coding == True:\n",
    "            #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            # else:\n",
    "            #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "\n",
    "        \n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST', gpu = '3',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "    # run = wandb.init(project=f'reservoir')\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "    wandb.run.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME=TIME_STEP, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, \n",
    "                            no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP, which_data)\n",
    "        wandb.log({\"epoch\": epoch})\n",
    "        # torch.save(net.state_dict(), 'net_save/reservoir_net.pth')\n",
    "        # artifact = wandb.Artifact('model', type='model')\n",
    "        # artifact.add_file('net_save/reservoir_net.pth')\n",
    "        # run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하기 싫을 때\n",
    "# wandb.init(project=f'reservoir')\n",
    "# main(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: rwy3z0gk\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/rwy3z0gk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qdbpmjqu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tFC_RESERVOIR: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.34453623111594467\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.053704718049556746\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 2.129277712339332\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_142609-qdbpmjqu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/qdbpmjqu' target=\"_blank\">fanciful-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/rwy3z0gk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/rwy3z0gk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/rwy3z0gk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/rwy3z0gk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/qdbpmjqu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/qdbpmjqu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'FC_RESERVOIR' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [/data2/gesture/duration_100000] already exists.\n",
      "The directory [/data2/gesture/duration_100000] already exists.\n",
      "RESERVOIR_NET(\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "Train Accuracy: 42.58%\n",
      "Test loss: 1.330, Val Accuracy: 46.97%\n",
      "Epoch 2\n"
     ]
    }
   ],
   "source": [
    "# sweep하고싶을 때\n",
    "def sweep_cover(data_path='/data2', which_data='CIFAR10', gpu = '2', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False, FC_RESERVOIR=False):\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.batch_size\n",
    "    TIME_STEP  =  wandb.config.time_step\n",
    "    v_decay  =  wandb.config.decay\n",
    "    pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "    which_data  =  wandb.config.which_data\n",
    "    data_path  =  wandb.config.data_path\n",
    "    rate_coding  =  wandb.config.rate_coding\n",
    "    EPOCH  =  wandb.config.EPOCH\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    no_reservoir  =  wandb.config.no_reservoir\n",
    "    FC_RESERVOIR  =  wandb.config.FC_RESERVOIR\n",
    "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping, no_reservoir = no_reservoir, FC_RESERVOIR=FC_RESERVOIR)\n",
    "\n",
    "\n",
    "\n",
    "which_data_hyper = 'DVS_GESTURE' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST', 'DVS_GESTURE'\n",
    "data_path_hyper = '/data2'\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': which_data_hyper,\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"min\": 0.00001, \"max\": 0.1},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64, 128, 256]},\n",
    "        \"time_step\": {\"values\": [4,5,6,7,8]},\n",
    "        \"decay\": {\"min\": 0.25, \"max\": 1.0},\n",
    "        \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "        \"which_data\": {\"values\": [which_data_hyper]},\n",
    "        \"data_path\": {\"values\": [data_path_hyper]},\n",
    "        \"rate_coding\": {\"values\": [True, False]},\n",
    "        \"EPOCH\": {\"values\": [50]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [16,32,48,128]},\n",
    "        \"dvs_duration\": {\"values\": [100000]},\n",
    "        \"dvs_clipping\": {\"values\": [True]},\n",
    "        \"no_reservoir\": {\"values\": [True, False]},\n",
    "        \"FC_RESERVOIR\": {\"values\": [True]},\n",
    "     }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "wandb.agent(sweep_id, function=sweep_cover, count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE하기\n",
    "\n",
    "# # Import\n",
    "# import wandb\n",
    "# # Save your model.\n",
    "# torch.save(model.state_dict(), 'save/to/path/model.pth')\n",
    "# # Save as artifact for version control.\n",
    "# run = wandb.init(project='your-project-name')\n",
    "# artifact = wandb.Artifact('model', type='model')\n",
    "# artifact.add_file('save/to/path/model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()\n",
    "\n",
    "\n",
    "# # LOAD 하기\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "\n",
    "\n",
    "# artifact = run.use_artifact('entity/your-project-name/model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
