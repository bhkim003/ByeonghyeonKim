{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17089/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # class CustomCriterion_mseloss(torch.autograd.Function):\n",
    "    #     @staticmethod\n",
    "    #     def forward(ctx, input, target):\n",
    "    #         \"\"\"\n",
    "    #         input: (batch_size, num_classes)\n",
    "    #         target: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "    #         \"\"\"\n",
    "    #         # targetÏùÑ one-hot Î≤°ÌÑ∞Î°ú Î≥ÄÌôò\n",
    "    #         target_one_hot = torch.zeros_like(input)\n",
    "    #         target_one_hot.scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "    #         ctx.save_for_backward(input, target_one_hot)\n",
    "            \n",
    "    #         # MSE Í≥ÑÏÇ∞\n",
    "    #         loss = F.mse_loss(input, target_one_hot, reduction='mean')\n",
    "    #         return loss\n",
    "\n",
    "    #     @staticmethod\n",
    "    #     def backward(ctx, grad_output):\n",
    "    #         input, target_one_hot = ctx.saved_tensors\n",
    "    #         # MSE gradient: 2 * (input - target) / N\n",
    "    #         N = input.numel()\n",
    "    #         grad_input = 2 * (input - target_one_hot) / N\n",
    "    #         # grad_input = grad_input * grad_output  # chain rule\n",
    "    #         return grad_input, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "        \n",
    "    # Wrapper module\n",
    "    class CustomCriterionMSE(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            # return CustomCriterion_mseloss.apply(input, target)\n",
    "\n",
    "            # targetÏùÑ one-hot Î≤°ÌÑ∞Î°ú Î≥ÄÌôò\n",
    "            target_one_hot = torch.zeros_like(input)\n",
    "            target_one_hot.scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # MSE Í≥ÑÏÇ∞\n",
    "            loss = F.mse_loss(input, target_one_hot, reduction='mean')\n",
    "            # print(f'input {input}', f'target_one_hot {target_one_hot}', f'loss {loss}')\n",
    "            return loss\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # criterion = CustomCriterion().to(device)\n",
    "    criterion = CustomCriterionMSE().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251118_195423-y06kq0x2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y06kq0x2' target=\"_blank\">faithful-cherry-18907</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y06kq0x2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y06kq0x2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20251118_195420_734', 'my_seed': 1, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0.0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000.0, 'lif_layer_sg_width': 4.0, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_main.pth', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1.0, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [], 'scale_exp': [[999, 998], [999, 999], [999, 999]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 0\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 0\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 0, v_exp: None\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 0\n",
      "weight exp, bias exp None None\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (2): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.25, v_reset=10000.0, sg_width=4.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (5): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.25, v_reset=10000.0, sg_width=4.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[], scale_exp=[[999, 998], [999, 999], [999, 999]], ANPI_MODE=False)\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([-0.0289,  0.2612, -0.0137, -0.0824,  0.0284, -0.0277, -0.1418,  0.2316,\n",
      "        -0.0017, -0.2721, -0.0370,  0.0688, -0.1362, -0.0122, -0.0904, -0.1509,\n",
      "        -0.0478,  0.0024, -0.1561, -0.0826, -0.0817, -0.0621, -0.1266, -0.0449,\n",
      "         0.0170,  0.1283,  0.2615,  0.1521,  0.0395,  0.0182, -0.1504,  0.1310,\n",
      "         0.0560,  0.0027,  0.0013, -0.2705,  0.0369, -0.0242,  0.0654,  0.1880,\n",
      "         0.0096,  0.0897,  0.0086, -0.0111,  0.2246, -0.0278,  0.0740,  0.1129,\n",
      "        -0.1065, -0.0122,  0.0353,  0.0830, -0.1050, -0.0288, -0.0971, -0.0476,\n",
      "        -0.0279, -0.0675, -0.0915, -0.0019,  0.0567, -0.0832,  0.0347, -0.1335,\n",
      "        -0.1402, -0.1483,  0.0224, -0.0132, -0.1896,  0.1671, -0.0371, -0.1254,\n",
      "        -0.1368, -0.1086,  0.1403,  0.0890,  0.0164, -0.0333, -0.0402,  0.1510,\n",
      "        -0.0440, -0.1341,  0.0406, -0.0527, -0.1006,  0.0394,  0.1795,  0.1660,\n",
      "        -0.1730,  0.1152,  0.1124, -0.1002, -0.1648, -0.0463, -0.1283, -0.0770,\n",
      "         0.0368, -0.0540,  0.0447, -0.0943,  0.1682,  0.1697,  0.0765, -0.1327,\n",
      "        -0.0175, -0.0235,  0.0173,  0.0228,  0.1165,  0.0874, -0.1202,  0.0937,\n",
      "         0.1093,  0.1436,  0.1297, -0.1175,  0.0643,  0.1767, -0.0473,  0.0127,\n",
      "         0.1104,  0.0541, -0.0634, -0.1238, -0.0421,  0.0424,  0.0746, -0.1028,\n",
      "        -0.0820,  0.0467,  0.0670,  0.0750,  0.0185,  0.0243, -0.0614,  0.0198,\n",
      "        -0.0903,  0.0785,  0.1405, -0.0424, -0.1308,  0.0891,  0.0093, -0.1343,\n",
      "        -0.0234, -0.0892, -0.0384,  0.1104,  0.0450, -0.0666,  0.0188, -0.2035,\n",
      "        -0.0915, -0.1403,  0.0193,  0.0649, -0.0759,  0.0499,  0.1186, -0.1927,\n",
      "        -0.0672,  0.0044,  0.0256, -0.0658, -0.0181,  0.0094,  0.0394, -0.0492,\n",
      "        -0.0107, -0.0724, -0.0017,  0.1776, -0.0289,  0.1369, -0.0624,  0.0131,\n",
      "         0.0021,  0.0042, -0.0312, -0.0447,  0.1171,  0.0114,  0.0296, -0.1796,\n",
      "         0.0512, -0.1628,  0.0020,  0.1117, -0.0968, -0.0534, -0.0548,  0.0009,\n",
      "        -0.0911, -0.0972,  0.0438, -0.0763, -0.0860, -0.0633, -0.1156, -0.1445],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 0.1477, -0.0170, -0.0279, -0.1294, -0.0037, -0.0808,  0.0684,  0.1816,\n",
      "        -0.0092,  0.0474, -0.0123, -0.0084, -0.1872, -0.0022,  0.1190,  0.1008,\n",
      "         0.1439, -0.0131, -0.0605, -0.1240,  0.0387, -0.1245, -0.0165,  0.0701,\n",
      "         0.0035,  0.0194,  0.0979, -0.0234, -0.0061, -0.0326, -0.0253,  0.1495,\n",
      "        -0.0554, -0.0481, -0.0317, -0.1282, -0.0425, -0.1463, -0.1635, -0.0943,\n",
      "         0.0890, -0.1438,  0.1454, -0.1620, -0.0886, -0.1028,  0.0065, -0.0905,\n",
      "         0.0891,  0.0470, -0.0340, -0.1081, -0.0550,  0.0080,  0.1655,  0.1666,\n",
      "         0.0586,  0.0178, -0.0931,  0.2031,  0.0631, -0.0838,  0.1022,  0.0690,\n",
      "        -0.0773,  0.0193, -0.0680, -0.0961, -0.0020,  0.2176, -0.2038,  0.1008,\n",
      "         0.1104, -0.0734, -0.0116,  0.1137, -0.0500, -0.0539,  0.0334,  0.0220,\n",
      "         0.0245, -0.1261, -0.0187,  0.0488,  0.0836,  0.0629, -0.0077,  0.0922,\n",
      "        -0.0342, -0.1331,  0.0362,  0.0937,  0.0099,  0.0172, -0.0554, -0.0782,\n",
      "         0.0710, -0.0568, -0.0460,  0.0087,  0.0623,  0.0275, -0.0128, -0.0305,\n",
      "         0.0452, -0.0241, -0.0682,  0.0138,  0.0454, -0.0046,  0.1632,  0.0767,\n",
      "         0.0401,  0.0831,  0.1341,  0.0349, -0.1578,  0.0524,  0.1510, -0.0578,\n",
      "        -0.1137,  0.0297, -0.0188,  0.1552, -0.2378,  0.1208, -0.1723, -0.0459,\n",
      "         0.2162, -0.0990, -0.0224,  0.0465, -0.0397,  0.0249, -0.1256, -0.1326,\n",
      "         0.0335, -0.0744,  0.0550,  0.2077,  0.0565, -0.1209,  0.1590, -0.0971,\n",
      "        -0.0752,  0.1574,  0.1014, -0.0930,  0.0396,  0.0478,  0.0975, -0.1638,\n",
      "        -0.1578, -0.0128,  0.0858, -0.1696, -0.1846,  0.1604,  0.0317,  0.1018,\n",
      "        -0.1080,  0.0083, -0.0626, -0.0889,  0.0755,  0.0923,  0.1501, -0.0724,\n",
      "        -0.1495,  0.1871, -0.0027, -0.0503,  0.0395, -0.0889,  0.1436, -0.0716,\n",
      "        -0.0965,  0.1172, -0.0532, -0.1287,  0.1186,  0.0490,  0.0976, -0.0501,\n",
      "        -0.0861, -0.0475,  0.1607,  0.0042,  0.0067,  0.1329,  0.1412, -0.0831,\n",
      "         0.0648,  0.1290,  0.2035,  0.0452,  0.0427, -0.1776,  0.0172, -0.0886],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([-0.2041, -0.0888, -0.0064,  0.0490,  0.0296, -0.1061,  0.0919, -0.1847,\n",
      "         0.0158,  0.1951,  0.0446, -0.0155,  0.0885,  0.0181, -0.0331,  0.1184,\n",
      "        -0.0026, -0.1646,  0.0796,  0.0801,  0.0593, -0.1767, -0.1097,  0.0355,\n",
      "        -0.0716, -0.0382,  0.2003, -0.2288,  0.0698, -0.0320, -0.1659, -0.1061,\n",
      "         0.0197, -0.0585, -0.0657, -0.1010, -0.0443, -0.1154, -0.1285,  0.0649,\n",
      "         0.1861, -0.0042, -0.0874,  0.0928, -0.0232,  0.0454, -0.0482, -0.3196,\n",
      "        -0.1972,  0.0931, -0.1504, -0.1500,  0.0560, -0.0309,  0.1398,  0.2406,\n",
      "         0.0596, -0.1162, -0.0399,  0.0485, -0.0816, -0.0221,  0.1075, -0.0622,\n",
      "         0.1140,  0.0101, -0.0953,  0.0495,  0.0754,  0.0333,  0.0396,  0.0939,\n",
      "         0.1437, -0.0427, -0.0248, -0.0151,  0.0027, -0.0731, -0.0503, -0.1152,\n",
      "        -0.0249,  0.0028, -0.1105, -0.1055,  0.0734,  0.0229,  0.1393, -0.1431,\n",
      "         0.0040, -0.0566,  0.1234, -0.1672, -0.0478, -0.0110,  0.0023,  0.0178,\n",
      "        -0.0054,  0.0885, -0.0101, -0.0833, -0.2042, -0.0301,  0.0701, -0.1631,\n",
      "        -0.0681, -0.0184,  0.1092,  0.0272,  0.2208, -0.0321,  0.0155, -0.1172,\n",
      "         0.1220,  0.0533, -0.0533,  0.0589, -0.0088, -0.1095,  0.0483,  0.0576,\n",
      "        -0.0542, -0.1000, -0.0060, -0.0041,  0.0909,  0.0789,  0.0629,  0.0125,\n",
      "        -0.1447,  0.1618,  0.0433, -0.1994, -0.1711, -0.2204,  0.2020, -0.1043,\n",
      "         0.0438, -0.0260, -0.0805,  0.1000, -0.0550, -0.0933,  0.2254, -0.0999,\n",
      "         0.0427, -0.1085,  0.2367,  0.0571, -0.1006, -0.0479, -0.0050, -0.0718,\n",
      "         0.0585, -0.0162,  0.0294,  0.0427,  0.0769, -0.0971, -0.1627, -0.0663,\n",
      "         0.0448,  0.1613, -0.1677, -0.1688, -0.1702,  0.0544, -0.1142, -0.0824,\n",
      "        -0.0331,  0.0448, -0.1315,  0.0517,  0.0845, -0.2306, -0.0228,  0.0928,\n",
      "         0.1453,  0.0334, -0.0638,  0.0287,  0.0892, -0.0804,  0.1342,  0.0943,\n",
      "         0.1607,  0.0720,  0.0619,  0.0113,  0.0834,  0.1369,  0.0418,  0.0296,\n",
      "        -0.0558,  0.0334,  0.1577,  0.0124,  0.0553,  0.0296,  0.2287, -0.0983],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([-0.0527,  0.0936,  0.1681, -0.0201, -0.0709,  0.1164,  0.0875,  0.1407,\n",
      "        -0.1918,  0.0272, -0.0170,  0.0330, -0.0967,  0.0124, -0.0170,  0.0301,\n",
      "        -0.0338,  0.0384, -0.0645,  0.0488, -0.0546, -0.1112,  0.2309,  0.1070,\n",
      "         0.0364,  0.0507,  0.1228, -0.1901, -0.0537, -0.0183,  0.0271,  0.0840,\n",
      "        -0.1839,  0.1022, -0.0924,  0.1476,  0.0182,  0.0492, -0.0189,  0.1518,\n",
      "        -0.0105, -0.1614,  0.0548, -0.0376, -0.0546, -0.0878, -0.0136, -0.0303,\n",
      "        -0.1152, -0.0619,  0.0674,  0.1298, -0.1089, -0.1112, -0.0334,  0.0625,\n",
      "         0.0524,  0.0215, -0.0628, -0.0232, -0.0768, -0.1626,  0.1207, -0.0119,\n",
      "         0.1574,  0.1062, -0.0810,  0.0539, -0.0747,  0.0928, -0.0161,  0.0091,\n",
      "        -0.1100,  0.0472,  0.0914, -0.0541, -0.0245,  0.0034, -0.1309, -0.0168,\n",
      "        -0.0061,  0.1077,  0.0294,  0.1110,  0.0330,  0.0467,  0.0016,  0.0900,\n",
      "        -0.0691, -0.0827,  0.1928, -0.1349, -0.0401,  0.0820, -0.0208,  0.0480,\n",
      "        -0.0744, -0.1483, -0.0622,  0.0979, -0.1113,  0.0518,  0.0161, -0.1097,\n",
      "         0.1178,  0.0848,  0.1452, -0.0195,  0.0353,  0.1514, -0.1510, -0.0293,\n",
      "         0.0930,  0.0266, -0.1338,  0.0621, -0.0789,  0.0726, -0.1096, -0.0656,\n",
      "        -0.1069, -0.0415, -0.1219, -0.0955, -0.1194, -0.0208,  0.0337, -0.0422,\n",
      "        -0.2046, -0.1437,  0.0067,  0.0880,  0.0095,  0.0192, -0.0168,  0.0875,\n",
      "         0.0004,  0.0108, -0.0418,  0.0052, -0.0473,  0.1388, -0.0542, -0.1211,\n",
      "         0.0627, -0.1328,  0.1576,  0.1601,  0.1099,  0.0654,  0.0995,  0.1794,\n",
      "         0.1989, -0.2214, -0.1635, -0.2214,  0.0918, -0.0508, -0.0297,  0.0767,\n",
      "         0.0303, -0.0333,  0.0683, -0.1669,  0.0792,  0.1304, -0.0803, -0.0598,\n",
      "         0.0161,  0.0504, -0.0646, -0.1656, -0.0676,  0.0235,  0.1045,  0.1070,\n",
      "        -0.1317,  0.0384, -0.0021,  0.0377,  0.0403,  0.0218,  0.0164, -0.0958,\n",
      "         0.0075,  0.0489, -0.1143, -0.0180, -0.0315,  0.0564,  0.0012, -0.0408,\n",
      "        -0.0525,  0.0466, -0.1575, -0.0788,  0.0109, -0.0492,  0.1610,  0.0399],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([ 0.0779, -0.0012, -0.1946, -0.1515,  0.2341, -0.1149,  0.1333,  0.0842,\n",
      "         0.0458, -0.0296,  0.0819,  0.0565,  0.0363,  0.1735, -0.1557, -0.1782,\n",
      "        -0.0516,  0.0061, -0.0853, -0.1051,  0.0748, -0.0044,  0.1955, -0.0618,\n",
      "        -0.0313,  0.1415, -0.0974,  0.1416,  0.0271,  0.0144, -0.0061,  0.0769,\n",
      "        -0.0550, -0.0430, -0.1162, -0.0388, -0.0237, -0.1276,  0.1261, -0.0359,\n",
      "         0.0930, -0.1092,  0.0447,  0.0404,  0.1131,  0.2846,  0.0163,  0.0446,\n",
      "        -0.1871, -0.0578,  0.0018,  0.0122,  0.1486,  0.0257, -0.0756, -0.1766,\n",
      "        -0.0228,  0.0134, -0.0633, -0.0345,  0.0255,  0.0838, -0.0832,  0.1274,\n",
      "         0.1023, -0.0200, -0.0588, -0.0317,  0.0701,  0.1696, -0.2173,  0.0747,\n",
      "         0.0343,  0.0013,  0.0253, -0.1741,  0.0725, -0.0922, -0.0710, -0.0130,\n",
      "         0.0433, -0.0342, -0.0076, -0.0680, -0.1161, -0.0260,  0.0854, -0.0139,\n",
      "         0.0889, -0.0238, -0.0245, -0.0012, -0.2037, -0.1151,  0.1325, -0.1250,\n",
      "         0.0237, -0.0628, -0.0440, -0.1071, -0.0644, -0.0035, -0.1814, -0.0202,\n",
      "         0.1690,  0.1054, -0.0525, -0.1351,  0.0166,  0.1159,  0.1570,  0.1098,\n",
      "         0.0501,  0.0803,  0.0197,  0.0301, -0.0814,  0.0785, -0.1304,  0.0779,\n",
      "        -0.0850,  0.0631, -0.1506,  0.0159,  0.0405,  0.0451, -0.0285,  0.0829,\n",
      "        -0.0541,  0.0389, -0.0154, -0.1709, -0.0699, -0.1380,  0.0785,  0.0333,\n",
      "        -0.0787, -0.2022,  0.0748, -0.1833, -0.0222,  0.0479,  0.2425,  0.0391,\n",
      "         0.0437, -0.1683, -0.0586,  0.1045,  0.0641,  0.1659,  0.0082, -0.1093,\n",
      "        -0.0511,  0.1409, -0.0252, -0.0737,  0.0216, -0.1000, -0.0091, -0.0923,\n",
      "        -0.0693, -0.2305, -0.0810, -0.1513, -0.1586,  0.1064, -0.1384, -0.1109,\n",
      "         0.0558,  0.0736,  0.0445, -0.1706, -0.0571, -0.0317, -0.2873,  0.1302,\n",
      "        -0.1169, -0.0084,  0.0510,  0.0768,  0.0491,  0.0925,  0.0389,  0.0541,\n",
      "        -0.0284, -0.0041, -0.0302, -0.0512, -0.0479, -0.0420, -0.0197, -0.0581,\n",
      "         0.1988, -0.0464,  0.1350, -0.0515,  0.0702,  0.0339, -0.0348,  0.0189],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 0.0616, -0.0080,  0.1167, -0.0671,  0.0188, -0.0101,  0.1574,  0.2411,\n",
      "        -0.1573, -0.0607, -0.0796,  0.0491,  0.0307, -0.1350, -0.0418,  0.0310,\n",
      "         0.2477, -0.0587,  0.1134, -0.0636, -0.0263,  0.0897,  0.0926,  0.1411,\n",
      "         0.0145, -0.2437,  0.1620, -0.0438, -0.1723,  0.0220,  0.0285,  0.0203,\n",
      "         0.0597, -0.1423, -0.0924, -0.0054, -0.1725, -0.0368,  0.3380,  0.0673,\n",
      "        -0.0753,  0.0222,  0.1161,  0.0918, -0.0554,  0.1198,  0.0696,  0.0837,\n",
      "         0.0755, -0.1738, -0.0427,  0.0464,  0.0251, -0.0193, -0.1224,  0.0654,\n",
      "        -0.0393,  0.0223,  0.0082, -0.0685, -0.1611, -0.0104, -0.1013, -0.2069,\n",
      "        -0.0504,  0.0234,  0.0239,  0.0997,  0.0081,  0.1377, -0.1235, -0.1601,\n",
      "         0.0202, -0.0230, -0.0331, -0.0233,  0.0339,  0.1243, -0.0442,  0.0329,\n",
      "         0.0172,  0.1671,  0.0566,  0.1336, -0.0447, -0.1084, -0.0236, -0.0323,\n",
      "        -0.1657, -0.1157, -0.1554,  0.0473,  0.0882, -0.0883, -0.0701,  0.0812,\n",
      "         0.1771, -0.0543, -0.0660, -0.0511, -0.1164,  0.1355, -0.1551,  0.1272,\n",
      "         0.0721,  0.0494,  0.1490, -0.0364,  0.0157,  0.0758,  0.1874, -0.1378,\n",
      "        -0.0155,  0.0483,  0.1508, -0.0321,  0.0389, -0.0222, -0.0614,  0.0906,\n",
      "         0.0074,  0.0238, -0.0417,  0.0274, -0.0564, -0.0313,  0.1983, -0.0227,\n",
      "         0.1325,  0.0221,  0.0096,  0.1401,  0.0674,  0.0046,  0.0210,  0.0092,\n",
      "        -0.0547,  0.0129,  0.0751,  0.0647, -0.0999,  0.0116, -0.0352,  0.0785,\n",
      "         0.0249,  0.0641, -0.0204,  0.0880,  0.0318,  0.0084, -0.0046, -0.0349,\n",
      "        -0.0044,  0.0675,  0.0699, -0.1088,  0.1077,  0.0877,  0.1094, -0.0311,\n",
      "         0.0340, -0.0096,  0.0311,  0.1554, -0.0744,  0.0478,  0.0126, -0.0162,\n",
      "         0.0513,  0.0051, -0.0470, -0.0501, -0.0900,  0.0030, -0.1304,  0.1563,\n",
      "         0.0321, -0.0376, -0.0620, -0.1164, -0.1837, -0.1469,  0.0262, -0.0717,\n",
      "         0.1650, -0.0361,  0.0933,  0.0099, -0.0219, -0.0065,  0.1879,  0.0417,\n",
      "        -0.0558,  0.0066, -0.1405, -0.0579,  0.0721,  0.0323, -0.0525,  0.0742],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([-0.0004, -0.0574, -0.2266,  0.0146, -0.0983,  0.0255,  0.0082, -0.0154,\n",
      "         0.0506,  0.0440, -0.0750, -0.1447,  0.0167,  0.1851, -0.0126,  0.1299,\n",
      "        -0.0899,  0.0265,  0.0707, -0.0115,  0.1639,  0.0242, -0.0100, -0.0212,\n",
      "         0.0197, -0.0049, -0.2268, -0.0929, -0.2117,  0.0488,  0.0147, -0.0749,\n",
      "        -0.0771,  0.0377, -0.0005,  0.0123,  0.0082, -0.1178,  0.0752,  0.0782,\n",
      "         0.2383,  0.2957,  0.1864, -0.0781, -0.0584,  0.0230,  0.0612,  0.0597,\n",
      "         0.1795, -0.0555, -0.0889,  0.0161,  0.1385, -0.1312, -0.0550,  0.0388,\n",
      "        -0.0112,  0.0312,  0.1030,  0.0466,  0.1128, -0.0140, -0.1141, -0.0364,\n",
      "         0.1179, -0.0736, -0.0842,  0.0820, -0.1137,  0.0065,  0.0118,  0.1166,\n",
      "        -0.0898, -0.0696, -0.1238,  0.0328,  0.0551, -0.2287, -0.0563,  0.0154,\n",
      "        -0.0125, -0.0462,  0.0692,  0.1069, -0.0366, -0.0364, -0.1279, -0.0685,\n",
      "         0.0326,  0.1306,  0.0165,  0.0817, -0.0079,  0.2614, -0.1455, -0.1237,\n",
      "        -0.0972, -0.0674,  0.0236,  0.0282, -0.1075,  0.0783,  0.1545, -0.0415,\n",
      "         0.1233,  0.0641,  0.1068,  0.0114,  0.0499,  0.0789, -0.0024,  0.0135,\n",
      "         0.1509, -0.0629, -0.1245,  0.0930, -0.1315,  0.0541, -0.1018, -0.0993,\n",
      "         0.2082, -0.0723,  0.0960,  0.0175,  0.0880, -0.0863, -0.0751, -0.0797,\n",
      "        -0.0359, -0.0704, -0.0381, -0.0278,  0.0995, -0.0103, -0.0036,  0.0588,\n",
      "        -0.2294,  0.1350,  0.1096, -0.2916, -0.2075, -0.0466,  0.0643, -0.0668,\n",
      "        -0.0549,  0.0899, -0.1061,  0.2563, -0.0485, -0.0878,  0.0493,  0.0456,\n",
      "        -0.0175,  0.0098, -0.0307,  0.0340,  0.0529,  0.0115,  0.1477,  0.0244,\n",
      "        -0.1026,  0.0145,  0.1479, -0.0005, -0.0872, -0.1680,  0.0050, -0.2332,\n",
      "        -0.1622, -0.0197, -0.0417,  0.0228,  0.1039, -0.0906, -0.0386, -0.0790,\n",
      "         0.0416,  0.0677, -0.0951, -0.0238,  0.0299,  0.1489,  0.1317,  0.0800,\n",
      "         0.0124,  0.1470, -0.0629, -0.0624,  0.0274,  0.3142, -0.0524,  0.2360,\n",
      "        -0.0008,  0.0720, -0.0762,  0.0272, -0.0691,  0.1077, -0.0174,  0.1817],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([ 0.0394,  0.0260,  0.0069, -0.1293,  0.0578,  0.1172,  0.0434,  0.0104,\n",
      "         0.0652, -0.0621, -0.0685,  0.0764, -0.0412,  0.1082,  0.0694,  0.0651,\n",
      "         0.1107, -0.0943,  0.0721,  0.0826, -0.0264, -0.1594, -0.0616,  0.0665,\n",
      "        -0.0077,  0.0594,  0.0190, -0.0279, -0.1240, -0.1050, -0.1828, -0.1179,\n",
      "        -0.0936,  0.1268, -0.2160, -0.1066, -0.0862, -0.1111, -0.0786, -0.0252,\n",
      "         0.0921,  0.0983, -0.0772,  0.0303, -0.0390, -0.0904, -0.0361, -0.0732,\n",
      "        -0.2411,  0.0439, -0.1491, -0.0143,  0.0537, -0.1084,  0.0393, -0.0940,\n",
      "         0.0086, -0.1397,  0.1619, -0.1423,  0.0703, -0.1434, -0.0842, -0.0489,\n",
      "         0.0620,  0.0475,  0.1270, -0.0032,  0.0352, -0.0648, -0.1381, -0.0118,\n",
      "        -0.0462, -0.0804,  0.0283,  0.0510, -0.1856, -0.0303, -0.0385, -0.0634,\n",
      "        -0.0613,  0.0238,  0.0602,  0.0649, -0.0359,  0.0818,  0.0225,  0.1412,\n",
      "         0.1165,  0.0352, -0.0024, -0.1393,  0.0410,  0.0343, -0.0161, -0.0308,\n",
      "         0.1336, -0.0459,  0.0659, -0.0415, -0.0477, -0.0775,  0.1419, -0.1565,\n",
      "        -0.0314,  0.0316,  0.0085, -0.0351, -0.0444,  0.0680,  0.0005,  0.0369,\n",
      "         0.0598, -0.1033,  0.0565, -0.0763, -0.0406, -0.0231, -0.1178,  0.0256,\n",
      "         0.0624, -0.0586,  0.0629,  0.1015, -0.0193,  0.0450, -0.0697,  0.0083,\n",
      "        -0.0270, -0.1787,  0.0856, -0.0382,  0.1618, -0.0641,  0.2068,  0.1216,\n",
      "         0.2067,  0.0144,  0.1390,  0.0112,  0.0418,  0.0814, -0.1848,  0.0122,\n",
      "        -0.0207,  0.0459, -0.0082, -0.0441, -0.0750, -0.0816, -0.0186,  0.1168,\n",
      "        -0.0539,  0.0792, -0.0287, -0.0275,  0.0003, -0.0434, -0.0176,  0.0996,\n",
      "        -0.0972,  0.0272,  0.0345,  0.0253, -0.1298,  0.1220, -0.0528,  0.0535,\n",
      "         0.1748,  0.1809, -0.0073,  0.0798, -0.1129, -0.0505,  0.0661,  0.0527,\n",
      "        -0.0379,  0.0157,  0.0908, -0.0752, -0.0363, -0.0491,  0.0154,  0.0773,\n",
      "        -0.0044,  0.0543, -0.0540,  0.0126,  0.0061,  0.0613, -0.0877, -0.1369,\n",
      "         0.0639, -0.0396,  0.0035,  0.0320,  0.1238, -0.1218,  0.0265,  0.1109],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([-0.0314,  0.0475,  0.0223, -0.0167,  0.0772, -0.1491, -0.1253, -0.0149,\n",
      "         0.0134, -0.0839,  0.0339,  0.1419, -0.0634,  0.0052,  0.0869, -0.1033,\n",
      "        -0.0328,  0.0674, -0.1195,  0.0894, -0.0227, -0.0682, -0.0961, -0.0622,\n",
      "        -0.0577, -0.0540, -0.0519,  0.1582,  0.0849,  0.0752, -0.0402,  0.0796,\n",
      "        -0.1240, -0.1024,  0.0672,  0.1713, -0.0938, -0.0064, -0.0023,  0.0171,\n",
      "        -0.1025, -0.1317,  0.0197,  0.1616, -0.0297,  0.0007,  0.0757, -0.0460,\n",
      "         0.0443, -0.1707,  0.1654, -0.1517, -0.0782,  0.1549, -0.0115, -0.0147,\n",
      "        -0.2042,  0.1254, -0.0911,  0.0244, -0.1523,  0.1652,  0.0141,  0.0480,\n",
      "        -0.0947, -0.0817, -0.0612, -0.0106, -0.1557, -0.0848,  0.0631, -0.0375,\n",
      "        -0.0235,  0.1857,  0.0500,  0.1774, -0.0129, -0.2781,  0.0671, -0.1331,\n",
      "        -0.0044, -0.0510,  0.0934, -0.0182, -0.0855, -0.0182, -0.2026, -0.1188,\n",
      "         0.0492, -0.0476, -0.0570,  0.0146, -0.1133, -0.1360, -0.1365,  0.1845,\n",
      "         0.0158, -0.0712,  0.0988, -0.0176,  0.0714, -0.0286, -0.0287, -0.0200,\n",
      "         0.1614,  0.0390,  0.2813, -0.1343, -0.0482,  0.0506, -0.0382,  0.0587,\n",
      "        -0.0467, -0.1628, -0.1223,  0.1008, -0.1930, -0.0866, -0.1498, -0.0819,\n",
      "         0.0255, -0.1351,  0.0476,  0.2038,  0.0967, -0.1650,  0.0280, -0.0326,\n",
      "        -0.0905, -0.0386, -0.0657,  0.0157, -0.1266,  0.1026,  0.0376,  0.2102,\n",
      "         0.0431, -0.0652, -0.1408, -0.0117, -0.0197,  0.0942, -0.0840,  0.0584,\n",
      "         0.0060, -0.0483,  0.1902, -0.1607, -0.1646, -0.2045,  0.1306, -0.0572,\n",
      "         0.0758,  0.0526,  0.1054, -0.0053, -0.1532, -0.0681, -0.0640,  0.0115,\n",
      "        -0.1111,  0.0572,  0.0975, -0.1608,  0.0380,  0.0632,  0.0860, -0.0226,\n",
      "        -0.1830,  0.1199, -0.0684,  0.0041, -0.0343, -0.0777, -0.0546,  0.0387,\n",
      "         0.1569,  0.0755, -0.0104,  0.0816, -0.0245, -0.0294,  0.0303, -0.0116,\n",
      "         0.1739, -0.0953,  0.0374,  0.0249,  0.1086, -0.1196,  0.0591,  0.0007,\n",
      "         0.0777, -0.0675,  0.0028, -0.0261, -0.0353,  0.0015, -0.0389, -0.2383],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([ 1.3979e-02, -1.3911e-02, -1.1985e-01,  2.0690e-02,  3.6181e-02,\n",
      "         1.6775e-02,  1.0729e-01,  3.2120e-02,  6.7534e-02, -9.8495e-03,\n",
      "         6.7884e-03,  1.7606e-01,  7.7206e-02,  4.3263e-02,  4.5469e-02,\n",
      "         9.5211e-02,  9.9391e-02,  6.6909e-02, -2.2377e-02, -4.6886e-02,\n",
      "        -5.7291e-03, -1.0073e-01,  2.4301e-02,  1.0707e-01,  1.2731e-02,\n",
      "         1.2589e-01,  1.9754e-02,  1.0216e-01,  8.0131e-02,  4.7178e-02,\n",
      "         2.6534e-02,  9.3718e-02,  9.7063e-02, -1.1227e-01, -1.7711e-02,\n",
      "         3.0842e-02,  7.3864e-02, -1.3816e-01, -8.1119e-02, -1.2927e-02,\n",
      "        -1.8243e-03,  1.3280e-04, -1.2234e-01,  7.7035e-02, -8.7595e-02,\n",
      "         2.0779e-01,  1.7796e-01, -1.2504e-01, -1.2741e-01,  3.3528e-02,\n",
      "        -2.2094e-02,  2.5073e-02,  6.5853e-02, -7.2761e-02,  8.1229e-02,\n",
      "        -1.5010e-02,  5.5768e-02,  1.7111e-01, -1.4666e-02, -5.6316e-02,\n",
      "         2.1799e-02, -7.0589e-02,  4.0128e-03,  2.9420e-01,  3.9436e-02,\n",
      "         4.1057e-02,  1.5825e-01, -1.1875e-01, -9.8189e-02,  1.3455e-01,\n",
      "         6.4241e-02, -3.2179e-02, -9.9394e-02, -1.0560e-01, -1.0021e-01,\n",
      "        -7.3862e-02, -1.6663e-02, -1.4995e-01,  1.1218e-02, -1.1019e-01,\n",
      "         1.4262e-02,  7.1316e-02, -4.8827e-02, -1.4175e-01, -2.3822e-02,\n",
      "        -9.3333e-02, -1.2582e-01,  9.8382e-02, -8.6568e-02, -7.2830e-02,\n",
      "         3.5475e-02,  7.8958e-03, -3.3995e-02, -5.7409e-02,  1.6111e-01,\n",
      "         1.1808e-01, -3.5267e-02,  1.3927e-01,  1.0361e-01, -5.7061e-02,\n",
      "         4.6825e-02,  5.7272e-02, -5.1540e-02,  1.8100e-02,  9.4374e-02,\n",
      "         1.0074e-01,  1.2902e-02, -2.6135e-03, -8.3204e-02,  3.7383e-02,\n",
      "         4.0829e-02,  6.4131e-02,  2.1529e-01, -9.0615e-02, -5.1402e-02,\n",
      "         1.0224e-01, -2.6783e-02, -3.7043e-02,  1.7837e-01,  6.5439e-02,\n",
      "         1.2405e-01,  4.7294e-02,  3.9902e-02,  4.6614e-02, -5.7397e-02,\n",
      "        -1.1948e-02,  9.6569e-02, -8.6357e-02, -1.2415e-02, -6.6724e-02,\n",
      "         8.5177e-02,  3.9636e-02, -6.9109e-02, -9.8633e-02, -7.4830e-02,\n",
      "        -1.2169e-01, -9.3715e-02, -1.2518e-01, -2.9737e-02,  7.4968e-02,\n",
      "        -1.5953e-01, -1.1313e-02, -1.0785e-01, -7.1701e-02, -5.7053e-02,\n",
      "         6.1507e-02,  9.4270e-02, -1.4999e-01, -5.7913e-02, -5.3546e-02,\n",
      "        -1.1397e-01,  1.1722e-02, -1.5179e-01,  5.6387e-02, -1.2858e-02,\n",
      "         5.9243e-02,  7.3249e-02,  6.8194e-02, -5.2159e-02, -1.1947e-01,\n",
      "         1.3285e-01,  3.7161e-02,  1.1279e-03,  1.2161e-01,  1.3462e-02,\n",
      "         1.8182e-01,  4.2557e-02, -9.2519e-02,  7.5969e-02, -1.5371e-02,\n",
      "         4.7073e-02,  3.0194e-02, -2.4528e-02,  8.0951e-02, -4.1227e-02,\n",
      "        -3.6816e-02, -1.0081e-01, -1.5359e-01,  6.5814e-02, -8.3603e-02,\n",
      "         1.2535e-01, -3.1021e-02, -8.8091e-02, -1.2611e-01, -3.6769e-02,\n",
      "        -1.3119e-02,  2.8919e-02,  5.1445e-02,  8.6765e-02,  8.1418e-02,\n",
      "        -1.4041e-01,  1.1084e-01,  1.2892e-01, -8.2344e-04, -6.1775e-02,\n",
      "         5.7038e-02,  1.3082e-01, -1.1051e-01,  1.2862e-01, -4.2578e-03],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "self.weight_fb[0] tensor([-6.1870e-02, -6.2763e-02, -6.2654e-02, -1.4122e-01,  4.3396e-02,\n",
      "        -5.9263e-02, -8.6564e-02, -9.2542e-02, -3.5556e-04, -4.4858e-02,\n",
      "         3.7574e-02,  1.1883e-01,  1.4588e-01,  5.1017e-03, -1.2994e-01,\n",
      "         1.0684e-01, -6.3763e-02,  1.6127e-01, -5.9857e-02, -4.9716e-02,\n",
      "         1.2088e-01, -3.0439e-02,  5.0000e-03, -7.2117e-03,  1.0896e-01,\n",
      "        -5.5239e-02,  1.6736e-02,  6.2158e-02,  2.8922e-02,  3.4220e-02,\n",
      "         3.7951e-02, -3.5904e-02, -3.3476e-02,  2.9390e-02,  7.0506e-02,\n",
      "        -9.2412e-02,  2.0155e-02,  6.5759e-02,  6.7800e-02, -1.5081e-01,\n",
      "         1.2845e-01,  2.2924e-01,  2.2292e-04, -1.4177e-01,  1.1385e-01,\n",
      "        -5.2078e-02,  9.6538e-02, -1.0606e-01, -1.4861e-01,  6.8852e-02,\n",
      "        -1.8763e-01, -4.0597e-02,  9.2861e-02,  4.9231e-03,  1.2072e-01,\n",
      "         9.4269e-03, -1.8970e-01, -1.2516e-01,  9.4061e-02,  9.4273e-02,\n",
      "         6.6180e-02, -6.8646e-02,  1.6542e-01, -2.1220e-02,  2.4611e-02,\n",
      "        -1.6053e-03, -1.3137e-01,  2.8868e-03, -1.4687e-01, -4.0802e-03,\n",
      "        -8.2664e-02,  6.7209e-02,  1.1512e-01, -4.0972e-02,  1.6833e-02,\n",
      "         5.1671e-02, -1.2450e-01, -2.8432e-02,  2.8871e-02,  7.9481e-02,\n",
      "         1.6831e-01, -2.3945e-02,  1.4201e-02,  6.5225e-02, -1.1502e-02,\n",
      "        -3.2115e-02, -3.6535e-02, -1.1036e-02, -2.2196e-02,  8.7490e-02,\n",
      "         5.1805e-02, -8.4337e-02, -2.7168e-02,  1.0938e-01, -9.0960e-02,\n",
      "        -6.4377e-02,  5.1735e-02, -8.5541e-02,  7.6011e-03, -2.2321e-01,\n",
      "        -1.0015e-02, -8.6348e-02, -4.6037e-02, -1.0351e-01,  1.8391e-01,\n",
      "        -8.6146e-02,  4.8618e-02, -1.2201e-01,  6.1461e-02,  5.0163e-02,\n",
      "        -1.0218e-01,  1.0960e-01, -1.3334e-02, -1.0508e-02,  1.1526e-01,\n",
      "         2.0714e-01, -1.2283e-01,  2.0920e-01,  5.6710e-02, -3.7023e-02,\n",
      "        -2.8073e-02, -8.3695e-02, -8.1271e-02, -3.3079e-02, -7.8959e-02,\n",
      "         7.4594e-02, -1.4765e-01,  1.0218e-01, -1.5748e-01,  1.0428e-01,\n",
      "        -4.7678e-02, -1.0045e-01, -1.3012e-01, -3.8958e-02,  2.1946e-01,\n",
      "         4.0591e-02,  6.0252e-02,  3.5542e-02, -8.4269e-02,  4.9231e-02,\n",
      "         2.1921e-02,  1.3501e-01,  1.7345e-01,  7.6810e-02, -1.1157e-01,\n",
      "        -2.8636e-02,  1.3121e-02,  9.0308e-02,  6.0350e-02, -1.3342e-01,\n",
      "         8.8706e-03, -9.2793e-02,  2.5497e-02,  3.2824e-02, -1.3066e-02,\n",
      "         2.3721e-02,  8.2154e-02, -5.4652e-02,  9.6261e-02, -1.9293e-01,\n",
      "         7.8505e-02, -1.0314e-01, -1.1350e-02,  9.6889e-02,  2.9741e-02,\n",
      "         4.3195e-02, -1.3071e-01,  8.4670e-02,  1.3636e-01,  1.4111e-01,\n",
      "        -1.1849e-01,  2.0688e-03,  4.1498e-02, -9.1041e-03,  9.0810e-02,\n",
      "        -1.0910e-01,  7.5433e-02,  7.2838e-02, -9.7462e-03,  1.0529e-01,\n",
      "        -9.9534e-02,  3.2246e-02,  3.7327e-02,  9.3890e-02,  1.2435e-01,\n",
      "        -1.5517e-01,  3.1685e-02, -5.6165e-03,  1.6025e-01, -1.5168e-01,\n",
      "        -1.4845e-01,  5.1059e-02, -2.6978e-02,  2.1934e-02,  2.0807e-02,\n",
      "         7.2738e-03,  3.9026e-02, -2.3090e-03,  7.3316e-02, -1.5519e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[1] tensor([ 3.9728e-03, -7.3524e-02,  1.6294e-02,  2.1089e-01, -9.0092e-02,\n",
      "         3.3362e-02, -5.8279e-03, -1.3281e-01, -4.5238e-02, -6.4674e-03,\n",
      "        -1.2432e-01, -1.9194e-03,  5.3200e-03,  3.8971e-01,  1.2851e-01,\n",
      "         1.9432e-02, -1.0308e-01,  2.6457e-02, -1.5819e-01, -6.9756e-02,\n",
      "         7.1999e-03,  8.9055e-02,  2.8267e-02,  4.2122e-02, -1.0188e-02,\n",
      "        -2.1903e-03,  3.8962e-02, -3.7538e-02, -1.5041e-01, -1.2934e-01,\n",
      "        -1.2043e-01,  3.5927e-02, -7.0173e-02,  1.5156e-02, -5.2709e-02,\n",
      "        -4.9970e-02,  8.8897e-02, -8.7405e-02, -4.1130e-02, -9.0494e-02,\n",
      "         1.5093e-01,  1.7089e-02,  2.2285e-03, -1.7867e-01, -3.9023e-02,\n",
      "         3.6104e-02, -9.9238e-02, -2.6198e-01, -4.8368e-02, -1.8873e-02,\n",
      "         2.8550e-01, -7.5669e-02,  5.5716e-02, -1.0409e-01,  2.1690e-01,\n",
      "         1.5653e-02, -2.7925e-02, -7.4226e-02, -1.4367e-02,  3.9615e-02,\n",
      "        -5.8226e-02, -2.0707e-02, -1.3132e-03, -2.5500e-01,  1.3597e-01,\n",
      "        -6.5147e-02,  1.0149e-01,  1.7085e-02,  1.0070e-02,  1.1868e-01,\n",
      "        -1.1022e-01, -1.5096e-01, -7.6378e-02,  1.1526e-01,  4.5284e-02,\n",
      "        -5.7636e-02,  2.5042e-02,  1.1170e-01,  1.9068e-02,  2.9679e-02,\n",
      "        -3.1938e-02,  3.5410e-02,  3.1389e-03, -4.4344e-02, -5.9055e-02,\n",
      "         7.6881e-02, -7.4985e-03, -7.7848e-02, -8.3300e-02, -2.1891e-01,\n",
      "        -1.1052e-02, -8.2107e-02, -8.0965e-02, -9.0501e-04, -7.1612e-02,\n",
      "         7.5600e-02, -3.3961e-02, -1.0477e-01, -4.8136e-02, -1.1034e-01,\n",
      "        -7.4978e-02, -2.4359e-01,  2.3003e-01, -5.1130e-02,  1.4514e-01,\n",
      "         4.1260e-02, -9.8144e-02,  1.4848e-01,  1.5925e-01, -2.8575e-02,\n",
      "         2.4836e-02,  4.9183e-03, -6.2617e-02,  2.4538e-02,  9.8451e-02,\n",
      "        -7.4590e-02,  1.4109e-01,  1.1406e-01, -4.5252e-02, -2.9583e-01,\n",
      "        -9.1155e-02, -1.9915e-01,  2.6170e-02, -1.1158e-01,  6.2205e-02,\n",
      "        -6.4982e-02, -2.0180e-02,  1.1741e-01, -9.5931e-02,  1.6191e-01,\n",
      "         2.3552e-02, -1.9746e-02,  7.3076e-02, -1.5658e-01,  3.3068e-02,\n",
      "        -6.1176e-02,  3.1169e-02,  5.0606e-02, -1.3180e-01, -6.1597e-02,\n",
      "         2.5113e-02, -2.9525e-03, -9.4172e-02,  1.3421e-01,  1.1159e-01,\n",
      "        -1.1387e-01,  4.2098e-02,  8.2559e-04, -1.0900e-01, -5.1050e-02,\n",
      "        -1.1954e-01, -4.1089e-02, -7.4815e-02,  1.4943e-01,  5.2440e-02,\n",
      "         1.4030e-01, -2.2766e-02, -2.1089e-01,  1.0797e-03, -7.6787e-02,\n",
      "        -9.7701e-02, -2.0112e-01,  6.4742e-03, -1.7217e-01, -1.0267e-01,\n",
      "         9.2035e-02,  2.0810e-02,  7.0022e-02,  8.5900e-02,  3.9608e-02,\n",
      "         1.9578e-01, -3.7497e-02,  9.8431e-02, -1.0775e-01,  9.3400e-03,\n",
      "         4.7345e-02,  6.4071e-02, -1.2331e-01,  3.8381e-03, -5.5601e-02,\n",
      "        -7.4040e-02,  7.7171e-02,  8.8795e-02,  2.4500e-01, -6.7051e-02,\n",
      "        -5.3225e-02,  3.8968e-02,  7.7060e-02, -6.3684e-03,  1.9607e-01,\n",
      "         4.0111e-02,  1.5504e-01,  2.4224e-02,  8.3664e-02,  8.0783e-02,\n",
      "        -1.4865e-01, -1.3644e-02,  1.6719e-02,  1.2246e-01,  2.3349e-04],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[2] tensor([ 0.0417, -0.1762, -0.1117,  0.1188,  0.0542,  0.0250,  0.2885, -0.0173,\n",
      "         0.0245, -0.0157, -0.1664,  0.1117, -0.1846,  0.1206, -0.0885,  0.0019,\n",
      "         0.0370, -0.1052, -0.0497, -0.1876,  0.1554, -0.0531,  0.0141,  0.1704,\n",
      "        -0.2869, -0.0033,  0.0998,  0.1030, -0.0074,  0.0547, -0.0476,  0.0327,\n",
      "        -0.0663,  0.0192, -0.0229, -0.0240,  0.2722, -0.0248, -0.0537,  0.0130,\n",
      "         0.0602, -0.0421, -0.0065, -0.0450,  0.2531, -0.0649,  0.0143,  0.0049,\n",
      "        -0.0263, -0.0437,  0.0555,  0.0088, -0.0438,  0.0458, -0.0951,  0.0321,\n",
      "        -0.0796, -0.0559,  0.0490,  0.0197, -0.0075, -0.1117,  0.1440,  0.0640,\n",
      "        -0.0372,  0.0465, -0.0740,  0.0301, -0.1574,  0.1190,  0.1643,  0.0525,\n",
      "        -0.0435, -0.1372, -0.0526, -0.0873,  0.2263,  0.0303,  0.0393,  0.0741,\n",
      "         0.0601, -0.0770,  0.0031,  0.1654,  0.0433, -0.0192,  0.2116,  0.0493,\n",
      "        -0.0322,  0.1928, -0.0754, -0.0324,  0.1310,  0.0978,  0.0231,  0.0876,\n",
      "         0.0595, -0.1828,  0.0922,  0.0204, -0.1251,  0.0918, -0.0327,  0.2516,\n",
      "         0.0107,  0.1048, -0.0265, -0.0230,  0.1867, -0.0498, -0.0138,  0.1156,\n",
      "        -0.0348, -0.2172, -0.1317, -0.0651,  0.0237,  0.0297, -0.0218, -0.0651,\n",
      "        -0.0624, -0.0337, -0.0537,  0.1240, -0.0692, -0.0235, -0.0797, -0.0290,\n",
      "        -0.1196, -0.0112,  0.1143,  0.0681,  0.1028,  0.0155, -0.0525, -0.0571,\n",
      "         0.0489, -0.0769, -0.0483, -0.0562,  0.1783, -0.0148,  0.1246,  0.1142,\n",
      "        -0.0167, -0.0183,  0.0599,  0.0653,  0.0381,  0.1051, -0.0486,  0.0308,\n",
      "         0.0632,  0.2124,  0.0168, -0.1670, -0.0011, -0.0862, -0.0907, -0.1048,\n",
      "         0.1018, -0.0088, -0.0297, -0.0284, -0.0288,  0.1239,  0.1482, -0.0475,\n",
      "         0.0914,  0.0086,  0.0481, -0.0587,  0.0406, -0.0134,  0.0551, -0.1047,\n",
      "         0.1554,  0.0231,  0.1211, -0.0485, -0.0948,  0.1502, -0.0493, -0.1490,\n",
      "         0.1869,  0.1108, -0.0508,  0.0212,  0.0654, -0.0689,  0.0944, -0.0505,\n",
      "         0.0601,  0.0698,  0.0631,  0.0319, -0.1234,  0.0844,  0.0118,  0.1960],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[3] tensor([ 3.7229e-03, -1.2595e-02,  1.7420e-01, -3.1672e-03, -5.2856e-02,\n",
      "         7.6175e-02, -7.1027e-03,  9.6481e-02,  1.6362e-01,  1.8737e-02,\n",
      "        -1.8550e-01,  3.3463e-03,  1.3537e-01,  1.6333e-02, -2.2672e-02,\n",
      "         1.3444e-02, -2.8420e-01,  9.4572e-02,  2.3005e-01, -3.8616e-02,\n",
      "        -3.0297e-02, -5.9469e-02, -4.5419e-02, -1.2708e-02, -4.4820e-02,\n",
      "        -3.1001e-02, -5.2363e-02, -4.3413e-02,  1.3407e-02, -1.2741e-01,\n",
      "         4.8774e-02, -2.4650e-02, -7.4278e-02, -1.6299e-02, -2.1139e-02,\n",
      "        -1.7276e-02,  1.1456e-01,  1.0551e-01,  6.1375e-02, -1.9237e-02,\n",
      "        -1.3890e-01,  1.3603e-01,  2.8346e-02, -1.8154e-01,  1.0431e-01,\n",
      "         1.0642e-01, -8.0557e-02,  2.2248e-02,  1.8163e-01,  6.2300e-02,\n",
      "        -1.5857e-01,  9.6966e-02, -6.8316e-02, -6.2120e-02, -2.5379e-02,\n",
      "         7.5444e-03, -1.3321e-01,  8.3204e-02, -4.9224e-02, -1.2339e-01,\n",
      "         1.0322e-01, -3.2595e-02,  2.3447e-02,  2.3445e-02,  2.2547e-02,\n",
      "         3.8509e-02, -2.0007e-02, -6.5523e-02,  1.3879e-02, -3.9385e-02,\n",
      "         3.5916e-02, -2.2805e-01, -1.2021e-01, -1.5939e-01, -1.5273e-01,\n",
      "         2.7466e-01,  8.7631e-02, -1.1186e-01,  6.2295e-02, -1.8220e-02,\n",
      "         9.3996e-02, -2.7239e-02, -1.7658e-01,  2.4093e-02, -6.5129e-02,\n",
      "         2.1871e-01, -1.2749e-01,  3.3614e-02,  3.1173e-02,  1.4515e-01,\n",
      "         1.6280e-02,  5.9555e-02,  8.9045e-02,  4.2754e-02, -1.0534e-01,\n",
      "         6.5100e-02, -2.6591e-01, -1.4692e-01, -2.1475e-02, -1.8257e-02,\n",
      "         1.4547e-01,  1.5479e-04, -8.2790e-02, -7.1589e-02,  7.6404e-02,\n",
      "        -2.0771e-01, -1.1344e-01, -3.3926e-03,  1.0683e-01, -1.2565e-01,\n",
      "         7.1926e-02,  1.1094e-01, -9.9572e-02,  1.0442e-01, -1.5828e-01,\n",
      "        -4.0917e-02,  1.0962e-01, -1.7219e-02,  4.7168e-02,  9.9607e-02,\n",
      "         1.7441e-01, -1.8981e-02,  1.7271e-02,  8.7641e-02, -1.7532e-01,\n",
      "        -9.5018e-02,  9.6621e-02, -1.2913e-01, -1.1084e-01,  4.7749e-02,\n",
      "        -8.2657e-02, -1.6724e-02, -1.0306e-02, -1.6134e-01, -7.2602e-03,\n",
      "         1.6080e-02, -2.2653e-02,  1.5684e-01,  2.8544e-01, -4.7241e-02,\n",
      "        -1.0567e-02, -3.8363e-02, -5.4891e-02,  5.6958e-03,  2.8807e-01,\n",
      "        -9.3448e-02,  7.9623e-02,  1.1257e-01,  1.1534e-01,  7.0215e-02,\n",
      "        -8.0252e-02, -1.5546e-02,  2.3133e-02,  1.0757e-01, -9.7446e-03,\n",
      "        -6.9094e-02,  7.8770e-03,  4.5920e-02, -1.8479e-01, -9.0383e-02,\n",
      "        -1.2509e-01,  5.6506e-02,  6.1723e-02, -1.5174e-01,  1.9297e-01,\n",
      "        -7.5414e-02,  1.0457e-01,  8.0773e-02,  3.7780e-03, -8.4189e-02,\n",
      "         3.1688e-02, -3.8497e-03,  8.4056e-02, -9.9316e-02,  8.7615e-02,\n",
      "        -6.8071e-02,  2.5084e-02,  5.3574e-02,  4.6666e-02,  1.2947e-02,\n",
      "         4.6195e-02,  9.2266e-02, -2.9378e-02, -1.0723e-01,  1.7183e-01,\n",
      "         4.2814e-02,  8.1333e-02, -4.9970e-02,  3.2418e-03, -1.3128e-01,\n",
      "         7.7707e-02, -3.8889e-02, -1.1558e-02,  8.1372e-02,  1.4607e-01,\n",
      "        -7.6217e-02,  1.4468e-01,  1.5896e-01, -9.3251e-02,  6.3844e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[4] tensor([-1.0302e-01, -7.7509e-03, -7.4870e-03,  2.4358e-01, -1.2544e-01,\n",
      "         1.6362e-02,  2.6464e-02,  2.9164e-02,  3.7399e-02,  1.3161e-01,\n",
      "        -4.2681e-02,  1.4904e-01, -9.0937e-02, -3.3656e-02,  2.8218e-02,\n",
      "        -9.3955e-02, -1.3286e-01, -6.3144e-02,  1.9410e-01,  4.0926e-02,\n",
      "        -3.6411e-02,  1.4837e-01,  7.9924e-03, -4.3758e-02,  1.5341e-02,\n",
      "        -7.4028e-02, -6.6781e-02,  4.5540e-02,  2.4593e-02,  4.7232e-02,\n",
      "        -4.8391e-02,  7.8726e-02, -2.8389e-02, -4.8237e-02, -1.6420e-02,\n",
      "        -8.5405e-03, -6.0271e-02,  4.7398e-02, -5.9538e-03, -9.4570e-03,\n",
      "         1.4137e-01, -2.0030e-03, -2.0963e-02,  2.6044e-02, -1.1906e-01,\n",
      "        -6.1033e-02,  1.9195e-01, -1.1498e-01,  7.4118e-02, -4.3078e-02,\n",
      "         5.2863e-03, -2.1767e-01,  3.0306e-02,  1.0708e-01,  7.0715e-02,\n",
      "        -2.4829e-02,  2.9306e-02, -8.5861e-02,  5.9368e-02,  9.2923e-03,\n",
      "        -7.9308e-02, -4.6438e-02,  5.0008e-02, -7.6247e-02,  1.1050e-01,\n",
      "        -1.7031e-02, -5.2862e-02,  1.1197e-01, -1.9923e-01,  3.0071e-03,\n",
      "         1.6545e-01,  1.1190e-03, -5.9068e-02,  4.3240e-02,  1.1128e-01,\n",
      "         5.8969e-03, -1.1048e-01,  6.5282e-02, -6.0153e-02,  1.3223e-01,\n",
      "        -1.5304e-02, -1.7250e-02,  1.2908e-01,  2.2995e-03,  1.1096e-01,\n",
      "        -2.0244e-01, -2.8327e-01, -1.4942e-01, -2.1503e-02, -8.0930e-03,\n",
      "         8.2775e-02,  9.9442e-03, -4.2479e-02,  1.8705e-01, -5.8489e-02,\n",
      "         3.8074e-02, -1.8708e-01,  6.8406e-02, -6.9420e-02,  1.9106e-02,\n",
      "        -3.9417e-02, -4.5432e-05, -6.4470e-02, -6.8615e-02,  1.1113e-02,\n",
      "         1.2353e-01, -3.0012e-02,  1.3205e-01, -2.5266e-02,  7.9964e-02,\n",
      "        -1.3544e-01, -1.5722e-02,  8.1702e-02,  2.5494e-02,  7.8534e-02,\n",
      "        -9.3808e-02, -1.5518e-01, -2.3512e-02, -6.1402e-02, -5.4612e-02,\n",
      "         1.1458e-01, -2.7796e-02, -1.3328e-01, -1.2011e-01, -7.3848e-02,\n",
      "         3.9711e-02,  1.1745e-01,  5.1109e-03,  5.8333e-02, -6.1241e-02,\n",
      "        -1.7331e-01, -9.6379e-02,  3.4767e-02, -1.5564e-01,  1.2040e-02,\n",
      "        -3.6924e-02,  3.6661e-03, -1.3521e-01,  1.1064e-01,  1.0018e-01,\n",
      "        -1.2725e-01, -4.3389e-02, -6.1483e-02, -4.3733e-02,  1.3003e-01,\n",
      "        -5.1490e-02, -2.5160e-02,  1.2371e-01, -6.1683e-02,  1.4167e-01,\n",
      "         6.5285e-02, -3.1724e-02,  9.4539e-02, -5.7676e-02, -1.4065e-01,\n",
      "        -2.2629e-02,  4.3304e-02, -1.3248e-01, -2.0677e-02,  6.3318e-02,\n",
      "        -2.0361e-02, -1.6661e-02,  3.6438e-02,  5.0278e-02,  2.8180e-02,\n",
      "        -9.1329e-02, -7.5576e-03, -8.8173e-02, -2.9350e-02,  3.9386e-02,\n",
      "        -3.6012e-02,  1.3277e-01,  3.8667e-03, -1.1344e-01,  2.6721e-01,\n",
      "         8.9100e-02, -8.3056e-02, -5.6132e-02,  4.5437e-02, -5.2353e-02,\n",
      "         1.0544e-03, -6.6587e-02,  5.2335e-02, -2.0228e-02, -1.0489e-01,\n",
      "         5.8334e-02, -1.1497e-01,  7.4012e-02,  1.0624e-02,  5.2788e-02,\n",
      "        -1.1195e-01, -7.7311e-02,  3.7675e-03, -3.2350e-02,  1.1414e-01,\n",
      "         1.7142e-01, -3.3949e-02,  6.4347e-02, -1.0133e-01,  3.4662e-02],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[5] tensor([ 1.9337e-01, -1.9199e-01, -1.4841e-01,  1.2894e-01,  7.2407e-02,\n",
      "        -5.7640e-02, -7.6368e-02, -4.8645e-02, -6.2393e-02,  5.2408e-02,\n",
      "        -4.4141e-02,  6.3411e-02, -9.1698e-02, -4.4849e-02, -3.3588e-03,\n",
      "         6.3982e-02, -9.1698e-02,  8.5604e-03, -9.5423e-03, -8.2421e-02,\n",
      "        -7.2068e-02, -1.8203e-02, -2.3280e-02, -7.2228e-02, -1.0983e-02,\n",
      "         2.4820e-02, -1.0261e-01, -3.1956e-02, -5.1363e-02, -5.5933e-02,\n",
      "         2.5700e-02,  6.9946e-02, -2.7094e-01, -1.2480e-02,  1.9912e-01,\n",
      "        -3.3759e-02,  1.2862e-02, -8.6291e-02,  1.2895e-01, -1.7852e-01,\n",
      "        -1.2706e-01,  2.8818e-02, -1.9385e-01, -6.9981e-02,  5.5638e-02,\n",
      "        -2.1827e-02, -2.2263e-02, -5.1278e-02, -9.1011e-02, -3.3960e-02,\n",
      "         1.5319e-02,  4.6578e-02,  2.2744e-04, -7.1812e-04, -4.3871e-02,\n",
      "         2.0047e-02, -2.6616e-02, -1.5639e-02, -4.3505e-03, -1.1097e-01,\n",
      "         1.2291e-01,  2.4989e-01, -1.3764e-01,  1.1554e-01, -3.7154e-02,\n",
      "         1.8237e-02, -2.1737e-02, -4.2553e-02, -2.1061e-02,  7.0750e-02,\n",
      "        -5.7224e-02,  3.1127e-02, -5.7017e-02, -3.3774e-02, -1.6438e-02,\n",
      "        -6.1806e-02, -2.7708e-01,  3.4427e-02, -3.3195e-02,  9.3408e-04,\n",
      "        -9.7844e-02,  5.1233e-02,  7.4044e-02, -1.0734e-01, -8.3491e-02,\n",
      "         1.7881e-02, -7.8670e-02,  3.3058e-02, -1.3821e-02, -1.0224e-01,\n",
      "         1.1658e-02,  4.2826e-02, -7.5251e-02, -1.0643e-01, -1.8144e-02,\n",
      "         7.8419e-03, -2.0314e-01,  7.7206e-02, -6.0397e-02, -2.3675e-03,\n",
      "         7.9493e-02, -9.5295e-03, -2.2778e-02,  3.6130e-03,  1.8215e-02,\n",
      "         3.2176e-03, -1.6590e-01,  5.1702e-02, -1.0828e-01, -4.2560e-02,\n",
      "         1.7831e-02, -2.1725e-01, -7.0203e-02, -6.3880e-03, -7.4311e-02,\n",
      "         1.9624e-01,  1.0977e-01, -1.5838e-03, -1.6711e-02, -3.7685e-02,\n",
      "        -2.2754e-03, -6.7416e-02,  6.5547e-02, -6.4914e-03, -7.9440e-03,\n",
      "         1.8788e-02,  9.8616e-02,  6.3292e-02,  6.9161e-02,  1.8430e-01,\n",
      "         1.4065e-01,  6.3812e-02,  1.2748e-02, -7.6659e-02,  4.4974e-02,\n",
      "        -6.4897e-03,  1.1740e-01,  3.7543e-02, -2.7840e-02,  1.0018e-01,\n",
      "        -6.1378e-03, -2.3791e-01,  2.1642e-02, -5.6330e-02,  2.4488e-02,\n",
      "         3.0186e-04, -2.3659e-02, -1.8172e-01,  3.0108e-02, -6.0988e-02,\n",
      "        -3.4989e-02,  5.2514e-02, -1.3205e-01,  1.5456e-01,  2.1256e-02,\n",
      "         2.6567e-01,  1.2649e-01, -3.3097e-02, -8.8255e-02,  4.5656e-02,\n",
      "         6.6722e-02, -2.1871e-02, -2.1532e-01, -7.8594e-02,  4.9632e-02,\n",
      "        -5.1547e-02, -9.3395e-02, -1.7774e-01, -1.1451e-01,  1.6514e-01,\n",
      "        -6.7115e-02,  1.1349e-01, -1.1352e-01,  4.3483e-02, -1.6765e-02,\n",
      "        -1.2465e-01,  4.2786e-02,  8.1259e-02, -9.9771e-02, -1.4122e-01,\n",
      "        -1.6993e-01,  1.1860e-01, -1.4119e-01, -8.5513e-02,  1.8765e-01,\n",
      "         4.6303e-02, -4.7695e-02,  3.8502e-02, -2.4808e-02, -2.0458e-02,\n",
      "        -5.9773e-02, -4.9367e-02, -7.4075e-02, -3.9861e-02, -8.0061e-02,\n",
      "        -7.9408e-04, -3.8035e-02, -8.2295e-03,  1.2578e-01, -2.8010e-01],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[6] tensor([ 0.0453,  0.0093,  0.0598, -0.1614, -0.0239,  0.0577, -0.1680, -0.0011,\n",
      "         0.0978, -0.0443, -0.1070,  0.1142, -0.1390, -0.0862, -0.1493, -0.1321,\n",
      "         0.0417,  0.0275, -0.1620,  0.0344,  0.0208,  0.1646, -0.2009, -0.0025,\n",
      "        -0.0051,  0.0964, -0.0968, -0.0739,  0.1643, -0.2293, -0.0054,  0.2785,\n",
      "         0.1224, -0.0233, -0.2082,  0.1138,  0.1534, -0.0253, -0.0051, -0.0912,\n",
      "        -0.0446, -0.1111, -0.0424,  0.0904,  0.0773,  0.1539,  0.0154, -0.0703,\n",
      "        -0.2068,  0.0388, -0.0595,  0.0846, -0.0933, -0.1346, -0.0406, -0.2227,\n",
      "        -0.0492, -0.0899,  0.0443,  0.0297, -0.1129, -0.1047,  0.0403,  0.0157,\n",
      "        -0.0338, -0.0795, -0.1059,  0.0531,  0.0087,  0.0673,  0.0318, -0.0718,\n",
      "         0.2130,  0.1238, -0.0553,  0.2205,  0.0012, -0.0335,  0.1135, -0.0347,\n",
      "        -0.0351,  0.0008,  0.0475, -0.0284,  0.0481, -0.0175,  0.0704,  0.0768,\n",
      "         0.0386,  0.0117, -0.0195, -0.0075,  0.1233, -0.0149, -0.0354,  0.0493,\n",
      "        -0.0991, -0.0512, -0.1630, -0.0817, -0.0644, -0.0426,  0.2096, -0.0493,\n",
      "         0.0183,  0.1914, -0.0007,  0.1238, -0.0126,  0.0793, -0.0703,  0.0624,\n",
      "         0.0183,  0.0684, -0.1078, -0.0503, -0.0572, -0.1032, -0.0040,  0.0218,\n",
      "        -0.0091, -0.0489, -0.2410, -0.0432,  0.0151, -0.1834, -0.0182, -0.0326,\n",
      "         0.0775, -0.0340, -0.1420, -0.0702, -0.0323,  0.1590, -0.1279, -0.0066,\n",
      "        -0.0144, -0.0058, -0.1332,  0.0215,  0.1146,  0.0153, -0.0029, -0.0849,\n",
      "         0.0495, -0.0699, -0.0891,  0.0283, -0.1119, -0.1213, -0.1083,  0.0066,\n",
      "        -0.1210,  0.0986, -0.0227, -0.0421,  0.0638, -0.1041, -0.0583, -0.0848,\n",
      "        -0.1220, -0.1037,  0.0560,  0.0696, -0.1111,  0.0972,  0.1087,  0.1321,\n",
      "         0.1843,  0.1330, -0.1607, -0.1805, -0.2262, -0.0398, -0.2152,  0.0013,\n",
      "        -0.1184, -0.0200,  0.1391, -0.0124,  0.2539,  0.0335,  0.0155,  0.0944,\n",
      "         0.0296, -0.2279,  0.0868, -0.0045,  0.0231,  0.0094, -0.0343, -0.0977,\n",
      "        -0.0073, -0.0455, -0.1681, -0.1171,  0.0072, -0.0190, -0.0629, -0.0396],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[7] tensor([-0.1039, -0.1012,  0.1090,  0.0266, -0.0008,  0.0209, -0.0584, -0.1099,\n",
      "         0.0363,  0.0108, -0.0902, -0.0527,  0.1290, -0.0173, -0.1851, -0.1777,\n",
      "        -0.1928,  0.0874,  0.0796, -0.0086, -0.1835, -0.0767,  0.0818,  0.1686,\n",
      "         0.1359, -0.1753, -0.1456,  0.1642, -0.0066, -0.1752, -0.0176, -0.0748,\n",
      "         0.1138, -0.0199,  0.1838, -0.0639,  0.0632,  0.0362,  0.0522, -0.0680,\n",
      "        -0.0224, -0.1674,  0.0598, -0.1038,  0.1133,  0.0242,  0.1488, -0.0526,\n",
      "        -0.1380, -0.0995,  0.0096,  0.1061, -0.0466,  0.1417,  0.1213, -0.0891,\n",
      "        -0.1539, -0.0474, -0.0361, -0.0076, -0.1681,  0.0219, -0.0655,  0.0233,\n",
      "         0.0129, -0.0407, -0.0803,  0.1116,  0.0556, -0.1112,  0.0332,  0.0073,\n",
      "         0.0267, -0.0162, -0.0657,  0.1069,  0.1258,  0.0064,  0.0148, -0.0187,\n",
      "        -0.0743,  0.0718,  0.0822,  0.0818, -0.2578,  0.0360, -0.0684, -0.0497,\n",
      "        -0.1130,  0.0107,  0.1463, -0.0769, -0.3091,  0.0577, -0.0153,  0.0247,\n",
      "         0.0764, -0.0594, -0.0151, -0.1741, -0.0102, -0.0420,  0.1231,  0.0436,\n",
      "         0.0044, -0.0278, -0.1438, -0.0316,  0.0309,  0.0007, -0.0829,  0.0518,\n",
      "        -0.0332,  0.0051, -0.0401,  0.1949,  0.0037,  0.1054, -0.0537, -0.0224,\n",
      "         0.0130,  0.0787,  0.1280, -0.1255, -0.0626,  0.0167, -0.3238,  0.0732,\n",
      "         0.1004, -0.0300, -0.0141,  0.1049,  0.0278,  0.0119, -0.0356,  0.0994,\n",
      "         0.0871, -0.0728,  0.0723, -0.0990, -0.0583, -0.0379, -0.1794, -0.0818,\n",
      "         0.0518,  0.0689, -0.1879,  0.0075, -0.0132,  0.1258,  0.0446,  0.0107,\n",
      "        -0.1404,  0.0237,  0.0206, -0.0650, -0.1555,  0.1526, -0.1461,  0.0027,\n",
      "        -0.0366,  0.1173, -0.0146, -0.0955,  0.0026,  0.0453,  0.0377, -0.0514,\n",
      "        -0.1188,  0.0689, -0.0926,  0.0885, -0.1499,  0.0902,  0.0522,  0.0511,\n",
      "        -0.0270, -0.0804,  0.0923, -0.0106,  0.0611, -0.0122, -0.1161,  0.1765,\n",
      "        -0.0544, -0.0488,  0.0291, -0.0920,  0.2061, -0.0696,  0.0787, -0.0039,\n",
      "        -0.1456, -0.0235, -0.0738,  0.0253,  0.0780, -0.0704,  0.1255, -0.1232],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[8] tensor([-0.2094,  0.0759,  0.0713, -0.2022, -0.0749, -0.0296,  0.0587,  0.1799,\n",
      "        -0.1106,  0.0130, -0.0325,  0.0073,  0.0825,  0.0009,  0.1984, -0.1146,\n",
      "        -0.0764,  0.1472,  0.2117,  0.0109,  0.0135,  0.0941,  0.1618,  0.0549,\n",
      "         0.0850,  0.0269, -0.0498,  0.0436,  0.1232,  0.0234,  0.0940, -0.0338,\n",
      "        -0.0574,  0.0609,  0.0004,  0.0831,  0.1437,  0.1179,  0.0231,  0.2039,\n",
      "        -0.0436, -0.1221, -0.0397,  0.0986, -0.0260, -0.0187, -0.0708, -0.1005,\n",
      "        -0.0548,  0.0261,  0.0063, -0.1241,  0.0037,  0.0791,  0.1223, -0.0428,\n",
      "        -0.0172,  0.1072, -0.0949,  0.0876, -0.0251, -0.0149, -0.0204, -0.1672,\n",
      "        -0.0286,  0.1146, -0.2240, -0.0898, -0.0419,  0.0091,  0.0046, -0.1665,\n",
      "         0.0040, -0.1374, -0.1155,  0.0167,  0.0257,  0.0207, -0.0262, -0.0110,\n",
      "        -0.1147, -0.0462,  0.0667, -0.1569, -0.1746,  0.0924,  0.0903, -0.2222,\n",
      "         0.1822, -0.0113,  0.1902, -0.0136,  0.1097,  0.0543, -0.0653, -0.0019,\n",
      "        -0.0292,  0.0318,  0.1614, -0.1093,  0.0175,  0.0085,  0.0427,  0.0389,\n",
      "        -0.0109,  0.0958, -0.1122, -0.0882, -0.0664,  0.0966,  0.1511,  0.0772,\n",
      "         0.0482, -0.0749, -0.0970, -0.0611, -0.0410, -0.1005,  0.1353, -0.0274,\n",
      "         0.1772,  0.0474, -0.0251,  0.0755, -0.0640, -0.0044,  0.1532,  0.0511,\n",
      "        -0.0908,  0.0845,  0.0981,  0.0400,  0.0767,  0.1775, -0.0778,  0.0025,\n",
      "         0.0418,  0.1133, -0.0134,  0.0689,  0.0730, -0.0053,  0.0212, -0.0204,\n",
      "        -0.0733, -0.0212, -0.0399,  0.0871, -0.1042,  0.0851,  0.0554,  0.0237,\n",
      "        -0.0564, -0.1144,  0.0628,  0.0475,  0.0123, -0.0755, -0.0189,  0.2273,\n",
      "         0.1400,  0.0792, -0.0681,  0.1371, -0.1029,  0.2021, -0.0319,  0.0588,\n",
      "         0.0368, -0.0164, -0.2124,  0.0258,  0.0117,  0.0360, -0.1010,  0.0922,\n",
      "         0.1263,  0.0907,  0.0534,  0.0013,  0.0098, -0.0061, -0.0625, -0.0873,\n",
      "        -0.0990,  0.0736,  0.0672,  0.0411, -0.0097,  0.0301,  0.0098,  0.0802,\n",
      "        -0.0619, -0.0682, -0.0570,  0.0209, -0.0713,  0.0489,  0.0197, -0.1051],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "self.weight_fb[9] tensor([ 0.0619, -0.0323,  0.1306,  0.0718,  0.0308, -0.0581,  0.0168, -0.1006,\n",
      "         0.0377,  0.0685,  0.0213,  0.0190,  0.0437,  0.0066,  0.0586,  0.1103,\n",
      "         0.0799,  0.0109, -0.0585, -0.0875, -0.0355,  0.1027,  0.1055,  0.0521,\n",
      "        -0.0699,  0.1047,  0.0462, -0.0418,  0.1740,  0.0336, -0.2174, -0.0270,\n",
      "         0.2230, -0.0110,  0.0951, -0.1243, -0.0965,  0.1664, -0.0013, -0.1457,\n",
      "        -0.0469,  0.0547, -0.0205, -0.0193, -0.1564, -0.0394,  0.0908, -0.1274,\n",
      "         0.0191,  0.0823, -0.0293, -0.0995, -0.0056, -0.0956, -0.1140,  0.0221,\n",
      "        -0.0677,  0.0622,  0.1915, -0.0069,  0.0864,  0.0210,  0.0295, -0.0080,\n",
      "        -0.0820,  0.0977, -0.0749,  0.0605,  0.0039,  0.0517, -0.1046, -0.0532,\n",
      "        -0.0534,  0.0600, -0.0555, -0.0607, -0.0706, -0.0534,  0.0453,  0.0275,\n",
      "        -0.0239, -0.0691,  0.0739, -0.0071, -0.1625,  0.0666, -0.0643,  0.0247,\n",
      "        -0.1265,  0.0894, -0.0415,  0.0756, -0.1569,  0.0803,  0.0619, -0.0909,\n",
      "        -0.0767, -0.0554, -0.0398, -0.1592, -0.1214, -0.0390, -0.2053, -0.0398,\n",
      "         0.1758,  0.0955, -0.0426, -0.0736,  0.1159, -0.0014,  0.0606, -0.0123,\n",
      "         0.0936,  0.1832, -0.1774, -0.0318,  0.1345,  0.2291, -0.0901, -0.0672,\n",
      "         0.0569, -0.1050,  0.1101, -0.0168, -0.0573,  0.0651, -0.1255, -0.0431,\n",
      "         0.0050, -0.0392,  0.0378,  0.0499, -0.2069,  0.0679, -0.0304, -0.0745,\n",
      "        -0.0952,  0.0132, -0.1228,  0.0512, -0.0277, -0.2837,  0.1178,  0.0967,\n",
      "         0.1293, -0.0600, -0.2102,  0.1692,  0.0636, -0.1341, -0.2376, -0.0536,\n",
      "        -0.0353, -0.0945, -0.0422,  0.0293, -0.0792,  0.0482, -0.1039, -0.0145,\n",
      "         0.1720, -0.0599,  0.0656,  0.1751, -0.1370, -0.0342,  0.0116, -0.1388,\n",
      "         0.0972,  0.1435,  0.0744,  0.1576,  0.0562,  0.1857,  0.0131, -0.1593,\n",
      "        -0.1371, -0.0651,  0.0031,  0.0946, -0.2178,  0.0778,  0.1374, -0.1658,\n",
      "        -0.0611, -0.0059, -0.0164,  0.0556,  0.0486,  0.1234, -0.0925,  0.1631,\n",
      "         0.1047,  0.0439, -0.0677, -0.1071, -0.0212, -0.0228,  0.1377, -0.1403],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  0.091363/  0.083734, val:  56.67%, val_best:  56.67%, tr:  45.56%, tr_best:  45.56%, epoch time: 70.05 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.0905%\n",
      "layer   2  Sparsity: 77.2847%\n",
      "layer   3  Sparsity: 81.0843%\n",
      "total_backward_count 9790 real_backward_count 6531  66.711%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  0.067964/  0.073492, val:  55.83%, val_best:  56.67%, tr:  67.21%, tr_best:  67.21%, epoch time: 68.57 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1014%\n",
      "layer   2  Sparsity: 71.5430%\n",
      "layer   3  Sparsity: 76.6728%\n",
      "total_backward_count 19580 real_backward_count 10859  55.460%\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  0.059442/  0.068434, val:  60.00%, val_best:  60.00%, tr:  69.77%, tr_best:  69.77%, epoch time: 69.54 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   2  Sparsity: 69.7602%\n",
      "layer   3  Sparsity: 74.6955%\n",
      "total_backward_count 29370 real_backward_count 14666  49.935%\n"
     ]
    }
   ],
   "source": [
    "for high_seed in [1,2,3]:\n",
    "    ### my_snn control board (Gesture) ########################\n",
    "    decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "    # nda 0.25 # ottt 0.5\n",
    "\n",
    "    unique_name = 'main'\n",
    "    run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "    wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "    my_snn_system(  devices = \"4\",\n",
    "                    single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "                    unique_name = run_name,\n",
    "                    my_seed = high_seed,\n",
    "                    TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "                    BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "                    IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                    # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "                    # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "                    which_data = 'DVS_GESTURE_TONIC',\n",
    "    # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "    # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                    rate_coding = False, # True # False\n",
    "\n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = decay,\n",
    "                    lif_layer_v_threshold = 0.25,   #nda 0.5  #ottt 1.0\n",
    "                    lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "                    lif_layer_sg_width = 4.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "                    synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False, # True # False\n",
    "                    convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                    # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                    # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "                    # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "                    # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "                    # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "                    cfg = [200, 200], \n",
    "                    # cfg = ['M', 'M', 64, 'M', 96], \n",
    "                    # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "                    # cfg = ['M', 'M', 64], \n",
    "                    # cfg = [64, 124, 64, 124],\n",
    "                    # cfg = ['M','M',512], \n",
    "                    # cfg = [512], \n",
    "                    # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                    # cfg = ['M','M',512],\n",
    "                    # cfg = ['M',200],\n",
    "                    # cfg = [200,200],\n",
    "                    # cfg = ['M','M',200,200],\n",
    "                    # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                    # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                    # cfg = ['M',200,200],\n",
    "                    # cfg = ['M','M',1024,512,256,128,64],\n",
    "                    # cfg = [200,200],\n",
    "                    # cfg = [12], #fc\n",
    "                    # cfg = [12, 'M', 48, 'M', 12], \n",
    "                    # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "                    # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                    # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                    # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                    # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                    # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                    # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                    # cfg = [20001,10001], # depthwise, separable\n",
    "                    # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                    # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                    # cfg = [],        \n",
    "                    \n",
    "                    net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                    \n",
    "                    pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "                    # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                    learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,  # True # False\n",
    "                    BN_on = False,  # True # False\n",
    "                    \n",
    "                    surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                    \n",
    "                    BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                    \n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "                    # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                    # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "                    dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                    # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                    # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "                    # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "                    # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "                    DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "                    trace_on = False,   # True # False\n",
    "                    OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "                    extra_train_dataset = -1, \n",
    "\n",
    "                    num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "                    chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "                    pin_memory = True, # True # False \n",
    "\n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = False, # True # False \n",
    "\n",
    "                    last_lif = False, # True # False \n",
    "\n",
    "                    temporal_filter = 5, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[[999,998],[999,999],[999,999]], # [[neuron_quant,feedback weight quant],[],[]]\n",
    "    # 1w -11~-9\n",
    "    # 1b -11~ -7\n",
    "    # 2w -10~-8\n",
    "    # 2b -10~-8\n",
    "    # 3w -10\n",
    "    # 3b -10\n",
    "                    ) \n",
    "\n",
    "    # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "    # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "    # num_workers = batch_size / num_GPU\n",
    "    # num_workers = batch_size / num_CPU\n",
    "\n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling  \n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# # Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# # wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'random', # 'random', 'bayes', 'grid'\n",
    "#     'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         # \"devices\": {\"values\": [\"1\"]},\n",
    "#         \"single_step\": {\"values\": [True]},\n",
    "#         # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "#         \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "#         # \"my_seed\": {\"values\": [42]},\n",
    "#         \"TIME\": {\"values\": [10]},\n",
    "#         \"BATCH\": {\"values\": [1]},\n",
    "#         \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "#         \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "#         \"data_path\": {\"values\": ['/data2']},\n",
    "#         \"rate_coding\": {\"values\": [False]},\n",
    "#         \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "#         \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "#         \"lif_layer_v_threshold\": {\"values\": [0.5]},\n",
    "#         \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "#         \"lif_layer_sg_width\": {\"values\": [4.5, 4.0, 3.5, 3.0, 2.5]},\n",
    "#         # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "#         \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "#         \"synapse_conv_stride\": {\"values\": [1]},\n",
    "#         \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "#         \"synapse_trace_const1\": {\"values\": [1]},\n",
    "#         \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "#         \"pre_trained\": {\"values\": [False]},\n",
    "#         \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "#         \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "#         \"net_print\": {\"values\": [True]},\n",
    "\n",
    "#         \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "#         \"learning_rate\": {\"values\": [1/512]}, \n",
    "#         \"epoch_num\": {\"values\": [200]}, \n",
    "#         \"tdBN_on\": {\"values\": [False]},\n",
    "#         \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "#         \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"optimizer_what\": {\"values\": ['SGD']},\n",
    "#         \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "#         \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "#         \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "#         \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "#         \"trace_on\": {\"values\": [False]},\n",
    "#         \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "#         \"merge_polarities\": {\"values\": [True]},\n",
    "#         \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"extra_train_dataset\": {\"values\": [9]},\n",
    "\n",
    "#         \"num_workers\": {\"values\": [2]},\n",
    "#         \"chaching_on\": {\"values\": [True]},\n",
    "#         \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "#         \"UDA_on\": {\"values\": [False]},\n",
    "#         \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "#         \"bias\": {\"values\": [False]},\n",
    "\n",
    "#         \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "#         \"temporal_filter\": {\"values\": [5]},\n",
    "#         \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "#         \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "#         \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "#         \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "#         \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "#         \"scale_exp_1w\": {\"values\": [-10]},\n",
    "#         # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "#         \"scale_exp_2w\": {\"values\": [-10]},\n",
    "#         # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "#         \"scale_exp_3w\": {\"values\": [-9]},\n",
    "#         # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "#     my_snn_system(  \n",
    "#         devices  =  \"4\",\n",
    "#         single_step  =  wandb.config.single_step,\n",
    "#         unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "#         my_seed  =  wandb.config.my_seed,\n",
    "#         TIME  =  wandb.config.TIME,\n",
    "#         BATCH  =  wandb.config.BATCH,\n",
    "#         IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "#         which_data  =  wandb.config.which_data,\n",
    "#         data_path  =  wandb.config.data_path,\n",
    "#         rate_coding  =  wandb.config.rate_coding,\n",
    "#         lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "#         lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "#         lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "#         lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "#         lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "#         synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "#         synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "#         synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "#         synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "#         synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "#         pre_trained  =  wandb.config.pre_trained,\n",
    "#         convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "#         cfg  =  wandb.config.cfg,\n",
    "#         net_print  =  wandb.config.net_print,\n",
    "#         pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "#         learning_rate  =  wandb.config.learning_rate,\n",
    "#         epoch_num  =  wandb.config.epoch_num,\n",
    "#         tdBN_on  =  wandb.config.tdBN_on,\n",
    "#         BN_on  =  wandb.config.BN_on,\n",
    "#         surrogate  =  wandb.config.surrogate,\n",
    "#         BPTT_on  =  wandb.config.BPTT_on,\n",
    "#         optimizer_what  =  wandb.config.optimizer_what,\n",
    "#         scheduler_name  =  wandb.config.scheduler_name,\n",
    "#         ddp_on  =  wandb.config.ddp_on,\n",
    "#         dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "#         dvs_duration  =  wandb.config.dvs_duration,\n",
    "#         DFA_on  =  wandb.config.DFA_on,\n",
    "#         trace_on  =  wandb.config.trace_on,\n",
    "#         OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "#         exclude_class  =  wandb.config.exclude_class,\n",
    "#         merge_polarities  =  wandb.config.merge_polarities,\n",
    "#         denoise_on  =  wandb.config.denoise_on,\n",
    "#         extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "#         num_workers  =  wandb.config.num_workers,\n",
    "#         chaching_on  =  wandb.config.chaching_on,\n",
    "#         pin_memory  =  wandb.config.pin_memory,\n",
    "#         UDA_on  =  wandb.config.UDA_on,\n",
    "#         alpha_uda  =  wandb.config.alpha_uda,\n",
    "#         bias  =  wandb.config.bias,\n",
    "#         last_lif  =  wandb.config.last_lif,\n",
    "#         temporal_filter  =  wandb.config.temporal_filter,\n",
    "#         initial_pooling  =  wandb.config.initial_pooling,\n",
    "#         temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "#         quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "#         scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "#                         ) \n",
    "#     # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "#     # average pooling\n",
    "#     # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "#     # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# # sweep_id = '4wosfk6x'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
