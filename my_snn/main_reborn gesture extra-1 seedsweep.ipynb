{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6056/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 20664,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 6.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-10,-10],[-10,-10],[-9,-9]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 7yr7ufe8\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/7yr7ufe8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xwdcxk3z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 5678\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251022_225050-xwdcxk3z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xwdcxk3z' target=\"_blank\">elated-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/7yr7ufe8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/7yr7ufe8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/7yr7ufe8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/7yr7ufe8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xwdcxk3z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xwdcxk3z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251022_225057_772', 'my_seed': 5678, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 328.0\n",
      "lif layer 1 self.abs_max_v: 328.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 1253.0\n",
      "lif layer 2 self.abs_max_v: 1253.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 421.0\n",
      "fc layer 1 self.abs_max_out: 497.0\n",
      "lif layer 1 self.abs_max_v: 592.5\n",
      "fc layer 2 self.abs_max_out: 1285.0\n",
      "lif layer 2 self.abs_max_v: 1805.5\n",
      "lif layer 1 self.abs_max_v: 705.5\n",
      "fc layer 3 self.abs_max_out: 551.0\n",
      "fc layer 1 self.abs_max_out: 507.0\n",
      "lif layer 1 self.abs_max_v: 735.5\n",
      "fc layer 1 self.abs_max_out: 591.0\n",
      "fc layer 3 self.abs_max_out: 566.0\n",
      "fc layer 1 self.abs_max_out: 696.0\n",
      "lif layer 1 self.abs_max_v: 739.0\n",
      "fc layer 3 self.abs_max_out: 596.0\n",
      "fc layer 1 self.abs_max_out: 717.0\n",
      "lif layer 2 self.abs_max_v: 1980.0\n",
      "lif layer 1 self.abs_max_v: 747.5\n",
      "fc layer 1 self.abs_max_out: 753.0\n",
      "lif layer 1 self.abs_max_v: 1016.0\n",
      "fc layer 1 self.abs_max_out: 796.0\n",
      "fc layer 1 self.abs_max_out: 844.0\n",
      "fc layer 1 self.abs_max_out: 899.0\n",
      "lif layer 1 self.abs_max_v: 1218.5\n",
      "lif layer 1 self.abs_max_v: 1273.5\n",
      "fc layer 1 self.abs_max_out: 906.0\n",
      "lif layer 2 self.abs_max_v: 1986.5\n",
      "fc layer 2 self.abs_max_out: 1330.0\n",
      "lif layer 2 self.abs_max_v: 2105.5\n",
      "lif layer 2 self.abs_max_v: 2202.5\n",
      "lif layer 2 self.abs_max_v: 2362.5\n",
      "fc layer 3 self.abs_max_out: 658.0\n",
      "fc layer 1 self.abs_max_out: 1091.0\n",
      "fc layer 1 self.abs_max_out: 1120.0\n",
      "lif layer 1 self.abs_max_v: 1306.0\n",
      "lif layer 1 self.abs_max_v: 1410.0\n",
      "fc layer 1 self.abs_max_out: 1132.0\n",
      "lif layer 1 self.abs_max_v: 1414.0\n",
      "lif layer 1 self.abs_max_v: 1520.0\n",
      "fc layer 2 self.abs_max_out: 1392.0\n",
      "fc layer 2 self.abs_max_out: 1433.0\n",
      "fc layer 1 self.abs_max_out: 1173.0\n",
      "lif layer 1 self.abs_max_v: 1551.5\n",
      "lif layer 1 self.abs_max_v: 1567.5\n",
      "lif layer 1 self.abs_max_v: 1714.0\n",
      "fc layer 1 self.abs_max_out: 1195.0\n",
      "lif layer 1 self.abs_max_v: 1970.5\n",
      "fc layer 1 self.abs_max_out: 1492.0\n",
      "fc layer 2 self.abs_max_out: 1460.0\n",
      "fc layer 2 self.abs_max_out: 1555.0\n",
      "lif layer 2 self.abs_max_v: 2365.5\n",
      "lif layer 2 self.abs_max_v: 2399.0\n",
      "lif layer 2 self.abs_max_v: 2414.5\n",
      "fc layer 1 self.abs_max_out: 1509.0\n",
      "lif layer 1 self.abs_max_v: 2130.5\n",
      "lif layer 1 self.abs_max_v: 2381.5\n",
      "fc layer 3 self.abs_max_out: 722.0\n",
      "fc layer 1 self.abs_max_out: 1526.0\n",
      "fc layer 2 self.abs_max_out: 1585.0\n",
      "lif layer 2 self.abs_max_v: 2547.5\n",
      "lif layer 2 self.abs_max_v: 2680.5\n",
      "fc layer 1 self.abs_max_out: 1529.0\n",
      "fc layer 3 self.abs_max_out: 745.0\n",
      "fc layer 2 self.abs_max_out: 1651.0\n",
      "fc layer 1 self.abs_max_out: 1598.0\n",
      "lif layer 1 self.abs_max_v: 2410.0\n",
      "fc layer 1 self.abs_max_out: 1740.0\n",
      "fc layer 2 self.abs_max_out: 1657.0\n",
      "lif layer 1 self.abs_max_v: 2489.0\n",
      "lif layer 1 self.abs_max_v: 2601.5\n",
      "lif layer 1 self.abs_max_v: 2639.0\n",
      "lif layer 1 self.abs_max_v: 2767.5\n",
      "fc layer 1 self.abs_max_out: 1784.0\n",
      "lif layer 1 self.abs_max_v: 2870.0\n",
      "fc layer 3 self.abs_max_out: 833.0\n",
      "lif layer 1 self.abs_max_v: 3102.0\n",
      "lif layer 1 self.abs_max_v: 3208.0\n",
      "fc layer 1 self.abs_max_out: 1792.0\n",
      "fc layer 1 self.abs_max_out: 1911.0\n",
      "lif layer 1 self.abs_max_v: 3248.5\n",
      "fc layer 1 self.abs_max_out: 1946.0\n",
      "lif layer 1 self.abs_max_v: 3565.5\n",
      "fc layer 1 self.abs_max_out: 1976.0\n",
      "lif layer 1 self.abs_max_v: 3571.0\n",
      "lif layer 1 self.abs_max_v: 3629.5\n",
      "lif layer 2 self.abs_max_v: 2703.5\n",
      "lif layer 2 self.abs_max_v: 2717.5\n",
      "fc layer 1 self.abs_max_out: 2000.0\n",
      "fc layer 2 self.abs_max_out: 1683.0\n",
      "lif layer 2 self.abs_max_v: 2822.5\n",
      "lif layer 2 self.abs_max_v: 2855.5\n",
      "fc layer 2 self.abs_max_out: 1781.0\n",
      "fc layer 1 self.abs_max_out: 2038.0\n",
      "fc layer 1 self.abs_max_out: 2127.0\n",
      "lif layer 2 self.abs_max_v: 2862.0\n",
      "fc layer 2 self.abs_max_out: 1824.0\n",
      "fc layer 2 self.abs_max_out: 1833.0\n",
      "fc layer 1 self.abs_max_out: 2159.0\n",
      "fc layer 1 self.abs_max_out: 2194.0\n",
      "lif layer 1 self.abs_max_v: 3644.5\n",
      "lif layer 1 self.abs_max_v: 3871.5\n",
      "fc layer 1 self.abs_max_out: 2363.0\n",
      "lif layer 1 self.abs_max_v: 3935.0\n",
      "lif layer 1 self.abs_max_v: 4132.0\n",
      "lif layer 1 self.abs_max_v: 4270.0\n",
      "fc layer 1 self.abs_max_out: 2381.0\n",
      "lif layer 1 self.abs_max_v: 4508.0\n",
      "fc layer 1 self.abs_max_out: 2516.0\n",
      "lif layer 1 self.abs_max_v: 4512.0\n",
      "lif layer 1 self.abs_max_v: 4645.0\n",
      "fc layer 2 self.abs_max_out: 1849.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.775056/  1.987457, val:  30.42%, val_best:  30.42%, tr:  95.61%, tr_best:  95.61%, epoch time: 61.70 seconds, 1.03 minutes\n",
      "total_backward_count 9790 real_backward_count 2225  22.727%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 2866.0\n",
      "lif layer 2 self.abs_max_v: 2875.5\n",
      "lif layer 2 self.abs_max_v: 3074.5\n",
      "lif layer 2 self.abs_max_v: 3090.5\n",
      "fc layer 2 self.abs_max_out: 1897.0\n",
      "lif layer 2 self.abs_max_v: 3194.5\n",
      "lif layer 2 self.abs_max_v: 3199.0\n",
      "lif layer 2 self.abs_max_v: 3461.5\n",
      "fc layer 3 self.abs_max_out: 921.0\n",
      "fc layer 2 self.abs_max_out: 1901.0\n",
      "fc layer 1 self.abs_max_out: 2611.0\n",
      "lif layer 1 self.abs_max_v: 4748.5\n",
      "lif layer 1 self.abs_max_v: 4985.5\n",
      "fc layer 2 self.abs_max_out: 1972.0\n",
      "fc layer 2 self.abs_max_out: 2004.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.724482/  1.944661, val:  43.33%, val_best:  43.33%, tr:  98.88%, tr_best:  98.88%, epoch time: 60.61 seconds, 1.01 minutes\n",
      "total_backward_count 19580 real_backward_count 3837  19.597%\n",
      "fc layer 2 self.abs_max_out: 2005.0\n",
      "lif layer 2 self.abs_max_v: 3557.5\n",
      "lif layer 2 self.abs_max_v: 3628.0\n",
      "fc layer 2 self.abs_max_out: 2044.0\n",
      "lif layer 2 self.abs_max_v: 3663.5\n",
      "fc layer 2 self.abs_max_out: 2105.0\n",
      "lif layer 2 self.abs_max_v: 3679.0\n",
      "fc layer 2 self.abs_max_out: 2114.0\n",
      "lif layer 2 self.abs_max_v: 3876.0\n",
      "fc layer 2 self.abs_max_out: 2186.0\n",
      "lif layer 2 self.abs_max_v: 4124.0\n",
      "fc layer 1 self.abs_max_out: 2658.0\n",
      "fc layer 1 self.abs_max_out: 2678.0\n",
      "fc layer 1 self.abs_max_out: 2722.0\n",
      "fc layer 1 self.abs_max_out: 2726.0\n",
      "fc layer 1 self.abs_max_out: 2747.0\n",
      "fc layer 1 self.abs_max_out: 2893.0\n",
      "fc layer 1 self.abs_max_out: 3006.0\n",
      "lif layer 1 self.abs_max_v: 5095.5\n",
      "lif layer 1 self.abs_max_v: 5422.0\n",
      "lif layer 1 self.abs_max_v: 5622.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.722124/  1.949470, val:  46.25%, val_best:  46.25%, tr:  99.39%, tr_best:  99.39%, epoch time: 62.29 seconds, 1.04 minutes\n",
      "total_backward_count 29370 real_backward_count 5283  17.988%\n",
      "fc layer 2 self.abs_max_out: 2239.0\n",
      "fc layer 1 self.abs_max_out: 3092.0\n",
      "fc layer 1 self.abs_max_out: 3181.0\n",
      "lif layer 1 self.abs_max_v: 5811.5\n",
      "lif layer 1 self.abs_max_v: 6029.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.731649/  1.923898, val:  47.50%, val_best:  47.50%, tr:  99.59%, tr_best:  99.59%, epoch time: 63.40 seconds, 1.06 minutes\n",
      "total_backward_count 39160 real_backward_count 6662  17.012%\n",
      "fc layer 1 self.abs_max_out: 3182.0\n",
      "lif layer 1 self.abs_max_v: 6092.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.714010/  1.926537, val:  37.92%, val_best:  47.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.99 seconds, 1.05 minutes\n",
      "total_backward_count 48950 real_backward_count 7934  16.208%\n",
      "fc layer 2 self.abs_max_out: 2243.0\n",
      "lif layer 2 self.abs_max_v: 4158.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.727312/  1.910446, val:  55.83%, val_best:  55.83%, tr:  99.39%, tr_best: 100.00%, epoch time: 63.68 seconds, 1.06 minutes\n",
      "total_backward_count 58740 real_backward_count 9205  15.671%\n",
      "fc layer 2 self.abs_max_out: 2341.0\n",
      "fc layer 1 self.abs_max_out: 3382.0\n",
      "fc layer 1 self.abs_max_out: 3393.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.707652/  1.886781, val:  55.00%, val_best:  55.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 66.93 seconds, 1.12 minutes\n",
      "total_backward_count 68530 real_backward_count 10388  15.158%\n",
      "fc layer 2 self.abs_max_out: 2357.0\n",
      "fc layer 1 self.abs_max_out: 3489.0\n",
      "fc layer 1 self.abs_max_out: 3535.0\n",
      "lif layer 1 self.abs_max_v: 6376.5\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.707481/  1.897555, val:  53.75%, val_best:  55.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.74 seconds, 1.18 minutes\n",
      "total_backward_count 78320 real_backward_count 11490  14.671%\n",
      "fc layer 2 self.abs_max_out: 2361.0\n",
      "fc layer 2 self.abs_max_out: 2365.0\n",
      "fc layer 2 self.abs_max_out: 2525.0\n",
      "fc layer 1 self.abs_max_out: 3974.0\n",
      "lif layer 1 self.abs_max_v: 6541.0\n",
      "lif layer 1 self.abs_max_v: 6677.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.719342/  1.900424, val:  52.92%, val_best:  55.83%, tr:  99.69%, tr_best: 100.00%, epoch time: 70.28 seconds, 1.17 minutes\n",
      "total_backward_count 88110 real_backward_count 12577  14.274%\n",
      "fc layer 1 self.abs_max_out: 4281.0\n",
      "lif layer 1 self.abs_max_v: 6954.5\n",
      "lif layer 1 self.abs_max_v: 7045.5\n",
      "lif layer 1 self.abs_max_v: 7535.0\n",
      "lif layer 1 self.abs_max_v: 7633.5\n",
      "lif layer 1 self.abs_max_v: 7882.0\n",
      "lif layer 1 self.abs_max_v: 8048.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.711459/  1.881817, val:  53.75%, val_best:  55.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 70.97 seconds, 1.18 minutes\n",
      "total_backward_count 97900 real_backward_count 13659  13.952%\n",
      "fc layer 2 self.abs_max_out: 2567.0\n",
      "lif layer 2 self.abs_max_v: 4266.0\n",
      "fc layer 2 self.abs_max_out: 2689.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.713334/  1.887902, val:  60.42%, val_best:  60.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 69.81 seconds, 1.16 minutes\n",
      "total_backward_count 107690 real_backward_count 14742  13.689%\n",
      "lif layer 2 self.abs_max_v: 4339.5\n",
      "lif layer 2 self.abs_max_v: 4526.0\n",
      "fc layer 2 self.abs_max_out: 2698.0\n",
      "fc layer 2 self.abs_max_out: 2732.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.701152/  1.863171, val:  52.50%, val_best:  60.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.25 seconds, 1.19 minutes\n",
      "total_backward_count 117480 real_backward_count 15777  13.430%\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.707419/  1.865807, val:  58.33%, val_best:  60.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.55 seconds, 1.18 minutes\n",
      "total_backward_count 127270 real_backward_count 16810  13.208%\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.709021/  1.890617, val:  62.50%, val_best:  62.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 70.63 seconds, 1.18 minutes\n",
      "total_backward_count 137060 real_backward_count 17843  13.018%\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.714025/  1.893733, val:  54.58%, val_best:  62.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 69.96 seconds, 1.17 minutes\n",
      "total_backward_count 146850 real_backward_count 18909  12.876%\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.694453/  1.887053, val:  47.92%, val_best:  62.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.14 seconds, 1.17 minutes\n",
      "total_backward_count 156640 real_backward_count 19904  12.707%\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.684144/  1.890040, val:  63.75%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.54 seconds, 1.18 minutes\n",
      "total_backward_count 166430 real_backward_count 20916  12.567%\n",
      "fc layer 1 self.abs_max_out: 4400.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.685820/  1.849090, val:  63.75%, val_best:  63.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.57 seconds, 1.18 minutes\n",
      "total_backward_count 176220 real_backward_count 21924  12.441%\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.674266/  1.848692, val:  68.75%, val_best:  68.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.27 seconds, 1.17 minutes\n",
      "total_backward_count 186010 real_backward_count 22967  12.347%\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.660166/  1.838454, val:  57.92%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.33 seconds, 1.17 minutes\n",
      "total_backward_count 195800 real_backward_count 23916  12.215%\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.662914/  1.823857, val:  70.00%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.76 seconds, 1.18 minutes\n",
      "total_backward_count 205590 real_backward_count 24832  12.078%\n",
      "fc layer 3 self.abs_max_out: 967.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.658161/  1.833144, val:  64.58%, val_best:  70.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.43 seconds, 1.17 minutes\n",
      "total_backward_count 215380 real_backward_count 25759  11.960%\n",
      "fc layer 1 self.abs_max_out: 4417.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.641536/  1.808398, val:  70.42%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.60 seconds, 1.18 minutes\n",
      "total_backward_count 225170 real_backward_count 26637  11.830%\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.646471/  1.850838, val:  63.75%, val_best:  70.42%, tr:  99.69%, tr_best: 100.00%, epoch time: 70.99 seconds, 1.18 minutes\n",
      "total_backward_count 234960 real_backward_count 27482  11.696%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.643834/  1.805196, val:  72.92%, val_best:  72.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.43 seconds, 1.17 minutes\n",
      "total_backward_count 244750 real_backward_count 28360  11.587%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.627599/  1.793752, val:  68.75%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.58 seconds, 1.18 minutes\n",
      "total_backward_count 254540 real_backward_count 29257  11.494%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.626871/  1.781344, val:  71.25%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.75 seconds, 1.16 minutes\n",
      "total_backward_count 264330 real_backward_count 30087  11.382%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.611097/  1.785406, val:  69.17%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.99 seconds, 1.17 minutes\n",
      "total_backward_count 274120 real_backward_count 30916  11.278%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.622391/  1.804715, val:  62.50%, val_best:  72.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 69.77 seconds, 1.16 minutes\n",
      "total_backward_count 283910 real_backward_count 31783  11.195%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.612450/  1.795390, val:  60.83%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.42 seconds, 1.17 minutes\n",
      "total_backward_count 293700 real_backward_count 32619  11.106%\n",
      "fc layer 1 self.abs_max_out: 4460.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.605096/  1.785677, val:  67.08%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.67 seconds, 1.18 minutes\n",
      "total_backward_count 303490 real_backward_count 33433  11.016%\n",
      "fc layer 1 self.abs_max_out: 4596.0\n",
      "lif layer 1 self.abs_max_v: 8170.5\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.606820/  1.757576, val:  73.75%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.35 seconds, 1.17 minutes\n",
      "total_backward_count 313280 real_backward_count 34294  10.947%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.587104/  1.799536, val:  63.33%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.72 seconds, 1.18 minutes\n",
      "total_backward_count 323070 real_backward_count 35088  10.861%\n",
      "fc layer 1 self.abs_max_out: 4686.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.591352/  1.770881, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.62 seconds, 1.18 minutes\n",
      "total_backward_count 332860 real_backward_count 35893  10.783%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.573174/  1.765016, val:  64.58%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.06 seconds, 1.17 minutes\n",
      "total_backward_count 342650 real_backward_count 36650  10.696%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.571897/  1.780842, val:  63.75%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.17 seconds, 1.17 minutes\n",
      "total_backward_count 352440 real_backward_count 37413  10.615%\n",
      "fc layer 1 self.abs_max_out: 4726.0\n",
      "lif layer 1 self.abs_max_v: 8245.5\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.575650/  1.760196, val:  67.92%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 69.83 seconds, 1.16 minutes\n",
      "total_backward_count 362230 real_backward_count 38205  10.547%\n",
      "fc layer 1 self.abs_max_out: 4821.0\n",
      "lif layer 1 self.abs_max_v: 8461.5\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.582850/  1.772784, val:  73.75%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.59 seconds, 1.18 minutes\n",
      "total_backward_count 372020 real_backward_count 38966  10.474%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.570756/  1.763660, val:  75.83%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.68 seconds, 1.18 minutes\n",
      "total_backward_count 381810 real_backward_count 39710  10.400%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.562002/  1.762675, val:  63.33%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.24 seconds, 1.17 minutes\n",
      "total_backward_count 391600 real_backward_count 40409  10.319%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.575000/  1.750852, val:  82.08%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.72 seconds, 1.18 minutes\n",
      "total_backward_count 401390 real_backward_count 41163  10.255%\n",
      "fc layer 1 self.abs_max_out: 4849.0\n",
      "lif layer 1 self.abs_max_v: 8472.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.558610/  1.736752, val:  74.58%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.94 seconds, 1.18 minutes\n",
      "total_backward_count 411180 real_backward_count 41866  10.182%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.557240/  1.755281, val:  69.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.66 seconds, 1.16 minutes\n",
      "total_backward_count 420970 real_backward_count 42546  10.107%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.556669/  1.744366, val:  68.75%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 69.97 seconds, 1.17 minutes\n",
      "total_backward_count 430760 real_backward_count 43212  10.032%\n",
      "fc layer 2 self.abs_max_out: 2739.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.557979/  1.722443, val:  77.50%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.88 seconds, 1.18 minutes\n",
      "total_backward_count 440550 real_backward_count 43906   9.966%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.531327/  1.716632, val:  77.92%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.60 seconds, 1.16 minutes\n",
      "total_backward_count 450340 real_backward_count 44541   9.891%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.538192/  1.700146, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.92 seconds, 1.17 minutes\n",
      "total_backward_count 460130 real_backward_count 45208   9.825%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.531412/  1.714566, val:  77.50%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.36 seconds, 1.17 minutes\n",
      "total_backward_count 469920 real_backward_count 45843   9.755%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.530291/  1.715997, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.05 seconds, 1.17 minutes\n",
      "total_backward_count 479710 real_backward_count 46499   9.693%\n",
      "fc layer 2 self.abs_max_out: 2854.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.530110/  1.697653, val:  80.83%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.13 seconds, 1.17 minutes\n",
      "total_backward_count 489500 real_backward_count 47141   9.630%\n",
      "fc layer 1 self.abs_max_out: 4870.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.514571/  1.686463, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.27 seconds, 1.17 minutes\n",
      "total_backward_count 499290 real_backward_count 47775   9.569%\n",
      "fc layer 1 self.abs_max_out: 5057.0\n",
      "lif layer 1 self.abs_max_v: 8526.5\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.517546/  1.684942, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.17 seconds, 1.17 minutes\n",
      "total_backward_count 509080 real_backward_count 48406   9.509%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.491886/  1.672553, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.79 seconds, 1.18 minutes\n",
      "total_backward_count 518870 real_backward_count 48975   9.439%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.494003/  1.682006, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.35 seconds, 1.17 minutes\n",
      "total_backward_count 528660 real_backward_count 49558   9.374%\n",
      "fc layer 3 self.abs_max_out: 969.0\n",
      "fc layer 3 self.abs_max_out: 973.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.501366/  1.681645, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.16 seconds, 1.17 minutes\n",
      "total_backward_count 538450 real_backward_count 50141   9.312%\n",
      "fc layer 3 self.abs_max_out: 998.0\n",
      "lif layer 2 self.abs_max_v: 4529.0\n",
      "lif layer 2 self.abs_max_v: 4712.5\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.518378/  1.699382, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.82 seconds, 1.18 minutes\n",
      "total_backward_count 548240 real_backward_count 50740   9.255%\n",
      "lif layer 2 self.abs_max_v: 4780.5\n",
      "lif layer 2 self.abs_max_v: 4831.5\n",
      "lif layer 2 self.abs_max_v: 4922.0\n",
      "lif layer 2 self.abs_max_v: 4948.0\n",
      "lif layer 2 self.abs_max_v: 4967.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.512501/  1.686203, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.28 seconds, 1.17 minutes\n",
      "total_backward_count 558030 real_backward_count 51323   9.197%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.504044/  1.698755, val:  72.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.04 seconds, 1.17 minutes\n",
      "total_backward_count 567820 real_backward_count 51883   9.137%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.511399/  1.676653, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.98 seconds, 1.17 minutes\n",
      "total_backward_count 577610 real_backward_count 52441   9.079%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.498854/  1.690338, val:  74.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.78 seconds, 1.18 minutes\n",
      "total_backward_count 587400 real_backward_count 52982   9.020%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.496974/  1.681030, val:  81.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.28 seconds, 1.19 minutes\n",
      "total_backward_count 597190 real_backward_count 53538   8.965%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.496641/  1.670376, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.38 seconds, 1.17 minutes\n",
      "total_backward_count 606980 real_backward_count 54088   8.911%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.496595/  1.666591, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.04 seconds, 1.17 minutes\n",
      "total_backward_count 616770 real_backward_count 54643   8.860%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.488608/  1.666317, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.03 seconds, 1.18 minutes\n",
      "total_backward_count 626560 real_backward_count 55225   8.814%\n",
      "lif layer 1 self.abs_max_v: 8595.5\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.496217/  1.672024, val:  76.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.22 seconds, 1.17 minutes\n",
      "total_backward_count 636350 real_backward_count 55754   8.762%\n",
      "lif layer 1 self.abs_max_v: 8616.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.493806/  1.675385, val:  81.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.90 seconds, 1.17 minutes\n",
      "total_backward_count 646140 real_backward_count 56287   8.711%\n",
      "fc layer 2 self.abs_max_out: 2919.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.498837/  1.675369, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.08 seconds, 1.17 minutes\n",
      "total_backward_count 655930 real_backward_count 56808   8.661%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.487512/  1.667872, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.29 seconds, 1.17 minutes\n",
      "total_backward_count 665720 real_backward_count 57301   8.607%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.486928/  1.688751, val:  79.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.01 seconds, 1.18 minutes\n",
      "total_backward_count 675510 real_backward_count 57823   8.560%\n",
      "lif layer 1 self.abs_max_v: 8656.0\n",
      "fc layer 3 self.abs_max_out: 1017.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.470946/  1.650059, val:  75.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.83 seconds, 1.18 minutes\n",
      "total_backward_count 685300 real_backward_count 58338   8.513%\n",
      "fc layer 3 self.abs_max_out: 1044.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.473760/  1.668659, val:  76.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.63 seconds, 1.18 minutes\n",
      "total_backward_count 695090 real_backward_count 58894   8.473%\n",
      "fc layer 2 self.abs_max_out: 2928.0\n",
      "fc layer 1 self.abs_max_out: 5082.0\n",
      "lif layer 1 self.abs_max_v: 8742.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.479856/  1.655533, val:  75.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.61 seconds, 1.16 minutes\n",
      "total_backward_count 704880 real_backward_count 59411   8.429%\n",
      "lif layer 1 self.abs_max_v: 8785.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.484011/  1.650996, val:  78.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.51 seconds, 1.18 minutes\n",
      "total_backward_count 714670 real_backward_count 59894   8.381%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.455509/  1.644657, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.12 seconds, 1.17 minutes\n",
      "total_backward_count 724460 real_backward_count 60378   8.334%\n",
      "fc layer 1 self.abs_max_out: 5125.0\n",
      "lif layer 1 self.abs_max_v: 8907.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.451591/  1.622033, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.39 seconds, 1.17 minutes\n",
      "total_backward_count 734250 real_backward_count 60861   8.289%\n",
      "fc layer 2 self.abs_max_out: 2932.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.456788/  1.661696, val:  74.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.03 seconds, 1.17 minutes\n",
      "total_backward_count 744040 real_backward_count 61359   8.247%\n",
      "fc layer 2 self.abs_max_out: 2987.0\n",
      "lif layer 2 self.abs_max_v: 4967.5\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.447749/  1.621749, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.53 seconds, 1.18 minutes\n",
      "total_backward_count 753830 real_backward_count 61795   8.197%\n",
      "lif layer 2 self.abs_max_v: 5014.0\n",
      "fc layer 1 self.abs_max_out: 5174.0\n",
      "lif layer 1 self.abs_max_v: 8953.5\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.439047/  1.639284, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.43 seconds, 1.16 minutes\n",
      "total_backward_count 763620 real_backward_count 62269   8.154%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.434249/  1.633931, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.82 seconds, 1.18 minutes\n",
      "total_backward_count 773410 real_backward_count 62736   8.112%\n",
      "lif layer 2 self.abs_max_v: 5026.0\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.442999/  1.638348, val:  81.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.15 seconds, 1.17 minutes\n",
      "total_backward_count 783200 real_backward_count 63168   8.065%\n",
      "lif layer 2 self.abs_max_v: 5082.5\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.443686/  1.615276, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.41 seconds, 1.17 minutes\n",
      "total_backward_count 792990 real_backward_count 63617   8.022%\n",
      "lif layer 2 self.abs_max_v: 5154.5\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.442606/  1.624067, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.01 seconds, 1.17 minutes\n",
      "total_backward_count 802780 real_backward_count 64067   7.981%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.441197/  1.637462, val:  77.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.17 seconds, 1.17 minutes\n",
      "total_backward_count 812570 real_backward_count 64506   7.939%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.443655/  1.636013, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.18 seconds, 1.17 minutes\n",
      "total_backward_count 822360 real_backward_count 64964   7.900%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.453127/  1.635299, val:  85.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.74 seconds, 1.16 minutes\n",
      "total_backward_count 832150 real_backward_count 65410   7.860%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.453380/  1.640256, val:  75.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.30 seconds, 1.17 minutes\n",
      "total_backward_count 841940 real_backward_count 65855   7.822%\n",
      "lif layer 1 self.abs_max_v: 9004.0\n",
      "lif layer 1 self.abs_max_v: 9037.0\n",
      "lif layer 1 self.abs_max_v: 9252.5\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.453912/  1.632792, val:  76.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.23 seconds, 1.17 minutes\n",
      "total_backward_count 851730 real_backward_count 66254   7.779%\n",
      "fc layer 1 self.abs_max_out: 5225.0\n",
      "lif layer 2 self.abs_max_v: 5184.0\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.447566/  1.627141, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.63 seconds, 1.16 minutes\n",
      "total_backward_count 861520 real_backward_count 66707   7.743%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.454228/  1.633131, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.31 seconds, 1.17 minutes\n",
      "total_backward_count 871310 real_backward_count 67158   7.708%\n",
      "fc layer 2 self.abs_max_out: 3025.0\n",
      "lif layer 2 self.abs_max_v: 5462.5\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.450852/  1.619407, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.82 seconds, 1.18 minutes\n",
      "total_backward_count 881100 real_backward_count 67598   7.672%\n",
      "fc layer 2 self.abs_max_out: 3114.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.433790/  1.625699, val:  82.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.14 seconds, 1.17 minutes\n",
      "total_backward_count 890890 real_backward_count 67991   7.632%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.433755/  1.633216, val:  83.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.82 seconds, 1.16 minutes\n",
      "total_backward_count 900680 real_backward_count 68411   7.595%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.427522/  1.634749, val:  79.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.72 seconds, 1.18 minutes\n",
      "total_backward_count 910470 real_backward_count 68781   7.554%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.432157/  1.609475, val:  79.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.53 seconds, 1.18 minutes\n",
      "total_backward_count 920260 real_backward_count 69207   7.520%\n",
      "fc layer 2 self.abs_max_out: 3129.0\n",
      "fc layer 3 self.abs_max_out: 1048.0\n",
      "lif layer 2 self.abs_max_v: 5481.5\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.430520/  1.606771, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.57 seconds, 1.18 minutes\n",
      "total_backward_count 930050 real_backward_count 69647   7.489%\n",
      "fc layer 2 self.abs_max_out: 3208.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.432428/  1.614398, val:  77.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.87 seconds, 1.16 minutes\n",
      "total_backward_count 939840 real_backward_count 70081   7.457%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.431092/  1.623388, val:  84.58%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.91 seconds, 1.18 minutes\n",
      "total_backward_count 949630 real_backward_count 70485   7.422%\n",
      "fc layer 3 self.abs_max_out: 1055.0\n",
      "fc layer 3 self.abs_max_out: 1063.0\n",
      "lif layer 2 self.abs_max_v: 5525.0\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.428531/  1.603369, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.78 seconds, 1.16 minutes\n",
      "total_backward_count 959420 real_backward_count 70914   7.391%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.417806/  1.598286, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.20 seconds, 1.17 minutes\n",
      "total_backward_count 969210 real_backward_count 71290   7.355%\n",
      "lif layer 2 self.abs_max_v: 5534.0\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.415998/  1.602492, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.03 seconds, 1.17 minutes\n",
      "total_backward_count 979000 real_backward_count 71696   7.323%\n",
      "fc layer 3 self.abs_max_out: 1091.0\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.410592/  1.610421, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.04 seconds, 1.17 minutes\n",
      "total_backward_count 988790 real_backward_count 72073   7.289%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.412049/  1.599521, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.71 seconds, 1.18 minutes\n",
      "total_backward_count 998580 real_backward_count 72462   7.257%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.400698/  1.594185, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.54 seconds, 1.18 minutes\n",
      "total_backward_count 1008370 real_backward_count 72766   7.216%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.403561/  1.631765, val:  80.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.72 seconds, 1.18 minutes\n",
      "total_backward_count 1018160 real_backward_count 73120   7.182%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.409357/  1.612764, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.40 seconds, 1.17 minutes\n",
      "total_backward_count 1027950 real_backward_count 73462   7.146%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.408201/  1.607154, val:  69.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.74 seconds, 1.18 minutes\n",
      "total_backward_count 1037740 real_backward_count 73810   7.113%\n",
      "fc layer 3 self.abs_max_out: 1094.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.399198/  1.581931, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.49 seconds, 1.17 minutes\n",
      "total_backward_count 1047530 real_backward_count 74174   7.081%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.393067/  1.593788, val:  80.83%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.79 seconds, 1.18 minutes\n",
      "total_backward_count 1057320 real_backward_count 74533   7.049%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.387228/  1.586264, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.09 seconds, 1.17 minutes\n",
      "total_backward_count 1067110 real_backward_count 74842   7.014%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.382374/  1.574256, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.61 seconds, 1.18 minutes\n",
      "total_backward_count 1076900 real_backward_count 75215   6.984%\n",
      "fc layer 3 self.abs_max_out: 1139.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.382831/  1.578371, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.61 seconds, 1.16 minutes\n",
      "total_backward_count 1086690 real_backward_count 75512   6.949%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.396340/  1.585143, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.31 seconds, 1.17 minutes\n",
      "total_backward_count 1096480 real_backward_count 75843   6.917%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.395536/  1.596995, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.32 seconds, 1.17 minutes\n",
      "total_backward_count 1106270 real_backward_count 76183   6.886%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.402814/  1.584962, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.88 seconds, 1.16 minutes\n",
      "total_backward_count 1116060 real_backward_count 76505   6.855%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.398150/  1.571337, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.85 seconds, 1.16 minutes\n",
      "total_backward_count 1125850 real_backward_count 76844   6.825%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.396021/  1.603749, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.63 seconds, 1.16 minutes\n",
      "total_backward_count 1135640 real_backward_count 77155   6.794%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.404950/  1.573695, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.36 seconds, 1.17 minutes\n",
      "total_backward_count 1145430 real_backward_count 77443   6.761%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.387494/  1.575619, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.08 seconds, 1.17 minutes\n",
      "total_backward_count 1155220 real_backward_count 77777   6.733%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.391389/  1.584542, val:  85.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.60 seconds, 1.18 minutes\n",
      "total_backward_count 1165010 real_backward_count 78076   6.702%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.406490/  1.583704, val:  85.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.69 seconds, 1.18 minutes\n",
      "total_backward_count 1174800 real_backward_count 78397   6.673%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.401539/  1.582422, val:  84.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.00 seconds, 1.17 minutes\n",
      "total_backward_count 1184590 real_backward_count 78712   6.645%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.394644/  1.568264, val:  84.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.33 seconds, 1.17 minutes\n",
      "total_backward_count 1194380 real_backward_count 79025   6.616%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.391804/  1.576283, val:  87.50%, val_best:  91.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.20 seconds, 1.17 minutes\n",
      "total_backward_count 1204170 real_backward_count 79319   6.587%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.395843/  1.588470, val:  82.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.29 seconds, 1.17 minutes\n",
      "total_backward_count 1213960 real_backward_count 79645   6.561%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.386543/  1.542434, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.36 seconds, 1.17 minutes\n",
      "total_backward_count 1223750 real_backward_count 79930   6.532%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.384191/  1.545671, val:  85.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.49 seconds, 1.17 minutes\n",
      "total_backward_count 1233540 real_backward_count 80230   6.504%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.380543/  1.557282, val:  84.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.08 seconds, 1.17 minutes\n",
      "total_backward_count 1243330 real_backward_count 80550   6.479%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.383118/  1.545472, val:  85.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.56 seconds, 1.18 minutes\n",
      "total_backward_count 1253120 real_backward_count 80843   6.451%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.375343/  1.571617, val:  78.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.87 seconds, 1.16 minutes\n",
      "total_backward_count 1262910 real_backward_count 81113   6.423%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.384422/  1.544219, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.91 seconds, 1.17 minutes\n",
      "total_backward_count 1272700 real_backward_count 81417   6.397%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.380400/  1.556555, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.09 seconds, 1.17 minutes\n",
      "total_backward_count 1282490 real_backward_count 81732   6.373%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.374629/  1.556411, val:  83.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.78 seconds, 1.18 minutes\n",
      "total_backward_count 1292280 real_backward_count 82006   6.346%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.383043/  1.557356, val:  84.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.81 seconds, 1.18 minutes\n",
      "total_backward_count 1302070 real_backward_count 82350   6.325%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.377294/  1.573873, val:  80.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.96 seconds, 1.17 minutes\n",
      "total_backward_count 1311860 real_backward_count 82661   6.301%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.368199/  1.531289, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.75 seconds, 1.18 minutes\n",
      "total_backward_count 1321650 real_backward_count 82980   6.279%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.349987/  1.533083, val:  84.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.47 seconds, 1.17 minutes\n",
      "total_backward_count 1331440 real_backward_count 83299   6.256%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.352898/  1.560919, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.33 seconds, 1.17 minutes\n",
      "total_backward_count 1341230 real_backward_count 83584   6.232%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.353541/  1.554576, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.44 seconds, 1.17 minutes\n",
      "total_backward_count 1351020 real_backward_count 83871   6.208%\n",
      "fc layer 2 self.abs_max_out: 3389.0\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.361899/  1.549675, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.81 seconds, 1.16 minutes\n",
      "total_backward_count 1360810 real_backward_count 84143   6.183%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.359678/  1.541690, val:  84.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.09 seconds, 1.17 minutes\n",
      "total_backward_count 1370600 real_backward_count 84420   6.159%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.350410/  1.534579, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.00 seconds, 1.17 minutes\n",
      "total_backward_count 1380390 real_backward_count 84665   6.133%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.353363/  1.540100, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.27 seconds, 1.17 minutes\n",
      "total_backward_count 1390180 real_backward_count 84906   6.108%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.351795/  1.544894, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.36 seconds, 1.17 minutes\n",
      "total_backward_count 1399970 real_backward_count 85147   6.082%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.344297/  1.532420, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.23 seconds, 1.17 minutes\n",
      "total_backward_count 1409760 real_backward_count 85420   6.059%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.342565/  1.542894, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.37 seconds, 1.17 minutes\n",
      "total_backward_count 1419550 real_backward_count 85671   6.035%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.349472/  1.540472, val:  85.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.64 seconds, 1.16 minutes\n",
      "total_backward_count 1429340 real_backward_count 85924   6.011%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.345833/  1.554903, val:  85.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.44 seconds, 1.17 minutes\n",
      "total_backward_count 1439130 real_backward_count 86195   5.989%\n",
      "fc layer 1 self.abs_max_out: 5230.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.336609/  1.532166, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.80 seconds, 1.16 minutes\n",
      "total_backward_count 1448920 real_backward_count 86457   5.967%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.336154/  1.546921, val:  85.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.77 seconds, 1.16 minutes\n",
      "total_backward_count 1458710 real_backward_count 86721   5.945%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.346822/  1.560815, val:  85.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.06 seconds, 1.17 minutes\n",
      "total_backward_count 1468500 real_backward_count 86988   5.924%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.341721/  1.540957, val:  84.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.03 seconds, 1.17 minutes\n",
      "total_backward_count 1478290 real_backward_count 87211   5.899%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.332909/  1.551108, val:  85.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.41 seconds, 1.17 minutes\n",
      "total_backward_count 1488080 real_backward_count 87422   5.875%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.334004/  1.525861, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.27 seconds, 1.15 minutes\n",
      "total_backward_count 1497870 real_backward_count 87660   5.852%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.334409/  1.534952, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.62 seconds, 1.18 minutes\n",
      "total_backward_count 1507660 real_backward_count 87895   5.830%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.341946/  1.535916, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.22 seconds, 1.17 minutes\n",
      "total_backward_count 1517450 real_backward_count 88143   5.809%\n",
      "fc layer 1 self.abs_max_out: 5299.0\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.341481/  1.530031, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.93 seconds, 1.17 minutes\n",
      "total_backward_count 1527240 real_backward_count 88373   5.786%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.333021/  1.546930, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.06 seconds, 1.17 minutes\n",
      "total_backward_count 1537030 real_backward_count 88605   5.765%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.331989/  1.528727, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "total_backward_count 1546820 real_backward_count 88842   5.744%\n",
      "lif layer 1 self.abs_max_v: 9518.0\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.320464/  1.541446, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.02 seconds, 1.17 minutes\n",
      "total_backward_count 1556610 real_backward_count 89106   5.724%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.327033/  1.520703, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.43 seconds, 1.17 minutes\n",
      "total_backward_count 1566400 real_backward_count 89329   5.703%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.323817/  1.521156, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.86 seconds, 1.16 minutes\n",
      "total_backward_count 1576190 real_backward_count 89580   5.683%\n",
      "fc layer 1 self.abs_max_out: 5313.0\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.311230/  1.511120, val:  85.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.91 seconds, 1.18 minutes\n",
      "total_backward_count 1585980 real_backward_count 89834   5.664%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.309821/  1.510680, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.27 seconds, 1.17 minutes\n",
      "total_backward_count 1595770 real_backward_count 90062   5.644%\n",
      "fc layer 1 self.abs_max_out: 5363.0\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.326399/  1.508675, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.10 seconds, 1.17 minutes\n",
      "total_backward_count 1605560 real_backward_count 90275   5.623%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.318772/  1.523174, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.82 seconds, 1.16 minutes\n",
      "total_backward_count 1615350 real_backward_count 90522   5.604%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.327175/  1.526501, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.15 seconds, 1.17 minutes\n",
      "total_backward_count 1625140 real_backward_count 90774   5.586%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.326798/  1.520615, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.50 seconds, 1.18 minutes\n",
      "total_backward_count 1634930 real_backward_count 90999   5.566%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.320330/  1.518523, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.47 seconds, 1.17 minutes\n",
      "total_backward_count 1644720 real_backward_count 91221   5.546%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.310266/  1.511753, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.20 seconds, 1.17 minutes\n",
      "total_backward_count 1654510 real_backward_count 91437   5.527%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.319113/  1.508655, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.55 seconds, 1.18 minutes\n",
      "total_backward_count 1664300 real_backward_count 91638   5.506%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.327254/  1.518013, val:  86.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.16 seconds, 1.17 minutes\n",
      "total_backward_count 1674090 real_backward_count 91837   5.486%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.309742/  1.507759, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.11 seconds, 1.17 minutes\n",
      "total_backward_count 1683880 real_backward_count 92053   5.467%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.312444/  1.508502, val:  86.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.79 seconds, 1.18 minutes\n",
      "total_backward_count 1693670 real_backward_count 92252   5.447%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.310594/  1.499905, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.29 seconds, 1.17 minutes\n",
      "total_backward_count 1703460 real_backward_count 92438   5.426%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.314473/  1.513712, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.52 seconds, 1.18 minutes\n",
      "total_backward_count 1713250 real_backward_count 92625   5.406%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.320639/  1.508054, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.17 seconds, 1.17 minutes\n",
      "total_backward_count 1723040 real_backward_count 92835   5.388%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.314299/  1.527025, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.40 seconds, 1.17 minutes\n",
      "total_backward_count 1732830 real_backward_count 93046   5.370%\n",
      "fc layer 3 self.abs_max_out: 1145.0\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.311175/  1.520762, val:  85.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.29 seconds, 1.17 minutes\n",
      "total_backward_count 1742620 real_backward_count 93237   5.350%\n",
      "fc layer 3 self.abs_max_out: 1163.0\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.308342/  1.507226, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.32 seconds, 1.17 minutes\n",
      "total_backward_count 1752410 real_backward_count 93463   5.333%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.304662/  1.507276, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.43 seconds, 1.17 minutes\n",
      "total_backward_count 1762200 real_backward_count 93622   5.313%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.308168/  1.503598, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.22 seconds, 1.17 minutes\n",
      "total_backward_count 1771990 real_backward_count 93775   5.292%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.305438/  1.510432, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.17 seconds, 1.17 minutes\n",
      "total_backward_count 1781780 real_backward_count 94002   5.276%\n",
      "fc layer 1 self.abs_max_out: 5381.0\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.310078/  1.507872, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.87 seconds, 1.16 minutes\n",
      "total_backward_count 1791570 real_backward_count 94227   5.259%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.307618/  1.512826, val:  83.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.57 seconds, 1.18 minutes\n",
      "total_backward_count 1801360 real_backward_count 94428   5.242%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.293083/  1.500687, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.21 seconds, 1.17 minutes\n",
      "total_backward_count 1811150 real_backward_count 94616   5.224%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.294955/  1.513619, val:  84.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.55 seconds, 1.16 minutes\n",
      "total_backward_count 1820940 real_backward_count 94804   5.206%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.301547/  1.509270, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.09 seconds, 1.17 minutes\n",
      "total_backward_count 1830730 real_backward_count 94978   5.188%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.295761/  1.503490, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.28 seconds, 1.17 minutes\n",
      "total_backward_count 1840520 real_backward_count 95177   5.171%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.296536/  1.508627, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.52 seconds, 1.18 minutes\n",
      "total_backward_count 1850310 real_backward_count 95347   5.153%\n",
      "fc layer 1 self.abs_max_out: 5546.0\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.303939/  1.505877, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.04 seconds, 1.17 minutes\n",
      "total_backward_count 1860100 real_backward_count 95549   5.137%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.307764/  1.508581, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.20 seconds, 1.17 minutes\n",
      "total_backward_count 1869890 real_backward_count 95736   5.120%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.308561/  1.511524, val:  86.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.52 seconds, 1.18 minutes\n",
      "total_backward_count 1879680 real_backward_count 95900   5.102%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.296027/  1.505782, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.55 seconds, 1.18 minutes\n",
      "total_backward_count 1889470 real_backward_count 96061   5.084%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.298053/  1.490772, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.40 seconds, 1.17 minutes\n",
      "total_backward_count 1899260 real_backward_count 96232   5.067%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.301395/  1.488017, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.26 seconds, 1.17 minutes\n",
      "total_backward_count 1909050 real_backward_count 96414   5.050%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.296847/  1.508962, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.56 seconds, 1.18 minutes\n",
      "total_backward_count 1918840 real_backward_count 96611   5.035%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.298095/  1.500429, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.24 seconds, 1.17 minutes\n",
      "total_backward_count 1928630 real_backward_count 96801   5.019%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.298757/  1.496717, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.31 seconds, 1.17 minutes\n",
      "total_backward_count 1938420 real_backward_count 96963   5.002%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.289148/  1.485497, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.27 seconds, 1.17 minutes\n",
      "total_backward_count 1948210 real_backward_count 97131   4.986%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.302507/  1.501492, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.74 seconds, 1.18 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a50e1665df4860890d21427e5b0237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.30251</td></tr><tr><td>val_acc_best</td><td>0.91667</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>1.50149</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">elated-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xwdcxk3z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/xwdcxk3z</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251022_225050-xwdcxk3z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ewdwilzt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 751\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251023_024512-ewdwilzt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ewdwilzt' target=\"_blank\">summer-sweep-10</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/7yr7ufe8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/7yr7ufe8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/7yr7ufe8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/7yr7ufe8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ewdwilzt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ewdwilzt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251023_024520_816', 'my_seed': 751, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 760.0\n",
      "lif layer 1 self.abs_max_v: 760.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 920.0\n",
      "lif layer 2 self.abs_max_v: 920.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 445.0\n",
      "lif layer 1 self.abs_max_v: 778.0\n",
      "fc layer 2 self.abs_max_out: 921.0\n",
      "lif layer 2 self.abs_max_v: 1238.0\n",
      "lif layer 1 self.abs_max_v: 904.0\n",
      "fc layer 2 self.abs_max_out: 1136.0\n",
      "lif layer 1 self.abs_max_v: 905.0\n",
      "lif layer 2 self.abs_max_v: 1370.5\n",
      "lif layer 2 self.abs_max_v: 1644.5\n",
      "lif layer 1 self.abs_max_v: 911.5\n",
      "lif layer 2 self.abs_max_v: 1937.5\n",
      "fc layer 1 self.abs_max_out: 777.0\n",
      "lif layer 1 self.abs_max_v: 1040.0\n",
      "fc layer 2 self.abs_max_out: 1212.0\n",
      "lif layer 2 self.abs_max_v: 2181.0\n",
      "fc layer 1 self.abs_max_out: 862.0\n",
      "lif layer 2 self.abs_max_v: 2299.5\n",
      "fc layer 1 self.abs_max_out: 957.0\n",
      "fc layer 2 self.abs_max_out: 1230.0\n",
      "fc layer 3 self.abs_max_out: 492.0\n",
      "fc layer 3 self.abs_max_out: 496.0\n",
      "fc layer 3 self.abs_max_out: 561.0\n",
      "lif layer 1 self.abs_max_v: 1139.0\n",
      "lif layer 1 self.abs_max_v: 1321.5\n",
      "fc layer 2 self.abs_max_out: 1264.0\n",
      "fc layer 1 self.abs_max_out: 1042.0\n",
      "fc layer 2 self.abs_max_out: 1326.0\n",
      "fc layer 1 self.abs_max_out: 1053.0\n",
      "fc layer 1 self.abs_max_out: 1233.0\n",
      "fc layer 1 self.abs_max_out: 1247.0\n",
      "lif layer 1 self.abs_max_v: 1467.0\n",
      "fc layer 3 self.abs_max_out: 619.0\n",
      "lif layer 1 self.abs_max_v: 1541.0\n",
      "lif layer 1 self.abs_max_v: 1679.5\n",
      "lif layer 1 self.abs_max_v: 1789.0\n",
      "fc layer 2 self.abs_max_out: 1343.0\n",
      "fc layer 2 self.abs_max_out: 1354.0\n",
      "fc layer 2 self.abs_max_out: 1416.0\n",
      "lif layer 2 self.abs_max_v: 2526.0\n",
      "lif layer 1 self.abs_max_v: 1846.0\n",
      "fc layer 1 self.abs_max_out: 1250.0\n",
      "fc layer 2 self.abs_max_out: 1435.0\n",
      "fc layer 1 self.abs_max_out: 1276.0\n",
      "lif layer 1 self.abs_max_v: 1922.5\n",
      "fc layer 1 self.abs_max_out: 1318.0\n",
      "fc layer 2 self.abs_max_out: 1437.0\n",
      "fc layer 2 self.abs_max_out: 1460.0\n",
      "fc layer 2 self.abs_max_out: 1463.0\n",
      "lif layer 2 self.abs_max_v: 2645.0\n",
      "lif layer 2 self.abs_max_v: 2727.5\n",
      "fc layer 1 self.abs_max_out: 1338.0\n",
      "fc layer 1 self.abs_max_out: 1560.0\n",
      "fc layer 1 self.abs_max_out: 1646.0\n",
      "fc layer 2 self.abs_max_out: 1516.0\n",
      "fc layer 3 self.abs_max_out: 654.0\n",
      "fc layer 3 self.abs_max_out: 670.0\n",
      "lif layer 1 self.abs_max_v: 1953.5\n",
      "lif layer 1 self.abs_max_v: 1974.0\n",
      "lif layer 1 self.abs_max_v: 1975.0\n",
      "lif layer 1 self.abs_max_v: 1978.0\n",
      "lif layer 1 self.abs_max_v: 1981.0\n",
      "lif layer 1 self.abs_max_v: 2020.0\n",
      "lif layer 1 self.abs_max_v: 2034.0\n",
      "lif layer 1 self.abs_max_v: 2279.5\n",
      "lif layer 1 self.abs_max_v: 2312.0\n",
      "lif layer 1 self.abs_max_v: 2427.5\n",
      "fc layer 1 self.abs_max_out: 1840.0\n",
      "fc layer 3 self.abs_max_out: 679.0\n",
      "lif layer 1 self.abs_max_v: 2517.0\n",
      "lif layer 1 self.abs_max_v: 2544.0\n",
      "lif layer 1 self.abs_max_v: 2897.0\n",
      "fc layer 1 self.abs_max_out: 1945.0\n",
      "lif layer 1 self.abs_max_v: 2966.5\n",
      "lif layer 1 self.abs_max_v: 3091.5\n",
      "lif layer 1 self.abs_max_v: 3156.0\n",
      "fc layer 2 self.abs_max_out: 1534.0\n",
      "fc layer 2 self.abs_max_out: 1535.0\n",
      "lif layer 1 self.abs_max_v: 3216.5\n",
      "lif layer 1 self.abs_max_v: 3241.0\n",
      "fc layer 1 self.abs_max_out: 1968.0\n",
      "lif layer 1 self.abs_max_v: 3364.5\n",
      "lif layer 1 self.abs_max_v: 3575.5\n",
      "fc layer 1 self.abs_max_out: 2047.0\n",
      "fc layer 1 self.abs_max_out: 2168.0\n",
      "fc layer 2 self.abs_max_out: 1609.0\n",
      "lif layer 1 self.abs_max_v: 3632.0\n",
      "lif layer 1 self.abs_max_v: 3699.0\n",
      "fc layer 1 self.abs_max_out: 2232.0\n",
      "lif layer 1 self.abs_max_v: 3735.0\n",
      "fc layer 3 self.abs_max_out: 711.0\n",
      "fc layer 2 self.abs_max_out: 1669.0\n",
      "lif layer 2 self.abs_max_v: 2738.5\n",
      "fc layer 1 self.abs_max_out: 2246.0\n",
      "fc layer 1 self.abs_max_out: 2353.0\n",
      "lif layer 1 self.abs_max_v: 3957.0\n",
      "lif layer 1 self.abs_max_v: 4315.5\n",
      "fc layer 1 self.abs_max_out: 2373.0\n",
      "fc layer 1 self.abs_max_out: 2391.0\n",
      "fc layer 2 self.abs_max_out: 1697.0\n",
      "lif layer 2 self.abs_max_v: 2895.5\n",
      "fc layer 1 self.abs_max_out: 2399.0\n",
      "fc layer 1 self.abs_max_out: 2411.0\n",
      "fc layer 1 self.abs_max_out: 2480.0\n",
      "lif layer 1 self.abs_max_v: 4429.5\n",
      "lif layer 1 self.abs_max_v: 4584.0\n",
      "fc layer 1 self.abs_max_out: 2537.0\n",
      "lif layer 1 self.abs_max_v: 4829.0\n",
      "fc layer 2 self.abs_max_out: 1699.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.820473/  2.020072, val:  30.00%, val_best:  30.00%, tr:  95.51%, tr_best:  95.51%, epoch time: 71.41 seconds, 1.19 minutes\n",
      "total_backward_count 9790 real_backward_count 2335  23.851%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 752.0\n",
      "fc layer 1 self.abs_max_out: 2644.0\n",
      "fc layer 1 self.abs_max_out: 2698.0\n",
      "fc layer 2 self.abs_max_out: 1742.0\n",
      "fc layer 3 self.abs_max_out: 757.0\n",
      "lif layer 2 self.abs_max_v: 3052.5\n",
      "lif layer 2 self.abs_max_v: 3082.5\n",
      "fc layer 2 self.abs_max_out: 1767.0\n",
      "lif layer 2 self.abs_max_v: 3187.0\n",
      "lif layer 2 self.abs_max_v: 3235.0\n",
      "fc layer 2 self.abs_max_out: 1774.0\n",
      "lif layer 2 self.abs_max_v: 3297.0\n",
      "fc layer 1 self.abs_max_out: 2931.0\n",
      "fc layer 1 self.abs_max_out: 2955.0\n",
      "lif layer 1 self.abs_max_v: 4972.0\n",
      "lif layer 1 self.abs_max_v: 5091.0\n",
      "fc layer 2 self.abs_max_out: 1798.0\n",
      "fc layer 2 self.abs_max_out: 1809.0\n",
      "fc layer 2 self.abs_max_out: 1869.0\n",
      "lif layer 2 self.abs_max_v: 3309.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.783822/  1.985846, val:  42.08%, val_best:  42.08%, tr:  98.37%, tr_best:  98.37%, epoch time: 70.42 seconds, 1.17 minutes\n",
      "total_backward_count 19580 real_backward_count 4032  20.592%\n",
      "lif layer 2 self.abs_max_v: 3348.0\n",
      "lif layer 2 self.abs_max_v: 3390.0\n",
      "lif layer 2 self.abs_max_v: 3508.0\n",
      "fc layer 2 self.abs_max_out: 1974.0\n",
      "fc layer 1 self.abs_max_out: 3107.0\n",
      "fc layer 3 self.abs_max_out: 775.0\n",
      "fc layer 1 self.abs_max_out: 3166.0\n",
      "fc layer 1 self.abs_max_out: 3204.0\n",
      "lif layer 1 self.abs_max_v: 5116.5\n",
      "lif layer 1 self.abs_max_v: 5402.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.774692/  1.947305, val:  46.25%, val_best:  46.25%, tr:  99.59%, tr_best:  99.59%, epoch time: 70.45 seconds, 1.17 minutes\n",
      "total_backward_count 29370 real_backward_count 5475  18.641%\n",
      "fc layer 3 self.abs_max_out: 779.0\n",
      "fc layer 3 self.abs_max_out: 789.0\n",
      "fc layer 2 self.abs_max_out: 2003.0\n",
      "fc layer 3 self.abs_max_out: 793.0\n",
      "fc layer 3 self.abs_max_out: 809.0\n",
      "lif layer 1 self.abs_max_v: 5520.5\n",
      "fc layer 3 self.abs_max_out: 813.0\n",
      "fc layer 1 self.abs_max_out: 3457.0\n",
      "lif layer 1 self.abs_max_v: 6018.0\n",
      "lif layer 1 self.abs_max_v: 6047.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.737251/  1.938677, val:  54.58%, val_best:  54.58%, tr:  99.18%, tr_best:  99.59%, epoch time: 70.54 seconds, 1.18 minutes\n",
      "total_backward_count 39160 real_backward_count 6829  17.439%\n",
      "fc layer 2 self.abs_max_out: 2012.0\n",
      "fc layer 2 self.abs_max_out: 2043.0\n",
      "lif layer 2 self.abs_max_v: 3539.5\n",
      "fc layer 2 self.abs_max_out: 2073.0\n",
      "fc layer 2 self.abs_max_out: 2088.0\n",
      "lif layer 2 self.abs_max_v: 3773.5\n",
      "lif layer 2 self.abs_max_v: 3810.5\n",
      "lif layer 2 self.abs_max_v: 3888.0\n",
      "fc layer 1 self.abs_max_out: 3525.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.754585/  1.924353, val:  55.42%, val_best:  55.42%, tr:  99.39%, tr_best:  99.59%, epoch time: 68.07 seconds, 1.13 minutes\n",
      "total_backward_count 48950 real_backward_count 8031  16.407%\n",
      "fc layer 3 self.abs_max_out: 840.0\n",
      "fc layer 2 self.abs_max_out: 2172.0\n",
      "fc layer 1 self.abs_max_out: 3883.0\n",
      "lif layer 1 self.abs_max_v: 6235.5\n",
      "lif layer 1 self.abs_max_v: 6383.0\n",
      "lif layer 1 self.abs_max_v: 6433.5\n",
      "lif layer 1 self.abs_max_v: 6705.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.739617/  1.926746, val:  46.67%, val_best:  55.42%, tr:  99.69%, tr_best:  99.69%, epoch time: 69.92 seconds, 1.17 minutes\n",
      "total_backward_count 58740 real_backward_count 9226  15.707%\n",
      "fc layer 1 self.abs_max_out: 4027.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.730444/  1.903652, val:  58.33%, val_best:  58.33%, tr:  99.59%, tr_best:  99.69%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "total_backward_count 68530 real_backward_count 10430  15.220%\n",
      "fc layer 2 self.abs_max_out: 2238.0\n",
      "fc layer 1 self.abs_max_out: 4066.0\n",
      "lif layer 1 self.abs_max_v: 6894.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.734547/  1.910449, val:  50.42%, val_best:  58.33%, tr:  99.59%, tr_best:  99.69%, epoch time: 70.88 seconds, 1.18 minutes\n",
      "total_backward_count 78320 real_backward_count 11581  14.787%\n",
      "lif layer 2 self.abs_max_v: 3901.5\n",
      "lif layer 2 self.abs_max_v: 3929.0\n",
      "lif layer 2 self.abs_max_v: 3961.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.742144/  1.894683, val:  65.83%, val_best:  65.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 70.78 seconds, 1.18 minutes\n",
      "total_backward_count 88110 real_backward_count 12745  14.465%\n",
      "fc layer 3 self.abs_max_out: 858.0\n",
      "lif layer 2 self.abs_max_v: 4090.0\n",
      "lif layer 2 self.abs_max_v: 4097.0\n",
      "fc layer 3 self.abs_max_out: 861.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.733987/  1.924984, val:  46.25%, val_best:  65.83%, tr:  99.59%, tr_best:  99.80%, epoch time: 70.81 seconds, 1.18 minutes\n",
      "total_backward_count 97900 real_backward_count 13884  14.182%\n",
      "fc layer 2 self.abs_max_out: 2336.0\n",
      "lif layer 2 self.abs_max_v: 4221.5\n",
      "fc layer 1 self.abs_max_out: 4103.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.728790/  1.917700, val:  53.75%, val_best:  65.83%, tr:  99.90%, tr_best:  99.90%, epoch time: 70.83 seconds, 1.18 minutes\n",
      "total_backward_count 107690 real_backward_count 14976  13.907%\n",
      "lif layer 2 self.abs_max_v: 4352.0\n",
      "fc layer 2 self.abs_max_out: 2374.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.723300/  1.917027, val:  53.33%, val_best:  65.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.50 seconds, 1.18 minutes\n",
      "total_backward_count 117480 real_backward_count 15996  13.616%\n",
      "fc layer 1 self.abs_max_out: 4147.0\n",
      "fc layer 3 self.abs_max_out: 869.0\n",
      "fc layer 3 self.abs_max_out: 872.0\n",
      "fc layer 1 self.abs_max_out: 4223.0\n",
      "lif layer 1 self.abs_max_v: 7148.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.710283/  1.884610, val:  47.08%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.05 seconds, 1.17 minutes\n",
      "total_backward_count 127270 real_backward_count 17072  13.414%\n",
      "fc layer 2 self.abs_max_out: 2489.0\n",
      "lif layer 1 self.abs_max_v: 7213.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.704779/  1.862947, val:  70.83%, val_best:  70.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 70.62 seconds, 1.18 minutes\n",
      "total_backward_count 137060 real_backward_count 18208  13.285%\n",
      "fc layer 1 self.abs_max_out: 4446.0\n",
      "lif layer 1 self.abs_max_v: 7650.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.701710/  1.868517, val:  58.75%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.81 seconds, 1.18 minutes\n",
      "total_backward_count 146850 real_backward_count 19232  13.096%\n",
      "fc layer 2 self.abs_max_out: 2493.0\n",
      "fc layer 1 self.abs_max_out: 4530.0\n",
      "lif layer 1 self.abs_max_v: 7822.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.708098/  1.850947, val:  66.67%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.49 seconds, 1.17 minutes\n",
      "total_backward_count 156640 real_backward_count 20227  12.913%\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.702790/  1.875723, val:  70.00%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.43 seconds, 1.17 minutes\n",
      "total_backward_count 166430 real_backward_count 21276  12.784%\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.715636/  1.873932, val:  63.75%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 69.95 seconds, 1.17 minutes\n",
      "total_backward_count 176220 real_backward_count 22323  12.668%\n",
      "fc layer 1 self.abs_max_out: 4578.0\n",
      "fc layer 1 self.abs_max_out: 4611.0\n",
      "lif layer 1 self.abs_max_v: 8020.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.700790/  1.854414, val:  60.42%, val_best:  70.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.55 seconds, 1.18 minutes\n",
      "total_backward_count 186010 real_backward_count 23303  12.528%\n",
      "fc layer 1 self.abs_max_out: 4804.0\n",
      "lif layer 1 self.abs_max_v: 8386.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.698066/  1.869788, val:  60.83%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.85 seconds, 1.18 minutes\n",
      "total_backward_count 195800 real_backward_count 24271  12.396%\n",
      "fc layer 2 self.abs_max_out: 2494.0\n",
      "fc layer 1 self.abs_max_out: 4811.0\n",
      "fc layer 1 self.abs_max_out: 4932.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.687541/  1.858582, val:  69.17%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.84 seconds, 1.18 minutes\n",
      "total_backward_count 205590 real_backward_count 25227  12.271%\n",
      "fc layer 3 self.abs_max_out: 915.0\n",
      "lif layer 2 self.abs_max_v: 4405.5\n",
      "lif layer 2 self.abs_max_v: 4600.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.684883/  1.845642, val:  60.83%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.48 seconds, 1.17 minutes\n",
      "total_backward_count 215380 real_backward_count 26120  12.127%\n",
      "fc layer 2 self.abs_max_out: 2509.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.684768/  1.833464, val:  61.67%, val_best:  70.83%, tr:  99.80%, tr_best: 100.00%, epoch time: 71.43 seconds, 1.19 minutes\n",
      "total_backward_count 225170 real_backward_count 27041  12.009%\n",
      "fc layer 2 self.abs_max_out: 2694.0\n",
      "lif layer 2 self.abs_max_v: 4751.5\n",
      "lif layer 2 self.abs_max_v: 4757.0\n",
      "fc layer 2 self.abs_max_out: 2743.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.655942/  1.824449, val:  72.08%, val_best:  72.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.46 seconds, 1.17 minutes\n",
      "total_backward_count 234960 real_backward_count 27980  11.908%\n",
      "lif layer 2 self.abs_max_v: 4780.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.667528/  1.839377, val:  74.17%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.42 seconds, 1.17 minutes\n",
      "total_backward_count 244750 real_backward_count 28842  11.784%\n",
      "fc layer 2 self.abs_max_out: 2855.0\n",
      "fc layer 1 self.abs_max_out: 4974.0\n",
      "lif layer 1 self.abs_max_v: 8536.5\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.651800/  1.827932, val:  60.42%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.67 seconds, 1.18 minutes\n",
      "total_backward_count 254540 real_backward_count 29735  11.682%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.648830/  1.825396, val:  66.25%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.02 seconds, 1.18 minutes\n",
      "total_backward_count 264330 real_backward_count 30620  11.584%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.636409/  1.779311, val:  66.67%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.67 seconds, 1.18 minutes\n",
      "total_backward_count 274120 real_backward_count 31517  11.498%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.614298/  1.785483, val:  63.75%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.99 seconds, 1.17 minutes\n",
      "total_backward_count 283910 real_backward_count 32375  11.403%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.616405/  1.786063, val:  59.58%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.25 seconds, 1.17 minutes\n",
      "total_backward_count 293700 real_backward_count 33266  11.327%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.626095/  1.787114, val:  72.50%, val_best:  74.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.14 seconds, 1.17 minutes\n",
      "total_backward_count 303490 real_backward_count 34058  11.222%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.620784/  1.783973, val:  72.50%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.95 seconds, 1.18 minutes\n",
      "total_backward_count 313280 real_backward_count 34903  11.141%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.634936/  1.772625, val:  76.67%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.97 seconds, 1.18 minutes\n",
      "total_backward_count 323070 real_backward_count 35765  11.070%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.615992/  1.767934, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.92 seconds, 1.17 minutes\n",
      "total_backward_count 332860 real_backward_count 36549  10.980%\n",
      "fc layer 2 self.abs_max_out: 2929.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.613824/  1.771097, val:  78.33%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.97 seconds, 1.18 minutes\n",
      "total_backward_count 342650 real_backward_count 37316  10.890%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.613171/  1.757727, val:  75.83%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.40 seconds, 1.17 minutes\n",
      "total_backward_count 352440 real_backward_count 38053  10.797%\n",
      "fc layer 2 self.abs_max_out: 2962.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.611331/  1.759852, val:  59.58%, val_best:  79.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.27 seconds, 1.17 minutes\n",
      "total_backward_count 362230 real_backward_count 38798  10.711%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.597485/  1.762379, val:  75.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.31 seconds, 1.17 minutes\n",
      "total_backward_count 372020 real_backward_count 39557  10.633%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.613845/  1.769181, val:  72.50%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.92 seconds, 1.18 minutes\n",
      "total_backward_count 381810 real_backward_count 40305  10.556%\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.589766/  1.750054, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.46 seconds, 1.17 minutes\n",
      "total_backward_count 391600 real_backward_count 41027  10.477%\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.596432/  1.762467, val:  80.00%, val_best:  80.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.55 seconds, 1.18 minutes\n",
      "total_backward_count 401390 real_backward_count 41728  10.396%\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.596140/  1.741214, val:  81.25%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.32 seconds, 1.17 minutes\n",
      "total_backward_count 411180 real_backward_count 42460  10.326%\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.590805/  1.761934, val:  74.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.82 seconds, 1.18 minutes\n",
      "total_backward_count 420970 real_backward_count 43190  10.260%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.581801/  1.752096, val:  81.67%, val_best:  81.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.44 seconds, 1.17 minutes\n",
      "total_backward_count 430760 real_backward_count 43872  10.185%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.586648/  1.749197, val:  82.08%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.83 seconds, 1.18 minutes\n",
      "total_backward_count 440550 real_backward_count 44572  10.117%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.581288/  1.753280, val:  77.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.02 seconds, 1.17 minutes\n",
      "total_backward_count 450340 real_backward_count 45281  10.055%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.580642/  1.759833, val:  77.50%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.38 seconds, 1.17 minutes\n",
      "total_backward_count 460130 real_backward_count 45957   9.988%\n",
      "fc layer 2 self.abs_max_out: 2967.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.581140/  1.750242, val:  78.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.96 seconds, 1.17 minutes\n",
      "total_backward_count 469920 real_backward_count 46638   9.925%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.578108/  1.723744, val:  80.42%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.10 seconds, 1.17 minutes\n",
      "total_backward_count 479710 real_backward_count 47315   9.863%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.566565/  1.752374, val:  71.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.04 seconds, 1.17 minutes\n",
      "total_backward_count 489500 real_backward_count 47985   9.803%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.565634/  1.736645, val:  77.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.57 seconds, 1.18 minutes\n",
      "total_backward_count 499290 real_backward_count 48632   9.740%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.571590/  1.723205, val:  75.42%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.53 seconds, 1.18 minutes\n",
      "total_backward_count 509080 real_backward_count 49278   9.680%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.566058/  1.720774, val:  77.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.72 seconds, 1.18 minutes\n",
      "total_backward_count 518870 real_backward_count 49955   9.628%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.561250/  1.715848, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.03 seconds, 1.17 minutes\n",
      "total_backward_count 528660 real_backward_count 50535   9.559%\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.561532/  1.720280, val:  76.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.22 seconds, 1.17 minutes\n",
      "total_backward_count 538450 real_backward_count 51135   9.497%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.569175/  1.735967, val:  81.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.57 seconds, 1.18 minutes\n",
      "total_backward_count 548240 real_backward_count 51694   9.429%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.560339/  1.717970, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.94 seconds, 1.17 minutes\n",
      "total_backward_count 558030 real_backward_count 52335   9.379%\n",
      "lif layer 1 self.abs_max_v: 8560.5\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.554801/  1.720006, val:  67.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.10 seconds, 1.17 minutes\n",
      "total_backward_count 567820 real_backward_count 52894   9.315%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.554558/  1.728132, val:  79.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.44 seconds, 1.17 minutes\n",
      "total_backward_count 577610 real_backward_count 53483   9.259%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.561188/  1.736119, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.25 seconds, 1.17 minutes\n",
      "total_backward_count 587400 real_backward_count 54062   9.204%\n",
      "lif layer 1 self.abs_max_v: 8620.0\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.560694/  1.707589, val:  79.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.28 seconds, 1.17 minutes\n",
      "total_backward_count 597190 real_backward_count 54616   9.145%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.543968/  1.728006, val:  76.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.87 seconds, 1.16 minutes\n",
      "total_backward_count 606980 real_backward_count 55211   9.096%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.539611/  1.696344, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.68 seconds, 1.18 minutes\n",
      "total_backward_count 616770 real_backward_count 55806   9.048%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.547941/  1.691774, val:  77.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.64 seconds, 1.18 minutes\n",
      "total_backward_count 626560 real_backward_count 56329   8.990%\n",
      "fc layer 3 self.abs_max_out: 940.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.548050/  1.730533, val:  75.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.59 seconds, 1.18 minutes\n",
      "total_backward_count 636350 real_backward_count 56833   8.931%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.546701/  1.707011, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.40 seconds, 1.17 minutes\n",
      "total_backward_count 646140 real_backward_count 57385   8.881%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.538691/  1.692708, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.36 seconds, 1.17 minutes\n",
      "total_backward_count 655930 real_backward_count 57910   8.829%\n",
      "fc layer 3 self.abs_max_out: 964.0\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.524334/  1.698587, val:  76.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.92 seconds, 1.17 minutes\n",
      "total_backward_count 665720 real_backward_count 58447   8.780%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.529612/  1.690444, val:  75.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.34 seconds, 1.17 minutes\n",
      "total_backward_count 675510 real_backward_count 58960   8.728%\n",
      "fc layer 1 self.abs_max_out: 5043.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.530379/  1.699952, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.24 seconds, 1.17 minutes\n",
      "total_backward_count 685300 real_backward_count 59505   8.683%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.515344/  1.691668, val:  78.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.57 seconds, 1.18 minutes\n",
      "total_backward_count 695090 real_backward_count 60059   8.640%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.519437/  1.679697, val:  75.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.93 seconds, 1.18 minutes\n",
      "total_backward_count 704880 real_backward_count 60623   8.600%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.509025/  1.671525, val:  79.17%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.02 seconds, 1.17 minutes\n",
      "total_backward_count 714670 real_backward_count 61095   8.549%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.517235/  1.688774, val:  76.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.34 seconds, 1.17 minutes\n",
      "total_backward_count 724460 real_backward_count 61641   8.509%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.503797/  1.676702, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.30 seconds, 1.17 minutes\n",
      "total_backward_count 734250 real_backward_count 62130   8.462%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.500837/  1.703224, val:  69.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.30 seconds, 1.17 minutes\n",
      "total_backward_count 744040 real_backward_count 62605   8.414%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.510568/  1.660809, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.65 seconds, 1.16 minutes\n",
      "total_backward_count 753830 real_backward_count 63100   8.371%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.492325/  1.668642, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.24 seconds, 1.17 minutes\n",
      "total_backward_count 763620 real_backward_count 63569   8.325%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.491690/  1.665370, val:  76.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.07 seconds, 1.17 minutes\n",
      "total_backward_count 773410 real_backward_count 64033   8.279%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.498518/  1.669793, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.36 seconds, 1.17 minutes\n",
      "total_backward_count 783200 real_backward_count 64514   8.237%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.485997/  1.654696, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.26 seconds, 1.17 minutes\n",
      "total_backward_count 792990 real_backward_count 65005   8.197%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.491342/  1.660332, val:  79.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.88 seconds, 1.16 minutes\n",
      "total_backward_count 802780 real_backward_count 65482   8.157%\n",
      "fc layer 1 self.abs_max_out: 5054.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.498718/  1.656368, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.02 seconds, 1.17 minutes\n",
      "total_backward_count 812570 real_backward_count 65944   8.115%\n",
      "fc layer 1 self.abs_max_out: 5132.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.497801/  1.664895, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.39 seconds, 1.17 minutes\n",
      "total_backward_count 822360 real_backward_count 66394   8.074%\n",
      "fc layer 1 self.abs_max_out: 5164.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.491596/  1.675259, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.85 seconds, 1.16 minutes\n",
      "total_backward_count 832150 real_backward_count 66851   8.034%\n",
      "fc layer 1 self.abs_max_out: 5182.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.482810/  1.643700, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.05 seconds, 1.17 minutes\n",
      "total_backward_count 841940 real_backward_count 67283   7.991%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.465983/  1.650446, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.31 seconds, 1.17 minutes\n",
      "total_backward_count 851730 real_backward_count 67750   7.954%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.469716/  1.636697, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.65 seconds, 1.18 minutes\n",
      "total_backward_count 861520 real_backward_count 68209   7.917%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.458988/  1.641406, val:  74.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.99 seconds, 1.17 minutes\n",
      "total_backward_count 871310 real_backward_count 68631   7.877%\n",
      "fc layer 1 self.abs_max_out: 5208.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.444530/  1.610197, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.92 seconds, 1.17 minutes\n",
      "total_backward_count 881100 real_backward_count 69092   7.842%\n",
      "fc layer 1 self.abs_max_out: 5350.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.449110/  1.615080, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.22 seconds, 1.17 minutes\n",
      "total_backward_count 890890 real_backward_count 69562   7.808%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.442724/  1.589966, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.40 seconds, 1.17 minutes\n",
      "total_backward_count 900680 real_backward_count 69962   7.768%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.447264/  1.617263, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.30 seconds, 1.17 minutes\n",
      "total_backward_count 910470 real_backward_count 70407   7.733%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.451772/  1.626340, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.73 seconds, 1.16 minutes\n",
      "total_backward_count 920260 real_backward_count 70847   7.699%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.452609/  1.643059, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.36 seconds, 1.17 minutes\n",
      "total_backward_count 930050 real_backward_count 71237   7.659%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.450038/  1.611537, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.41 seconds, 1.17 minutes\n",
      "total_backward_count 939840 real_backward_count 71622   7.621%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.448563/  1.613352, val:  88.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.71 seconds, 1.18 minutes\n",
      "total_backward_count 949630 real_backward_count 72085   7.591%\n",
      "lif layer 1 self.abs_max_v: 8727.0\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.438605/  1.616418, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.54 seconds, 1.18 minutes\n",
      "total_backward_count 959420 real_backward_count 72486   7.555%\n",
      "fc layer 1 self.abs_max_out: 5367.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.437724/  1.637831, val:  80.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.13 seconds, 1.17 minutes\n",
      "total_backward_count 969210 real_backward_count 72903   7.522%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.452157/  1.624429, val:  80.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.27 seconds, 1.17 minutes\n",
      "total_backward_count 979000 real_backward_count 73336   7.491%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.432672/  1.616930, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.73 seconds, 1.18 minutes\n",
      "total_backward_count 988790 real_backward_count 73743   7.458%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.436992/  1.629601, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.71 seconds, 1.16 minutes\n",
      "total_backward_count 998580 real_backward_count 74130   7.424%\n",
      "fc layer 1 self.abs_max_out: 5373.0\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.437176/  1.604629, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.99 seconds, 1.17 minutes\n",
      "total_backward_count 1008370 real_backward_count 74529   7.391%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.438340/  1.626522, val:  80.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.93 seconds, 1.17 minutes\n",
      "total_backward_count 1018160 real_backward_count 74911   7.357%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.442117/  1.611525, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.57 seconds, 1.18 minutes\n",
      "total_backward_count 1027950 real_backward_count 75245   7.320%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.442522/  1.625475, val:  80.00%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.60 seconds, 1.18 minutes\n",
      "total_backward_count 1037740 real_backward_count 75656   7.290%\n",
      "fc layer 1 self.abs_max_out: 5439.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.453201/  1.636530, val:  81.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.16 seconds, 1.17 minutes\n",
      "total_backward_count 1047530 real_backward_count 76068   7.262%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.449923/  1.634120, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.01 seconds, 1.17 minutes\n",
      "total_backward_count 1057320 real_backward_count 76423   7.228%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.445947/  1.617720, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.85 seconds, 1.18 minutes\n",
      "total_backward_count 1067110 real_backward_count 76799   7.197%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.440282/  1.624046, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.60 seconds, 1.18 minutes\n",
      "total_backward_count 1076900 real_backward_count 77139   7.163%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.441441/  1.612191, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.27 seconds, 1.17 minutes\n",
      "total_backward_count 1086690 real_backward_count 77489   7.131%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.433905/  1.603019, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.60 seconds, 1.18 minutes\n",
      "total_backward_count 1096480 real_backward_count 77849   7.100%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.430827/  1.611700, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.07 seconds, 1.18 minutes\n",
      "total_backward_count 1106270 real_backward_count 78228   7.071%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.428478/  1.636629, val:  75.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.33 seconds, 1.17 minutes\n",
      "total_backward_count 1116060 real_backward_count 78570   7.040%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.434358/  1.584600, val:  87.50%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.44 seconds, 1.17 minutes\n",
      "total_backward_count 1125850 real_backward_count 78932   7.011%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.410621/  1.592140, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.96 seconds, 1.18 minutes\n",
      "total_backward_count 1135640 real_backward_count 79280   6.981%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.408560/  1.609557, val:  85.00%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.13 seconds, 1.17 minutes\n",
      "total_backward_count 1145430 real_backward_count 79625   6.952%\n",
      "fc layer 3 self.abs_max_out: 980.0\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.423967/  1.606440, val:  82.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.81 seconds, 1.16 minutes\n",
      "total_backward_count 1155220 real_backward_count 79967   6.922%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.424275/  1.606902, val:  82.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.12 seconds, 1.17 minutes\n",
      "total_backward_count 1165010 real_backward_count 80297   6.892%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.425223/  1.583878, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.39 seconds, 1.17 minutes\n",
      "total_backward_count 1174800 real_backward_count 80643   6.864%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.417209/  1.594225, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.43 seconds, 1.17 minutes\n",
      "total_backward_count 1184590 real_backward_count 80968   6.835%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.413929/  1.578378, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.61 seconds, 1.18 minutes\n",
      "total_backward_count 1194380 real_backward_count 81275   6.805%\n",
      "fc layer 1 self.abs_max_out: 5440.0\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.410698/  1.594339, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.08 seconds, 1.17 minutes\n",
      "total_backward_count 1204170 real_backward_count 81572   6.774%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.412203/  1.586764, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.13 seconds, 1.17 minutes\n",
      "total_backward_count 1213960 real_backward_count 81887   6.745%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.410897/  1.600405, val:  80.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.52 seconds, 1.16 minutes\n",
      "total_backward_count 1223750 real_backward_count 82225   6.719%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.410042/  1.593781, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.21 seconds, 1.17 minutes\n",
      "total_backward_count 1233540 real_backward_count 82512   6.689%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.413250/  1.591612, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.11 seconds, 1.17 minutes\n",
      "total_backward_count 1243330 real_backward_count 82829   6.662%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.418122/  1.594558, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.15 seconds, 1.17 minutes\n",
      "total_backward_count 1253120 real_backward_count 83155   6.636%\n",
      "fc layer 1 self.abs_max_out: 5478.0\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.410907/  1.590883, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.22 seconds, 1.17 minutes\n",
      "total_backward_count 1262910 real_backward_count 83465   6.609%\n",
      "fc layer 1 self.abs_max_out: 5497.0\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.395378/  1.583916, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.10 seconds, 1.17 minutes\n",
      "total_backward_count 1272700 real_backward_count 83785   6.583%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.393896/  1.594284, val:  73.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.16 seconds, 1.17 minutes\n",
      "total_backward_count 1282490 real_backward_count 84112   6.558%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.404315/  1.583571, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.09 seconds, 1.17 minutes\n",
      "total_backward_count 1292280 real_backward_count 84432   6.534%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.404292/  1.579018, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.78 seconds, 1.16 minutes\n",
      "total_backward_count 1302070 real_backward_count 84762   6.510%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.405844/  1.581426, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.84 seconds, 1.16 minutes\n",
      "total_backward_count 1311860 real_backward_count 85049   6.483%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.396516/  1.588442, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.23 seconds, 1.17 minutes\n",
      "total_backward_count 1321650 real_backward_count 85339   6.457%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.405368/  1.586647, val:  77.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.64 seconds, 1.18 minutes\n",
      "total_backward_count 1331440 real_backward_count 85642   6.432%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.408530/  1.572847, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.96 seconds, 1.17 minutes\n",
      "total_backward_count 1341230 real_backward_count 85951   6.408%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.400242/  1.575562, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.30 seconds, 1.17 minutes\n",
      "total_backward_count 1351020 real_backward_count 86239   6.383%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.394284/  1.580793, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.17 seconds, 1.17 minutes\n",
      "total_backward_count 1360810 real_backward_count 86506   6.357%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.392393/  1.583459, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.52 seconds, 1.18 minutes\n",
      "total_backward_count 1370600 real_backward_count 86764   6.330%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.395033/  1.570488, val:  80.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.94 seconds, 1.17 minutes\n",
      "total_backward_count 1380390 real_backward_count 87021   6.304%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.394176/  1.579190, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.96 seconds, 1.17 minutes\n",
      "total_backward_count 1390180 real_backward_count 87331   6.282%\n",
      "lif layer 2 self.abs_max_v: 4900.0\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.391870/  1.577529, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.20 seconds, 1.17 minutes\n",
      "total_backward_count 1399970 real_backward_count 87604   6.258%\n",
      "fc layer 1 self.abs_max_out: 5523.0\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.398830/  1.575723, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.36 seconds, 1.17 minutes\n",
      "total_backward_count 1409760 real_backward_count 87891   6.234%\n",
      "fc layer 1 self.abs_max_out: 5524.0\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.401876/  1.572555, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.35 seconds, 1.17 minutes\n",
      "total_backward_count 1419550 real_backward_count 88147   6.210%\n",
      "lif layer 2 self.abs_max_v: 4917.5\n",
      "lif layer 1 self.abs_max_v: 8839.0\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.400165/  1.570250, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.65 seconds, 1.18 minutes\n",
      "total_backward_count 1429340 real_backward_count 88430   6.187%\n",
      "lif layer 2 self.abs_max_v: 4992.5\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.399815/  1.570833, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.15 seconds, 1.17 minutes\n",
      "total_backward_count 1439130 real_backward_count 88697   6.163%\n",
      "lif layer 2 self.abs_max_v: 5066.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.393996/  1.575520, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.69 seconds, 1.16 minutes\n",
      "total_backward_count 1448920 real_backward_count 88956   6.139%\n",
      "fc layer 1 self.abs_max_out: 5539.0\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.392267/  1.583620, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.68 seconds, 1.18 minutes\n",
      "total_backward_count 1458710 real_backward_count 89220   6.116%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.387741/  1.590201, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.88 seconds, 1.16 minutes\n",
      "total_backward_count 1468500 real_backward_count 89481   6.093%\n",
      "lif layer 2 self.abs_max_v: 5136.0\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.403617/  1.588934, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.93 seconds, 1.17 minutes\n",
      "total_backward_count 1478290 real_backward_count 89771   6.073%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.398915/  1.587842, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.33 seconds, 1.19 minutes\n",
      "total_backward_count 1488080 real_backward_count 90041   6.051%\n",
      "lif layer 2 self.abs_max_v: 5137.0\n",
      "lif layer 2 self.abs_max_v: 5259.5\n",
      "lif layer 1 self.abs_max_v: 9059.0\n",
      "fc layer 1 self.abs_max_out: 5552.0\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.391901/  1.568240, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.28 seconds, 1.17 minutes\n",
      "total_backward_count 1497870 real_backward_count 90291   6.028%\n",
      "fc layer 1 self.abs_max_out: 5648.0\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.388315/  1.555543, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.06 seconds, 1.17 minutes\n",
      "total_backward_count 1507660 real_backward_count 90526   6.004%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.376634/  1.567945, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.86 seconds, 1.16 minutes\n",
      "total_backward_count 1517450 real_backward_count 90786   5.983%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.376600/  1.549193, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.24 seconds, 1.17 minutes\n",
      "total_backward_count 1527240 real_backward_count 91041   5.961%\n",
      "fc layer 1 self.abs_max_out: 5654.0\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.368483/  1.546520, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.20 seconds, 1.17 minutes\n",
      "total_backward_count 1537030 real_backward_count 91275   5.938%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.374203/  1.566640, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.52 seconds, 1.18 minutes\n",
      "total_backward_count 1546820 real_backward_count 91510   5.916%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.373398/  1.564929, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.10 seconds, 1.17 minutes\n",
      "total_backward_count 1556610 real_backward_count 91744   5.894%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.376043/  1.573509, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.42 seconds, 1.17 minutes\n",
      "total_backward_count 1566400 real_backward_count 91964   5.871%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.375044/  1.582996, val:  82.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.24 seconds, 1.17 minutes\n",
      "total_backward_count 1576190 real_backward_count 92223   5.851%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.370163/  1.548691, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.57 seconds, 1.18 minutes\n",
      "total_backward_count 1585980 real_backward_count 92443   5.829%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.361729/  1.542307, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.99 seconds, 1.17 minutes\n",
      "total_backward_count 1595770 real_backward_count 92698   5.809%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.373433/  1.558772, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.95 seconds, 1.17 minutes\n",
      "total_backward_count 1605560 real_backward_count 92918   5.787%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.368232/  1.559707, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.71 seconds, 1.18 minutes\n",
      "total_backward_count 1615350 real_backward_count 93188   5.769%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.365880/  1.548162, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.06 seconds, 1.18 minutes\n",
      "total_backward_count 1625140 real_backward_count 93416   5.748%\n",
      "fc layer 3 self.abs_max_out: 999.0\n",
      "fc layer 1 self.abs_max_out: 5660.0\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.369552/  1.553589, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.05 seconds, 1.17 minutes\n",
      "total_backward_count 1634930 real_backward_count 93654   5.728%\n",
      "fc layer 1 self.abs_max_out: 5687.0\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.374267/  1.537662, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.16 seconds, 1.17 minutes\n",
      "total_backward_count 1644720 real_backward_count 93878   5.708%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.362196/  1.532187, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.36 seconds, 1.17 minutes\n",
      "total_backward_count 1654510 real_backward_count 94109   5.688%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.347104/  1.562034, val:  82.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.77 seconds, 1.18 minutes\n",
      "total_backward_count 1664300 real_backward_count 94330   5.668%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.343626/  1.524493, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.89 seconds, 1.16 minutes\n",
      "total_backward_count 1674090 real_backward_count 94551   5.648%\n",
      "fc layer 1 self.abs_max_out: 5691.0\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.341575/  1.544006, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.19 seconds, 1.19 minutes\n",
      "total_backward_count 1683880 real_backward_count 94769   5.628%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.347181/  1.570633, val:  81.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.35 seconds, 1.17 minutes\n",
      "total_backward_count 1693670 real_backward_count 94972   5.607%\n",
      "fc layer 1 self.abs_max_out: 5712.0\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.362884/  1.551825, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.75 seconds, 1.18 minutes\n",
      "total_backward_count 1703460 real_backward_count 95191   5.588%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.365378/  1.559113, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.17 seconds, 1.17 minutes\n",
      "total_backward_count 1713250 real_backward_count 95440   5.571%\n",
      "fc layer 1 self.abs_max_out: 5718.0\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.363544/  1.545588, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.49 seconds, 1.17 minutes\n",
      "total_backward_count 1723040 real_backward_count 95664   5.552%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.353747/  1.525512, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.13 seconds, 1.17 minutes\n",
      "total_backward_count 1732830 real_backward_count 95883   5.533%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.348173/  1.539689, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.54 seconds, 1.18 minutes\n",
      "total_backward_count 1742620 real_backward_count 96079   5.513%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.351295/  1.531639, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.40 seconds, 1.17 minutes\n",
      "total_backward_count 1752410 real_backward_count 96303   5.495%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.345120/  1.540958, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.96 seconds, 1.17 minutes\n",
      "total_backward_count 1762200 real_backward_count 96489   5.475%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.351202/  1.531777, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.20 seconds, 1.17 minutes\n",
      "total_backward_count 1771990 real_backward_count 96677   5.456%\n",
      "fc layer 1 self.abs_max_out: 5761.0\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.345058/  1.530517, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.82 seconds, 1.16 minutes\n",
      "total_backward_count 1781780 real_backward_count 96909   5.439%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.340554/  1.532034, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.50 seconds, 1.16 minutes\n",
      "total_backward_count 1791570 real_backward_count 97135   5.422%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.333745/  1.531459, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.96 seconds, 1.17 minutes\n",
      "total_backward_count 1801360 real_backward_count 97364   5.405%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.326132/  1.525271, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.29 seconds, 1.17 minutes\n",
      "total_backward_count 1811150 real_backward_count 97564   5.387%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.331807/  1.524522, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.38 seconds, 1.17 minutes\n",
      "total_backward_count 1820940 real_backward_count 97766   5.369%\n",
      "fc layer 1 self.abs_max_out: 5791.0\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.331954/  1.524628, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.90 seconds, 1.16 minutes\n",
      "total_backward_count 1830730 real_backward_count 97979   5.352%\n",
      "lif layer 2 self.abs_max_v: 5330.0\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.328613/  1.531309, val:  78.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.01 seconds, 1.17 minutes\n",
      "total_backward_count 1840520 real_backward_count 98155   5.333%\n",
      "fc layer 3 self.abs_max_out: 1002.0\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.331404/  1.549289, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.16 seconds, 1.17 minutes\n",
      "total_backward_count 1850310 real_backward_count 98389   5.317%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.350841/  1.554504, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.07 seconds, 1.17 minutes\n",
      "total_backward_count 1860100 real_backward_count 98590   5.300%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.349273/  1.541076, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.08 seconds, 1.17 minutes\n",
      "total_backward_count 1869890 real_backward_count 98782   5.283%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.335803/  1.528495, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.85 seconds, 1.16 minutes\n",
      "total_backward_count 1879680 real_backward_count 98964   5.265%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.334252/  1.527296, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.29 seconds, 1.17 minutes\n",
      "total_backward_count 1889470 real_backward_count 99131   5.246%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.328526/  1.525656, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.71 seconds, 1.16 minutes\n",
      "total_backward_count 1899260 real_backward_count 99307   5.229%\n",
      "fc layer 3 self.abs_max_out: 1008.0\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.331334/  1.512967, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.89 seconds, 1.16 minutes\n",
      "total_backward_count 1909050 real_backward_count 99496   5.212%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.327611/  1.514488, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.29 seconds, 1.17 minutes\n",
      "total_backward_count 1918840 real_backward_count 99686   5.195%\n",
      "fc layer 2 self.abs_max_out: 2979.0\n",
      "lif layer 2 self.abs_max_v: 5483.0\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.335769/  1.526220, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.68 seconds, 1.18 minutes\n",
      "total_backward_count 1928630 real_backward_count 99882   5.179%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.332900/  1.528766, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.62 seconds, 1.18 minutes\n",
      "total_backward_count 1938420 real_backward_count 100049   5.161%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.330039/  1.533596, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.25 seconds, 1.17 minutes\n",
      "total_backward_count 1948210 real_backward_count 100229   5.145%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.328903/  1.514654, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.04 seconds, 1.17 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7278e6f4d52c4d989d2d814306723a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñÜ‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñÜ‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.3289</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.88333</td></tr><tr><td>val_loss</td><td>1.51465</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-sweep-10</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ewdwilzt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ewdwilzt</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251023_024512-ewdwilzt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z3rc2ubf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.0625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 2249\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251023_064024-z3rc2ubf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z3rc2ubf' target=\"_blank\">super-sweep-18</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/7yr7ufe8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/7yr7ufe8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/7yr7ufe8' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/7yr7ufe8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z3rc2ubf' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z3rc2ubf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251023_064032_082', 'my_seed': 2249, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.0625, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 10, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.0625, v_reset=10000, sg_width=10, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 426.0\n",
      "lif layer 1 self.abs_max_v: 426.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 930.0\n",
      "lif layer 2 self.abs_max_v: 930.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "fc layer 1 self.abs_max_out: 468.0\n",
      "lif layer 1 self.abs_max_v: 563.5\n",
      "fc layer 2 self.abs_max_out: 1002.0\n",
      "lif layer 2 self.abs_max_v: 1360.5\n",
      "fc layer 3 self.abs_max_out: 383.0\n",
      "fc layer 1 self.abs_max_out: 573.0\n",
      "lif layer 1 self.abs_max_v: 624.5\n",
      "lif layer 2 self.abs_max_v: 1665.5\n",
      "lif layer 1 self.abs_max_v: 786.5\n",
      "fc layer 2 self.abs_max_out: 1152.0\n",
      "lif layer 2 self.abs_max_v: 1825.5\n",
      "fc layer 3 self.abs_max_out: 445.0\n",
      "lif layer 1 self.abs_max_v: 829.5\n",
      "fc layer 1 self.abs_max_out: 636.0\n",
      "lif layer 1 self.abs_max_v: 890.0\n",
      "fc layer 3 self.abs_max_out: 472.0\n",
      "fc layer 1 self.abs_max_out: 680.0\n",
      "fc layer 1 self.abs_max_out: 705.0\n",
      "lif layer 1 self.abs_max_v: 993.5\n",
      "lif layer 1 self.abs_max_v: 1134.0\n",
      "fc layer 2 self.abs_max_out: 1162.0\n",
      "lif layer 2 self.abs_max_v: 1837.0\n",
      "fc layer 1 self.abs_max_out: 717.0\n",
      "lif layer 1 self.abs_max_v: 1162.5\n",
      "fc layer 2 self.abs_max_out: 1328.0\n",
      "lif layer 2 self.abs_max_v: 2001.5\n",
      "fc layer 1 self.abs_max_out: 946.0\n",
      "lif layer 1 self.abs_max_v: 1244.5\n",
      "fc layer 3 self.abs_max_out: 479.0\n",
      "fc layer 1 self.abs_max_out: 988.0\n",
      "fc layer 2 self.abs_max_out: 1344.0\n",
      "lif layer 2 self.abs_max_v: 2064.0\n",
      "fc layer 1 self.abs_max_out: 1354.0\n",
      "lif layer 1 self.abs_max_v: 1354.0\n",
      "fc layer 3 self.abs_max_out: 533.0\n",
      "fc layer 3 self.abs_max_out: 559.0\n",
      "lif layer 2 self.abs_max_v: 2084.5\n",
      "lif layer 2 self.abs_max_v: 2220.5\n",
      "lif layer 1 self.abs_max_v: 1508.5\n",
      "lif layer 1 self.abs_max_v: 1575.0\n",
      "fc layer 3 self.abs_max_out: 639.0\n",
      "lif layer 1 self.abs_max_v: 1582.0\n",
      "fc layer 1 self.abs_max_out: 1438.0\n",
      "lif layer 1 self.abs_max_v: 1675.0\n",
      "fc layer 2 self.abs_max_out: 1420.0\n",
      "lif layer 2 self.abs_max_v: 2359.0\n",
      "lif layer 1 self.abs_max_v: 1713.0\n",
      "lif layer 2 self.abs_max_v: 2433.5\n",
      "fc layer 2 self.abs_max_out: 1656.0\n",
      "lif layer 2 self.abs_max_v: 2766.5\n",
      "lif layer 2 self.abs_max_v: 2803.5\n",
      "lif layer 1 self.abs_max_v: 1791.5\n",
      "lif layer 1 self.abs_max_v: 1795.5\n",
      "lif layer 2 self.abs_max_v: 2862.5\n",
      "lif layer 1 self.abs_max_v: 2142.0\n",
      "lif layer 2 self.abs_max_v: 2944.5\n",
      "fc layer 2 self.abs_max_out: 1742.0\n",
      "lif layer 2 self.abs_max_v: 3214.5\n",
      "fc layer 3 self.abs_max_out: 701.0\n",
      "fc layer 1 self.abs_max_out: 1560.0\n",
      "lif layer 1 self.abs_max_v: 2266.5\n",
      "lif layer 1 self.abs_max_v: 2382.5\n",
      "lif layer 2 self.abs_max_v: 3234.0\n",
      "fc layer 2 self.abs_max_out: 1802.0\n",
      "lif layer 2 self.abs_max_v: 3294.0\n",
      "lif layer 2 self.abs_max_v: 3330.0\n",
      "fc layer 2 self.abs_max_out: 1809.0\n",
      "fc layer 2 self.abs_max_out: 1815.0\n",
      "fc layer 2 self.abs_max_out: 1837.0\n",
      "fc layer 2 self.abs_max_out: 1880.0\n",
      "fc layer 1 self.abs_max_out: 1571.0\n",
      "fc layer 2 self.abs_max_out: 1897.0\n",
      "lif layer 2 self.abs_max_v: 3405.0\n",
      "lif layer 2 self.abs_max_v: 3405.5\n",
      "lif layer 2 self.abs_max_v: 3538.0\n",
      "lif layer 1 self.abs_max_v: 2581.5\n",
      "fc layer 1 self.abs_max_out: 1729.0\n",
      "lif layer 1 self.abs_max_v: 2841.5\n",
      "fc layer 1 self.abs_max_out: 1958.0\n",
      "lif layer 1 self.abs_max_v: 2882.5\n",
      "lif layer 1 self.abs_max_v: 3166.0\n",
      "fc layer 3 self.abs_max_out: 736.0\n",
      "lif layer 1 self.abs_max_v: 3294.5\n",
      "lif layer 1 self.abs_max_v: 3504.5\n",
      "lif layer 1 self.abs_max_v: 3536.5\n",
      "fc layer 1 self.abs_max_out: 2192.0\n",
      "lif layer 1 self.abs_max_v: 3614.0\n",
      "lif layer 1 self.abs_max_v: 3620.0\n",
      "lif layer 1 self.abs_max_v: 3694.5\n",
      "fc layer 1 self.abs_max_out: 2466.0\n",
      "fc layer 1 self.abs_max_out: 2607.0\n",
      "lif layer 1 self.abs_max_v: 4219.5\n",
      "lif layer 1 self.abs_max_v: 4475.0\n",
      "lif layer 1 self.abs_max_v: 4597.0\n",
      "lif layer 1 self.abs_max_v: 4614.0\n",
      "fc layer 3 self.abs_max_out: 745.0\n",
      "fc layer 3 self.abs_max_out: 750.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  1.809545/  1.980043, val:  35.00%, val_best:  35.00%, tr:  96.12%, tr_best:  96.12%, epoch time: 70.34 seconds, 1.17 minutes\n",
      "total_backward_count 9790 real_backward_count 2237  22.850%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 2613.0\n",
      "lif layer 1 self.abs_max_v: 4664.0\n",
      "lif layer 1 self.abs_max_v: 4689.0\n",
      "lif layer 1 self.abs_max_v: 4774.5\n",
      "fc layer 1 self.abs_max_out: 2661.0\n",
      "fc layer 1 self.abs_max_out: 2738.0\n",
      "fc layer 1 self.abs_max_out: 2753.0\n",
      "lif layer 1 self.abs_max_v: 4815.5\n",
      "lif layer 1 self.abs_max_v: 4991.0\n",
      "fc layer 1 self.abs_max_out: 2772.0\n",
      "lif layer 1 self.abs_max_v: 5267.5\n",
      "lif layer 1 self.abs_max_v: 5346.0\n",
      "fc layer 1 self.abs_max_out: 3056.0\n",
      "lif layer 1 self.abs_max_v: 5729.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  1.758507/  1.956940, val:  42.08%, val_best:  42.08%, tr:  99.39%, tr_best:  99.39%, epoch time: 70.53 seconds, 1.18 minutes\n",
      "total_backward_count 19580 real_backward_count 3848  19.653%\n",
      "fc layer 2 self.abs_max_out: 1914.0\n",
      "fc layer 3 self.abs_max_out: 762.0\n",
      "fc layer 2 self.abs_max_out: 1953.0\n",
      "fc layer 2 self.abs_max_out: 1991.0\n",
      "fc layer 1 self.abs_max_out: 3141.0\n",
      "fc layer 1 self.abs_max_out: 3219.0\n",
      "lif layer 1 self.abs_max_v: 5893.5\n",
      "lif layer 1 self.abs_max_v: 5927.5\n",
      "lif layer 1 self.abs_max_v: 6173.0\n",
      "lif layer 1 self.abs_max_v: 6200.5\n",
      "fc layer 2 self.abs_max_out: 2179.0\n",
      "lif layer 2 self.abs_max_v: 3636.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  1.759753/  1.954504, val:  44.58%, val_best:  44.58%, tr:  99.39%, tr_best:  99.39%, epoch time: 69.35 seconds, 1.16 minutes\n",
      "total_backward_count 29370 real_backward_count 5319  18.110%\n",
      "fc layer 3 self.abs_max_out: 774.0\n",
      "fc layer 3 self.abs_max_out: 804.0\n",
      "lif layer 2 self.abs_max_v: 3663.5\n",
      "lif layer 2 self.abs_max_v: 3673.5\n",
      "lif layer 2 self.abs_max_v: 3685.5\n",
      "fc layer 1 self.abs_max_out: 3229.0\n",
      "fc layer 1 self.abs_max_out: 3366.0\n",
      "lif layer 1 self.abs_max_v: 6234.0\n",
      "fc layer 1 self.abs_max_out: 3396.0\n",
      "lif layer 1 self.abs_max_v: 6513.0\n",
      "lif layer 1 self.abs_max_v: 6582.5\n",
      "lif layer 2 self.abs_max_v: 3748.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  1.734635/  1.920115, val:  46.25%, val_best:  46.25%, tr:  99.49%, tr_best:  99.49%, epoch time: 69.11 seconds, 1.15 minutes\n",
      "total_backward_count 39160 real_backward_count 6623  16.913%\n",
      "fc layer 3 self.abs_max_out: 810.0\n",
      "fc layer 3 self.abs_max_out: 826.0\n",
      "fc layer 3 self.abs_max_out: 880.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  1.716580/  1.904201, val:  40.42%, val_best:  46.25%, tr:  99.39%, tr_best:  99.49%, epoch time: 70.63 seconds, 1.18 minutes\n",
      "total_backward_count 48950 real_backward_count 7863  16.063%\n",
      "fc layer 2 self.abs_max_out: 2210.0\n",
      "lif layer 2 self.abs_max_v: 3783.5\n",
      "lif layer 2 self.abs_max_v: 3922.0\n",
      "lif layer 2 self.abs_max_v: 3939.0\n",
      "fc layer 1 self.abs_max_out: 3440.0\n",
      "fc layer 1 self.abs_max_out: 3485.0\n",
      "fc layer 1 self.abs_max_out: 3688.0\n",
      "lif layer 1 self.abs_max_v: 6690.0\n",
      "lif layer 1 self.abs_max_v: 6710.0\n",
      "lif layer 1 self.abs_max_v: 6985.0\n",
      "lif layer 1 self.abs_max_v: 7090.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.731752/  1.893950, val:  57.92%, val_best:  57.92%, tr:  99.59%, tr_best:  99.59%, epoch time: 69.35 seconds, 1.16 minutes\n",
      "total_backward_count 58740 real_backward_count 9041  15.392%\n",
      "fc layer 2 self.abs_max_out: 2335.0\n",
      "lif layer 2 self.abs_max_v: 4067.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.712702/  1.909915, val:  45.42%, val_best:  57.92%, tr:  99.59%, tr_best:  99.59%, epoch time: 70.30 seconds, 1.17 minutes\n",
      "total_backward_count 68530 real_backward_count 10182  14.858%\n",
      "fc layer 2 self.abs_max_out: 2545.0\n",
      "lif layer 2 self.abs_max_v: 4100.5\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.727842/  1.923161, val:  48.75%, val_best:  57.92%, tr:  99.69%, tr_best:  99.69%, epoch time: 69.86 seconds, 1.16 minutes\n",
      "total_backward_count 78320 real_backward_count 11371  14.519%\n",
      "fc layer 2 self.abs_max_out: 2548.0\n",
      "lif layer 2 self.abs_max_v: 4211.5\n",
      "fc layer 2 self.abs_max_out: 2738.0\n",
      "lif layer 2 self.abs_max_v: 4565.0\n",
      "fc layer 1 self.abs_max_out: 3809.0\n",
      "lif layer 1 self.abs_max_v: 7197.0\n",
      "lif layer 1 self.abs_max_v: 7247.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.730839/  1.901108, val:  57.50%, val_best:  57.92%, tr:  99.39%, tr_best:  99.69%, epoch time: 70.68 seconds, 1.18 minutes\n",
      "total_backward_count 88110 real_backward_count 12524  14.214%\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.712554/  1.877622, val:  67.50%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.27 seconds, 1.17 minutes\n",
      "total_backward_count 97900 real_backward_count 13659  13.952%\n",
      "fc layer 1 self.abs_max_out: 3880.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.714571/  1.893700, val:  56.67%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 70.45 seconds, 1.17 minutes\n",
      "total_backward_count 107690 real_backward_count 14763  13.709%\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.715518/  1.914628, val:  47.50%, val_best:  67.50%, tr:  99.69%, tr_best: 100.00%, epoch time: 69.98 seconds, 1.17 minutes\n",
      "total_backward_count 117480 real_backward_count 15824  13.470%\n",
      "fc layer 2 self.abs_max_out: 2753.0\n",
      "fc layer 1 self.abs_max_out: 4018.0\n",
      "lif layer 1 self.abs_max_v: 7299.5\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.718796/  1.893577, val:  58.75%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.44 seconds, 1.17 minutes\n",
      "total_backward_count 127270 real_backward_count 16920  13.295%\n",
      "fc layer 2 self.abs_max_out: 2770.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.703399/  1.887302, val:  58.75%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.33 seconds, 1.17 minutes\n",
      "total_backward_count 137060 real_backward_count 17967  13.109%\n",
      "lif layer 2 self.abs_max_v: 4578.0\n",
      "lif layer 2 self.abs_max_v: 4765.0\n",
      "lif layer 1 self.abs_max_v: 7391.0\n",
      "lif layer 1 self.abs_max_v: 7426.5\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.696025/  1.852503, val:  65.42%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.34 seconds, 1.17 minutes\n",
      "total_backward_count 146850 real_backward_count 19073  12.988%\n",
      "fc layer 1 self.abs_max_out: 4171.0\n",
      "lif layer 1 self.abs_max_v: 7736.0\n",
      "lif layer 1 self.abs_max_v: 7774.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.692018/  1.880663, val:  64.17%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.71 seconds, 1.18 minutes\n",
      "total_backward_count 156640 real_backward_count 20149  12.863%\n",
      "fc layer 1 self.abs_max_out: 4197.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.713658/  1.873974, val:  61.25%, val_best:  67.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.76 seconds, 1.18 minutes\n",
      "total_backward_count 166430 real_backward_count 21223  12.752%\n",
      "lif layer 1 self.abs_max_v: 7797.5\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.706459/  1.859634, val:  56.25%, val_best:  67.50%, tr:  99.49%, tr_best: 100.00%, epoch time: 70.41 seconds, 1.17 minutes\n",
      "total_backward_count 176220 real_backward_count 22255  12.629%\n",
      "lif layer 1 self.abs_max_v: 7817.5\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.693160/  1.866475, val:  53.33%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.60 seconds, 1.18 minutes\n",
      "total_backward_count 186010 real_backward_count 23285  12.518%\n",
      "lif layer 1 self.abs_max_v: 7888.5\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.690193/  1.866826, val:  57.92%, val_best:  67.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.05 seconds, 1.17 minutes\n",
      "total_backward_count 195800 real_backward_count 24238  12.379%\n",
      "fc layer 1 self.abs_max_out: 4265.0\n",
      "lif layer 1 self.abs_max_v: 7920.5\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.693498/  1.838909, val:  68.33%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.38 seconds, 1.17 minutes\n",
      "total_backward_count 205590 real_backward_count 25162  12.239%\n",
      "fc layer 1 self.abs_max_out: 4493.0\n",
      "lif layer 1 self.abs_max_v: 7932.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.667652/  1.848317, val:  62.92%, val_best:  68.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.56 seconds, 1.18 minutes\n",
      "total_backward_count 215380 real_backward_count 26131  12.133%\n",
      "lif layer 1 self.abs_max_v: 7940.5\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.675244/  1.842396, val:  68.33%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.42 seconds, 1.17 minutes\n",
      "total_backward_count 225170 real_backward_count 27088  12.030%\n",
      "fc layer 1 self.abs_max_out: 4653.0\n",
      "lif layer 1 self.abs_max_v: 8074.0\n",
      "lif layer 1 self.abs_max_v: 8108.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.681281/  1.860073, val:  56.67%, val_best:  68.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 69.98 seconds, 1.17 minutes\n",
      "total_backward_count 234960 real_backward_count 28035  11.932%\n",
      "lif layer 2 self.abs_max_v: 4905.0\n",
      "lif layer 2 self.abs_max_v: 4949.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.695073/  1.852090, val:  62.08%, val_best:  68.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 69.83 seconds, 1.16 minutes\n",
      "total_backward_count 244750 real_backward_count 28940  11.824%\n",
      "lif layer 1 self.abs_max_v: 8537.5\n",
      "lif layer 1 self.abs_max_v: 8661.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.691151/  1.835392, val:  75.00%, val_best:  75.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.57 seconds, 1.18 minutes\n",
      "total_backward_count 254540 real_backward_count 29860  11.731%\n",
      "lif layer 1 self.abs_max_v: 8776.5\n",
      "lif layer 1 self.abs_max_v: 8888.5\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.672069/  1.847807, val:  70.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.04 seconds, 1.17 minutes\n",
      "total_backward_count 264330 real_backward_count 30732  11.626%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.683656/  1.851831, val:  65.00%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.41 seconds, 1.17 minutes\n",
      "total_backward_count 274120 real_backward_count 31638  11.542%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.673146/  1.830928, val:  76.25%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 70.23 seconds, 1.17 minutes\n",
      "total_backward_count 283910 real_backward_count 32524  11.456%\n",
      "fc layer 2 self.abs_max_out: 2776.0\n",
      "fc layer 1 self.abs_max_out: 4673.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.661198/  1.828997, val:  69.58%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.44 seconds, 1.17 minutes\n",
      "total_backward_count 293700 real_backward_count 33410  11.376%\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.672242/  1.828119, val:  72.50%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 69.82 seconds, 1.16 minutes\n",
      "total_backward_count 303490 real_backward_count 34271  11.292%\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.663032/  1.830182, val:  74.17%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.66 seconds, 1.18 minutes\n",
      "total_backward_count 313280 real_backward_count 35099  11.204%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.658652/  1.804553, val:  77.08%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 69.88 seconds, 1.16 minutes\n",
      "total_backward_count 323070 real_backward_count 35973  11.135%\n",
      "fc layer 1 self.abs_max_out: 4705.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.636514/  1.804460, val:  60.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.73 seconds, 1.18 minutes\n",
      "total_backward_count 332860 real_backward_count 36793  11.054%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.642161/  1.801205, val:  70.00%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.49 seconds, 1.17 minutes\n",
      "total_backward_count 342650 real_backward_count 37579  10.967%\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.638557/  1.805002, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.27 seconds, 1.17 minutes\n",
      "total_backward_count 352440 real_backward_count 38366  10.886%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.645404/  1.820757, val:  64.58%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.59 seconds, 1.18 minutes\n",
      "total_backward_count 362230 real_backward_count 39126  10.801%\n",
      "fc layer 1 self.abs_max_out: 4842.0\n",
      "fc layer 1 self.abs_max_out: 4935.0\n",
      "fc layer 1 self.abs_max_out: 5125.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.648393/  1.821023, val:  65.00%, val_best:  77.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.67 seconds, 1.18 minutes\n",
      "total_backward_count 372020 real_backward_count 39900  10.725%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.646720/  1.794212, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.96 seconds, 1.17 minutes\n",
      "total_backward_count 381810 real_backward_count 40652  10.647%\n",
      "fc layer 3 self.abs_max_out: 893.0\n",
      "fc layer 1 self.abs_max_out: 5159.0\n",
      "fc layer 1 self.abs_max_out: 5314.0\n",
      "fc layer 1 self.abs_max_out: 5511.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.634747/  1.800663, val:  68.75%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.78 seconds, 1.18 minutes\n",
      "total_backward_count 391600 real_backward_count 41411  10.575%\n",
      "fc layer 1 self.abs_max_out: 5560.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.627133/  1.793471, val:  77.08%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.30 seconds, 1.17 minutes\n",
      "total_backward_count 401390 real_backward_count 42105  10.490%\n",
      "fc layer 1 self.abs_max_out: 5635.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.634833/  1.805721, val:  79.58%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.86 seconds, 1.18 minutes\n",
      "total_backward_count 411180 real_backward_count 42809  10.411%\n",
      "fc layer 1 self.abs_max_out: 5715.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.637217/  1.793361, val:  75.83%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.31 seconds, 1.17 minutes\n",
      "total_backward_count 420970 real_backward_count 43523  10.339%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.637385/  1.782627, val:  76.67%, val_best:  79.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.75 seconds, 1.16 minutes\n",
      "total_backward_count 430760 real_backward_count 44242  10.271%\n",
      "fc layer 1 self.abs_max_out: 5767.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.629703/  1.794939, val:  73.33%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.20 seconds, 1.17 minutes\n",
      "total_backward_count 440550 real_backward_count 44982  10.210%\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.642803/  1.829479, val:  66.25%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 71.36 seconds, 1.19 minutes\n",
      "total_backward_count 450340 real_backward_count 45688  10.145%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.642693/  1.795242, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.54 seconds, 1.18 minutes\n",
      "total_backward_count 460130 real_backward_count 46391  10.082%\n",
      "fc layer 1 self.abs_max_out: 5784.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.641160/  1.776006, val:  78.33%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.56 seconds, 1.18 minutes\n",
      "total_backward_count 469920 real_backward_count 47071  10.017%\n",
      "fc layer 1 self.abs_max_out: 5827.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.631493/  1.802064, val:  71.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.41 seconds, 1.17 minutes\n",
      "total_backward_count 479710 real_backward_count 47734   9.951%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.629372/  1.772392, val:  78.75%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.40 seconds, 1.17 minutes\n",
      "total_backward_count 489500 real_backward_count 48400   9.888%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.630235/  1.811656, val:  69.17%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.65 seconds, 1.18 minutes\n",
      "total_backward_count 499290 real_backward_count 49069   9.828%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.631532/  1.799361, val:  65.42%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.16 seconds, 1.17 minutes\n",
      "total_backward_count 509080 real_backward_count 49768   9.776%\n",
      "fc layer 1 self.abs_max_out: 5859.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.615033/  1.777694, val:  75.00%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.06 seconds, 1.17 minutes\n",
      "total_backward_count 518870 real_backward_count 50374   9.708%\n",
      "fc layer 3 self.abs_max_out: 913.0\n",
      "fc layer 3 self.abs_max_out: 917.0\n",
      "fc layer 1 self.abs_max_out: 5918.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.620696/  1.787911, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.21 seconds, 1.17 minutes\n",
      "total_backward_count 528660 real_backward_count 51000   9.647%\n",
      "fc layer 2 self.abs_max_out: 2873.0\n",
      "fc layer 1 self.abs_max_out: 6107.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.620847/  1.772948, val:  79.17%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.56 seconds, 1.18 minutes\n",
      "total_backward_count 538450 real_backward_count 51616   9.586%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.619166/  1.801119, val:  63.75%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.43 seconds, 1.16 minutes\n",
      "total_backward_count 548240 real_backward_count 52234   9.528%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.610512/  1.778597, val:  79.58%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.28 seconds, 1.17 minutes\n",
      "total_backward_count 558030 real_backward_count 52852   9.471%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.600303/  1.759473, val:  80.42%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.30 seconds, 1.17 minutes\n",
      "total_backward_count 567820 real_backward_count 53466   9.416%\n",
      "lif layer 1 self.abs_max_v: 9185.5\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.603339/  1.769804, val:  81.25%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.71 seconds, 1.18 minutes\n",
      "total_backward_count 577610 real_backward_count 54020   9.352%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.602804/  1.759674, val:  79.58%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.41 seconds, 1.17 minutes\n",
      "total_backward_count 587400 real_backward_count 54596   9.295%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.610117/  1.785010, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.90 seconds, 1.16 minutes\n",
      "total_backward_count 597190 real_backward_count 55174   9.239%\n",
      "fc layer 3 self.abs_max_out: 927.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.610220/  1.782162, val:  67.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.37 seconds, 1.17 minutes\n",
      "total_backward_count 606980 real_backward_count 55747   9.184%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.613470/  1.756886, val:  81.67%, val_best:  83.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.23 seconds, 1.17 minutes\n",
      "total_backward_count 616770 real_backward_count 56283   9.125%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.618522/  1.782003, val:  75.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.19 seconds, 1.19 minutes\n",
      "total_backward_count 626560 real_backward_count 56846   9.073%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.605224/  1.748651, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.82 seconds, 1.18 minutes\n",
      "total_backward_count 636350 real_backward_count 57355   9.013%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.604987/  1.765500, val:  75.83%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.11 seconds, 1.17 minutes\n",
      "total_backward_count 646140 real_backward_count 57920   8.964%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.606660/  1.731289, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.07 seconds, 1.17 minutes\n",
      "total_backward_count 655930 real_backward_count 58454   8.912%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.588416/  1.744417, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.66 seconds, 1.16 minutes\n",
      "total_backward_count 665720 real_backward_count 59031   8.867%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.584935/  1.737027, val:  80.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.39 seconds, 1.17 minutes\n",
      "total_backward_count 675510 real_backward_count 59508   8.809%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.592267/  1.762344, val:  75.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.65 seconds, 1.18 minutes\n",
      "total_backward_count 685300 real_backward_count 60088   8.768%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.582486/  1.735363, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.93 seconds, 1.17 minutes\n",
      "total_backward_count 695090 real_backward_count 60631   8.723%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.580850/  1.746035, val:  79.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.48 seconds, 1.17 minutes\n",
      "total_backward_count 704880 real_backward_count 61140   8.674%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.585395/  1.747790, val:  78.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.18 seconds, 1.17 minutes\n",
      "total_backward_count 714670 real_backward_count 61689   8.632%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.578834/  1.754391, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.30 seconds, 1.17 minutes\n",
      "total_backward_count 724460 real_backward_count 62141   8.578%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.591767/  1.746490, val:  79.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.28 seconds, 1.17 minutes\n",
      "total_backward_count 734250 real_backward_count 62634   8.530%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.584713/  1.732980, val:  79.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.28 seconds, 1.17 minutes\n",
      "total_backward_count 744040 real_backward_count 63108   8.482%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.580578/  1.748484, val:  73.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.42 seconds, 1.17 minutes\n",
      "total_backward_count 753830 real_backward_count 63631   8.441%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.578886/  1.711511, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.42 seconds, 1.17 minutes\n",
      "total_backward_count 763620 real_backward_count 64102   8.394%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.572676/  1.724585, val:  77.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.86 seconds, 1.18 minutes\n",
      "total_backward_count 773410 real_backward_count 64600   8.353%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.577705/  1.734785, val:  76.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.66 seconds, 1.18 minutes\n",
      "total_backward_count 783200 real_backward_count 65085   8.310%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.578393/  1.721890, val:  85.42%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.58 seconds, 1.18 minutes\n",
      "total_backward_count 792990 real_backward_count 65550   8.266%\n",
      "fc layer 2 self.abs_max_out: 2889.0\n",
      "lif layer 1 self.abs_max_v: 9323.5\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.568338/  1.721470, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.35 seconds, 1.17 minutes\n",
      "total_backward_count 802780 real_backward_count 65993   8.221%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.562837/  1.725328, val:  80.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.09 seconds, 1.18 minutes\n",
      "total_backward_count 812570 real_backward_count 66433   8.176%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.571046/  1.716123, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.37 seconds, 1.17 minutes\n",
      "total_backward_count 822360 real_backward_count 66906   8.136%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.568734/  1.736244, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.29 seconds, 1.17 minutes\n",
      "total_backward_count 832150 real_backward_count 67337   8.092%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.564356/  1.722818, val:  86.67%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.81 seconds, 1.18 minutes\n",
      "total_backward_count 841940 real_backward_count 67797   8.052%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.566840/  1.718643, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.18 seconds, 1.17 minutes\n",
      "total_backward_count 851730 real_backward_count 68239   8.012%\n",
      "lif layer 1 self.abs_max_v: 9530.5\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.563584/  1.719346, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.96 seconds, 1.17 minutes\n",
      "total_backward_count 861520 real_backward_count 68671   7.971%\n",
      "lif layer 1 self.abs_max_v: 9700.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.557393/  1.700893, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.04 seconds, 1.17 minutes\n",
      "total_backward_count 871310 real_backward_count 69104   7.931%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.553654/  1.720088, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.73 seconds, 1.16 minutes\n",
      "total_backward_count 881100 real_backward_count 69517   7.890%\n",
      "fc layer 3 self.abs_max_out: 952.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.554139/  1.732443, val:  75.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.80 seconds, 1.18 minutes\n",
      "total_backward_count 890890 real_backward_count 69907   7.847%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.552209/  1.713167, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.86 seconds, 1.18 minutes\n",
      "total_backward_count 900680 real_backward_count 70354   7.811%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.541812/  1.721986, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.66 seconds, 1.18 minutes\n",
      "total_backward_count 910470 real_backward_count 70766   7.772%\n",
      "fc layer 3 self.abs_max_out: 953.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.553413/  1.726411, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.67 seconds, 1.18 minutes\n",
      "total_backward_count 920260 real_backward_count 71166   7.733%\n",
      "fc layer 3 self.abs_max_out: 958.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.540284/  1.712715, val:  77.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.20 seconds, 1.19 minutes\n",
      "total_backward_count 930050 real_backward_count 71583   7.697%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.534425/  1.699082, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.04 seconds, 1.17 minutes\n",
      "total_backward_count 939840 real_backward_count 71995   7.660%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.530573/  1.692719, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.83 seconds, 1.18 minutes\n",
      "total_backward_count 949630 real_backward_count 72397   7.624%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.529621/  1.692157, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.36 seconds, 1.17 minutes\n",
      "total_backward_count 959420 real_backward_count 72795   7.587%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.533944/  1.681795, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.28 seconds, 1.17 minutes\n",
      "total_backward_count 969210 real_backward_count 73205   7.553%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.521725/  1.675679, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.69 seconds, 1.18 minutes\n",
      "total_backward_count 979000 real_backward_count 73628   7.521%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.533841/  1.693625, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.56 seconds, 1.18 minutes\n",
      "total_backward_count 988790 real_backward_count 73953   7.479%\n",
      "lif layer 1 self.abs_max_v: 9727.5\n",
      "lif layer 1 self.abs_max_v: 10116.0\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.521424/  1.694652, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.38 seconds, 1.19 minutes\n",
      "total_backward_count 998580 real_backward_count 74325   7.443%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.523158/  1.682692, val:  81.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.85 seconds, 1.18 minutes\n",
      "total_backward_count 1008370 real_backward_count 74632   7.401%\n",
      "lif layer 1 self.abs_max_v: 10149.5\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.517347/  1.701268, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.70 seconds, 1.16 minutes\n",
      "total_backward_count 1018160 real_backward_count 74991   7.365%\n",
      "lif layer 1 self.abs_max_v: 10189.0\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.528494/  1.691526, val:  85.42%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.55 seconds, 1.18 minutes\n",
      "total_backward_count 1027950 real_backward_count 75367   7.332%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.534929/  1.697374, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.97 seconds, 1.18 minutes\n",
      "total_backward_count 1037740 real_backward_count 75729   7.297%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.529579/  1.693911, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.79 seconds, 1.16 minutes\n",
      "total_backward_count 1047530 real_backward_count 76080   7.263%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.528455/  1.705800, val:  81.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.67 seconds, 1.16 minutes\n",
      "total_backward_count 1057320 real_backward_count 76459   7.231%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.522047/  1.678110, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.50 seconds, 1.17 minutes\n",
      "total_backward_count 1067110 real_backward_count 76812   7.198%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.519599/  1.673441, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.13 seconds, 1.17 minutes\n",
      "total_backward_count 1076900 real_backward_count 77175   7.166%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.524063/  1.706173, val:  72.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.58 seconds, 1.16 minutes\n",
      "total_backward_count 1086690 real_backward_count 77529   7.134%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.533214/  1.676131, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.49 seconds, 1.16 minutes\n",
      "total_backward_count 1096480 real_backward_count 77897   7.104%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.514395/  1.686631, val:  80.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.13 seconds, 1.12 minutes\n",
      "total_backward_count 1106270 real_backward_count 78258   7.074%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.516590/  1.665417, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.82 seconds, 1.16 minutes\n",
      "total_backward_count 1116060 real_backward_count 78627   7.045%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.484801/  1.648301, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.10 seconds, 1.17 minutes\n",
      "total_backward_count 1125850 real_backward_count 78935   7.011%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.483552/  1.655950, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.71 seconds, 1.18 minutes\n",
      "total_backward_count 1135640 real_backward_count 79256   6.979%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.489870/  1.653260, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.13 seconds, 1.17 minutes\n",
      "total_backward_count 1145430 real_backward_count 79541   6.944%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.486077/  1.655156, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.68 seconds, 1.16 minutes\n",
      "total_backward_count 1155220 real_backward_count 79887   6.915%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.482357/  1.634321, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.75 seconds, 1.16 minutes\n",
      "total_backward_count 1165010 real_backward_count 80236   6.887%\n",
      "fc layer 3 self.abs_max_out: 982.0\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.479414/  1.649083, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.84 seconds, 1.16 minutes\n",
      "total_backward_count 1174800 real_backward_count 80580   6.859%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.481703/  1.645388, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.96 seconds, 1.18 minutes\n",
      "total_backward_count 1184590 real_backward_count 80902   6.830%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.491321/  1.674846, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.81 seconds, 1.16 minutes\n",
      "total_backward_count 1194380 real_backward_count 81237   6.802%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.493335/  1.655474, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.16 seconds, 1.17 minutes\n",
      "total_backward_count 1204170 real_backward_count 81558   6.773%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.485098/  1.683607, val:  80.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.99 seconds, 1.17 minutes\n",
      "total_backward_count 1213960 real_backward_count 81887   6.745%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.491997/  1.672250, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.50 seconds, 1.16 minutes\n",
      "total_backward_count 1223750 real_backward_count 82195   6.717%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.492410/  1.674170, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.46 seconds, 1.17 minutes\n",
      "total_backward_count 1233540 real_backward_count 82506   6.689%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.492852/  1.668856, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.15 seconds, 1.17 minutes\n",
      "total_backward_count 1243330 real_backward_count 82805   6.660%\n",
      "fc layer 3 self.abs_max_out: 1006.0\n",
      "lif layer 1 self.abs_max_v: 10558.5\n",
      "lif layer 1 self.abs_max_v: 11020.5\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.473249/  1.642991, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.61 seconds, 1.16 minutes\n",
      "total_backward_count 1253120 real_backward_count 83121   6.633%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.468375/  1.638167, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.64 seconds, 1.16 minutes\n",
      "total_backward_count 1262910 real_backward_count 83399   6.604%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.471106/  1.645614, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.68 seconds, 1.16 minutes\n",
      "total_backward_count 1272700 real_backward_count 83744   6.580%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.477084/  1.654582, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.33 seconds, 1.16 minutes\n",
      "total_backward_count 1282490 real_backward_count 84066   6.555%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.476424/  1.659914, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.94 seconds, 1.17 minutes\n",
      "total_backward_count 1292280 real_backward_count 84383   6.530%\n",
      "fc layer 2 self.abs_max_out: 3040.0\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.493640/  1.655378, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.59 seconds, 1.16 minutes\n",
      "total_backward_count 1302070 real_backward_count 84699   6.505%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.480551/  1.635745, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.60 seconds, 1.16 minutes\n",
      "total_backward_count 1311860 real_backward_count 85020   6.481%\n",
      "fc layer 3 self.abs_max_out: 1036.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.462652/  1.637656, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.84 seconds, 1.16 minutes\n",
      "total_backward_count 1321650 real_backward_count 85295   6.454%\n",
      "lif layer 1 self.abs_max_v: 11298.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.460170/  1.636794, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.71 seconds, 1.16 minutes\n",
      "total_backward_count 1331440 real_backward_count 85605   6.430%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.459439/  1.620664, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.73 seconds, 1.16 minutes\n",
      "total_backward_count 1341230 real_backward_count 85873   6.403%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.458910/  1.636189, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.19 seconds, 1.17 minutes\n",
      "total_backward_count 1351020 real_backward_count 86154   6.377%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.451794/  1.621451, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.14 seconds, 1.17 minutes\n",
      "total_backward_count 1360810 real_backward_count 86447   6.353%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.449126/  1.631564, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.67 seconds, 1.16 minutes\n",
      "total_backward_count 1370600 real_backward_count 86749   6.329%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.453449/  1.627822, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.15 seconds, 1.17 minutes\n",
      "total_backward_count 1380390 real_backward_count 87025   6.304%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.449595/  1.618594, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.22 seconds, 1.17 minutes\n",
      "total_backward_count 1390180 real_backward_count 87289   6.279%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.437938/  1.628835, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.43 seconds, 1.17 minutes\n",
      "total_backward_count 1399970 real_backward_count 87592   6.257%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.439951/  1.615854, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.16 seconds, 1.17 minutes\n",
      "total_backward_count 1409760 real_backward_count 87857   6.232%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.430280/  1.604371, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.85 seconds, 1.16 minutes\n",
      "total_backward_count 1419550 real_backward_count 88102   6.206%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.431076/  1.608295, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.82 seconds, 1.16 minutes\n",
      "total_backward_count 1429340 real_backward_count 88342   6.181%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.446064/  1.643666, val:  81.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.51 seconds, 1.18 minutes\n",
      "total_backward_count 1439130 real_backward_count 88597   6.156%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.438055/  1.620861, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.43 seconds, 1.17 minutes\n",
      "total_backward_count 1448920 real_backward_count 88861   6.133%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.440521/  1.627457, val:  80.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.82 seconds, 1.16 minutes\n",
      "total_backward_count 1458710 real_backward_count 89134   6.110%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.434064/  1.624719, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.61 seconds, 1.16 minutes\n",
      "total_backward_count 1468500 real_backward_count 89379   6.086%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.422674/  1.630189, val:  81.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.53 seconds, 1.18 minutes\n",
      "total_backward_count 1478290 real_backward_count 89627   6.063%\n",
      "lif layer 1 self.abs_max_v: 11400.5\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.435705/  1.617569, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.82 seconds, 1.16 minutes\n",
      "total_backward_count 1488080 real_backward_count 89888   6.041%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.431715/  1.610059, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.85 seconds, 1.16 minutes\n",
      "total_backward_count 1497870 real_backward_count 90155   6.019%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.422566/  1.612277, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.01 seconds, 1.17 minutes\n",
      "total_backward_count 1507660 real_backward_count 90416   5.997%\n",
      "lif layer 1 self.abs_max_v: 11456.0\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.414546/  1.603396, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.20 seconds, 1.17 minutes\n",
      "total_backward_count 1517450 real_backward_count 90670   5.975%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.419824/  1.607440, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.90 seconds, 1.16 minutes\n",
      "total_backward_count 1527240 real_backward_count 90917   5.953%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.418622/  1.610201, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.01 seconds, 1.17 minutes\n",
      "total_backward_count 1537030 real_backward_count 91151   5.930%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.430293/  1.614176, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.65 seconds, 1.16 minutes\n",
      "total_backward_count 1546820 real_backward_count 91372   5.907%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.427863/  1.612771, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.18 seconds, 1.17 minutes\n",
      "total_backward_count 1556610 real_backward_count 91608   5.885%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.425174/  1.600861, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.83 seconds, 1.16 minutes\n",
      "total_backward_count 1566400 real_backward_count 91864   5.865%\n",
      "fc layer 1 self.abs_max_out: 6150.0\n",
      "fc layer 1 self.abs_max_out: 6227.0\n",
      "lif layer 1 self.abs_max_v: 11953.5\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.421918/  1.605365, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.84 seconds, 1.16 minutes\n",
      "total_backward_count 1576190 real_backward_count 92091   5.843%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.422699/  1.600438, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.49 seconds, 1.16 minutes\n",
      "total_backward_count 1585980 real_backward_count 92310   5.820%\n",
      "fc layer 1 self.abs_max_out: 6251.0\n",
      "lif layer 1 self.abs_max_v: 11994.0\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.437919/  1.613841, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.59 seconds, 1.16 minutes\n",
      "total_backward_count 1595770 real_backward_count 92548   5.800%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.437464/  1.615865, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.86 seconds, 1.16 minutes\n",
      "total_backward_count 1605560 real_backward_count 92809   5.780%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.440143/  1.604563, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.71 seconds, 1.16 minutes\n",
      "total_backward_count 1615350 real_backward_count 93064   5.761%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.439535/  1.607774, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.80 seconds, 1.16 minutes\n",
      "total_backward_count 1625140 real_backward_count 93345   5.744%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.442659/  1.619367, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.31 seconds, 1.17 minutes\n",
      "total_backward_count 1634930 real_backward_count 93577   5.724%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'random', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        # \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.0625]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [10]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/512]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        # \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        # \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_1w + 1,wandb.config.scale_exp_1w + 1]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = '7yr7ufe8'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
