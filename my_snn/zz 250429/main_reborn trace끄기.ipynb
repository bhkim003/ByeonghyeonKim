{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1052/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True and trace_on == True\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels, IMAGE_SIZE, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #     criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8f765663f24023bb80c077f9074841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113583876027, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250429_195238-1c0ccbmj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1c0ccbmj' target=\"_blank\">eager-glade-6834</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1c0ccbmj' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/1c0ccbmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0.0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.720291189014991, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 3.555718888923306, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': ['M', 'M', 200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_main.pth', 'learning_rate': 0.001, 'epoch_num': 10000, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 100000, 'DFA_on': False, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1.0, 'bias': True, 'last_lif': False, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ffa516e60c3efd5e0208f72b4c36cb84\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): DimChanger_for_FC()\n",
      "      (3): SYNAPSE_FC(in_features=2048, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0.0, v_decay=0.25, v_threshold=0.720291189014991, v_reset=10000, sg_width=3.555718888923306, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (6): LIF_layer(v_init=0.0, v_decay=0.25, v_threshold=0.720291189014991, v_reset=10000, sg_width=3.555718888923306, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 452,010\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.305365/  2.302841, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:   8.17%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.304994/  2.302639, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:   8.17%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.305079/  2.302667, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:   8.99%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.304731/  2.302708, val:  10.00%, val_best:  10.00%, tr:   8.38%, tr_best:   8.99%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  2.304833/  2.302682, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:   8.99%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  2.304569/  2.302663, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:   8.99%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  2.305086/  2.302687, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:   9.70%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  2.304632/  2.302694, val:  10.00%, val_best:  10.00%, tr:   7.46%, tr_best:   9.70%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  2.304545/  2.302613, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:   9.70%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  2.305080/  2.302782, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:   9.70%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  2.304688/  2.302772, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  2.304834/  2.302622, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  10.01%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  2.304284/  2.302666, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.01%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  2.304813/  2.302634, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.01%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  2.305131/  2.302411, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  10.01%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  2.301059/  2.291211, val:  13.75%, val_best:  13.75%, tr:   9.50%, tr_best:  10.01%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  2.246874/  2.214486, val:  21.67%, val_best:  21.67%, tr:  14.71%, tr_best:  14.71%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  2.157224/  2.140454, val:  27.08%, val_best:  27.08%, tr:  24.00%, tr_best:  24.00%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  2.046482/  2.040472, val:  31.25%, val_best:  31.25%, tr:  33.50%, tr_best:  33.50%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  1.906550/  1.928588, val:  39.17%, val_best:  39.17%, tr:  35.24%, tr_best:  35.24%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  1.779065/  1.819623, val:  45.00%, val_best:  45.00%, tr:  42.80%, tr_best:  42.80%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  1.647228/  1.702483, val:  45.00%, val_best:  45.00%, tr:  47.91%, tr_best:  47.91%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  1.539742/  1.636667, val:  47.50%, val_best:  47.50%, tr:  50.36%, tr_best:  50.36%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  1.447676/  1.569222, val:  51.67%, val_best:  51.67%, tr:  52.40%, tr_best:  52.40%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  1.377799/  1.508536, val:  56.67%, val_best:  56.67%, tr:  55.87%, tr_best:  55.87%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  1.319633/  1.453578, val:  56.67%, val_best:  56.67%, tr:  55.77%, tr_best:  55.87%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  1.259101/  1.410303, val:  57.08%, val_best:  57.08%, tr:  59.45%, tr_best:  59.45%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  1.206326/  1.422126, val:  57.08%, val_best:  57.08%, tr:  61.70%, tr_best:  61.70%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  1.170764/  1.393855, val:  58.33%, val_best:  58.33%, tr:  63.13%, tr_best:  63.13%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  1.110889/  1.382864, val:  59.17%, val_best:  59.17%, tr:  64.76%, tr_best:  64.76%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  1.064930/  1.368314, val:  60.42%, val_best:  60.42%, tr:  66.91%, tr_best:  66.91%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  1.029238/  1.379051, val:  59.58%, val_best:  60.42%, tr:  67.62%, tr_best:  67.62%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  1.004348/  1.391749, val:  62.92%, val_best:  62.92%, tr:  69.15%, tr_best:  69.15%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.984327/  1.376553, val:  62.08%, val_best:  62.92%, tr:  70.38%, tr_best:  70.38%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.942123/  1.364465, val:  62.50%, val_best:  62.92%, tr:  71.71%, tr_best:  71.71%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.916009/  1.388233, val:  62.92%, val_best:  62.92%, tr:  70.38%, tr_best:  71.71%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.880888/  1.374822, val:  63.33%, val_best:  63.33%, tr:  75.28%, tr_best:  75.28%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.862979/  1.387281, val:  63.33%, val_best:  63.33%, tr:  73.44%, tr_best:  75.28%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.837995/  1.392341, val:  64.17%, val_best:  64.17%, tr:  77.83%, tr_best:  77.83%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.817310/  1.402145, val:  63.75%, val_best:  64.17%, tr:  74.77%, tr_best:  77.83%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.786543/  1.383829, val:  67.50%, val_best:  67.50%, tr:  74.87%, tr_best:  77.83%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.763899/  1.438366, val:  67.08%, val_best:  67.50%, tr:  79.06%, tr_best:  79.06%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.735006/  1.421077, val:  63.75%, val_best:  67.50%, tr:  79.67%, tr_best:  79.67%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.716589/  1.442309, val:  67.08%, val_best:  67.50%, tr:  80.59%, tr_best:  80.59%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.691561/  1.435302, val:  67.08%, val_best:  67.50%, tr:  79.88%, tr_best:  80.59%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.662523/  1.445426, val:  66.67%, val_best:  67.50%, tr:  83.15%, tr_best:  83.15%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.651053/  1.460211, val:  69.58%, val_best:  69.58%, tr:  82.12%, tr_best:  83.15%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.628077/  1.509695, val:  68.33%, val_best:  69.58%, tr:  84.78%, tr_best:  84.78%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.616177/  1.488567, val:  68.33%, val_best:  69.58%, tr:  86.11%, tr_best:  86.11%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.595709/  1.480934, val:  76.25%, val_best:  76.25%, tr:  85.90%, tr_best:  86.11%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.582885/  1.501230, val:  74.58%, val_best:  76.25%, tr:  87.33%, tr_best:  87.33%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.559949/  1.531457, val:  70.00%, val_best:  76.25%, tr:  89.79%, tr_best:  89.79%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.537734/  1.568645, val:  65.83%, val_best:  76.25%, tr:  88.46%, tr_best:  89.79%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.524128/  1.583236, val:  72.50%, val_best:  76.25%, tr:  90.60%, tr_best:  90.60%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.498878/  1.572215, val:  70.83%, val_best:  76.25%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.487043/  1.572023, val:  77.92%, val_best:  77.92%, tr:  90.81%, tr_best:  93.36%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.465037/  1.650933, val:  68.75%, val_best:  77.92%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.455659/  1.600802, val:  76.25%, val_best:  77.92%, tr:  94.69%, tr_best:  95.40%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.430767/  1.647601, val:  68.75%, val_best:  77.92%, tr:  93.56%, tr_best:  95.40%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.414021/  1.648790, val:  72.92%, val_best:  77.92%, tr:  95.81%, tr_best:  95.81%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.392766/  1.667076, val:  74.58%, val_best:  77.92%, tr:  95.61%, tr_best:  95.81%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.376952/  1.713549, val:  69.17%, val_best:  77.92%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.367818/  1.730100, val:  73.75%, val_best:  77.92%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.342953/  1.720965, val:  74.17%, val_best:  77.92%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.341144/  1.770978, val:  72.92%, val_best:  77.92%, tr:  96.63%, tr_best:  98.37%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.313121/  1.803763, val:  75.00%, val_best:  77.92%, tr:  98.47%, tr_best:  98.47%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.301778/  1.837045, val:  73.75%, val_best:  77.92%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.289067/  1.819489, val:  75.42%, val_best:  77.92%, tr:  98.67%, tr_best:  99.39%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.274718/  1.842854, val:  74.58%, val_best:  77.92%, tr:  98.77%, tr_best:  99.39%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.261057/  1.855280, val:  75.42%, val_best:  77.92%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.237833/  1.892429, val:  74.17%, val_best:  77.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.237163/  1.898232, val:  76.25%, val_best:  77.92%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.223867/  1.953255, val:  74.58%, val_best:  77.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.214352/  1.961457, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.197200/  1.963575, val:  76.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.186714/  1.998319, val:  76.25%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.176272/  2.044681, val:  75.42%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.177085/  2.084101, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.168121/  2.067551, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.155393/  2.096540, val:  78.75%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.148840/  2.111411, val:  77.92%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.139930/  2.112511, val:  77.08%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.129695/  2.145187, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.127196/  2.157320, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.119350/  2.167019, val:  77.92%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.112914/  2.190427, val:  80.00%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.106095/  2.232597, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.102832/  2.252678, val:  78.75%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.094414/  2.232775, val:  79.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.087249/  2.263632, val:  77.08%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.088859/  2.286685, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.081102/  2.315728, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.070640/  2.326679, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.068382/  2.363111, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.062927/  2.332861, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.060397/  2.396302, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.054629/  2.402686, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.054549/  2.422150, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.052539/  2.427524, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.048499/  2.460320, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0010000'], tr/val_loss:  0.048337/  2.474601, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0010000'], tr/val_loss:  0.046210/  2.490639, val:  79.17%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0010000'], tr/val_loss:  0.043703/  2.492774, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0010000'], tr/val_loss:  0.044263/  2.522631, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0010000'], tr/val_loss:  0.039139/  2.536856, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0010000'], tr/val_loss:  0.037440/  2.566981, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0010000'], tr/val_loss:  0.037184/  2.533966, val:  78.75%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0010000'], tr/val_loss:  0.036256/  2.576372, val:  78.33%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0010000'], tr/val_loss:  0.036530/  2.585717, val:  80.00%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0010000'], tr/val_loss:  0.033886/  2.603129, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0010000'], tr/val_loss:  0.029403/  2.605687, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0010000'], tr/val_loss:  0.027988/  2.612505, val:  80.42%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0010000'], tr/val_loss:  0.025802/  2.619475, val:  80.83%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0010000'], tr/val_loss:  0.027739/  2.656355, val:  79.58%, val_best:  80.83%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0010000'], tr/val_loss:  0.025069/  2.626835, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0010000'], tr/val_loss:  0.022793/  2.639851, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0010000'], tr/val_loss:  0.023099/  2.646680, val:  80.83%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0010000'], tr/val_loss:  0.022326/  2.652161, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0010000'], tr/val_loss:  0.023502/  2.657084, val:  81.25%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0010000'], tr/val_loss:  0.022524/  2.674578, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0010000'], tr/val_loss:  0.021688/  2.676769, val:  81.25%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0010000'], tr/val_loss:  0.020754/  2.687460, val:  80.83%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0010000'], tr/val_loss:  0.019754/  2.678514, val:  81.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0010000'], tr/val_loss:  0.019028/  2.687661, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0010000'], tr/val_loss:  0.017905/  2.693928, val:  82.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0010000'], tr/val_loss:  0.016173/  2.696233, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0010000'], tr/val_loss:  0.016645/  2.761513, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0010000'], tr/val_loss:  0.017283/  2.737446, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0010000'], tr/val_loss:  0.015077/  2.764356, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0010000'], tr/val_loss:  0.015886/  2.764879, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0010000'], tr/val_loss:  0.015796/  2.769456, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0010000'], tr/val_loss:  0.013152/  2.763759, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0010000'], tr/val_loss:  0.014328/  2.750794, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0010000'], tr/val_loss:  0.013577/  2.774553, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0010000'], tr/val_loss:  0.012867/  2.778715, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0010000'], tr/val_loss:  0.014015/  2.806327, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0010000'], tr/val_loss:  0.012947/  2.795919, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0010000'], tr/val_loss:  0.012685/  2.802842, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0010000'], tr/val_loss:  0.013789/  2.827946, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0010000'], tr/val_loss:  0.012221/  2.812040, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0010000'], tr/val_loss:  0.011139/  2.825524, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0010000'], tr/val_loss:  0.012024/  2.829945, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0010000'], tr/val_loss:  0.010200/  2.831162, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0010000'], tr/val_loss:  0.010620/  2.814115, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0010000'], tr/val_loss:  0.010256/  2.831364, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0010000'], tr/val_loss:  0.009461/  2.830969, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0010000'], tr/val_loss:  0.010709/  2.843766, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0010000'], tr/val_loss:  0.010505/  2.858856, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0010000'], tr/val_loss:  0.011387/  2.860211, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0010000'], tr/val_loss:  0.009776/  2.877971, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0010000'], tr/val_loss:  0.008940/  2.870523, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0010000'], tr/val_loss:  0.008947/  2.912827, val:  79.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0010000'], tr/val_loss:  0.009092/  2.909052, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0010000'], tr/val_loss:  0.007941/  2.905471, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0010000'], tr/val_loss:  0.009147/  2.919741, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0010000'], tr/val_loss:  0.009800/  2.901637, val:  81.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0010000'], tr/val_loss:  0.008089/  2.916532, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0010000'], tr/val_loss:  0.009019/  2.964920, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0010000'], tr/val_loss:  0.009210/  2.937397, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0010000'], tr/val_loss:  0.009979/  2.926639, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0010000'], tr/val_loss:  0.008880/  2.963444, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0010000'], tr/val_loss:  0.008507/  2.951836, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0010000'], tr/val_loss:  0.010780/  2.953554, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0010000'], tr/val_loss:  0.011819/  2.944610, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0010000'], tr/val_loss:  0.009285/  2.952406, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0010000'], tr/val_loss:  0.008005/  2.949431, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0010000'], tr/val_loss:  0.007007/  2.954980, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0010000'], tr/val_loss:  0.006260/  2.977053, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0010000'], tr/val_loss:  0.006506/  2.967524, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0010000'], tr/val_loss:  0.006921/  2.973262, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0010000'], tr/val_loss:  0.006502/  2.968084, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0010000'], tr/val_loss:  0.007188/  2.946402, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0010000'], tr/val_loss:  0.006988/  2.962504, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0010000'], tr/val_loss:  0.006358/  2.970559, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0010000'], tr/val_loss:  0.005556/  2.997624, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0010000'], tr/val_loss:  0.005666/  2.980729, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0010000'], tr/val_loss:  0.005740/  2.994641, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0010000'], tr/val_loss:  0.005172/  2.992623, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0010000'], tr/val_loss:  0.004961/  2.989002, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0010000'], tr/val_loss:  0.004549/  3.007992, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0010000'], tr/val_loss:  0.004728/  2.993139, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0010000'], tr/val_loss:  0.004868/  3.006486, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0010000'], tr/val_loss:  0.004525/  3.008109, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0010000'], tr/val_loss:  0.004386/  3.015358, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0010000'], tr/val_loss:  0.004765/  3.024579, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0010000'], tr/val_loss:  0.004715/  3.017975, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0010000'], tr/val_loss:  0.004797/  3.030107, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0010000'], tr/val_loss:  0.004797/  3.016716, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0010000'], tr/val_loss:  0.005874/  3.032844, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0010000'], tr/val_loss:  0.005367/  3.029695, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0010000'], tr/val_loss:  0.004827/  3.030664, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0010000'], tr/val_loss:  0.004506/  3.031784, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0010000'], tr/val_loss:  0.004542/  3.036873, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0010000'], tr/val_loss:  0.004707/  3.039166, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0010000'], tr/val_loss:  0.004359/  3.031314, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0010000'], tr/val_loss:  0.004404/  3.050957, val:  82.92%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0010000'], tr/val_loss:  0.004002/  3.064269, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0010000'], tr/val_loss:  0.004300/  3.061490, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0010000'], tr/val_loss:  0.004643/  3.045301, val:  83.33%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0010000'], tr/val_loss:  0.004557/  3.087048, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-200 lr=['0.0010000'], tr/val_loss:  0.003933/  3.076805, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-201 lr=['0.0010000'], tr/val_loss:  0.003840/  3.067926, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-202 lr=['0.0010000'], tr/val_loss:  0.004728/  3.058529, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-203 lr=['0.0010000'], tr/val_loss:  0.004783/  3.072273, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-204 lr=['0.0010000'], tr/val_loss:  0.003734/  3.089427, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-205 lr=['0.0010000'], tr/val_loss:  0.003219/  3.068876, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-206 lr=['0.0010000'], tr/val_loss:  0.003558/  3.085804, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-207 lr=['0.0010000'], tr/val_loss:  0.004401/  3.102281, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-208 lr=['0.0010000'], tr/val_loss:  0.003806/  3.100791, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-209 lr=['0.0010000'], tr/val_loss:  0.003325/  3.096868, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-210 lr=['0.0010000'], tr/val_loss:  0.002807/  3.084644, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-211 lr=['0.0010000'], tr/val_loss:  0.003008/  3.090421, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-212 lr=['0.0010000'], tr/val_loss:  0.003044/  3.094747, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-213 lr=['0.0010000'], tr/val_loss:  0.003446/  3.097567, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-214 lr=['0.0010000'], tr/val_loss:  0.004245/  3.096041, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-215 lr=['0.0010000'], tr/val_loss:  0.003905/  3.100218, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-216 lr=['0.0010000'], tr/val_loss:  0.003144/  3.092268, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-217 lr=['0.0010000'], tr/val_loss:  0.002835/  3.118009, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-218 lr=['0.0010000'], tr/val_loss:  0.003103/  3.112363, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-219 lr=['0.0010000'], tr/val_loss:  0.003312/  3.123936, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-220 lr=['0.0010000'], tr/val_loss:  0.002973/  3.113744, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-221 lr=['0.0010000'], tr/val_loss:  0.003451/  3.123430, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-222 lr=['0.0010000'], tr/val_loss:  0.002684/  3.127594, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-223 lr=['0.0010000'], tr/val_loss:  0.002857/  3.131162, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-224 lr=['0.0010000'], tr/val_loss:  0.002619/  3.128996, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-225 lr=['0.0010000'], tr/val_loss:  0.002669/  3.130789, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-226 lr=['0.0010000'], tr/val_loss:  0.002497/  3.140108, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-227 lr=['0.0010000'], tr/val_loss:  0.002702/  3.146151, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-228 lr=['0.0010000'], tr/val_loss:  0.003053/  3.148934, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-229 lr=['0.0010000'], tr/val_loss:  0.003683/  3.151145, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-230 lr=['0.0010000'], tr/val_loss:  0.003478/  3.164140, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-231 lr=['0.0010000'], tr/val_loss:  0.002801/  3.165917, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-232 lr=['0.0010000'], tr/val_loss:  0.002658/  3.170740, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-233 lr=['0.0010000'], tr/val_loss:  0.002454/  3.160964, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-234 lr=['0.0010000'], tr/val_loss:  0.003354/  3.167829, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-235 lr=['0.0010000'], tr/val_loss:  0.002327/  3.183622, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-236 lr=['0.0010000'], tr/val_loss:  0.002565/  3.160883, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-237 lr=['0.0010000'], tr/val_loss:  0.002810/  3.164658, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-238 lr=['0.0010000'], tr/val_loss:  0.002470/  3.161611, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-239 lr=['0.0010000'], tr/val_loss:  0.002527/  3.175166, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-240 lr=['0.0010000'], tr/val_loss:  0.002276/  3.178074, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-241 lr=['0.0010000'], tr/val_loss:  0.002413/  3.179018, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-242 lr=['0.0010000'], tr/val_loss:  0.002706/  3.188923, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-243 lr=['0.0010000'], tr/val_loss:  0.002705/  3.184858, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-244 lr=['0.0010000'], tr/val_loss:  0.002703/  3.166435, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-245 lr=['0.0010000'], tr/val_loss:  0.002811/  3.166690, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-246 lr=['0.0010000'], tr/val_loss:  0.002339/  3.188822, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-247 lr=['0.0010000'], tr/val_loss:  0.002316/  3.197238, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-248 lr=['0.0010000'], tr/val_loss:  0.002305/  3.189850, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-249 lr=['0.0010000'], tr/val_loss:  0.002075/  3.200443, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-250 lr=['0.0010000'], tr/val_loss:  0.002065/  3.199030, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-251 lr=['0.0010000'], tr/val_loss:  0.001974/  3.187665, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-252 lr=['0.0010000'], tr/val_loss:  0.002162/  3.192446, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-253 lr=['0.0010000'], tr/val_loss:  0.002338/  3.206575, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-254 lr=['0.0010000'], tr/val_loss:  0.002384/  3.202934, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-255 lr=['0.0010000'], tr/val_loss:  0.002251/  3.210193, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-256 lr=['0.0010000'], tr/val_loss:  0.002280/  3.193865, val:  79.58%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-257 lr=['0.0010000'], tr/val_loss:  0.002129/  3.204525, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-258 lr=['0.0010000'], tr/val_loss:  0.002321/  3.197817, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-259 lr=['0.0010000'], tr/val_loss:  0.002041/  3.205860, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-260 lr=['0.0010000'], tr/val_loss:  0.002134/  3.209007, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-261 lr=['0.0010000'], tr/val_loss:  0.001965/  3.213535, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-262 lr=['0.0010000'], tr/val_loss:  0.002147/  3.219009, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-263 lr=['0.0010000'], tr/val_loss:  0.002036/  3.207272, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-264 lr=['0.0010000'], tr/val_loss:  0.001851/  3.212140, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-265 lr=['0.0010000'], tr/val_loss:  0.002570/  3.212275, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-266 lr=['0.0010000'], tr/val_loss:  0.002135/  3.209070, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-267 lr=['0.0010000'], tr/val_loss:  0.002125/  3.202330, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-268 lr=['0.0010000'], tr/val_loss:  0.001825/  3.210245, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-269 lr=['0.0010000'], tr/val_loss:  0.001853/  3.224265, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-270 lr=['0.0010000'], tr/val_loss:  0.002037/  3.201537, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-271 lr=['0.0010000'], tr/val_loss:  0.001936/  3.214823, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-272 lr=['0.0010000'], tr/val_loss:  0.001743/  3.213583, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-273 lr=['0.0010000'], tr/val_loss:  0.001737/  3.206705, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-274 lr=['0.0010000'], tr/val_loss:  0.001767/  3.210405, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-275 lr=['0.0010000'], tr/val_loss:  0.002486/  3.211688, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-276 lr=['0.0010000'], tr/val_loss:  0.002750/  3.209519, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-277 lr=['0.0010000'], tr/val_loss:  0.002640/  3.209299, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-278 lr=['0.0010000'], tr/val_loss:  0.002168/  3.213821, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-279 lr=['0.0010000'], tr/val_loss:  0.002137/  3.224471, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-280 lr=['0.0010000'], tr/val_loss:  0.001759/  3.207580, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-281 lr=['0.0010000'], tr/val_loss:  0.001857/  3.205284, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-282 lr=['0.0010000'], tr/val_loss:  0.001722/  3.201559, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-283 lr=['0.0010000'], tr/val_loss:  0.001921/  3.228464, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-284 lr=['0.0010000'], tr/val_loss:  0.002008/  3.209709, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-285 lr=['0.0010000'], tr/val_loss:  0.001713/  3.217916, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-286 lr=['0.0010000'], tr/val_loss:  0.001745/  3.209325, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-287 lr=['0.0010000'], tr/val_loss:  0.001758/  3.219100, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-288 lr=['0.0010000'], tr/val_loss:  0.001670/  3.226394, val:  80.42%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-289 lr=['0.0010000'], tr/val_loss:  0.002083/  3.233474, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-290 lr=['0.0010000'], tr/val_loss:  0.001939/  3.232357, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-291 lr=['0.0010000'], tr/val_loss:  0.001797/  3.234903, val:  80.00%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-292 lr=['0.0010000'], tr/val_loss:  0.001504/  3.240698, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-293 lr=['0.0010000'], tr/val_loss:  0.001584/  3.240027, val:  81.25%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-294 lr=['0.0010000'], tr/val_loss:  0.001674/  3.237931, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-295 lr=['0.0010000'], tr/val_loss:  0.001462/  3.237288, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-296 lr=['0.0010000'], tr/val_loss:  0.001414/  3.243937, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-297 lr=['0.0010000'], tr/val_loss:  0.001462/  3.244657, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-298 lr=['0.0010000'], tr/val_loss:  0.001436/  3.246149, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-299 lr=['0.0010000'], tr/val_loss:  0.001429/  3.228052, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-300 lr=['0.0010000'], tr/val_loss:  0.001393/  3.238138, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-301 lr=['0.0010000'], tr/val_loss:  0.001512/  3.234794, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-302 lr=['0.0010000'], tr/val_loss:  0.002095/  3.229439, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-303 lr=['0.0010000'], tr/val_loss:  0.001486/  3.246434, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-304 lr=['0.0010000'], tr/val_loss:  0.001439/  3.243761, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-305 lr=['0.0010000'], tr/val_loss:  0.001420/  3.236313, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-306 lr=['0.0010000'], tr/val_loss:  0.001400/  3.245198, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-307 lr=['0.0010000'], tr/val_loss:  0.002039/  3.239228, val:  82.08%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-308 lr=['0.0010000'], tr/val_loss:  0.001990/  3.249087, val:  80.83%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-309 lr=['0.0010000'], tr/val_loss:  0.001690/  3.249173, val:  81.67%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-310 lr=['0.0010000'], tr/val_loss:  0.001834/  3.242669, val:  82.50%, val_best:  83.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-311 lr=['0.0010000'], tr/val_loss:  0.001988/  3.235399, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-312 lr=['0.0010000'], tr/val_loss:  0.001591/  3.233252, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-313 lr=['0.0010000'], tr/val_loss:  0.001508/  3.225528, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-314 lr=['0.0010000'], tr/val_loss:  0.001621/  3.227367, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-315 lr=['0.0010000'], tr/val_loss:  0.001744/  3.227365, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-316 lr=['0.0010000'], tr/val_loss:  0.002013/  3.227311, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-317 lr=['0.0010000'], tr/val_loss:  0.001496/  3.223330, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-318 lr=['0.0010000'], tr/val_loss:  0.001409/  3.229378, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-319 lr=['0.0010000'], tr/val_loss:  0.001371/  3.228546, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-320 lr=['0.0010000'], tr/val_loss:  0.001280/  3.238190, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-321 lr=['0.0010000'], tr/val_loss:  0.001296/  3.240667, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-322 lr=['0.0010000'], tr/val_loss:  0.001284/  3.238460, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-323 lr=['0.0010000'], tr/val_loss:  0.001210/  3.237297, val:  83.33%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-324 lr=['0.0010000'], tr/val_loss:  0.001337/  3.243460, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-325 lr=['0.0010000'], tr/val_loss:  0.001294/  3.246680, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-326 lr=['0.0010000'], tr/val_loss:  0.001372/  3.252444, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-327 lr=['0.0010000'], tr/val_loss:  0.001257/  3.260471, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-328 lr=['0.0010000'], tr/val_loss:  0.001206/  3.256068, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-329 lr=['0.0010000'], tr/val_loss:  0.001185/  3.249894, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-330 lr=['0.0010000'], tr/val_loss:  0.001202/  3.251355, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-331 lr=['0.0010000'], tr/val_loss:  0.001067/  3.241937, val:  82.08%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-332 lr=['0.0010000'], tr/val_loss:  0.001138/  3.244302, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-333 lr=['0.0010000'], tr/val_loss:  0.001028/  3.254745, val:  82.92%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-334 lr=['0.0010000'], tr/val_loss:  0.001134/  3.257388, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-335 lr=['0.0010000'], tr/val_loss:  0.001411/  3.269102, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-336 lr=['0.0010000'], tr/val_loss:  0.001767/  3.274840, val:  82.50%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-337 lr=['0.0010000'], tr/val_loss:  0.001752/  3.264851, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-338 lr=['0.0010000'], tr/val_loss:  0.001281/  3.265296, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-339 lr=['0.0010000'], tr/val_loss:  0.001244/  3.277285, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-340 lr=['0.0010000'], tr/val_loss:  0.001244/  3.281476, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-341 lr=['0.0010000'], tr/val_loss:  0.001155/  3.275005, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-342 lr=['0.0010000'], tr/val_loss:  0.001056/  3.269998, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-343 lr=['0.0010000'], tr/val_loss:  0.001059/  3.280682, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-344 lr=['0.0010000'], tr/val_loss:  0.001264/  3.267319, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-345 lr=['0.0010000'], tr/val_loss:  0.001168/  3.267478, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-346 lr=['0.0010000'], tr/val_loss:  0.001076/  3.274683, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-347 lr=['0.0010000'], tr/val_loss:  0.001022/  3.266863, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-348 lr=['0.0010000'], tr/val_loss:  0.001009/  3.269362, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-349 lr=['0.0010000'], tr/val_loss:  0.001030/  3.272383, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-350 lr=['0.0010000'], tr/val_loss:  0.001000/  3.276534, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-351 lr=['0.0010000'], tr/val_loss:  0.000967/  3.275871, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-352 lr=['0.0010000'], tr/val_loss:  0.001016/  3.277778, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-353 lr=['0.0010000'], tr/val_loss:  0.000986/  3.271424, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-354 lr=['0.0010000'], tr/val_loss:  0.001062/  3.284751, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-355 lr=['0.0010000'], tr/val_loss:  0.001140/  3.287698, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-356 lr=['0.0010000'], tr/val_loss:  0.000981/  3.284851, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-357 lr=['0.0010000'], tr/val_loss:  0.001487/  3.303236, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-358 lr=['0.0010000'], tr/val_loss:  0.001062/  3.299107, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-359 lr=['0.0010000'], tr/val_loss:  0.001157/  3.308832, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-360 lr=['0.0010000'], tr/val_loss:  0.002257/  3.295772, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-361 lr=['0.0010000'], tr/val_loss:  0.002233/  3.299445, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-362 lr=['0.0010000'], tr/val_loss:  0.001318/  3.317572, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-363 lr=['0.0010000'], tr/val_loss:  0.001236/  3.309669, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-364 lr=['0.0010000'], tr/val_loss:  0.001049/  3.309901, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-365 lr=['0.0010000'], tr/val_loss:  0.001049/  3.315524, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-366 lr=['0.0010000'], tr/val_loss:  0.001095/  3.320472, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-367 lr=['0.0010000'], tr/val_loss:  0.001050/  3.322719, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-368 lr=['0.0010000'], tr/val_loss:  0.001059/  3.314306, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-369 lr=['0.0010000'], tr/val_loss:  0.001174/  3.306911, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-370 lr=['0.0010000'], tr/val_loss:  0.000975/  3.310217, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-371 lr=['0.0010000'], tr/val_loss:  0.000925/  3.311270, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-372 lr=['0.0010000'], tr/val_loss:  0.001014/  3.306650, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-373 lr=['0.0010000'], tr/val_loss:  0.000997/  3.323182, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-374 lr=['0.0010000'], tr/val_loss:  0.001149/  3.326858, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-375 lr=['0.0010000'], tr/val_loss:  0.001603/  3.323170, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-376 lr=['0.0010000'], tr/val_loss:  0.001160/  3.320234, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-377 lr=['0.0010000'], tr/val_loss:  0.001059/  3.321217, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-378 lr=['0.0010000'], tr/val_loss:  0.001147/  3.311374, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-379 lr=['0.0010000'], tr/val_loss:  0.001084/  3.321954, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-380 lr=['0.0010000'], tr/val_loss:  0.001351/  3.333640, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-381 lr=['0.0010000'], tr/val_loss:  0.001153/  3.329052, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-382 lr=['0.0010000'], tr/val_loss:  0.001271/  3.325783, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-383 lr=['0.0010000'], tr/val_loss:  0.001419/  3.321440, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-384 lr=['0.0010000'], tr/val_loss:  0.001314/  3.344387, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-385 lr=['0.0010000'], tr/val_loss:  0.001425/  3.329370, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-386 lr=['0.0010000'], tr/val_loss:  0.001175/  3.342660, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-387 lr=['0.0010000'], tr/val_loss:  0.001303/  3.326134, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-388 lr=['0.0010000'], tr/val_loss:  0.001528/  3.332499, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-389 lr=['0.0010000'], tr/val_loss:  0.000922/  3.328892, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-390 lr=['0.0010000'], tr/val_loss:  0.001077/  3.318189, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-391 lr=['0.0010000'], tr/val_loss:  0.001117/  3.332006, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-392 lr=['0.0010000'], tr/val_loss:  0.001520/  3.329953, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-393 lr=['0.0010000'], tr/val_loss:  0.001217/  3.339116, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-394 lr=['0.0010000'], tr/val_loss:  0.001689/  3.337136, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-395 lr=['0.0010000'], tr/val_loss:  0.001107/  3.330253, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-396 lr=['0.0010000'], tr/val_loss:  0.001072/  3.341819, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-397 lr=['0.0010000'], tr/val_loss:  0.001013/  3.330948, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-398 lr=['0.0010000'], tr/val_loss:  0.000980/  3.340837, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-399 lr=['0.0010000'], tr/val_loss:  0.000901/  3.345080, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-400 lr=['0.0010000'], tr/val_loss:  0.000911/  3.349955, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-401 lr=['0.0010000'], tr/val_loss:  0.000907/  3.342353, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-402 lr=['0.0010000'], tr/val_loss:  0.000993/  3.338104, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-403 lr=['0.0010000'], tr/val_loss:  0.000986/  3.347858, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-404 lr=['0.0010000'], tr/val_loss:  0.000909/  3.346534, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-405 lr=['0.0010000'], tr/val_loss:  0.000858/  3.347287, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-406 lr=['0.0010000'], tr/val_loss:  0.000842/  3.356613, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-407 lr=['0.0010000'], tr/val_loss:  0.001249/  3.355021, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-408 lr=['0.0010000'], tr/val_loss:  0.001390/  3.338103, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-409 lr=['0.0010000'], tr/val_loss:  0.001389/  3.370532, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-410 lr=['0.0010000'], tr/val_loss:  0.001422/  3.371368, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-411 lr=['0.0010000'], tr/val_loss:  0.001082/  3.376470, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-412 lr=['0.0010000'], tr/val_loss:  0.000983/  3.375028, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-413 lr=['0.0010000'], tr/val_loss:  0.001081/  3.360982, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-414 lr=['0.0010000'], tr/val_loss:  0.001026/  3.372080, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-415 lr=['0.0010000'], tr/val_loss:  0.001003/  3.377238, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-416 lr=['0.0010000'], tr/val_loss:  0.001144/  3.376603, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-417 lr=['0.0010000'], tr/val_loss:  0.000946/  3.367211, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-418 lr=['0.0010000'], tr/val_loss:  0.000903/  3.375266, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-419 lr=['0.0010000'], tr/val_loss:  0.000816/  3.372098, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-420 lr=['0.0010000'], tr/val_loss:  0.000794/  3.374378, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-421 lr=['0.0010000'], tr/val_loss:  0.000864/  3.376764, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-422 lr=['0.0010000'], tr/val_loss:  0.001060/  3.371819, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-423 lr=['0.0010000'], tr/val_loss:  0.000867/  3.363461, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-424 lr=['0.0010000'], tr/val_loss:  0.000823/  3.367350, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-425 lr=['0.0010000'], tr/val_loss:  0.000956/  3.358727, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-426 lr=['0.0010000'], tr/val_loss:  0.000833/  3.366980, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-427 lr=['0.0010000'], tr/val_loss:  0.001049/  3.368818, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-428 lr=['0.0010000'], tr/val_loss:  0.000829/  3.364534, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-429 lr=['0.0010000'], tr/val_loss:  0.001002/  3.366633, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-430 lr=['0.0010000'], tr/val_loss:  0.001029/  3.374088, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-431 lr=['0.0010000'], tr/val_loss:  0.000844/  3.363534, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-432 lr=['0.0010000'], tr/val_loss:  0.000802/  3.362466, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-433 lr=['0.0010000'], tr/val_loss:  0.000841/  3.367131, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-434 lr=['0.0010000'], tr/val_loss:  0.000783/  3.359391, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-435 lr=['0.0010000'], tr/val_loss:  0.000759/  3.364097, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-436 lr=['0.0010000'], tr/val_loss:  0.000741/  3.371312, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-437 lr=['0.0010000'], tr/val_loss:  0.000726/  3.369679, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-438 lr=['0.0010000'], tr/val_loss:  0.000865/  3.379870, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-439 lr=['0.0010000'], tr/val_loss:  0.000840/  3.374754, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-440 lr=['0.0010000'], tr/val_loss:  0.000919/  3.363961, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-441 lr=['0.0010000'], tr/val_loss:  0.001134/  3.372466, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-442 lr=['0.0010000'], tr/val_loss:  0.001068/  3.353609, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-443 lr=['0.0010000'], tr/val_loss:  0.000957/  3.372988, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-444 lr=['0.0010000'], tr/val_loss:  0.000889/  3.358466, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-445 lr=['0.0010000'], tr/val_loss:  0.000902/  3.360354, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-446 lr=['0.0010000'], tr/val_loss:  0.001081/  3.361780, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-447 lr=['0.0010000'], tr/val_loss:  0.000824/  3.374124, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-448 lr=['0.0010000'], tr/val_loss:  0.000768/  3.374450, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-449 lr=['0.0010000'], tr/val_loss:  0.000747/  3.382710, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-450 lr=['0.0010000'], tr/val_loss:  0.000753/  3.378120, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-451 lr=['0.0010000'], tr/val_loss:  0.000724/  3.377005, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-452 lr=['0.0010000'], tr/val_loss:  0.000798/  3.387846, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-453 lr=['0.0010000'], tr/val_loss:  0.000799/  3.381541, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-454 lr=['0.0010000'], tr/val_loss:  0.000848/  3.379011, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-455 lr=['0.0010000'], tr/val_loss:  0.000848/  3.390049, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-456 lr=['0.0010000'], tr/val_loss:  0.000742/  3.386371, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-457 lr=['0.0010000'], tr/val_loss:  0.000717/  3.385743, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-458 lr=['0.0010000'], tr/val_loss:  0.000733/  3.380027, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-459 lr=['0.0010000'], tr/val_loss:  0.000735/  3.386899, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-460 lr=['0.0010000'], tr/val_loss:  0.000703/  3.393646, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-461 lr=['0.0010000'], tr/val_loss:  0.000765/  3.390457, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-462 lr=['0.0010000'], tr/val_loss:  0.000690/  3.397156, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-463 lr=['0.0010000'], tr/val_loss:  0.000832/  3.374769, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-464 lr=['0.0010000'], tr/val_loss:  0.000929/  3.389332, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-465 lr=['0.0010000'], tr/val_loss:  0.000854/  3.383537, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-466 lr=['0.0010000'], tr/val_loss:  0.000810/  3.396672, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-467 lr=['0.0010000'], tr/val_loss:  0.000706/  3.386885, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-468 lr=['0.0010000'], tr/val_loss:  0.000733/  3.397193, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-469 lr=['0.0010000'], tr/val_loss:  0.000739/  3.388939, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-470 lr=['0.0010000'], tr/val_loss:  0.000675/  3.392156, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-471 lr=['0.0010000'], tr/val_loss:  0.000882/  3.402047, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-472 lr=['0.0010000'], tr/val_loss:  0.000832/  3.400324, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-473 lr=['0.0010000'], tr/val_loss:  0.000702/  3.399815, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-474 lr=['0.0010000'], tr/val_loss:  0.000687/  3.398211, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-475 lr=['0.0010000'], tr/val_loss:  0.000654/  3.398428, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-476 lr=['0.0010000'], tr/val_loss:  0.000655/  3.401263, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-477 lr=['0.0010000'], tr/val_loss:  0.000649/  3.406978, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-478 lr=['0.0010000'], tr/val_loss:  0.000630/  3.402957, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-479 lr=['0.0010000'], tr/val_loss:  0.000790/  3.408355, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-480 lr=['0.0010000'], tr/val_loss:  0.000978/  3.400112, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-481 lr=['0.0010000'], tr/val_loss:  0.000719/  3.404710, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-482 lr=['0.0010000'], tr/val_loss:  0.000660/  3.396399, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-483 lr=['0.0010000'], tr/val_loss:  0.000649/  3.403023, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-484 lr=['0.0010000'], tr/val_loss:  0.000677/  3.401779, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-485 lr=['0.0010000'], tr/val_loss:  0.000617/  3.402360, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-486 lr=['0.0010000'], tr/val_loss:  0.000615/  3.398856, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-487 lr=['0.0010000'], tr/val_loss:  0.000618/  3.397679, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-488 lr=['0.0010000'], tr/val_loss:  0.000601/  3.396345, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-489 lr=['0.0010000'], tr/val_loss:  0.000688/  3.396214, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-490 lr=['0.0010000'], tr/val_loss:  0.000666/  3.401121, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-491 lr=['0.0010000'], tr/val_loss:  0.000621/  3.410697, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-492 lr=['0.0010000'], tr/val_loss:  0.000620/  3.414876, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-493 lr=['0.0010000'], tr/val_loss:  0.000612/  3.410573, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-494 lr=['0.0010000'], tr/val_loss:  0.000641/  3.426819, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-495 lr=['0.0010000'], tr/val_loss:  0.000588/  3.419513, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-496 lr=['0.0010000'], tr/val_loss:  0.000607/  3.425968, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-497 lr=['0.0010000'], tr/val_loss:  0.001139/  3.427960, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-498 lr=['0.0010000'], tr/val_loss:  0.001046/  3.424562, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-499 lr=['0.0010000'], tr/val_loss:  0.000728/  3.432018, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-500 lr=['0.0010000'], tr/val_loss:  0.000708/  3.438240, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-501 lr=['0.0010000'], tr/val_loss:  0.000688/  3.445059, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-502 lr=['0.0010000'], tr/val_loss:  0.000622/  3.438284, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-503 lr=['0.0010000'], tr/val_loss:  0.000617/  3.438665, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-504 lr=['0.0010000'], tr/val_loss:  0.000604/  3.437924, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-505 lr=['0.0010000'], tr/val_loss:  0.000603/  3.435699, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-506 lr=['0.0010000'], tr/val_loss:  0.000601/  3.434702, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-507 lr=['0.0010000'], tr/val_loss:  0.000587/  3.439461, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-508 lr=['0.0010000'], tr/val_loss:  0.000564/  3.435792, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-509 lr=['0.0010000'], tr/val_loss:  0.000636/  3.449289, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-510 lr=['0.0010000'], tr/val_loss:  0.000593/  3.444910, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-511 lr=['0.0010000'], tr/val_loss:  0.000558/  3.438431, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-512 lr=['0.0010000'], tr/val_loss:  0.000574/  3.437453, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-513 lr=['0.0010000'], tr/val_loss:  0.000568/  3.439438, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-514 lr=['0.0010000'], tr/val_loss:  0.000602/  3.439879, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-515 lr=['0.0010000'], tr/val_loss:  0.000646/  3.440443, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-516 lr=['0.0010000'], tr/val_loss:  0.000647/  3.432547, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-517 lr=['0.0010000'], tr/val_loss:  0.000603/  3.445614, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-518 lr=['0.0010000'], tr/val_loss:  0.000656/  3.434288, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-519 lr=['0.0010000'], tr/val_loss:  0.000576/  3.432142, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-520 lr=['0.0010000'], tr/val_loss:  0.000612/  3.435920, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-521 lr=['0.0010000'], tr/val_loss:  0.000585/  3.438756, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-522 lr=['0.0010000'], tr/val_loss:  0.000566/  3.445132, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-523 lr=['0.0010000'], tr/val_loss:  0.000603/  3.443527, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-524 lr=['0.0010000'], tr/val_loss:  0.000600/  3.447043, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-525 lr=['0.0010000'], tr/val_loss:  0.000633/  3.455560, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-526 lr=['0.0010000'], tr/val_loss:  0.000628/  3.458411, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-527 lr=['0.0010000'], tr/val_loss:  0.000591/  3.464422, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-528 lr=['0.0010000'], tr/val_loss:  0.000606/  3.464994, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-529 lr=['0.0010000'], tr/val_loss:  0.000832/  3.473602, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-530 lr=['0.0010000'], tr/val_loss:  0.000749/  3.472241, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-531 lr=['0.0010000'], tr/val_loss:  0.000735/  3.466619, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-532 lr=['0.0010000'], tr/val_loss:  0.000643/  3.463323, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-533 lr=['0.0010000'], tr/val_loss:  0.000625/  3.464051, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-534 lr=['0.0010000'], tr/val_loss:  0.000594/  3.462428, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-535 lr=['0.0010000'], tr/val_loss:  0.000617/  3.456517, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-536 lr=['0.0010000'], tr/val_loss:  0.000575/  3.465540, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-537 lr=['0.0010000'], tr/val_loss:  0.000616/  3.456115, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-538 lr=['0.0010000'], tr/val_loss:  0.000560/  3.466069, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-539 lr=['0.0010000'], tr/val_loss:  0.000550/  3.466090, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-540 lr=['0.0010000'], tr/val_loss:  0.000570/  3.467949, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-541 lr=['0.0010000'], tr/val_loss:  0.000555/  3.462699, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-542 lr=['0.0010000'], tr/val_loss:  0.000590/  3.462413, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-543 lr=['0.0010000'], tr/val_loss:  0.000596/  3.457556, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-544 lr=['0.0010000'], tr/val_loss:  0.000583/  3.459813, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-545 lr=['0.0010000'], tr/val_loss:  0.000608/  3.458655, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-546 lr=['0.0010000'], tr/val_loss:  0.000614/  3.455851, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-547 lr=['0.0010000'], tr/val_loss:  0.000591/  3.463310, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-548 lr=['0.0010000'], tr/val_loss:  0.000613/  3.470059, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-549 lr=['0.0010000'], tr/val_loss:  0.000596/  3.474216, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-550 lr=['0.0010000'], tr/val_loss:  0.000714/  3.470387, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-551 lr=['0.0010000'], tr/val_loss:  0.001153/  3.481940, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-552 lr=['0.0010000'], tr/val_loss:  0.000807/  3.487417, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-553 lr=['0.0010000'], tr/val_loss:  0.000830/  3.483156, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-554 lr=['0.0010000'], tr/val_loss:  0.000655/  3.480651, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-555 lr=['0.0010000'], tr/val_loss:  0.000630/  3.479664, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-556 lr=['0.0010000'], tr/val_loss:  0.000615/  3.474157, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-557 lr=['0.0010000'], tr/val_loss:  0.000596/  3.476466, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-558 lr=['0.0010000'], tr/val_loss:  0.000588/  3.474387, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-559 lr=['0.0010000'], tr/val_loss:  0.000555/  3.477023, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-560 lr=['0.0010000'], tr/val_loss:  0.000555/  3.471972, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-561 lr=['0.0010000'], tr/val_loss:  0.000542/  3.471170, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-562 lr=['0.0010000'], tr/val_loss:  0.000526/  3.467563, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-563 lr=['0.0010000'], tr/val_loss:  0.000541/  3.469676, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-564 lr=['0.0010000'], tr/val_loss:  0.000527/  3.472668, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-565 lr=['0.0010000'], tr/val_loss:  0.001098/  3.466392, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-566 lr=['0.0010000'], tr/val_loss:  0.000654/  3.477063, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-567 lr=['0.0010000'], tr/val_loss:  0.000786/  3.475454, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-568 lr=['0.0010000'], tr/val_loss:  0.000576/  3.480962, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-569 lr=['0.0010000'], tr/val_loss:  0.000561/  3.471498, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-570 lr=['0.0010000'], tr/val_loss:  0.000530/  3.471138, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-571 lr=['0.0010000'], tr/val_loss:  0.000569/  3.459598, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-572 lr=['0.0010000'], tr/val_loss:  0.000567/  3.465278, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-573 lr=['0.0010000'], tr/val_loss:  0.000568/  3.476533, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-574 lr=['0.0010000'], tr/val_loss:  0.000575/  3.479355, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-575 lr=['0.0010000'], tr/val_loss:  0.000585/  3.483347, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-576 lr=['0.0010000'], tr/val_loss:  0.000586/  3.478156, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-577 lr=['0.0010000'], tr/val_loss:  0.000545/  3.485710, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-578 lr=['0.0010000'], tr/val_loss:  0.000512/  3.481962, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-579 lr=['0.0010000'], tr/val_loss:  0.000511/  3.481427, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-580 lr=['0.0010000'], tr/val_loss:  0.000504/  3.489131, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-581 lr=['0.0010000'], tr/val_loss:  0.000491/  3.484554, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-582 lr=['0.0010000'], tr/val_loss:  0.000498/  3.485423, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-583 lr=['0.0010000'], tr/val_loss:  0.000493/  3.483730, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-584 lr=['0.0010000'], tr/val_loss:  0.000573/  3.486207, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-585 lr=['0.0010000'], tr/val_loss:  0.000705/  3.481670, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-586 lr=['0.0010000'], tr/val_loss:  0.000624/  3.469307, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-587 lr=['0.0010000'], tr/val_loss:  0.000579/  3.479839, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-588 lr=['0.0010000'], tr/val_loss:  0.000515/  3.481560, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-589 lr=['0.0010000'], tr/val_loss:  0.000485/  3.479474, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-590 lr=['0.0010000'], tr/val_loss:  0.000500/  3.482837, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-591 lr=['0.0010000'], tr/val_loss:  0.000490/  3.482506, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-592 lr=['0.0010000'], tr/val_loss:  0.000475/  3.490084, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-593 lr=['0.0010000'], tr/val_loss:  0.000651/  3.480475, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-594 lr=['0.0010000'], tr/val_loss:  0.000508/  3.484020, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-595 lr=['0.0010000'], tr/val_loss:  0.000492/  3.485827, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-596 lr=['0.0010000'], tr/val_loss:  0.000495/  3.489601, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-597 lr=['0.0010000'], tr/val_loss:  0.000497/  3.483639, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-598 lr=['0.0010000'], tr/val_loss:  0.000496/  3.481401, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-599 lr=['0.0010000'], tr/val_loss:  0.000557/  3.480802, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-600 lr=['0.0010000'], tr/val_loss:  0.000492/  3.486405, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-601 lr=['0.0010000'], tr/val_loss:  0.000538/  3.488708, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-602 lr=['0.0010000'], tr/val_loss:  0.000485/  3.481790, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-603 lr=['0.0010000'], tr/val_loss:  0.000646/  3.482757, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-604 lr=['0.0010000'], tr/val_loss:  0.000516/  3.484445, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-605 lr=['0.0010000'], tr/val_loss:  0.000483/  3.489482, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-606 lr=['0.0010000'], tr/val_loss:  0.000479/  3.488383, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-607 lr=['0.0010000'], tr/val_loss:  0.000467/  3.488962, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-608 lr=['0.0010000'], tr/val_loss:  0.000461/  3.495022, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-609 lr=['0.0010000'], tr/val_loss:  0.000515/  3.491896, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-610 lr=['0.0010000'], tr/val_loss:  0.000533/  3.496017, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-611 lr=['0.0010000'], tr/val_loss:  0.000469/  3.498457, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-612 lr=['0.0010000'], tr/val_loss:  0.000467/  3.496932, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-613 lr=['0.0010000'], tr/val_loss:  0.000498/  3.494280, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-614 lr=['0.0010000'], tr/val_loss:  0.000489/  3.486370, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-615 lr=['0.0010000'], tr/val_loss:  0.000715/  3.494301, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-616 lr=['0.0010000'], tr/val_loss:  0.000833/  3.491863, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-617 lr=['0.0010000'], tr/val_loss:  0.000708/  3.491571, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-618 lr=['0.0010000'], tr/val_loss:  0.001384/  3.501884, val:  79.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-619 lr=['0.0010000'], tr/val_loss:  0.000797/  3.493596, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-620 lr=['0.0010000'], tr/val_loss:  0.000618/  3.493855, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-621 lr=['0.0010000'], tr/val_loss:  0.000642/  3.482090, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-622 lr=['0.0010000'], tr/val_loss:  0.000588/  3.482954, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-623 lr=['0.0010000'], tr/val_loss:  0.000606/  3.509645, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-624 lr=['0.0010000'], tr/val_loss:  0.000528/  3.506884, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-625 lr=['0.0010000'], tr/val_loss:  0.000475/  3.500549, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-626 lr=['0.0010000'], tr/val_loss:  0.000473/  3.503048, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-627 lr=['0.0010000'], tr/val_loss:  0.000497/  3.504372, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-628 lr=['0.0010000'], tr/val_loss:  0.000484/  3.506022, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-629 lr=['0.0010000'], tr/val_loss:  0.000463/  3.503213, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-630 lr=['0.0010000'], tr/val_loss:  0.000461/  3.502159, val:  79.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-631 lr=['0.0010000'], tr/val_loss:  0.000453/  3.507825, val:  79.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-632 lr=['0.0010000'], tr/val_loss:  0.000458/  3.505233, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-633 lr=['0.0010000'], tr/val_loss:  0.000436/  3.505903, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-634 lr=['0.0010000'], tr/val_loss:  0.000437/  3.505313, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-635 lr=['0.0010000'], tr/val_loss:  0.000433/  3.509272, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-636 lr=['0.0010000'], tr/val_loss:  0.000435/  3.510270, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-637 lr=['0.0010000'], tr/val_loss:  0.000457/  3.512732, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-638 lr=['0.0010000'], tr/val_loss:  0.000442/  3.508229, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-639 lr=['0.0010000'], tr/val_loss:  0.000423/  3.513804, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-640 lr=['0.0010000'], tr/val_loss:  0.000415/  3.513069, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-641 lr=['0.0010000'], tr/val_loss:  0.000418/  3.518782, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-642 lr=['0.0010000'], tr/val_loss:  0.000443/  3.518022, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-643 lr=['0.0010000'], tr/val_loss:  0.000414/  3.522029, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-644 lr=['0.0010000'], tr/val_loss:  0.000417/  3.515907, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-645 lr=['0.0010000'], tr/val_loss:  0.000402/  3.521123, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-646 lr=['0.0010000'], tr/val_loss:  0.000405/  3.522974, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-647 lr=['0.0010000'], tr/val_loss:  0.000409/  3.526588, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-648 lr=['0.0010000'], tr/val_loss:  0.000422/  3.516638, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-649 lr=['0.0010000'], tr/val_loss:  0.000473/  3.529047, val:  80.00%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-650 lr=['0.0010000'], tr/val_loss:  0.000464/  3.519832, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-651 lr=['0.0010000'], tr/val_loss:  0.000523/  3.527036, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-652 lr=['0.0010000'], tr/val_loss:  0.000596/  3.522524, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-653 lr=['0.0010000'], tr/val_loss:  0.000488/  3.526253, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-654 lr=['0.0010000'], tr/val_loss:  0.000446/  3.532101, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-655 lr=['0.0010000'], tr/val_loss:  0.000426/  3.528047, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-656 lr=['0.0010000'], tr/val_loss:  0.000427/  3.531946, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-657 lr=['0.0010000'], tr/val_loss:  0.000455/  3.525586, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-658 lr=['0.0010000'], tr/val_loss:  0.000431/  3.524209, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-659 lr=['0.0010000'], tr/val_loss:  0.000412/  3.522709, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-660 lr=['0.0010000'], tr/val_loss:  0.000934/  3.516125, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-661 lr=['0.0010000'], tr/val_loss:  0.000792/  3.513306, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-662 lr=['0.0010000'], tr/val_loss:  0.000560/  3.517623, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-663 lr=['0.0010000'], tr/val_loss:  0.000575/  3.519378, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-664 lr=['0.0010000'], tr/val_loss:  0.000500/  3.514013, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-665 lr=['0.0010000'], tr/val_loss:  0.000440/  3.510454, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-666 lr=['0.0010000'], tr/val_loss:  0.000426/  3.508844, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-667 lr=['0.0010000'], tr/val_loss:  0.000422/  3.505733, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-668 lr=['0.0010000'], tr/val_loss:  0.000436/  3.507038, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-669 lr=['0.0010000'], tr/val_loss:  0.000420/  3.512955, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-670 lr=['0.0010000'], tr/val_loss:  0.000435/  3.506403, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-671 lr=['0.0010000'], tr/val_loss:  0.000409/  3.512492, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-672 lr=['0.0010000'], tr/val_loss:  0.000404/  3.510148, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-673 lr=['0.0010000'], tr/val_loss:  0.000390/  3.519840, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-674 lr=['0.0010000'], tr/val_loss:  0.000382/  3.514098, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-675 lr=['0.0010000'], tr/val_loss:  0.000402/  3.516813, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-676 lr=['0.0010000'], tr/val_loss:  0.000382/  3.517521, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-677 lr=['0.0010000'], tr/val_loss:  0.000391/  3.518543, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-678 lr=['0.0010000'], tr/val_loss:  0.000604/  3.517978, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-679 lr=['0.0010000'], tr/val_loss:  0.000551/  3.522062, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-680 lr=['0.0010000'], tr/val_loss:  0.000493/  3.521785, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-681 lr=['0.0010000'], tr/val_loss:  0.000448/  3.518045, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-682 lr=['0.0010000'], tr/val_loss:  0.000436/  3.513719, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-683 lr=['0.0010000'], tr/val_loss:  0.000408/  3.523357, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-684 lr=['0.0010000'], tr/val_loss:  0.000425/  3.526562, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-685 lr=['0.0010000'], tr/val_loss:  0.000395/  3.518889, val:  80.42%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-686 lr=['0.0010000'], tr/val_loss:  0.000398/  3.514772, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-687 lr=['0.0010000'], tr/val_loss:  0.000402/  3.520130, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-688 lr=['0.0010000'], tr/val_loss:  0.000381/  3.524032, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-689 lr=['0.0010000'], tr/val_loss:  0.000381/  3.527669, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-690 lr=['0.0010000'], tr/val_loss:  0.000385/  3.528156, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-691 lr=['0.0010000'], tr/val_loss:  0.000419/  3.527544, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-692 lr=['0.0010000'], tr/val_loss:  0.000397/  3.523035, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-693 lr=['0.0010000'], tr/val_loss:  0.000421/  3.540269, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-694 lr=['0.0010000'], tr/val_loss:  0.000365/  3.533205, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-695 lr=['0.0010000'], tr/val_loss:  0.000379/  3.542941, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-696 lr=['0.0010000'], tr/val_loss:  0.000373/  3.537777, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-697 lr=['0.0010000'], tr/val_loss:  0.000379/  3.546180, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-698 lr=['0.0010000'], tr/val_loss:  0.000372/  3.541604, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-699 lr=['0.0010000'], tr/val_loss:  0.000371/  3.543259, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-700 lr=['0.0010000'], tr/val_loss:  0.000372/  3.551358, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-701 lr=['0.0010000'], tr/val_loss:  0.000390/  3.538102, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-702 lr=['0.0010000'], tr/val_loss:  0.000361/  3.543654, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-703 lr=['0.0010000'], tr/val_loss:  0.000375/  3.545488, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-704 lr=['0.0010000'], tr/val_loss:  0.000350/  3.546610, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-705 lr=['0.0010000'], tr/val_loss:  0.000377/  3.538201, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-706 lr=['0.0010000'], tr/val_loss:  0.000390/  3.547831, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-707 lr=['0.0010000'], tr/val_loss:  0.000372/  3.543004, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-708 lr=['0.0010000'], tr/val_loss:  0.000364/  3.541126, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-709 lr=['0.0010000'], tr/val_loss:  0.000370/  3.543597, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-710 lr=['0.0010000'], tr/val_loss:  0.000361/  3.536598, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-711 lr=['0.0010000'], tr/val_loss:  0.000387/  3.538947, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-712 lr=['0.0010000'], tr/val_loss:  0.000394/  3.539160, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-713 lr=['0.0010000'], tr/val_loss:  0.000387/  3.530011, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-714 lr=['0.0010000'], tr/val_loss:  0.000415/  3.541857, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-715 lr=['0.0010000'], tr/val_loss:  0.000372/  3.538797, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-716 lr=['0.0010000'], tr/val_loss:  0.000371/  3.542372, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-717 lr=['0.0010000'], tr/val_loss:  0.000373/  3.541075, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-718 lr=['0.0010000'], tr/val_loss:  0.000350/  3.539704, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-719 lr=['0.0010000'], tr/val_loss:  0.000346/  3.540504, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-720 lr=['0.0010000'], tr/val_loss:  0.000355/  3.546610, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-721 lr=['0.0010000'], tr/val_loss:  0.000366/  3.548745, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-722 lr=['0.0010000'], tr/val_loss:  0.000446/  3.554203, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-723 lr=['0.0010000'], tr/val_loss:  0.000412/  3.556056, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-724 lr=['0.0010000'], tr/val_loss:  0.000403/  3.554657, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-725 lr=['0.0010000'], tr/val_loss:  0.000525/  3.557326, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-726 lr=['0.0010000'], tr/val_loss:  0.000827/  3.544822, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-727 lr=['0.0010000'], tr/val_loss:  0.001334/  3.550331, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-728 lr=['0.0010000'], tr/val_loss:  0.000462/  3.537170, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-729 lr=['0.0010000'], tr/val_loss:  0.000381/  3.541143, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-730 lr=['0.0010000'], tr/val_loss:  0.000383/  3.543360, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-731 lr=['0.0010000'], tr/val_loss:  0.000373/  3.543008, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-732 lr=['0.0010000'], tr/val_loss:  0.000379/  3.553641, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-733 lr=['0.0010000'], tr/val_loss:  0.000382/  3.551834, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-734 lr=['0.0010000'], tr/val_loss:  0.000360/  3.551394, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-735 lr=['0.0010000'], tr/val_loss:  0.000361/  3.549254, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-736 lr=['0.0010000'], tr/val_loss:  0.000390/  3.548163, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-737 lr=['0.0010000'], tr/val_loss:  0.000372/  3.555888, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-738 lr=['0.0010000'], tr/val_loss:  0.000385/  3.554703, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-739 lr=['0.0010000'], tr/val_loss:  0.001034/  3.548488, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-740 lr=['0.0010000'], tr/val_loss:  0.000851/  3.557161, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-741 lr=['0.0010000'], tr/val_loss:  0.000542/  3.559722, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-742 lr=['0.0010000'], tr/val_loss:  0.000692/  3.554928, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-743 lr=['0.0010000'], tr/val_loss:  0.000450/  3.564216, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-744 lr=['0.0010000'], tr/val_loss:  0.000428/  3.560596, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-745 lr=['0.0010000'], tr/val_loss:  0.000403/  3.565890, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-746 lr=['0.0010000'], tr/val_loss:  0.000404/  3.563974, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-747 lr=['0.0010000'], tr/val_loss:  0.000476/  3.569992, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-748 lr=['0.0010000'], tr/val_loss:  0.000485/  3.563923, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-749 lr=['0.0010000'], tr/val_loss:  0.000437/  3.552267, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-750 lr=['0.0010000'], tr/val_loss:  0.000419/  3.553957, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-751 lr=['0.0010000'], tr/val_loss:  0.000404/  3.551716, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-752 lr=['0.0010000'], tr/val_loss:  0.000377/  3.559711, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-753 lr=['0.0010000'], tr/val_loss:  0.000362/  3.562545, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-754 lr=['0.0010000'], tr/val_loss:  0.000365/  3.563832, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-755 lr=['0.0010000'], tr/val_loss:  0.000355/  3.560974, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-756 lr=['0.0010000'], tr/val_loss:  0.000355/  3.567177, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-757 lr=['0.0010000'], tr/val_loss:  0.000378/  3.566123, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-758 lr=['0.0010000'], tr/val_loss:  0.000370/  3.565435, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-759 lr=['0.0010000'], tr/val_loss:  0.000349/  3.565562, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-760 lr=['0.0010000'], tr/val_loss:  0.000366/  3.563612, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-761 lr=['0.0010000'], tr/val_loss:  0.000347/  3.566751, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-762 lr=['0.0010000'], tr/val_loss:  0.000347/  3.567621, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-763 lr=['0.0010000'], tr/val_loss:  0.000570/  3.567536, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-764 lr=['0.0010000'], tr/val_loss:  0.000486/  3.564033, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-765 lr=['0.0010000'], tr/val_loss:  0.000377/  3.554580, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-766 lr=['0.0010000'], tr/val_loss:  0.000356/  3.557769, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-767 lr=['0.0010000'], tr/val_loss:  0.000356/  3.558291, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-768 lr=['0.0010000'], tr/val_loss:  0.000367/  3.563467, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-769 lr=['0.0010000'], tr/val_loss:  0.000397/  3.569081, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-770 lr=['0.0010000'], tr/val_loss:  0.000416/  3.572953, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-771 lr=['0.0010000'], tr/val_loss:  0.000368/  3.574837, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-772 lr=['0.0010000'], tr/val_loss:  0.000372/  3.570503, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-773 lr=['0.0010000'], tr/val_loss:  0.000501/  3.573923, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-774 lr=['0.0010000'], tr/val_loss:  0.000434/  3.587671, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-775 lr=['0.0010000'], tr/val_loss:  0.000437/  3.579937, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-776 lr=['0.0010000'], tr/val_loss:  0.000369/  3.581769, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-777 lr=['0.0010000'], tr/val_loss:  0.000352/  3.581204, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-778 lr=['0.0010000'], tr/val_loss:  0.000353/  3.578153, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-779 lr=['0.0010000'], tr/val_loss:  0.000344/  3.581066, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-780 lr=['0.0010000'], tr/val_loss:  0.000344/  3.579953, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-781 lr=['0.0010000'], tr/val_loss:  0.000330/  3.582812, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-782 lr=['0.0010000'], tr/val_loss:  0.000345/  3.577726, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-783 lr=['0.0010000'], tr/val_loss:  0.000326/  3.582860, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-784 lr=['0.0010000'], tr/val_loss:  0.000332/  3.580719, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-785 lr=['0.0010000'], tr/val_loss:  0.000379/  3.583499, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-786 lr=['0.0010000'], tr/val_loss:  0.000339/  3.586171, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-787 lr=['0.0010000'], tr/val_loss:  0.000333/  3.585187, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-788 lr=['0.0010000'], tr/val_loss:  0.000344/  3.589234, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-789 lr=['0.0010000'], tr/val_loss:  0.000335/  3.589966, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-790 lr=['0.0010000'], tr/val_loss:  0.000341/  3.591943, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-791 lr=['0.0010000'], tr/val_loss:  0.000334/  3.590877, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-792 lr=['0.0010000'], tr/val_loss:  0.000346/  3.601880, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-793 lr=['0.0010000'], tr/val_loss:  0.000338/  3.606352, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-794 lr=['0.0010000'], tr/val_loss:  0.000341/  3.598583, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-795 lr=['0.0010000'], tr/val_loss:  0.000338/  3.596909, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-796 lr=['0.0010000'], tr/val_loss:  0.000338/  3.600744, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-797 lr=['0.0010000'], tr/val_loss:  0.000327/  3.603688, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-798 lr=['0.0010000'], tr/val_loss:  0.000336/  3.600746, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-799 lr=['0.0010000'], tr/val_loss:  0.000326/  3.595967, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-800 lr=['0.0010000'], tr/val_loss:  0.000341/  3.604680, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-801 lr=['0.0010000'], tr/val_loss:  0.000330/  3.597338, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-802 lr=['0.0010000'], tr/val_loss:  0.000336/  3.605941, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-803 lr=['0.0010000'], tr/val_loss:  0.000318/  3.607752, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-804 lr=['0.0010000'], tr/val_loss:  0.000327/  3.612500, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-805 lr=['0.0010000'], tr/val_loss:  0.000492/  3.590535, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-806 lr=['0.0010000'], tr/val_loss:  0.000369/  3.593103, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-807 lr=['0.0010000'], tr/val_loss:  0.000372/  3.593100, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-808 lr=['0.0010000'], tr/val_loss:  0.000379/  3.596701, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-809 lr=['0.0010000'], tr/val_loss:  0.000350/  3.599912, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-810 lr=['0.0010000'], tr/val_loss:  0.000320/  3.605264, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-811 lr=['0.0010000'], tr/val_loss:  0.000327/  3.597200, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-812 lr=['0.0010000'], tr/val_loss:  0.000340/  3.599289, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-813 lr=['0.0010000'], tr/val_loss:  0.000336/  3.599657, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-814 lr=['0.0010000'], tr/val_loss:  0.000308/  3.600411, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-815 lr=['0.0010000'], tr/val_loss:  0.000321/  3.597775, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-816 lr=['0.0010000'], tr/val_loss:  0.000307/  3.600801, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-817 lr=['0.0010000'], tr/val_loss:  0.000507/  3.604033, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-818 lr=['0.0010000'], tr/val_loss:  0.000416/  3.614391, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-819 lr=['0.0010000'], tr/val_loss:  0.000384/  3.618340, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-820 lr=['0.0010000'], tr/val_loss:  0.000404/  3.602571, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-821 lr=['0.0010000'], tr/val_loss:  0.000331/  3.611743, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-822 lr=['0.0010000'], tr/val_loss:  0.000349/  3.603924, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-823 lr=['0.0010000'], tr/val_loss:  0.000322/  3.605259, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-824 lr=['0.0010000'], tr/val_loss:  0.000317/  3.603165, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-825 lr=['0.0010000'], tr/val_loss:  0.000300/  3.603902, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-826 lr=['0.0010000'], tr/val_loss:  0.000309/  3.606453, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-827 lr=['0.0010000'], tr/val_loss:  0.000316/  3.602894, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-828 lr=['0.0010000'], tr/val_loss:  0.000329/  3.604977, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-829 lr=['0.0010000'], tr/val_loss:  0.000308/  3.616006, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-830 lr=['0.0010000'], tr/val_loss:  0.000339/  3.612598, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-831 lr=['0.0010000'], tr/val_loss:  0.000308/  3.617705, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-832 lr=['0.0010000'], tr/val_loss:  0.000318/  3.613707, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-833 lr=['0.0010000'], tr/val_loss:  0.000352/  3.605267, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-834 lr=['0.0010000'], tr/val_loss:  0.000355/  3.597664, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-835 lr=['0.0010000'], tr/val_loss:  0.000358/  3.599411, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-836 lr=['0.0010000'], tr/val_loss:  0.000338/  3.602817, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-837 lr=['0.0010000'], tr/val_loss:  0.000347/  3.596592, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-838 lr=['0.0010000'], tr/val_loss:  0.000380/  3.608933, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-839 lr=['0.0010000'], tr/val_loss:  0.000325/  3.606454, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-840 lr=['0.0010000'], tr/val_loss:  0.000312/  3.615999, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-841 lr=['0.0010000'], tr/val_loss:  0.000319/  3.605723, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-842 lr=['0.0010000'], tr/val_loss:  0.000355/  3.612063, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-843 lr=['0.0010000'], tr/val_loss:  0.000408/  3.613186, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-844 lr=['0.0010000'], tr/val_loss:  0.000335/  3.610302, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-845 lr=['0.0010000'], tr/val_loss:  0.000321/  3.616193, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-846 lr=['0.0010000'], tr/val_loss:  0.000305/  3.610910, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-847 lr=['0.0010000'], tr/val_loss:  0.000307/  3.615222, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-848 lr=['0.0010000'], tr/val_loss:  0.000293/  3.610521, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-849 lr=['0.0010000'], tr/val_loss:  0.000300/  3.604744, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-850 lr=['0.0010000'], tr/val_loss:  0.000291/  3.612654, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-851 lr=['0.0010000'], tr/val_loss:  0.000307/  3.617813, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-852 lr=['0.0010000'], tr/val_loss:  0.000287/  3.612486, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-853 lr=['0.0010000'], tr/val_loss:  0.000297/  3.617087, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-854 lr=['0.0010000'], tr/val_loss:  0.000285/  3.618659, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-855 lr=['0.0010000'], tr/val_loss:  0.000274/  3.619148, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-856 lr=['0.0010000'], tr/val_loss:  0.000283/  3.624291, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-857 lr=['0.0010000'], tr/val_loss:  0.000303/  3.620557, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-858 lr=['0.0010000'], tr/val_loss:  0.000314/  3.621239, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-859 lr=['0.0010000'], tr/val_loss:  0.000321/  3.609241, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-860 lr=['0.0010000'], tr/val_loss:  0.000298/  3.608810, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-861 lr=['0.0010000'], tr/val_loss:  0.000285/  3.610102, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-862 lr=['0.0010000'], tr/val_loss:  0.000283/  3.615848, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-863 lr=['0.0010000'], tr/val_loss:  0.000278/  3.615298, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-864 lr=['0.0010000'], tr/val_loss:  0.000282/  3.622857, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-865 lr=['0.0010000'], tr/val_loss:  0.000280/  3.622946, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-866 lr=['0.0010000'], tr/val_loss:  0.000308/  3.616711, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-867 lr=['0.0010000'], tr/val_loss:  0.000340/  3.617136, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-868 lr=['0.0010000'], tr/val_loss:  0.000363/  3.615520, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-869 lr=['0.0010000'], tr/val_loss:  0.000516/  3.613702, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-870 lr=['0.0010000'], tr/val_loss:  0.000343/  3.614482, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-871 lr=['0.0010000'], tr/val_loss:  0.000350/  3.619457, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-872 lr=['0.0010000'], tr/val_loss:  0.000311/  3.614212, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-873 lr=['0.0010000'], tr/val_loss:  0.000304/  3.616617, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-874 lr=['0.0010000'], tr/val_loss:  0.000289/  3.612707, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-875 lr=['0.0010000'], tr/val_loss:  0.000287/  3.616923, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-876 lr=['0.0010000'], tr/val_loss:  0.000297/  3.623628, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-877 lr=['0.0010000'], tr/val_loss:  0.000287/  3.618597, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-878 lr=['0.0010000'], tr/val_loss:  0.000284/  3.620440, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-879 lr=['0.0010000'], tr/val_loss:  0.000335/  3.624053, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-880 lr=['0.0010000'], tr/val_loss:  0.000306/  3.629775, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-881 lr=['0.0010000'], tr/val_loss:  0.000296/  3.620240, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-882 lr=['0.0010000'], tr/val_loss:  0.000283/  3.620580, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-883 lr=['0.0010000'], tr/val_loss:  0.000294/  3.626609, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-884 lr=['0.0010000'], tr/val_loss:  0.000286/  3.621542, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-885 lr=['0.0010000'], tr/val_loss:  0.000282/  3.624308, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-886 lr=['0.0010000'], tr/val_loss:  0.000294/  3.622505, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-887 lr=['0.0010000'], tr/val_loss:  0.000296/  3.622353, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-888 lr=['0.0010000'], tr/val_loss:  0.000279/  3.620167, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-889 lr=['0.0010000'], tr/val_loss:  0.000283/  3.622175, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-890 lr=['0.0010000'], tr/val_loss:  0.000272/  3.619876, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-891 lr=['0.0010000'], tr/val_loss:  0.000277/  3.617439, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-892 lr=['0.0010000'], tr/val_loss:  0.000273/  3.620509, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-893 lr=['0.0010000'], tr/val_loss:  0.000283/  3.630414, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-894 lr=['0.0010000'], tr/val_loss:  0.000269/  3.627845, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-895 lr=['0.0010000'], tr/val_loss:  0.000269/  3.624202, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-896 lr=['0.0010000'], tr/val_loss:  0.000272/  3.619740, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-897 lr=['0.0010000'], tr/val_loss:  0.000275/  3.622755, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-898 lr=['0.0010000'], tr/val_loss:  0.000272/  3.621024, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-899 lr=['0.0010000'], tr/val_loss:  0.000272/  3.618614, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-900 lr=['0.0010000'], tr/val_loss:  0.000265/  3.628663, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-901 lr=['0.0010000'], tr/val_loss:  0.000270/  3.628247, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-902 lr=['0.0010000'], tr/val_loss:  0.000290/  3.628766, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-903 lr=['0.0010000'], tr/val_loss:  0.000274/  3.631534, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-904 lr=['0.0010000'], tr/val_loss:  0.000276/  3.634015, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-905 lr=['0.0010000'], tr/val_loss:  0.000267/  3.637572, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-906 lr=['0.0010000'], tr/val_loss:  0.000270/  3.641730, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-907 lr=['0.0010000'], tr/val_loss:  0.000452/  3.619279, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-908 lr=['0.0010000'], tr/val_loss:  0.000343/  3.620889, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-909 lr=['0.0010000'], tr/val_loss:  0.000300/  3.630271, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-910 lr=['0.0010000'], tr/val_loss:  0.000317/  3.625755, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-911 lr=['0.0010000'], tr/val_loss:  0.000296/  3.624247, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-912 lr=['0.0010000'], tr/val_loss:  0.000298/  3.625483, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-913 lr=['0.0010000'], tr/val_loss:  0.000292/  3.623474, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-914 lr=['0.0010000'], tr/val_loss:  0.000356/  3.615041, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-915 lr=['0.0010000'], tr/val_loss:  0.000294/  3.629570, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-916 lr=['0.0010000'], tr/val_loss:  0.000328/  3.622284, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-917 lr=['0.0010000'], tr/val_loss:  0.000310/  3.626143, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-918 lr=['0.0010000'], tr/val_loss:  0.000472/  3.625434, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-919 lr=['0.0010000'], tr/val_loss:  0.000367/  3.628050, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-920 lr=['0.0010000'], tr/val_loss:  0.000444/  3.619541, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-921 lr=['0.0010000'], tr/val_loss:  0.000332/  3.632174, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-922 lr=['0.0010000'], tr/val_loss:  0.000297/  3.624207, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-923 lr=['0.0010000'], tr/val_loss:  0.000293/  3.621464, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-924 lr=['0.0010000'], tr/val_loss:  0.000274/  3.622918, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-925 lr=['0.0010000'], tr/val_loss:  0.000266/  3.620963, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-926 lr=['0.0010000'], tr/val_loss:  0.000266/  3.615864, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-927 lr=['0.0010000'], tr/val_loss:  0.000267/  3.610923, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-928 lr=['0.0010000'], tr/val_loss:  0.000271/  3.613271, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-929 lr=['0.0010000'], tr/val_loss:  0.000278/  3.615227, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-930 lr=['0.0010000'], tr/val_loss:  0.000295/  3.610508, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-931 lr=['0.0010000'], tr/val_loss:  0.000272/  3.615906, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-932 lr=['0.0010000'], tr/val_loss:  0.000260/  3.619624, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-933 lr=['0.0010000'], tr/val_loss:  0.000257/  3.621982, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-934 lr=['0.0010000'], tr/val_loss:  0.000269/  3.625161, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-935 lr=['0.0010000'], tr/val_loss:  0.000257/  3.620017, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-936 lr=['0.0010000'], tr/val_loss:  0.000259/  3.620725, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-937 lr=['0.0010000'], tr/val_loss:  0.000264/  3.624150, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-938 lr=['0.0010000'], tr/val_loss:  0.000260/  3.621964, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-939 lr=['0.0010000'], tr/val_loss:  0.001040/  3.623033, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-940 lr=['0.0010000'], tr/val_loss:  0.000947/  3.628633, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-941 lr=['0.0010000'], tr/val_loss:  0.001029/  3.637771, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-942 lr=['0.0010000'], tr/val_loss:  0.000521/  3.631969, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-943 lr=['0.0010000'], tr/val_loss:  0.000639/  3.614821, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-944 lr=['0.0010000'], tr/val_loss:  0.000384/  3.616599, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-945 lr=['0.0010000'], tr/val_loss:  0.000351/  3.609584, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-946 lr=['0.0010000'], tr/val_loss:  0.000515/  3.626404, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-947 lr=['0.0010000'], tr/val_loss:  0.000344/  3.633132, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-948 lr=['0.0010000'], tr/val_loss:  0.000305/  3.629060, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-949 lr=['0.0010000'], tr/val_loss:  0.000295/  3.633726, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-950 lr=['0.0010000'], tr/val_loss:  0.000284/  3.631125, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-951 lr=['0.0010000'], tr/val_loss:  0.000295/  3.628234, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-952 lr=['0.0010000'], tr/val_loss:  0.000278/  3.628563, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-953 lr=['0.0010000'], tr/val_loss:  0.000277/  3.632942, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-954 lr=['0.0010000'], tr/val_loss:  0.000275/  3.634204, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-955 lr=['0.0010000'], tr/val_loss:  0.000321/  3.633659, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-956 lr=['0.0010000'], tr/val_loss:  0.000294/  3.641015, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-957 lr=['0.0010000'], tr/val_loss:  0.000276/  3.633589, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-958 lr=['0.0010000'], tr/val_loss:  0.000278/  3.628192, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-959 lr=['0.0010000'], tr/val_loss:  0.000278/  3.638381, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-960 lr=['0.0010000'], tr/val_loss:  0.000277/  3.632153, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-961 lr=['0.0010000'], tr/val_loss:  0.000284/  3.630883, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-962 lr=['0.0010000'], tr/val_loss:  0.000279/  3.631621, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-963 lr=['0.0010000'], tr/val_loss:  0.000293/  3.639564, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-964 lr=['0.0010000'], tr/val_loss:  0.000283/  3.637569, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-965 lr=['0.0010000'], tr/val_loss:  0.000270/  3.634249, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-966 lr=['0.0010000'], tr/val_loss:  0.000267/  3.633318, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-967 lr=['0.0010000'], tr/val_loss:  0.000460/  3.631820, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-968 lr=['0.0010000'], tr/val_loss:  0.000463/  3.631444, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-969 lr=['0.0010000'], tr/val_loss:  0.000278/  3.630751, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-970 lr=['0.0010000'], tr/val_loss:  0.000271/  3.629070, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-971 lr=['0.0010000'], tr/val_loss:  0.000277/  3.629775, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-972 lr=['0.0010000'], tr/val_loss:  0.000303/  3.633728, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-973 lr=['0.0010000'], tr/val_loss:  0.000303/  3.633809, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-974 lr=['0.0010000'], tr/val_loss:  0.000284/  3.632947, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-975 lr=['0.0010000'], tr/val_loss:  0.000320/  3.629030, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-976 lr=['0.0010000'], tr/val_loss:  0.000294/  3.633071, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-977 lr=['0.0010000'], tr/val_loss:  0.000272/  3.638020, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-978 lr=['0.0010000'], tr/val_loss:  0.000260/  3.642727, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-979 lr=['0.0010000'], tr/val_loss:  0.000258/  3.638855, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-980 lr=['0.0010000'], tr/val_loss:  0.000262/  3.640827, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-981 lr=['0.0010000'], tr/val_loss:  0.000264/  3.639960, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-982 lr=['0.0010000'], tr/val_loss:  0.000285/  3.644499, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-983 lr=['0.0010000'], tr/val_loss:  0.000265/  3.643046, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-984 lr=['0.0010000'], tr/val_loss:  0.000264/  3.644410, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-985 lr=['0.0010000'], tr/val_loss:  0.000264/  3.643880, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-986 lr=['0.0010000'], tr/val_loss:  0.000276/  3.640335, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-987 lr=['0.0010000'], tr/val_loss:  0.000274/  3.635713, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-988 lr=['0.0010000'], tr/val_loss:  0.000260/  3.638462, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-989 lr=['0.0010000'], tr/val_loss:  0.000261/  3.639770, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-990 lr=['0.0010000'], tr/val_loss:  0.000262/  3.640953, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-991 lr=['0.0010000'], tr/val_loss:  0.000259/  3.644033, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-992 lr=['0.0010000'], tr/val_loss:  0.000927/  3.633459, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-993 lr=['0.0010000'], tr/val_loss:  0.000328/  3.641847, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-994 lr=['0.0010000'], tr/val_loss:  0.000285/  3.651986, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-995 lr=['0.0010000'], tr/val_loss:  0.000276/  3.640193, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-996 lr=['0.0010000'], tr/val_loss:  0.000278/  3.650714, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-997 lr=['0.0010000'], tr/val_loss:  0.000303/  3.641116, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-998 lr=['0.0010000'], tr/val_loss:  0.000293/  3.642304, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-999 lr=['0.0010000'], tr/val_loss:  0.000268/  3.648283, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1000 lr=['0.0010000'], tr/val_loss:  0.000305/  3.644235, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1001 lr=['0.0010000'], tr/val_loss:  0.000290/  3.647492, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1002 lr=['0.0010000'], tr/val_loss:  0.000266/  3.647209, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1003 lr=['0.0010000'], tr/val_loss:  0.000257/  3.648320, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1004 lr=['0.0010000'], tr/val_loss:  0.000255/  3.648165, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1005 lr=['0.0010000'], tr/val_loss:  0.000256/  3.648253, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1006 lr=['0.0010000'], tr/val_loss:  0.000257/  3.653479, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1007 lr=['0.0010000'], tr/val_loss:  0.000264/  3.648783, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1008 lr=['0.0010000'], tr/val_loss:  0.000259/  3.647278, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1009 lr=['0.0010000'], tr/val_loss:  0.000263/  3.657427, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1010 lr=['0.0010000'], tr/val_loss:  0.000268/  3.655071, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1011 lr=['0.0010000'], tr/val_loss:  0.000261/  3.654994, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1012 lr=['0.0010000'], tr/val_loss:  0.000270/  3.657809, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1013 lr=['0.0010000'], tr/val_loss:  0.000251/  3.656674, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1014 lr=['0.0010000'], tr/val_loss:  0.000255/  3.654031, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1015 lr=['0.0010000'], tr/val_loss:  0.000245/  3.658132, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1016 lr=['0.0010000'], tr/val_loss:  0.000250/  3.652206, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1017 lr=['0.0010000'], tr/val_loss:  0.000256/  3.652181, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1018 lr=['0.0010000'], tr/val_loss:  0.000260/  3.655699, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1019 lr=['0.0010000'], tr/val_loss:  0.000242/  3.650593, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1020 lr=['0.0010000'], tr/val_loss:  0.000278/  3.648490, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1021 lr=['0.0010000'], tr/val_loss:  0.000254/  3.649004, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1022 lr=['0.0010000'], tr/val_loss:  0.000246/  3.650882, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1023 lr=['0.0010000'], tr/val_loss:  0.000248/  3.649477, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1024 lr=['0.0010000'], tr/val_loss:  0.000264/  3.656378, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1025 lr=['0.0010000'], tr/val_loss:  0.000261/  3.652224, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1026 lr=['0.0010000'], tr/val_loss:  0.000251/  3.652720, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1027 lr=['0.0010000'], tr/val_loss:  0.000248/  3.654944, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1028 lr=['0.0010000'], tr/val_loss:  0.000244/  3.657400, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1029 lr=['0.0010000'], tr/val_loss:  0.000247/  3.655986, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1030 lr=['0.0010000'], tr/val_loss:  0.000247/  3.648999, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1031 lr=['0.0010000'], tr/val_loss:  0.000243/  3.653488, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1032 lr=['0.0010000'], tr/val_loss:  0.000236/  3.655061, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1033 lr=['0.0010000'], tr/val_loss:  0.000237/  3.652609, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1034 lr=['0.0010000'], tr/val_loss:  0.000245/  3.654949, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1035 lr=['0.0010000'], tr/val_loss:  0.000237/  3.656812, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1036 lr=['0.0010000'], tr/val_loss:  0.000241/  3.651335, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1037 lr=['0.0010000'], tr/val_loss:  0.000247/  3.657794, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1038 lr=['0.0010000'], tr/val_loss:  0.000301/  3.655493, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1039 lr=['0.0010000'], tr/val_loss:  0.000270/  3.653747, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1040 lr=['0.0010000'], tr/val_loss:  0.000294/  3.662129, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1041 lr=['0.0010000'], tr/val_loss:  0.000271/  3.657426, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1042 lr=['0.0010000'], tr/val_loss:  0.000265/  3.656018, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1043 lr=['0.0010000'], tr/val_loss:  0.000256/  3.655660, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1044 lr=['0.0010000'], tr/val_loss:  0.000256/  3.654289, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1045 lr=['0.0010000'], tr/val_loss:  0.000257/  3.661137, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1046 lr=['0.0010000'], tr/val_loss:  0.000249/  3.660756, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1047 lr=['0.0010000'], tr/val_loss:  0.000251/  3.665920, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1048 lr=['0.0010000'], tr/val_loss:  0.000238/  3.664539, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1049 lr=['0.0010000'], tr/val_loss:  0.000238/  3.664851, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1050 lr=['0.0010000'], tr/val_loss:  0.000231/  3.667480, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1051 lr=['0.0010000'], tr/val_loss:  0.000244/  3.665943, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1052 lr=['0.0010000'], tr/val_loss:  0.000226/  3.665822, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1053 lr=['0.0010000'], tr/val_loss:  0.000243/  3.669226, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1054 lr=['0.0010000'], tr/val_loss:  0.000228/  3.672020, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1055 lr=['0.0010000'], tr/val_loss:  0.000234/  3.671298, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1056 lr=['0.0010000'], tr/val_loss:  0.000224/  3.675522, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1057 lr=['0.0010000'], tr/val_loss:  0.000225/  3.680448, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1058 lr=['0.0010000'], tr/val_loss:  0.000232/  3.676569, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1059 lr=['0.0010000'], tr/val_loss:  0.000796/  3.680036, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1060 lr=['0.0010000'], tr/val_loss:  0.000555/  3.680483, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1061 lr=['0.0010000'], tr/val_loss:  0.000352/  3.686327, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1062 lr=['0.0010000'], tr/val_loss:  0.000283/  3.686150, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1063 lr=['0.0010000'], tr/val_loss:  0.000276/  3.681789, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1064 lr=['0.0010000'], tr/val_loss:  0.000282/  3.684111, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1065 lr=['0.0010000'], tr/val_loss:  0.000304/  3.688053, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1066 lr=['0.0010000'], tr/val_loss:  0.000262/  3.681751, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1067 lr=['0.0010000'], tr/val_loss:  0.001191/  3.680552, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1068 lr=['0.0010000'], tr/val_loss:  0.000935/  3.672225, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1069 lr=['0.0010000'], tr/val_loss:  0.000505/  3.688949, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1070 lr=['0.0010000'], tr/val_loss:  0.000382/  3.668305, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1071 lr=['0.0010000'], tr/val_loss:  0.000390/  3.682688, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1072 lr=['0.0010000'], tr/val_loss:  0.000290/  3.676236, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1073 lr=['0.0010000'], tr/val_loss:  0.000270/  3.683384, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1074 lr=['0.0010000'], tr/val_loss:  0.000269/  3.683520, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1075 lr=['0.0010000'], tr/val_loss:  0.000270/  3.688121, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1076 lr=['0.0010000'], tr/val_loss:  0.000246/  3.689826, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1077 lr=['0.0010000'], tr/val_loss:  0.000245/  3.685246, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1078 lr=['0.0010000'], tr/val_loss:  0.000243/  3.685266, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1079 lr=['0.0010000'], tr/val_loss:  0.000245/  3.684247, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1080 lr=['0.0010000'], tr/val_loss:  0.000236/  3.682508, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1081 lr=['0.0010000'], tr/val_loss:  0.000240/  3.683503, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1082 lr=['0.0010000'], tr/val_loss:  0.000242/  3.686574, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1083 lr=['0.0010000'], tr/val_loss:  0.000238/  3.689069, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1084 lr=['0.0010000'], tr/val_loss:  0.000242/  3.682975, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1085 lr=['0.0010000'], tr/val_loss:  0.000231/  3.685486, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1086 lr=['0.0010000'], tr/val_loss:  0.000245/  3.682037, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1087 lr=['0.0010000'], tr/val_loss:  0.000227/  3.679081, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1088 lr=['0.0010000'], tr/val_loss:  0.000230/  3.677646, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1089 lr=['0.0010000'], tr/val_loss:  0.000241/  3.673523, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1090 lr=['0.0010000'], tr/val_loss:  0.000259/  3.681535, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1091 lr=['0.0010000'], tr/val_loss:  0.000325/  3.679753, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1092 lr=['0.0010000'], tr/val_loss:  0.000448/  3.670353, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1093 lr=['0.0010000'], tr/val_loss:  0.000354/  3.683783, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1094 lr=['0.0010000'], tr/val_loss:  0.000301/  3.680169, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1095 lr=['0.0010000'], tr/val_loss:  0.000277/  3.675394, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1096 lr=['0.0010000'], tr/val_loss:  0.000264/  3.677379, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1097 lr=['0.0010000'], tr/val_loss:  0.000276/  3.667223, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1098 lr=['0.0010000'], tr/val_loss:  0.000257/  3.673707, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1099 lr=['0.0010000'], tr/val_loss:  0.000243/  3.670466, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1100 lr=['0.0010000'], tr/val_loss:  0.000249/  3.674088, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1101 lr=['0.0010000'], tr/val_loss:  0.000249/  3.674381, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1102 lr=['0.0010000'], tr/val_loss:  0.000253/  3.681405, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1103 lr=['0.0010000'], tr/val_loss:  0.000278/  3.677688, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1104 lr=['0.0010000'], tr/val_loss:  0.000245/  3.678210, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1105 lr=['0.0010000'], tr/val_loss:  0.000244/  3.684658, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1106 lr=['0.0010000'], tr/val_loss:  0.000228/  3.683846, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1107 lr=['0.0010000'], tr/val_loss:  0.000239/  3.685115, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1108 lr=['0.0010000'], tr/val_loss:  0.000232/  3.677428, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1109 lr=['0.0010000'], tr/val_loss:  0.000230/  3.679069, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1110 lr=['0.0010000'], tr/val_loss:  0.000252/  3.679288, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1111 lr=['0.0010000'], tr/val_loss:  0.000242/  3.674523, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1112 lr=['0.0010000'], tr/val_loss:  0.000241/  3.677327, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1113 lr=['0.0010000'], tr/val_loss:  0.000244/  3.683498, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1114 lr=['0.0010000'], tr/val_loss:  0.000279/  3.693907, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1115 lr=['0.0010000'], tr/val_loss:  0.000710/  3.695294, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1116 lr=['0.0010000'], tr/val_loss:  0.000304/  3.694023, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1117 lr=['0.0010000'], tr/val_loss:  0.000298/  3.695417, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1118 lr=['0.0010000'], tr/val_loss:  0.000261/  3.700490, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1119 lr=['0.0010000'], tr/val_loss:  0.000260/  3.703471, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1120 lr=['0.0010000'], tr/val_loss:  0.000256/  3.690648, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1121 lr=['0.0010000'], tr/val_loss:  0.000258/  3.698291, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1122 lr=['0.0010000'], tr/val_loss:  0.000235/  3.697311, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1123 lr=['0.0010000'], tr/val_loss:  0.000232/  3.698672, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1124 lr=['0.0010000'], tr/val_loss:  0.000240/  3.696378, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1125 lr=['0.0010000'], tr/val_loss:  0.000241/  3.704149, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1126 lr=['0.0010000'], tr/val_loss:  0.001334/  3.682657, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1127 lr=['0.0010000'], tr/val_loss:  0.000315/  3.691411, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1128 lr=['0.0010000'], tr/val_loss:  0.000259/  3.689582, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1129 lr=['0.0010000'], tr/val_loss:  0.000243/  3.689514, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1130 lr=['0.0010000'], tr/val_loss:  0.000236/  3.689682, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1131 lr=['0.0010000'], tr/val_loss:  0.000237/  3.683339, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1132 lr=['0.0010000'], tr/val_loss:  0.000237/  3.687161, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1133 lr=['0.0010000'], tr/val_loss:  0.000258/  3.687767, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1134 lr=['0.0010000'], tr/val_loss:  0.000235/  3.682548, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1135 lr=['0.0010000'], tr/val_loss:  0.000223/  3.684106, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1136 lr=['0.0010000'], tr/val_loss:  0.000220/  3.685833, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1137 lr=['0.0010000'], tr/val_loss:  0.000226/  3.679388, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1138 lr=['0.0010000'], tr/val_loss:  0.000235/  3.680891, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1139 lr=['0.0010000'], tr/val_loss:  0.000281/  3.693010, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1140 lr=['0.0010000'], tr/val_loss:  0.000242/  3.683607, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1141 lr=['0.0010000'], tr/val_loss:  0.000240/  3.684310, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1142 lr=['0.0010000'], tr/val_loss:  0.000223/  3.690561, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1143 lr=['0.0010000'], tr/val_loss:  0.000220/  3.689599, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1144 lr=['0.0010000'], tr/val_loss:  0.000222/  3.686049, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1145 lr=['0.0010000'], tr/val_loss:  0.000228/  3.683545, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1146 lr=['0.0010000'], tr/val_loss:  0.000251/  3.687119, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1147 lr=['0.0010000'], tr/val_loss:  0.000232/  3.696240, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1148 lr=['0.0010000'], tr/val_loss:  0.000217/  3.694212, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1149 lr=['0.0010000'], tr/val_loss:  0.000221/  3.694889, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1150 lr=['0.0010000'], tr/val_loss:  0.000215/  3.695041, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1151 lr=['0.0010000'], tr/val_loss:  0.000227/  3.692937, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1152 lr=['0.0010000'], tr/val_loss:  0.000218/  3.692797, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1153 lr=['0.0010000'], tr/val_loss:  0.000229/  3.694900, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1154 lr=['0.0010000'], tr/val_loss:  0.000494/  3.694818, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1155 lr=['0.0010000'], tr/val_loss:  0.000291/  3.691627, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1156 lr=['0.0010000'], tr/val_loss:  0.000315/  3.698205, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1157 lr=['0.0010000'], tr/val_loss:  0.000275/  3.690865, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1158 lr=['0.0010000'], tr/val_loss:  0.000240/  3.687645, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1159 lr=['0.0010000'], tr/val_loss:  0.000226/  3.691755, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1160 lr=['0.0010000'], tr/val_loss:  0.000226/  3.698155, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1161 lr=['0.0010000'], tr/val_loss:  0.000225/  3.700634, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1162 lr=['0.0010000'], tr/val_loss:  0.000214/  3.691856, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1163 lr=['0.0010000'], tr/val_loss:  0.000209/  3.690225, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1164 lr=['0.0010000'], tr/val_loss:  0.000210/  3.689553, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1165 lr=['0.0010000'], tr/val_loss:  0.000214/  3.688848, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1166 lr=['0.0010000'], tr/val_loss:  0.000213/  3.687967, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1167 lr=['0.0010000'], tr/val_loss:  0.000206/  3.687526, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1168 lr=['0.0010000'], tr/val_loss:  0.000209/  3.690096, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1169 lr=['0.0010000'], tr/val_loss:  0.000210/  3.685134, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1170 lr=['0.0010000'], tr/val_loss:  0.000206/  3.685197, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1171 lr=['0.0010000'], tr/val_loss:  0.000207/  3.691608, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1172 lr=['0.0010000'], tr/val_loss:  0.000201/  3.691274, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1173 lr=['0.0010000'], tr/val_loss:  0.000214/  3.696101, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1174 lr=['0.0010000'], tr/val_loss:  0.000212/  3.700440, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1175 lr=['0.0010000'], tr/val_loss:  0.000210/  3.697398, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1176 lr=['0.0010000'], tr/val_loss:  0.000203/  3.699175, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1177 lr=['0.0010000'], tr/val_loss:  0.000205/  3.700482, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1178 lr=['0.0010000'], tr/val_loss:  0.000211/  3.697383, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1179 lr=['0.0010000'], tr/val_loss:  0.000208/  3.700547, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1180 lr=['0.0010000'], tr/val_loss:  0.000208/  3.695971, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1181 lr=['0.0010000'], tr/val_loss:  0.000209/  3.694314, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1182 lr=['0.0010000'], tr/val_loss:  0.000204/  3.692200, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1183 lr=['0.0010000'], tr/val_loss:  0.000201/  3.689798, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1184 lr=['0.0010000'], tr/val_loss:  0.000205/  3.688252, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1185 lr=['0.0010000'], tr/val_loss:  0.000217/  3.694390, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1186 lr=['0.0010000'], tr/val_loss:  0.000211/  3.694669, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1187 lr=['0.0010000'], tr/val_loss:  0.000207/  3.694009, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1188 lr=['0.0010000'], tr/val_loss:  0.000204/  3.695341, val:  83.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1189 lr=['0.0010000'], tr/val_loss:  0.000215/  3.699579, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1190 lr=['0.0010000'], tr/val_loss:  0.000204/  3.702422, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1191 lr=['0.0010000'], tr/val_loss:  0.000365/  3.691998, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1192 lr=['0.0010000'], tr/val_loss:  0.000281/  3.685370, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1193 lr=['0.0010000'], tr/val_loss:  0.000249/  3.688787, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1194 lr=['0.0010000'], tr/val_loss:  0.000217/  3.690105, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1195 lr=['0.0010000'], tr/val_loss:  0.000219/  3.687129, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1196 lr=['0.0010000'], tr/val_loss:  0.000215/  3.688083, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1197 lr=['0.0010000'], tr/val_loss:  0.000217/  3.688468, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1198 lr=['0.0010000'], tr/val_loss:  0.000219/  3.686640, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1199 lr=['0.0010000'], tr/val_loss:  0.000223/  3.686073, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1200 lr=['0.0010000'], tr/val_loss:  0.000223/  3.682765, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1201 lr=['0.0010000'], tr/val_loss:  0.000209/  3.685480, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1202 lr=['0.0010000'], tr/val_loss:  0.000214/  3.687569, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1203 lr=['0.0010000'], tr/val_loss:  0.000217/  3.687267, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1204 lr=['0.0010000'], tr/val_loss:  0.000277/  3.681255, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1205 lr=['0.0010000'], tr/val_loss:  0.000245/  3.684115, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1206 lr=['0.0010000'], tr/val_loss:  0.000245/  3.692015, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1207 lr=['0.0010000'], tr/val_loss:  0.000221/  3.690464, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1208 lr=['0.0010000'], tr/val_loss:  0.000217/  3.687208, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1209 lr=['0.0010000'], tr/val_loss:  0.000218/  3.694750, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1210 lr=['0.0010000'], tr/val_loss:  0.000211/  3.693954, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1211 lr=['0.0010000'], tr/val_loss:  0.000211/  3.692087, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1212 lr=['0.0010000'], tr/val_loss:  0.000220/  3.692569, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1213 lr=['0.0010000'], tr/val_loss:  0.000212/  3.695807, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1214 lr=['0.0010000'], tr/val_loss:  0.000204/  3.694591, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1215 lr=['0.0010000'], tr/val_loss:  0.000206/  3.701272, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1216 lr=['0.0010000'], tr/val_loss:  0.000215/  3.699025, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1217 lr=['0.0010000'], tr/val_loss:  0.000218/  3.696789, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1218 lr=['0.0010000'], tr/val_loss:  0.000212/  3.703307, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1219 lr=['0.0010000'], tr/val_loss:  0.000202/  3.706585, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1220 lr=['0.0010000'], tr/val_loss:  0.000198/  3.706984, val:  83.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1221 lr=['0.0010000'], tr/val_loss:  0.000198/  3.709331, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1222 lr=['0.0010000'], tr/val_loss:  0.000221/  3.706942, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1223 lr=['0.0010000'], tr/val_loss:  0.000321/  3.698895, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1224 lr=['0.0010000'], tr/val_loss:  0.000217/  3.698508, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1225 lr=['0.0010000'], tr/val_loss:  0.000217/  3.701805, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1226 lr=['0.0010000'], tr/val_loss:  0.000223/  3.702276, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1227 lr=['0.0010000'], tr/val_loss:  0.000205/  3.696796, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1228 lr=['0.0010000'], tr/val_loss:  0.000208/  3.699068, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1229 lr=['0.0010000'], tr/val_loss:  0.000211/  3.695513, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1230 lr=['0.0010000'], tr/val_loss:  0.000208/  3.693827, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1231 lr=['0.0010000'], tr/val_loss:  0.000199/  3.694649, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1232 lr=['0.0010000'], tr/val_loss:  0.000211/  3.695220, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1233 lr=['0.0010000'], tr/val_loss:  0.000199/  3.694808, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1234 lr=['0.0010000'], tr/val_loss:  0.000201/  3.695069, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1235 lr=['0.0010000'], tr/val_loss:  0.000192/  3.696088, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1236 lr=['0.0010000'], tr/val_loss:  0.000194/  3.697703, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1237 lr=['0.0010000'], tr/val_loss:  0.000197/  3.689181, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1238 lr=['0.0010000'], tr/val_loss:  0.000199/  3.699779, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1239 lr=['0.0010000'], tr/val_loss:  0.000189/  3.697390, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1240 lr=['0.0010000'], tr/val_loss:  0.000188/  3.700859, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1241 lr=['0.0010000'], tr/val_loss:  0.000197/  3.697479, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1242 lr=['0.0010000'], tr/val_loss:  0.000194/  3.705674, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1243 lr=['0.0010000'], tr/val_loss:  0.000208/  3.699668, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1244 lr=['0.0010000'], tr/val_loss:  0.000202/  3.707071, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1245 lr=['0.0010000'], tr/val_loss:  0.000230/  3.708057, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1246 lr=['0.0010000'], tr/val_loss:  0.000254/  3.703916, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1247 lr=['0.0010000'], tr/val_loss:  0.000204/  3.704057, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1248 lr=['0.0010000'], tr/val_loss:  0.000198/  3.707528, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1249 lr=['0.0010000'], tr/val_loss:  0.000195/  3.715729, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-1250 lr=['0.0010000'], tr/val_loss:  0.000202/  3.709253, val:  82.08%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    }
   ],
   "source": [
    "### my_snn control board (Gesture) ########################\n",
    "decay = 0.25 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "const2 = True \n",
    "\n",
    "unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "if const2 == True:\n",
    "    const2 = decay\n",
    "else:\n",
    "    const2 = 0.0\n",
    "\n",
    "wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "my_snn_system(  devices = \"5\",\n",
    "                single_step = True, # True # False # DFA_on이랑 같이 가라\n",
    "                unique_name = run_name,\n",
    "                my_seed = 42,\n",
    "                TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "                BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 128, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "                # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = 'DVS_GESTURE_TONIC',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 0.720291189014991,   #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "                lif_layer_sg_width = 3.555718888923306, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "\n",
    "                synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "                synapse_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "                # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "                cfg = ['M', 'M', 200, 200], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "                # cfg = ['M', 'M', 64], \n",
    "                # cfg = [64, 124, 64, 124],\n",
    "                # cfg = ['M','M',512], \n",
    "                # cfg = [512], \n",
    "                # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                # cfg = ['M','M',512],\n",
    "                # cfg = ['M',200],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = ['M','M',200,200],\n",
    "                # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = ['M',200,200],\n",
    "                # cfg = ['M','M',1024,512,256,128,64],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = [12], #fc\n",
    "                # cfg = [12, 'M', 48, 'M', 12], \n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [],        \n",
    "                \n",
    "                net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "                learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                epoch_num = 10000,\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "                BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                dvs_clipping = 5, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "                # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "                dvs_duration = 100_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "                # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "                # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "                DFA_on = False, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "                trace_on = False,   # True # False\n",
    "                OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용\n",
    "\n",
    "                exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "                extra_train_dataset = 0, \n",
    "\n",
    "                num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "                chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "                pin_memory = True, # True # False \n",
    "\n",
    "                UDA_on = False,  # DECREPATED # uda\n",
    "                alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                bias = True, # True # False \n",
    "\n",
    "                last_lif = False,\n",
    "                ) \n",
    "\n",
    "# num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# num_workers = batch_size / num_GPU\n",
    "# num_workers = batch_size / num_CPU\n",
    "\n",
    "# sigmoid와 BN이 있어야 잘된다.\n",
    "# average pooling  \n",
    "# 이 낫다. \n",
    "\n",
    "# nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# # 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# # wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# run_name = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'random', # 'random', 'bayes'\n",
    "#     'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         \"learning_rate\": {\"values\": [0.001]}, #0.00936191669529645\n",
    "#         \"BATCH\": {\"values\": [16]},\n",
    "#         \"decay\": {\"values\": [0.25]},\n",
    "#         \"IMAGE_SIZE\": {\"values\": [128]},\n",
    "#         \"TIME\": {\"values\": [10]},\n",
    "#         \"epoch_num\": {\"values\": [200]},\n",
    "#         \"dvs_duration\": {\"values\": [25_000,50_000,100_000]},\n",
    "#         \"dvs_clipping\": {\"values\": [1,2,3,4,5]},\n",
    "#         \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "#         \"const2\": {\"values\": [False]},\n",
    "#         \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "#         \"DFA_on\": {\"values\": [False]},\n",
    "#         \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "#         \"cfg\": {\"values\": [['M','M',200,200]]},\n",
    "#         \"e_transport_swap\": {\"values\": [0]},\n",
    "#         \"e_transport_swap_tr\": {\"values\": [0]},\n",
    "#         \"drop_rate\": {\"values\": [0.0]}, # \"drop_rate\": {\"values\": [0.25,0.5,0.75]}, #\"drop_rate\": {\"min\": 0.25, \"max\": 0.75},\n",
    "#         \"exclude_class\": {\"values\": [True]},\n",
    "#         \"merge_polarities\": {\"values\": [False]},\n",
    "#         \"lif_layer_v_reset\": {\"values\": [10000]},\n",
    "#         \"lif_layer_sg_width\": {\"values\": [3.555718888923306]},\n",
    "#         \"e_transport_swap_coin\": {\"values\": [1]},\n",
    "#         \"lif_layer_v_threshold\": {\"values\": [0.720291189014991]},\n",
    "#         \"scheduler_name\": {\"values\": ['no']},  # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "#         \"denoise_on\": {\"values\": [True,False]}, \n",
    "#         \"I_wanna_sweep_at_this_epoch\": {\"values\": [-1]}, \n",
    "#         \"dvs_duration_domain\": {\"values\": [[]]}, \n",
    "#         \"dvs_relative_timestep\": {\"values\": [[False]]}, \n",
    "#         \"extra_train_dataset\": {\"values\": [0]}, \n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     unique_name = unique_name_hyper ## 이거 설정하면 새로운 경로에 모두 save\n",
    "    \n",
    "#     wandb.init(save_code = True)\n",
    "#     learning_rate  =  wandb.config.learning_rate\n",
    "#     BATCH  =  wandb.config.BATCH\n",
    "#     decay  =  wandb.config.decay\n",
    "#     IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "#     TIME  =  wandb.config.TIME\n",
    "#     epoch_num  =  wandb.config.epoch_num \n",
    "#     dvs_duration  =  wandb.config.dvs_duration\n",
    "#     dvs_clipping  =  wandb.config.dvs_clipping\n",
    "#     which_data  =  wandb.config.which_data\n",
    "#     const2  =  wandb.config.const2\n",
    "#     surrogate  =  wandb.config.surrogate\n",
    "#     DFA_on  =  wandb.config.DFA_on\n",
    "#     OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on\n",
    "#     cfg  =  wandb.config.cfg\n",
    "#     e_transport_swap  =  wandb.config.e_transport_swap\n",
    "#     e_transport_swap_tr  =  wandb.config.e_transport_swap_tr\n",
    "#     drop_rate  =  wandb.config.drop_rate\n",
    "#     exclude_class  =  wandb.config.exclude_class\n",
    "#     merge_polarities  =  wandb.config.merge_polarities\n",
    "#     lif_layer_v_reset  =  wandb.config.lif_layer_v_reset\n",
    "#     lif_layer_sg_width  =  wandb.config.lif_layer_sg_width\n",
    "#     e_transport_swap_coin  =  wandb.config.e_transport_swap_coin\n",
    "#     lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold\n",
    "#     scheduler_name  =  wandb.config.scheduler_name\n",
    "#     denoise_on  =  wandb.config.denoise_on\n",
    "#     I_wanna_sweep_at_this_epoch  =  wandb.config.I_wanna_sweep_at_this_epoch\n",
    "#     dvs_duration_domain  =  wandb.config.dvs_duration_domain\n",
    "#     dvs_relative_timestep  =  wandb.config.dvs_relative_timestep\n",
    "#     extra_train_dataset  =  wandb.config.extra_train_dataset\n",
    "#     if const2 == True:\n",
    "#         const2 = decay\n",
    "#     else:\n",
    "#         const2 = 0.0\n",
    "\n",
    "#     my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = TIME , # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "#                 BATCH = BATCH, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = IMAGE_SIZE, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "#                 #pmnist는 28로 해야 됨. 나머지는 바꿔도 돌아는 감.\n",
    "\n",
    "#                 # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "#                 which_data = which_data,\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = lif_layer_v_threshold,  # 10000이상으로 하면 NDA LIF 씀. #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = lif_layer_v_reset, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "#                 lif_layer_sg_width = lif_layer_sg_width, # # surrogate sigmoid 쓸 때는 의미없음\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "#                 synapse_conv_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_conv_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "#                 synapse_fc_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_fc_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "#                 # cfg = [64, 64],\n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 cfg = cfg,\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [], \n",
    "                \n",
    "#                 net_print = True, # True # False # True로 하길 추천\n",
    "#                 weight_count_print = False, # True # False\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 learning_rate = learning_rate, # default 0.001  # ottt 0.1 # nda 0.001 \n",
    "#                 epoch_num = epoch_num,\n",
    "#                 verbose_interval = 999999999, #숫자 크게 하면 꺼짐 #걍 중간중간 iter에서 끊어서 출력\n",
    "#                 validation_interval =  999999999,#999999999, #숫자 크게 하면 에포크 마지막 iter 때 val 함\n",
    "\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = surrogate, # 'rectangle' 'sigmoid' 'rough_rectangle'\n",
    "                \n",
    "#                 gradient_verbose = False,  # True # False  # weight gradient 각 layer마다 띄워줌\n",
    "\n",
    "#                 BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = scheduler_name, # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False,   # True # False \n",
    "#                 # 지원 DATASET: cifar10, mnist\n",
    "\n",
    "#                 nda_net = False,   # True # False\n",
    "\n",
    "#                 domain_il_epoch = 0, # over 0, then domain il mode on # pmnist 쓸거면 HLOP 코드보고 더 디벨롭하셈. 지금 개발 hold함.\n",
    "                \n",
    "#                 dvs_clipping = dvs_clipping, # 숫자만큼 크면 spike 아니면 걍 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "\n",
    "#                 dvs_duration = dvs_duration, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "#                 # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "\n",
    "#                 DFA_on = DFA_on, # True # False # residual은 dfa지원안함.\n",
    "#                 OTTT_input_trace_on = OTTT_input_trace_on, # True # False # 맨 처음 input에 trace 적용\n",
    "                 \n",
    "#                 e_transport_swap = e_transport_swap, # 1 이상이면 해당 숫자 에포크만큼 val_acc_best가 변화가 없으면 e_transport scheme (BP vs DFA) swap\n",
    "#                 e_transport_swap_tr = e_transport_swap_tr, # 1 이상이면 해당 숫자 에포크만큼 tr_acc_best가 변화가 없으면 e_transport scheme (BP vs DFA) swap\n",
    "#                 e_transport_swap_coin = e_transport_swap_coin, # swap할 수 있는 coin 개수\n",
    "                    \n",
    "#                 drop_rate = drop_rate,\n",
    "\n",
    "#                 exclude_class = exclude_class, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "#                 merge_polarities = merge_polarities, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "#                 denoise_on = denoise_on,\n",
    "\n",
    "#                 I_wanna_sweep_at_this_epoch = I_wanna_sweep_at_this_epoch,\n",
    "#                 dvs_duration_domain = dvs_duration_domain,\n",
    "#                 dvs_relative_timestep = dvs_relative_timestep, # True # False \n",
    "\n",
    "#                 extra_train_dataset = extra_train_dataset,\n",
    "\n",
    "#                 num_workers = 2,\n",
    "#                 chaching_on = True,\n",
    "#                 pin_memory = True, # True # False\n",
    "#                     ) \n",
    "#     # sigmoid와 BN이 있어야 잘된다.\n",
    "#     # average pooling\n",
    "#     # 이 낫다. \n",
    "    \n",
    "#     # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import json\n",
    "# run_name = 'main_FINAL_TEST'\n",
    "\n",
    "# unique_name = run_name\n",
    "# def pad_array_to_match_length(array1, array2):\n",
    "#     if len(array1) > len(array2):\n",
    "#         padded_array2 = np.pad(array2, (0, len(array1) - len(array2)), 'constant')\n",
    "#         return array1, padded_array2\n",
    "#     elif len(array2) > len(array1):\n",
    "#         padded_array1 = np.pad(array1, (0, len(array2) - len(array1)), 'constant')\n",
    "#         return padded_array1, array2\n",
    "#     else:\n",
    "#         return array1, array2\n",
    "# def load_hyperparameters(filename=f'result_save/hyperparameters_{unique_name}.json'):\n",
    "#     with open(filename, 'r') as f:\n",
    "#         return json.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# current_time = '20240628_110116'\n",
    "# base_name = f'{current_time}'\n",
    "# iter_acc_file_name = f'result_save/{base_name}_iter_acc_array_{unique_name}.npy'\n",
    "# val_acc_file_name = f'result_save/{base_name}_val_acc_now_array_{unique_name}.npy'\n",
    "# hyperparameters_file_name = f'result_save/{base_name}_hyperparameters_{unique_name}.json'\n",
    "\n",
    "# ### if you want to just see most recent train and val acc###########################\n",
    "# iter_acc_file_name = f'result_save/iter_acc_array_{unique_name}.npy'\n",
    "# tr_acc_file_name = f'result_save/tr_acc_array_{unique_name}.npy'\n",
    "# val_acc_file_name = f'result_save/val_acc_now_array_{unique_name}.npy'\n",
    "# hyperparameters_file_name = f'result_save/hyperparameters_{unique_name}.json'\n",
    "\n",
    "# loaded_iter_acc_array = np.load(iter_acc_file_name)*100\n",
    "# loaded_tr_acc_array = np.load(tr_acc_file_name)*100\n",
    "# loaded_val_acc_array = np.load(val_acc_file_name)*100\n",
    "# hyperparameters = load_hyperparameters(hyperparameters_file_name)\n",
    "\n",
    "# loaded_iter_acc_array, loaded_val_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_val_acc_array)\n",
    "# loaded_iter_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_iter_acc_array, loaded_tr_acc_array)\n",
    "# loaded_val_acc_array, loaded_tr_acc_array = pad_array_to_match_length(loaded_val_acc_array, loaded_tr_acc_array)\n",
    "\n",
    "# top_iter_acc = np.max(loaded_iter_acc_array)\n",
    "# top_tr_acc = np.max(loaded_tr_acc_array)\n",
    "# top_val_acc = np.max(loaded_val_acc_array)\n",
    "\n",
    "# which_data = hyperparameters['which_data']\n",
    "# BPTT_on = hyperparameters['BPTT_on']\n",
    "# current_epoch = hyperparameters['current epoch']\n",
    "# surrogate = hyperparameters['surrogate']\n",
    "# cfg = hyperparameters['cfg']\n",
    "# tdBN_on = hyperparameters['tdBN_on']\n",
    "# BN_on = hyperparameters['BN_on']\n",
    "\n",
    "\n",
    "# iterations = np.arange(len(loaded_iter_acc_array))\n",
    "\n",
    "# # 그래프 그리기\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(iterations, loaded_iter_acc_array, label='Iter Accuracy', color='g', alpha=0.2)\n",
    "# plt.plot(iterations, loaded_tr_acc_array, label='Training Accuracy', color='b')\n",
    "# plt.plot(iterations, loaded_val_acc_array, label='Validation Accuracy', color='r')\n",
    "\n",
    "# # # 텍스트 추가\n",
    "# # plt.text(0.05, 0.95, f'Top Training Accuracy: {100*top_iter_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='blue')\n",
    "# # plt.text(0.05, 0.90, f'Top Validation Accuracy: {100*top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='left', color='red')\n",
    "# # 텍스트 추가\n",
    "# plt.text(0.5, 0.10, f'Top Training Accuracy: {top_tr_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='blue')\n",
    "# plt.text(0.5, 0.05, f'Top Validation Accuracy: {top_val_acc:.2f}%', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', horizontalalignment='center', color='red')\n",
    "\n",
    "# plt.xlabel('Iterations')\n",
    "# plt.ylabel('Accuracy [%]')\n",
    "\n",
    "# # 그래프 제목에 하이퍼파라미터 정보 추가\n",
    "# title = f'Training and Validation Accuracy over Iterations\\n\\nData: {which_data}, BPTT: {\"On\" if BPTT_on else \"Off\"}, Current Epoch: {current_epoch}, Surrogate: {surrogate},\\nCFG: {cfg}, tdBN: {\"On\" if tdBN_on else \"Off\"}, BN: {\"On\" if BN_on else \"Off\"}'\n",
    "\n",
    "# plt.title(title)\n",
    "\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.xlim(0)  # x축을 0부터 시작\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
