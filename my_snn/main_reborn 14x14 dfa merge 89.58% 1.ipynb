{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12382/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #     criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    slice_bucket.append(slice_concat)\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                slice_bucket.append(slice_concat)\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb 과거 하이퍼파라미터 가져와서 붙여넣기 (devices unique_name은 니가 할당해라)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],)\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_164850-rl3u9krs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rl3u9krs' target=\"_blank\">dauntless-vortex-7435</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rl3u9krs' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/rl3u9krs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0.0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1.0, 'lif_layer_v_reset': 10000.0, 'lif_layer_sg_width': 3.0, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_main.pth', 'learning_rate': 0.01, 'epoch_num': 10000, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 5, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1.0, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1} \n",
      "\n",
      "dataset_hash = 14688881ebd6a7fed8e9c9f512432e6a\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0.0, v_decay=0.25, v_threshold=1.0, v_reset=10000.0, sg_width=3.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0.0, v_decay=0.25, v_threshold=1.0, v_reset=10000.0, sg_width=3.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.231682/  1.725753, val:  33.75%, val_best:  33.75%, tr:  14.20%, tr_best:  14.20%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.502246/  1.526380, val:  56.25%, val_best:  56.25%, tr:  51.38%, tr_best:  51.38%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.485144/  1.639171, val:  57.08%, val_best:  57.08%, tr:  57.10%, tr_best:  57.10%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.264549/  1.770136, val:  52.92%, val_best:  57.08%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.248263/  1.324783, val:  62.50%, val_best:  62.50%, tr:  62.21%, tr_best:  62.72%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.102589/  1.560552, val:  56.25%, val_best:  62.50%, tr:  68.13%, tr_best:  68.13%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.127105/  1.391897, val:  62.08%, val_best:  62.50%, tr:  67.82%, tr_best:  68.13%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.157054/  1.464881, val:  60.42%, val_best:  62.50%, tr:  67.82%, tr_best:  68.13%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.123250/  1.456251, val:  70.00%, val_best:  70.00%, tr:  70.17%, tr_best:  70.17%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.170959/  1.314588, val:  65.42%, val_best:  70.00%, tr:  71.71%, tr_best:  71.71%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  1.111409/  1.223261, val:  69.17%, val_best:  70.00%, tr:  73.75%, tr_best:  73.75%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.970058/  1.358844, val:  69.58%, val_best:  70.00%, tr:  76.10%, tr_best:  76.10%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.911124/  1.255785, val:  74.58%, val_best:  74.58%, tr:  76.10%, tr_best:  76.10%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.830981/  1.404512, val:  66.25%, val_best:  74.58%, tr:  81.61%, tr_best:  81.61%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.867591/  1.586107, val:  69.17%, val_best:  74.58%, tr:  79.88%, tr_best:  81.61%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.696896/  1.371113, val:  75.42%, val_best:  75.42%, tr:  87.44%, tr_best:  87.44%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.695881/  1.313211, val:  78.75%, val_best:  78.75%, tr:  87.44%, tr_best:  87.44%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.645288/  1.370996, val:  75.00%, val_best:  78.75%, tr:  91.22%, tr_best:  91.22%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.608394/  1.420346, val:  74.58%, val_best:  78.75%, tr:  92.85%, tr_best:  92.85%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.569320/  1.282387, val:  81.25%, val_best:  81.25%, tr:  93.46%, tr_best:  93.46%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.537541/  1.425604, val:  75.00%, val_best:  81.25%, tr:  93.97%, tr_best:  93.97%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.520091/  1.365157, val:  77.50%, val_best:  81.25%, tr:  93.16%, tr_best:  93.97%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.447665/  1.458992, val:  77.50%, val_best:  81.25%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.486769/  1.445451, val:  83.75%, val_best:  83.75%, tr:  94.48%, tr_best:  96.12%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.419895/  1.405383, val:  83.75%, val_best:  83.75%, tr:  96.63%, tr_best:  96.63%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.387360/  1.488586, val:  78.75%, val_best:  83.75%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.396446/  1.496646, val:  82.08%, val_best:  83.75%, tr:  97.24%, tr_best:  97.65%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.347656/  1.606530, val:  78.33%, val_best:  83.75%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.354080/  1.495938, val:  82.08%, val_best:  83.75%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.326816/  1.728316, val:  77.50%, val_best:  83.75%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.314989/  1.548785, val:  82.08%, val_best:  83.75%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.286499/  1.576852, val:  82.50%, val_best:  83.75%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.379415/  1.764387, val:  75.42%, val_best:  83.75%, tr:  96.42%, tr_best:  99.08%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.294208/  1.791745, val:  80.00%, val_best:  83.75%, tr:  98.57%, tr_best:  99.08%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.280195/  1.779899, val:  80.00%, val_best:  83.75%, tr:  98.77%, tr_best:  99.08%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.259849/  1.654489, val:  85.42%, val_best:  85.42%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.225250/  1.648249, val:  84.58%, val_best:  85.42%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.219818/  1.683023, val:  82.50%, val_best:  85.42%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.221132/  1.722826, val:  85.00%, val_best:  85.42%, tr:  99.18%, tr_best:  99.59%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.219920/  1.733050, val:  85.42%, val_best:  85.42%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.187905/  1.726974, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.186896/  1.821142, val:  85.83%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.171857/  1.825525, val:  83.75%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.157112/  1.833568, val:  83.75%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.155573/  1.922752, val:  80.42%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.149704/  1.890066, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.155295/  1.944295, val:  83.75%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.145255/  1.938594, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.162080/  1.923778, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.122033/  1.910164, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.120061/  1.946535, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.111476/  1.999621, val:  85.42%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.125914/  2.069286, val:  82.92%, val_best:  87.50%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.121611/  2.029336, val:  85.00%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.119072/  2.003530, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.099003/  2.111725, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.100748/  2.161664, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.091105/  2.142911, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.081562/  2.164029, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.083714/  2.180006, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.074662/  2.172743, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.077839/  2.196053, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.076606/  2.234782, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.071461/  2.238532, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.089436/  2.279516, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.069939/  2.289559, val:  87.08%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.066738/  2.295385, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.056468/  2.333027, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.061742/  2.374368, val:  87.08%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.050225/  2.401031, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.047095/  2.325894, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.050537/  2.386467, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.041838/  2.464985, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.048119/  2.462368, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.046133/  2.480167, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.045559/  2.485793, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.042508/  2.490080, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.045580/  2.545574, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.041018/  2.587059, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.042720/  2.553449, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.036753/  2.608487, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.037326/  2.615348, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.034386/  2.606033, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.034790/  2.609849, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.041508/  2.590512, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.029394/  2.614373, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.029366/  2.604161, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.033145/  2.642275, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.034232/  2.688861, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.033353/  2.659101, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.028558/  2.669632, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.026406/  2.739908, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.027827/  2.740283, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.026233/  2.731324, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.028431/  2.766816, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.027781/  2.768856, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.019218/  2.781971, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.018251/  2.833440, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.020688/  2.887144, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.023139/  2.857838, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-100 lr=['0.0100000'], tr/val_loss:  0.025445/  2.863735, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-101 lr=['0.0100000'], tr/val_loss:  0.022251/  2.868055, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-102 lr=['0.0100000'], tr/val_loss:  0.020591/  2.906389, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-103 lr=['0.0100000'], tr/val_loss:  0.020175/  2.917064, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-104 lr=['0.0100000'], tr/val_loss:  0.023405/  2.921782, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-105 lr=['0.0100000'], tr/val_loss:  0.019924/  2.888725, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-106 lr=['0.0100000'], tr/val_loss:  0.017671/  2.909557, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-107 lr=['0.0100000'], tr/val_loss:  0.016954/  2.912626, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-108 lr=['0.0100000'], tr/val_loss:  0.018955/  2.913003, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-109 lr=['0.0100000'], tr/val_loss:  0.019886/  2.918146, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-110 lr=['0.0100000'], tr/val_loss:  0.017117/  2.917315, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-111 lr=['0.0100000'], tr/val_loss:  0.020947/  2.977052, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-112 lr=['0.0100000'], tr/val_loss:  0.020859/  2.904686, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-113 lr=['0.0100000'], tr/val_loss:  0.015906/  3.024947, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-114 lr=['0.0100000'], tr/val_loss:  0.016652/  3.044051, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-115 lr=['0.0100000'], tr/val_loss:  0.015716/  3.048381, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-116 lr=['0.0100000'], tr/val_loss:  0.018135/  2.984577, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-117 lr=['0.0100000'], tr/val_loss:  0.020441/  2.982991, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-118 lr=['0.0100000'], tr/val_loss:  0.017327/  3.054592, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-119 lr=['0.0100000'], tr/val_loss:  0.013800/  3.040455, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-120 lr=['0.0100000'], tr/val_loss:  0.013403/  3.011140, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-121 lr=['0.0100000'], tr/val_loss:  0.014037/  3.068401, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-122 lr=['0.0100000'], tr/val_loss:  0.017997/  3.121901, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-123 lr=['0.0100000'], tr/val_loss:  0.014658/  3.078465, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-124 lr=['0.0100000'], tr/val_loss:  0.015637/  3.094901, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-125 lr=['0.0100000'], tr/val_loss:  0.015020/  3.093808, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-126 lr=['0.0100000'], tr/val_loss:  0.014910/  3.147661, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-127 lr=['0.0100000'], tr/val_loss:  0.013318/  3.146467, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-128 lr=['0.0100000'], tr/val_loss:  0.013286/  3.144099, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-129 lr=['0.0100000'], tr/val_loss:  0.011120/  3.163739, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-130 lr=['0.0100000'], tr/val_loss:  0.014925/  3.104382, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-131 lr=['0.0100000'], tr/val_loss:  0.014631/  3.156178, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-132 lr=['0.0100000'], tr/val_loss:  0.010936/  3.248726, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-133 lr=['0.0100000'], tr/val_loss:  0.011396/  3.223279, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-134 lr=['0.0100000'], tr/val_loss:  0.012304/  3.226165, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-135 lr=['0.0100000'], tr/val_loss:  0.014382/  3.231908, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-136 lr=['0.0100000'], tr/val_loss:  0.015629/  3.224138, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-137 lr=['0.0100000'], tr/val_loss:  0.012740/  3.213380, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-138 lr=['0.0100000'], tr/val_loss:  0.014563/  3.264351, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-139 lr=['0.0100000'], tr/val_loss:  0.014359/  3.253829, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-140 lr=['0.0100000'], tr/val_loss:  0.015281/  3.269230, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-141 lr=['0.0100000'], tr/val_loss:  0.021871/  3.203746, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-142 lr=['0.0100000'], tr/val_loss:  0.012871/  3.212361, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-143 lr=['0.0100000'], tr/val_loss:  0.010423/  3.197139, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-144 lr=['0.0100000'], tr/val_loss:  0.011795/  3.212224, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-145 lr=['0.0100000'], tr/val_loss:  0.012298/  3.219350, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-146 lr=['0.0100000'], tr/val_loss:  0.012475/  3.201828, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-147 lr=['0.0100000'], tr/val_loss:  0.010244/  3.230706, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-148 lr=['0.0100000'], tr/val_loss:  0.010510/  3.296210, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-149 lr=['0.0100000'], tr/val_loss:  0.009617/  3.285963, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-150 lr=['0.0100000'], tr/val_loss:  0.009011/  3.320431, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-151 lr=['0.0100000'], tr/val_loss:  0.010810/  3.325761, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-152 lr=['0.0100000'], tr/val_loss:  0.009682/  3.302352, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-153 lr=['0.0100000'], tr/val_loss:  0.009118/  3.318408, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-154 lr=['0.0100000'], tr/val_loss:  0.007893/  3.371507, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-155 lr=['0.0100000'], tr/val_loss:  0.008527/  3.383905, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-156 lr=['0.0100000'], tr/val_loss:  0.012495/  3.353760, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-157 lr=['0.0100000'], tr/val_loss:  0.011473/  3.342953, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-158 lr=['0.0100000'], tr/val_loss:  0.010103/  3.378057, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-159 lr=['0.0100000'], tr/val_loss:  0.017047/  3.355890, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-160 lr=['0.0100000'], tr/val_loss:  0.017555/  3.393826, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-161 lr=['0.0100000'], tr/val_loss:  0.011341/  3.388094, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-162 lr=['0.0100000'], tr/val_loss:  0.011526/  3.347811, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-163 lr=['0.0100000'], tr/val_loss:  0.011917/  3.372674, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-164 lr=['0.0100000'], tr/val_loss:  0.012501/  3.365704, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-165 lr=['0.0100000'], tr/val_loss:  0.010761/  3.376103, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-166 lr=['0.0100000'], tr/val_loss:  0.010473/  3.394904, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-167 lr=['0.0100000'], tr/val_loss:  0.014855/  3.424744, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-168 lr=['0.0100000'], tr/val_loss:  0.010258/  3.394464, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-169 lr=['0.0100000'], tr/val_loss:  0.009305/  3.420663, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-170 lr=['0.0100000'], tr/val_loss:  0.008184/  3.466568, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-171 lr=['0.0100000'], tr/val_loss:  0.009743/  3.455068, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-172 lr=['0.0100000'], tr/val_loss:  0.008446/  3.434962, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-173 lr=['0.0100000'], tr/val_loss:  0.009297/  3.422692, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-174 lr=['0.0100000'], tr/val_loss:  0.010013/  3.440474, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-175 lr=['0.0100000'], tr/val_loss:  0.008139/  3.464007, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-176 lr=['0.0100000'], tr/val_loss:  0.007856/  3.422937, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-177 lr=['0.0100000'], tr/val_loss:  0.006932/  3.428756, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-178 lr=['0.0100000'], tr/val_loss:  0.009265/  3.430463, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-179 lr=['0.0100000'], tr/val_loss:  0.009489/  3.460381, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-180 lr=['0.0100000'], tr/val_loss:  0.008173/  3.455068, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-181 lr=['0.0100000'], tr/val_loss:  0.005516/  3.468563, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-182 lr=['0.0100000'], tr/val_loss:  0.008039/  3.493440, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-183 lr=['0.0100000'], tr/val_loss:  0.007540/  3.527032, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-184 lr=['0.0100000'], tr/val_loss:  0.006840/  3.513713, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-185 lr=['0.0100000'], tr/val_loss:  0.007127/  3.526192, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-186 lr=['0.0100000'], tr/val_loss:  0.006253/  3.543428, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-187 lr=['0.0100000'], tr/val_loss:  0.006944/  3.530000, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-188 lr=['0.0100000'], tr/val_loss:  0.006175/  3.515532, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-189 lr=['0.0100000'], tr/val_loss:  0.006722/  3.523660, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-190 lr=['0.0100000'], tr/val_loss:  0.006665/  3.533414, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-191 lr=['0.0100000'], tr/val_loss:  0.006764/  3.572186, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-192 lr=['0.0100000'], tr/val_loss:  0.007157/  3.578498, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-193 lr=['0.0100000'], tr/val_loss:  0.004611/  3.561882, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-194 lr=['0.0100000'], tr/val_loss:  0.006488/  3.569865, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-195 lr=['0.0100000'], tr/val_loss:  0.005514/  3.563774, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-196 lr=['0.0100000'], tr/val_loss:  0.004866/  3.589757, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-197 lr=['0.0100000'], tr/val_loss:  0.004986/  3.587655, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-198 lr=['0.0100000'], tr/val_loss:  0.006311/  3.549649, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-199 lr=['0.0100000'], tr/val_loss:  0.006824/  3.536846, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-200 lr=['0.0100000'], tr/val_loss:  0.006253/  3.588607, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-201 lr=['0.0100000'], tr/val_loss:  0.007586/  3.548480, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-202 lr=['0.0100000'], tr/val_loss:  0.011230/  3.588450, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-203 lr=['0.0100000'], tr/val_loss:  0.005760/  3.565206, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-204 lr=['0.0100000'], tr/val_loss:  0.006246/  3.557893, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-205 lr=['0.0100000'], tr/val_loss:  0.006024/  3.579781, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-206 lr=['0.0100000'], tr/val_loss:  0.005471/  3.569299, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-207 lr=['0.0100000'], tr/val_loss:  0.004802/  3.643720, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-208 lr=['0.0100000'], tr/val_loss:  0.005819/  3.635112, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-209 lr=['0.0100000'], tr/val_loss:  0.004667/  3.607188, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-210 lr=['0.0100000'], tr/val_loss:  0.005452/  3.614346, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-211 lr=['0.0100000'], tr/val_loss:  0.004967/  3.625681, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-212 lr=['0.0100000'], tr/val_loss:  0.004098/  3.629213, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-213 lr=['0.0100000'], tr/val_loss:  0.004671/  3.620958, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-214 lr=['0.0100000'], tr/val_loss:  0.005990/  3.642831, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-215 lr=['0.0100000'], tr/val_loss:  0.005858/  3.637437, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-216 lr=['0.0100000'], tr/val_loss:  0.004725/  3.639525, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-217 lr=['0.0100000'], tr/val_loss:  0.004490/  3.584068, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-218 lr=['0.0100000'], tr/val_loss:  0.004266/  3.651774, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-219 lr=['0.0100000'], tr/val_loss:  0.006758/  3.644658, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-220 lr=['0.0100000'], tr/val_loss:  0.005636/  3.653434, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-221 lr=['0.0100000'], tr/val_loss:  0.005697/  3.648500, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-222 lr=['0.0100000'], tr/val_loss:  0.005519/  3.672572, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-223 lr=['0.0100000'], tr/val_loss:  0.003948/  3.669549, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-224 lr=['0.0100000'], tr/val_loss:  0.004079/  3.676240, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-225 lr=['0.0100000'], tr/val_loss:  0.004924/  3.685038, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-226 lr=['0.0100000'], tr/val_loss:  0.004579/  3.714756, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-227 lr=['0.0100000'], tr/val_loss:  0.004514/  3.682518, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-228 lr=['0.0100000'], tr/val_loss:  0.005349/  3.707568, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-229 lr=['0.0100000'], tr/val_loss:  0.003685/  3.714847, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-230 lr=['0.0100000'], tr/val_loss:  0.004771/  3.706364, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-231 lr=['0.0100000'], tr/val_loss:  0.004951/  3.731692, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-232 lr=['0.0100000'], tr/val_loss:  0.005597/  3.675605, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-233 lr=['0.0100000'], tr/val_loss:  0.004840/  3.715599, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-234 lr=['0.0100000'], tr/val_loss:  0.005032/  3.744400, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-235 lr=['0.0100000'], tr/val_loss:  0.004227/  3.737130, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-236 lr=['0.0100000'], tr/val_loss:  0.004723/  3.739229, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-237 lr=['0.0100000'], tr/val_loss:  0.006617/  3.776785, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-238 lr=['0.0100000'], tr/val_loss:  0.006434/  3.775503, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-239 lr=['0.0100000'], tr/val_loss:  0.004823/  3.770920, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-240 lr=['0.0100000'], tr/val_loss:  0.004965/  3.741003, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-241 lr=['0.0100000'], tr/val_loss:  0.004746/  3.763694, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-242 lr=['0.0100000'], tr/val_loss:  0.004827/  3.735247, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-243 lr=['0.0100000'], tr/val_loss:  0.004776/  3.725288, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-244 lr=['0.0100000'], tr/val_loss:  0.004524/  3.732648, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-245 lr=['0.0100000'], tr/val_loss:  0.004842/  3.730836, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-246 lr=['0.0100000'], tr/val_loss:  0.004214/  3.775402, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-247 lr=['0.0100000'], tr/val_loss:  0.004437/  3.788750, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-248 lr=['0.0100000'], tr/val_loss:  0.004451/  3.754287, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-249 lr=['0.0100000'], tr/val_loss:  0.005789/  3.809562, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-250 lr=['0.0100000'], tr/val_loss:  0.007024/  3.838013, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-251 lr=['0.0100000'], tr/val_loss:  0.006002/  3.746388, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-252 lr=['0.0100000'], tr/val_loss:  0.004628/  3.773348, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-253 lr=['0.0100000'], tr/val_loss:  0.005400/  3.761872, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-254 lr=['0.0100000'], tr/val_loss:  0.004659/  3.745012, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-255 lr=['0.0100000'], tr/val_loss:  0.004062/  3.821501, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-256 lr=['0.0100000'], tr/val_loss:  0.004699/  3.784387, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-257 lr=['0.0100000'], tr/val_loss:  0.004505/  3.771975, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-258 lr=['0.0100000'], tr/val_loss:  0.004363/  3.851925, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-259 lr=['0.0100000'], tr/val_loss:  0.003485/  3.840487, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-260 lr=['0.0100000'], tr/val_loss:  0.003830/  3.853129, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-261 lr=['0.0100000'], tr/val_loss:  0.003911/  3.852779, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-262 lr=['0.0100000'], tr/val_loss:  0.003409/  3.855592, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-263 lr=['0.0100000'], tr/val_loss:  0.003230/  3.873628, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-264 lr=['0.0100000'], tr/val_loss:  0.002923/  3.857155, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-265 lr=['0.0100000'], tr/val_loss:  0.003710/  3.880980, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-266 lr=['0.0100000'], tr/val_loss:  0.003638/  3.868954, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-267 lr=['0.0100000'], tr/val_loss:  0.002882/  3.897359, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-268 lr=['0.0100000'], tr/val_loss:  0.002760/  3.868839, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-269 lr=['0.0100000'], tr/val_loss:  0.003160/  3.899330, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-270 lr=['0.0100000'], tr/val_loss:  0.002367/  3.905375, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-271 lr=['0.0100000'], tr/val_loss:  0.002725/  3.909132, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-272 lr=['0.0100000'], tr/val_loss:  0.003286/  3.890689, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-273 lr=['0.0100000'], tr/val_loss:  0.004438/  3.893637, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-274 lr=['0.0100000'], tr/val_loss:  0.003393/  3.855768, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-275 lr=['0.0100000'], tr/val_loss:  0.003526/  3.924760, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-276 lr=['0.0100000'], tr/val_loss:  0.002722/  3.887510, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-277 lr=['0.0100000'], tr/val_loss:  0.002191/  3.885218, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-278 lr=['0.0100000'], tr/val_loss:  0.002435/  3.917199, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-279 lr=['0.0100000'], tr/val_loss:  0.003801/  3.924166, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-280 lr=['0.0100000'], tr/val_loss:  0.003082/  3.917334, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-281 lr=['0.0100000'], tr/val_loss:  0.004031/  3.910631, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-282 lr=['0.0100000'], tr/val_loss:  0.003305/  3.901529, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-283 lr=['0.0100000'], tr/val_loss:  0.002964/  3.900760, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-284 lr=['0.0100000'], tr/val_loss:  0.003108/  3.885180, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-285 lr=['0.0100000'], tr/val_loss:  0.004636/  3.934656, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-286 lr=['0.0100000'], tr/val_loss:  0.004515/  3.908193, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-287 lr=['0.0100000'], tr/val_loss:  0.003650/  3.922615, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-288 lr=['0.0100000'], tr/val_loss:  0.003374/  3.935406, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-289 lr=['0.0100000'], tr/val_loss:  0.003753/  3.940037, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-290 lr=['0.0100000'], tr/val_loss:  0.005196/  3.925987, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-291 lr=['0.0100000'], tr/val_loss:  0.004752/  3.933098, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-292 lr=['0.0100000'], tr/val_loss:  0.004707/  3.912637, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-293 lr=['0.0100000'], tr/val_loss:  0.003441/  3.895349, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-294 lr=['0.0100000'], tr/val_loss:  0.003616/  3.881175, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-295 lr=['0.0100000'], tr/val_loss:  0.003449/  3.868139, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-296 lr=['0.0100000'], tr/val_loss:  0.002718/  3.899866, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-297 lr=['0.0100000'], tr/val_loss:  0.003011/  3.895520, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-298 lr=['0.0100000'], tr/val_loss:  0.003399/  3.904043, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-299 lr=['0.0100000'], tr/val_loss:  0.004061/  3.917742, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-300 lr=['0.0100000'], tr/val_loss:  0.003305/  3.928257, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-301 lr=['0.0100000'], tr/val_loss:  0.002898/  3.929695, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-302 lr=['0.0100000'], tr/val_loss:  0.005151/  3.915512, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-303 lr=['0.0100000'], tr/val_loss:  0.003578/  3.883052, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-304 lr=['0.0100000'], tr/val_loss:  0.003427/  3.962795, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-305 lr=['0.0100000'], tr/val_loss:  0.003879/  3.921412, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-306 lr=['0.0100000'], tr/val_loss:  0.004668/  3.928778, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-307 lr=['0.0100000'], tr/val_loss:  0.004801/  3.875471, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-308 lr=['0.0100000'], tr/val_loss:  0.004897/  3.874480, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-309 lr=['0.0100000'], tr/val_loss:  0.002786/  3.910188, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-310 lr=['0.0100000'], tr/val_loss:  0.004847/  3.930798, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-311 lr=['0.0100000'], tr/val_loss:  0.004093/  3.987553, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-312 lr=['0.0100000'], tr/val_loss:  0.003339/  3.928721, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-313 lr=['0.0100000'], tr/val_loss:  0.003389/  3.920229, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-314 lr=['0.0100000'], tr/val_loss:  0.003088/  3.912109, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-315 lr=['0.0100000'], tr/val_loss:  0.003769/  3.881257, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-316 lr=['0.0100000'], tr/val_loss:  0.003403/  3.898621, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-317 lr=['0.0100000'], tr/val_loss:  0.002824/  3.951240, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-318 lr=['0.0100000'], tr/val_loss:  0.003384/  3.918875, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-319 lr=['0.0100000'], tr/val_loss:  0.003933/  3.941184, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-320 lr=['0.0100000'], tr/val_loss:  0.002764/  3.948943, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-321 lr=['0.0100000'], tr/val_loss:  0.002311/  3.954612, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-322 lr=['0.0100000'], tr/val_loss:  0.002671/  4.014452, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-323 lr=['0.0100000'], tr/val_loss:  0.002481/  4.017926, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-324 lr=['0.0100000'], tr/val_loss:  0.003295/  3.981480, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-325 lr=['0.0100000'], tr/val_loss:  0.002928/  3.974622, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-326 lr=['0.0100000'], tr/val_loss:  0.003518/  3.962953, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-327 lr=['0.0100000'], tr/val_loss:  0.003295/  3.962568, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-328 lr=['0.0100000'], tr/val_loss:  0.003868/  3.961834, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-329 lr=['0.0100000'], tr/val_loss:  0.003732/  3.949797, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-330 lr=['0.0100000'], tr/val_loss:  0.004449/  3.951201, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-331 lr=['0.0100000'], tr/val_loss:  0.002918/  3.977037, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-332 lr=['0.0100000'], tr/val_loss:  0.003314/  3.953604, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-333 lr=['0.0100000'], tr/val_loss:  0.003552/  3.947245, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-334 lr=['0.0100000'], tr/val_loss:  0.003341/  3.945304, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-335 lr=['0.0100000'], tr/val_loss:  0.002858/  3.976331, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-336 lr=['0.0100000'], tr/val_loss:  0.002481/  4.002452, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-337 lr=['0.0100000'], tr/val_loss:  0.002768/  3.997074, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-338 lr=['0.0100000'], tr/val_loss:  0.003345/  4.015417, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-339 lr=['0.0100000'], tr/val_loss:  0.003968/  3.983488, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-340 lr=['0.0100000'], tr/val_loss:  0.011341/  3.965805, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-341 lr=['0.0100000'], tr/val_loss:  0.008492/  3.960039, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-342 lr=['0.0100000'], tr/val_loss:  0.005669/  3.959754, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-343 lr=['0.0100000'], tr/val_loss:  0.004818/  3.963078, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-344 lr=['0.0100000'], tr/val_loss:  0.007369/  3.944365, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-345 lr=['0.0100000'], tr/val_loss:  0.005105/  3.929004, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-346 lr=['0.0100000'], tr/val_loss:  0.003839/  3.950947, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-347 lr=['0.0100000'], tr/val_loss:  0.003967/  3.941105, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-348 lr=['0.0100000'], tr/val_loss:  0.004794/  3.965055, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-349 lr=['0.0100000'], tr/val_loss:  0.002461/  3.952553, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-350 lr=['0.0100000'], tr/val_loss:  0.002301/  3.922203, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-351 lr=['0.0100000'], tr/val_loss:  0.002082/  3.936622, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-352 lr=['0.0100000'], tr/val_loss:  0.002699/  3.928237, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-353 lr=['0.0100000'], tr/val_loss:  0.002534/  3.936601, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-354 lr=['0.0100000'], tr/val_loss:  0.003680/  3.934025, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-355 lr=['0.0100000'], tr/val_loss:  0.002577/  3.987322, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-356 lr=['0.0100000'], tr/val_loss:  0.004005/  3.959317, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-357 lr=['0.0100000'], tr/val_loss:  0.004247/  3.961874, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-358 lr=['0.0100000'], tr/val_loss:  0.003947/  3.968707, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-359 lr=['0.0100000'], tr/val_loss:  0.003284/  3.998772, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-360 lr=['0.0100000'], tr/val_loss:  0.002944/  3.984522, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-361 lr=['0.0100000'], tr/val_loss:  0.004272/  4.013693, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-362 lr=['0.0100000'], tr/val_loss:  0.002902/  3.963469, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-363 lr=['0.0100000'], tr/val_loss:  0.004121/  4.020265, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-364 lr=['0.0100000'], tr/val_loss:  0.002792/  4.020011, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-365 lr=['0.0100000'], tr/val_loss:  0.003013/  4.049416, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-366 lr=['0.0100000'], tr/val_loss:  0.002209/  4.004428, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-367 lr=['0.0100000'], tr/val_loss:  0.002310/  4.032753, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-368 lr=['0.0100000'], tr/val_loss:  0.002023/  4.025091, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-369 lr=['0.0100000'], tr/val_loss:  0.002607/  4.040488, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-370 lr=['0.0100000'], tr/val_loss:  0.002341/  4.003832, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-371 lr=['0.0100000'], tr/val_loss:  0.003617/  4.019680, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-372 lr=['0.0100000'], tr/val_loss:  0.003847/  4.082625, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-373 lr=['0.0100000'], tr/val_loss:  0.004503/  4.064405, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-374 lr=['0.0100000'], tr/val_loss:  0.003619/  4.077991, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-375 lr=['0.0100000'], tr/val_loss:  0.002784/  4.078507, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-376 lr=['0.0100000'], tr/val_loss:  0.002782/  4.065075, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-377 lr=['0.0100000'], tr/val_loss:  0.003960/  4.025286, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-378 lr=['0.0100000'], tr/val_loss:  0.004171/  4.061108, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-379 lr=['0.0100000'], tr/val_loss:  0.005036/  3.997276, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-380 lr=['0.0100000'], tr/val_loss:  0.005142/  4.013837, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-381 lr=['0.0100000'], tr/val_loss:  0.003817/  4.004523, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-382 lr=['0.0100000'], tr/val_loss:  0.003852/  4.030110, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-383 lr=['0.0100000'], tr/val_loss:  0.002535/  4.019034, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-384 lr=['0.0100000'], tr/val_loss:  0.002954/  4.026846, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-385 lr=['0.0100000'], tr/val_loss:  0.002820/  4.028759, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-386 lr=['0.0100000'], tr/val_loss:  0.003675/  4.007246, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-387 lr=['0.0100000'], tr/val_loss:  0.003113/  4.043044, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-388 lr=['0.0100000'], tr/val_loss:  0.002609/  4.031510, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-389 lr=['0.0100000'], tr/val_loss:  0.002448/  4.040084, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-390 lr=['0.0100000'], tr/val_loss:  0.002146/  4.057330, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-391 lr=['0.0100000'], tr/val_loss:  0.002344/  4.054148, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-392 lr=['0.0100000'], tr/val_loss:  0.002644/  4.035435, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-393 lr=['0.0100000'], tr/val_loss:  0.004513/  4.060939, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-394 lr=['0.0100000'], tr/val_loss:  0.003791/  4.082671, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-395 lr=['0.0100000'], tr/val_loss:  0.002607/  4.032923, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-396 lr=['0.0100000'], tr/val_loss:  0.002008/  4.026483, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-397 lr=['0.0100000'], tr/val_loss:  0.001660/  4.024891, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-398 lr=['0.0100000'], tr/val_loss:  0.002509/  4.052989, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-399 lr=['0.0100000'], tr/val_loss:  0.002556/  4.039218, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-400 lr=['0.0100000'], tr/val_loss:  0.003093/  4.015075, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-401 lr=['0.0100000'], tr/val_loss:  0.003774/  4.068491, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-402 lr=['0.0100000'], tr/val_loss:  0.001945/  4.074216, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-403 lr=['0.0100000'], tr/val_loss:  0.002179/  4.077132, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-404 lr=['0.0100000'], tr/val_loss:  0.001538/  4.068634, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-405 lr=['0.0100000'], tr/val_loss:  0.001707/  4.092792, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-406 lr=['0.0100000'], tr/val_loss:  0.001371/  4.060305, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-407 lr=['0.0100000'], tr/val_loss:  0.002194/  4.080693, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-408 lr=['0.0100000'], tr/val_loss:  0.001872/  4.085724, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-409 lr=['0.0100000'], tr/val_loss:  0.001859/  4.111009, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-410 lr=['0.0100000'], tr/val_loss:  0.002082/  4.102949, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-411 lr=['0.0100000'], tr/val_loss:  0.001674/  4.095022, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-412 lr=['0.0100000'], tr/val_loss:  0.001536/  4.085409, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-413 lr=['0.0100000'], tr/val_loss:  0.001610/  4.092319, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-414 lr=['0.0100000'], tr/val_loss:  0.001489/  4.106639, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-415 lr=['0.0100000'], tr/val_loss:  0.001537/  4.065209, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-416 lr=['0.0100000'], tr/val_loss:  0.002429/  4.080243, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-417 lr=['0.0100000'], tr/val_loss:  0.001909/  4.084761, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-418 lr=['0.0100000'], tr/val_loss:  0.001631/  4.091321, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-419 lr=['0.0100000'], tr/val_loss:  0.001615/  4.067075, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-420 lr=['0.0100000'], tr/val_loss:  0.002857/  4.069695, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-421 lr=['0.0100000'], tr/val_loss:  0.001648/  4.094993, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-422 lr=['0.0100000'], tr/val_loss:  0.002576/  4.088299, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-423 lr=['0.0100000'], tr/val_loss:  0.001997/  4.121150, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-424 lr=['0.0100000'], tr/val_loss:  0.001367/  4.092005, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-425 lr=['0.0100000'], tr/val_loss:  0.001552/  4.086417, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-426 lr=['0.0100000'], tr/val_loss:  0.001555/  4.107097, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-427 lr=['0.0100000'], tr/val_loss:  0.001677/  4.064524, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-428 lr=['0.0100000'], tr/val_loss:  0.001788/  4.109183, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-429 lr=['0.0100000'], tr/val_loss:  0.001859/  4.124374, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-430 lr=['0.0100000'], tr/val_loss:  0.002264/  4.119263, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-431 lr=['0.0100000'], tr/val_loss:  0.002316/  4.112582, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-432 lr=['0.0100000'], tr/val_loss:  0.002623/  4.122414, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-433 lr=['0.0100000'], tr/val_loss:  0.003188/  4.092892, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-434 lr=['0.0100000'], tr/val_loss:  0.003759/  4.087650, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-435 lr=['0.0100000'], tr/val_loss:  0.003277/  4.058832, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-436 lr=['0.0100000'], tr/val_loss:  0.002575/  4.084873, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-437 lr=['0.0100000'], tr/val_loss:  0.002427/  4.105066, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-438 lr=['0.0100000'], tr/val_loss:  0.002236/  4.095710, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-439 lr=['0.0100000'], tr/val_loss:  0.002689/  4.104258, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-440 lr=['0.0100000'], tr/val_loss:  0.001782/  4.082998, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-441 lr=['0.0100000'], tr/val_loss:  0.001626/  4.068690, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-442 lr=['0.0100000'], tr/val_loss:  0.001763/  4.094985, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-443 lr=['0.0100000'], tr/val_loss:  0.001474/  4.106332, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-444 lr=['0.0100000'], tr/val_loss:  0.001830/  4.101236, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-445 lr=['0.0100000'], tr/val_loss:  0.003111/  4.133937, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-446 lr=['0.0100000'], tr/val_loss:  0.002088/  4.115340, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-447 lr=['0.0100000'], tr/val_loss:  0.002451/  4.127721, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-448 lr=['0.0100000'], tr/val_loss:  0.002224/  4.134558, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-449 lr=['0.0100000'], tr/val_loss:  0.002670/  4.151412, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-450 lr=['0.0100000'], tr/val_loss:  0.002312/  4.177738, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-451 lr=['0.0100000'], tr/val_loss:  0.001797/  4.148759, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-452 lr=['0.0100000'], tr/val_loss:  0.001982/  4.140681, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-453 lr=['0.0100000'], tr/val_loss:  0.002083/  4.166950, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-454 lr=['0.0100000'], tr/val_loss:  0.002249/  4.157336, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-455 lr=['0.0100000'], tr/val_loss:  0.002395/  4.167812, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-456 lr=['0.0100000'], tr/val_loss:  0.002686/  4.175796, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-457 lr=['0.0100000'], tr/val_loss:  0.001480/  4.168274, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-458 lr=['0.0100000'], tr/val_loss:  0.001595/  4.167020, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-459 lr=['0.0100000'], tr/val_loss:  0.001278/  4.177026, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-460 lr=['0.0100000'], tr/val_loss:  0.001358/  4.158728, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-461 lr=['0.0100000'], tr/val_loss:  0.002469/  4.153755, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-462 lr=['0.0100000'], tr/val_loss:  0.003746/  4.202907, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-463 lr=['0.0100000'], tr/val_loss:  0.002110/  4.215119, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-464 lr=['0.0100000'], tr/val_loss:  0.003153/  4.227532, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-465 lr=['0.0100000'], tr/val_loss:  0.002685/  4.208178, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-466 lr=['0.0100000'], tr/val_loss:  0.002661/  4.194204, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-467 lr=['0.0100000'], tr/val_loss:  0.002240/  4.230222, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-468 lr=['0.0100000'], tr/val_loss:  0.001970/  4.225862, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-469 lr=['0.0100000'], tr/val_loss:  0.002834/  4.201573, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-470 lr=['0.0100000'], tr/val_loss:  0.002593/  4.177679, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-471 lr=['0.0100000'], tr/val_loss:  0.002302/  4.143183, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-472 lr=['0.0100000'], tr/val_loss:  0.002077/  4.170267, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-473 lr=['0.0100000'], tr/val_loss:  0.002378/  4.154171, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-474 lr=['0.0100000'], tr/val_loss:  0.002409/  4.133892, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-475 lr=['0.0100000'], tr/val_loss:  0.001954/  4.133292, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-476 lr=['0.0100000'], tr/val_loss:  0.002334/  4.141598, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-477 lr=['0.0100000'], tr/val_loss:  0.002243/  4.165004, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-478 lr=['0.0100000'], tr/val_loss:  0.002912/  4.148501, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-479 lr=['0.0100000'], tr/val_loss:  0.001942/  4.155037, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-480 lr=['0.0100000'], tr/val_loss:  0.002087/  4.162575, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-481 lr=['0.0100000'], tr/val_loss:  0.002723/  4.159500, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-482 lr=['0.0100000'], tr/val_loss:  0.001841/  4.179918, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-483 lr=['0.0100000'], tr/val_loss:  0.001190/  4.176531, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-484 lr=['0.0100000'], tr/val_loss:  0.001094/  4.183583, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-485 lr=['0.0100000'], tr/val_loss:  0.001070/  4.167794, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-486 lr=['0.0100000'], tr/val_loss:  0.000947/  4.155649, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-487 lr=['0.0100000'], tr/val_loss:  0.001552/  4.172530, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-488 lr=['0.0100000'], tr/val_loss:  0.001063/  4.166621, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-489 lr=['0.0100000'], tr/val_loss:  0.001245/  4.166206, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-490 lr=['0.0100000'], tr/val_loss:  0.001879/  4.188468, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-491 lr=['0.0100000'], tr/val_loss:  0.001840/  4.198883, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-492 lr=['0.0100000'], tr/val_loss:  0.001473/  4.192252, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-493 lr=['0.0100000'], tr/val_loss:  0.001185/  4.167810, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-494 lr=['0.0100000'], tr/val_loss:  0.001129/  4.175146, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-495 lr=['0.0100000'], tr/val_loss:  0.001067/  4.186283, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-496 lr=['0.0100000'], tr/val_loss:  0.001722/  4.184981, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-497 lr=['0.0100000'], tr/val_loss:  0.001217/  4.213731, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-498 lr=['0.0100000'], tr/val_loss:  0.001781/  4.222923, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-499 lr=['0.0100000'], tr/val_loss:  0.001099/  4.225196, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-500 lr=['0.0100000'], tr/val_loss:  0.001046/  4.222373, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-501 lr=['0.0100000'], tr/val_loss:  0.001382/  4.222422, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-502 lr=['0.0100000'], tr/val_loss:  0.001935/  4.244118, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-503 lr=['0.0100000'], tr/val_loss:  0.002816/  4.231129, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-504 lr=['0.0100000'], tr/val_loss:  0.002469/  4.227204, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-505 lr=['0.0100000'], tr/val_loss:  0.002391/  4.193129, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-506 lr=['0.0100000'], tr/val_loss:  0.003560/  4.259355, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-507 lr=['0.0100000'], tr/val_loss:  0.005214/  4.224146, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-508 lr=['0.0100000'], tr/val_loss:  0.002881/  4.214557, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-509 lr=['0.0100000'], tr/val_loss:  0.003629/  4.224168, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-510 lr=['0.0100000'], tr/val_loss:  0.002383/  4.242849, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-511 lr=['0.0100000'], tr/val_loss:  0.002779/  4.226090, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-512 lr=['0.0100000'], tr/val_loss:  0.002960/  4.237489, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-513 lr=['0.0100000'], tr/val_loss:  0.002435/  4.224294, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-514 lr=['0.0100000'], tr/val_loss:  0.001899/  4.207771, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-515 lr=['0.0100000'], tr/val_loss:  0.001159/  4.245735, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-516 lr=['0.0100000'], tr/val_loss:  0.001264/  4.265224, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-517 lr=['0.0100000'], tr/val_loss:  0.001140/  4.254259, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-518 lr=['0.0100000'], tr/val_loss:  0.001467/  4.267540, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-519 lr=['0.0100000'], tr/val_loss:  0.001119/  4.252826, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-520 lr=['0.0100000'], tr/val_loss:  0.001222/  4.240532, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-521 lr=['0.0100000'], tr/val_loss:  0.001300/  4.217060, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-522 lr=['0.0100000'], tr/val_loss:  0.002713/  4.238122, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-523 lr=['0.0100000'], tr/val_loss:  0.002441/  4.263953, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-524 lr=['0.0100000'], tr/val_loss:  0.002542/  4.252823, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-525 lr=['0.0100000'], tr/val_loss:  0.002110/  4.304710, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-526 lr=['0.0100000'], tr/val_loss:  0.001366/  4.283119, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-527 lr=['0.0100000'], tr/val_loss:  0.001935/  4.299280, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-528 lr=['0.0100000'], tr/val_loss:  0.001556/  4.304631, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-529 lr=['0.0100000'], tr/val_loss:  0.002594/  4.279514, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-530 lr=['0.0100000'], tr/val_loss:  0.001923/  4.284928, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-531 lr=['0.0100000'], tr/val_loss:  0.001681/  4.253680, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-532 lr=['0.0100000'], tr/val_loss:  0.001337/  4.269289, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-533 lr=['0.0100000'], tr/val_loss:  0.001264/  4.290847, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-534 lr=['0.0100000'], tr/val_loss:  0.001578/  4.349877, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-535 lr=['0.0100000'], tr/val_loss:  0.001978/  4.347125, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-536 lr=['0.0100000'], tr/val_loss:  0.001554/  4.323519, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-537 lr=['0.0100000'], tr/val_loss:  0.000844/  4.307495, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-538 lr=['0.0100000'], tr/val_loss:  0.001124/  4.325992, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-539 lr=['0.0100000'], tr/val_loss:  0.001545/  4.345901, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-540 lr=['0.0100000'], tr/val_loss:  0.001665/  4.311812, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-541 lr=['0.0100000'], tr/val_loss:  0.000863/  4.301745, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-542 lr=['0.0100000'], tr/val_loss:  0.000867/  4.302031, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-543 lr=['0.0100000'], tr/val_loss:  0.001103/  4.320508, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-544 lr=['0.0100000'], tr/val_loss:  0.000914/  4.331422, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-545 lr=['0.0100000'], tr/val_loss:  0.000940/  4.339705, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-546 lr=['0.0100000'], tr/val_loss:  0.000959/  4.335157, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-547 lr=['0.0100000'], tr/val_loss:  0.000846/  4.355548, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-548 lr=['0.0100000'], tr/val_loss:  0.000821/  4.347403, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-549 lr=['0.0100000'], tr/val_loss:  0.000857/  4.319990, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-550 lr=['0.0100000'], tr/val_loss:  0.001115/  4.355226, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-551 lr=['0.0100000'], tr/val_loss:  0.001720/  4.281500, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-552 lr=['0.0100000'], tr/val_loss:  0.001146/  4.293270, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-553 lr=['0.0100000'], tr/val_loss:  0.001673/  4.309191, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-554 lr=['0.0100000'], tr/val_loss:  0.002069/  4.316923, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-555 lr=['0.0100000'], tr/val_loss:  0.001090/  4.309102, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-556 lr=['0.0100000'], tr/val_loss:  0.000965/  4.334853, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-557 lr=['0.0100000'], tr/val_loss:  0.000782/  4.323441, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-558 lr=['0.0100000'], tr/val_loss:  0.000675/  4.308872, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-559 lr=['0.0100000'], tr/val_loss:  0.000659/  4.334216, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-560 lr=['0.0100000'], tr/val_loss:  0.000790/  4.375432, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-561 lr=['0.0100000'], tr/val_loss:  0.000659/  4.345899, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-562 lr=['0.0100000'], tr/val_loss:  0.000695/  4.344460, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-563 lr=['0.0100000'], tr/val_loss:  0.000745/  4.332277, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-564 lr=['0.0100000'], tr/val_loss:  0.000756/  4.360391, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-565 lr=['0.0100000'], tr/val_loss:  0.000731/  4.339022, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-566 lr=['0.0100000'], tr/val_loss:  0.000698/  4.360036, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-567 lr=['0.0100000'], tr/val_loss:  0.000990/  4.334873, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-568 lr=['0.0100000'], tr/val_loss:  0.001784/  4.370721, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-569 lr=['0.0100000'], tr/val_loss:  0.001783/  4.378601, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-570 lr=['0.0100000'], tr/val_loss:  0.000938/  4.348313, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-571 lr=['0.0100000'], tr/val_loss:  0.000958/  4.361639, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-572 lr=['0.0100000'], tr/val_loss:  0.000865/  4.382322, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-573 lr=['0.0100000'], tr/val_loss:  0.000713/  4.380136, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-574 lr=['0.0100000'], tr/val_loss:  0.000680/  4.380422, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-575 lr=['0.0100000'], tr/val_loss:  0.000891/  4.400964, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-576 lr=['0.0100000'], tr/val_loss:  0.000952/  4.367959, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-577 lr=['0.0100000'], tr/val_loss:  0.000785/  4.374655, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-578 lr=['0.0100000'], tr/val_loss:  0.000704/  4.392104, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-579 lr=['0.0100000'], tr/val_loss:  0.000775/  4.404389, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-580 lr=['0.0100000'], tr/val_loss:  0.001584/  4.416096, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-581 lr=['0.0100000'], tr/val_loss:  0.000926/  4.401109, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-582 lr=['0.0100000'], tr/val_loss:  0.001052/  4.391713, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-583 lr=['0.0100000'], tr/val_loss:  0.000955/  4.392146, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-584 lr=['0.0100000'], tr/val_loss:  0.001003/  4.417159, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-585 lr=['0.0100000'], tr/val_loss:  0.001074/  4.388367, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-586 lr=['0.0100000'], tr/val_loss:  0.000854/  4.385318, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-587 lr=['0.0100000'], tr/val_loss:  0.000781/  4.364306, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-588 lr=['0.0100000'], tr/val_loss:  0.000653/  4.372696, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-589 lr=['0.0100000'], tr/val_loss:  0.000753/  4.365314, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-590 lr=['0.0100000'], tr/val_loss:  0.000616/  4.374198, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-591 lr=['0.0100000'], tr/val_loss:  0.000820/  4.363930, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-592 lr=['0.0100000'], tr/val_loss:  0.000667/  4.368730, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-593 lr=['0.0100000'], tr/val_loss:  0.000811/  4.381721, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-594 lr=['0.0100000'], tr/val_loss:  0.000714/  4.359586, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-595 lr=['0.0100000'], tr/val_loss:  0.000654/  4.381614, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-596 lr=['0.0100000'], tr/val_loss:  0.001665/  4.344970, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-597 lr=['0.0100000'], tr/val_loss:  0.001033/  4.339423, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-598 lr=['0.0100000'], tr/val_loss:  0.000993/  4.347600, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-599 lr=['0.0100000'], tr/val_loss:  0.001070/  4.357595, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-600 lr=['0.0100000'], tr/val_loss:  0.000756/  4.355906, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-601 lr=['0.0100000'], tr/val_loss:  0.000891/  4.368787, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-602 lr=['0.0100000'], tr/val_loss:  0.001216/  4.349104, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-603 lr=['0.0100000'], tr/val_loss:  0.000997/  4.392416, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-604 lr=['0.0100000'], tr/val_loss:  0.001262/  4.384433, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-605 lr=['0.0100000'], tr/val_loss:  0.001200/  4.351895, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-606 lr=['0.0100000'], tr/val_loss:  0.001423/  4.373383, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-607 lr=['0.0100000'], tr/val_loss:  0.004578/  4.358775, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-608 lr=['0.0100000'], tr/val_loss:  0.007625/  4.343971, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-609 lr=['0.0100000'], tr/val_loss:  0.003377/  4.335977, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-610 lr=['0.0100000'], tr/val_loss:  0.001803/  4.311398, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-611 lr=['0.0100000'], tr/val_loss:  0.001612/  4.373996, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-612 lr=['0.0100000'], tr/val_loss:  0.001664/  4.357066, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-613 lr=['0.0100000'], tr/val_loss:  0.001196/  4.369041, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-614 lr=['0.0100000'], tr/val_loss:  0.001011/  4.367114, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-615 lr=['0.0100000'], tr/val_loss:  0.001164/  4.359584, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-616 lr=['0.0100000'], tr/val_loss:  0.000757/  4.350798, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-617 lr=['0.0100000'], tr/val_loss:  0.000846/  4.353124, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-618 lr=['0.0100000'], tr/val_loss:  0.000710/  4.372510, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-619 lr=['0.0100000'], tr/val_loss:  0.000902/  4.354894, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-620 lr=['0.0100000'], tr/val_loss:  0.000843/  4.370709, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-621 lr=['0.0100000'], tr/val_loss:  0.000642/  4.375569, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-622 lr=['0.0100000'], tr/val_loss:  0.001059/  4.378881, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-623 lr=['0.0100000'], tr/val_loss:  0.002192/  4.395202, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-624 lr=['0.0100000'], tr/val_loss:  0.001559/  4.371195, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-625 lr=['0.0100000'], tr/val_loss:  0.000888/  4.360798, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-626 lr=['0.0100000'], tr/val_loss:  0.000945/  4.346405, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-627 lr=['0.0100000'], tr/val_loss:  0.001218/  4.370595, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-628 lr=['0.0100000'], tr/val_loss:  0.000713/  4.364110, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-629 lr=['0.0100000'], tr/val_loss:  0.000802/  4.378867, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-630 lr=['0.0100000'], tr/val_loss:  0.000749/  4.373121, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-631 lr=['0.0100000'], tr/val_loss:  0.000754/  4.390419, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-632 lr=['0.0100000'], tr/val_loss:  0.001026/  4.393075, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-633 lr=['0.0100000'], tr/val_loss:  0.001180/  4.392186, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-634 lr=['0.0100000'], tr/val_loss:  0.001195/  4.368359, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-635 lr=['0.0100000'], tr/val_loss:  0.001167/  4.364425, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-636 lr=['0.0100000'], tr/val_loss:  0.001069/  4.345626, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-637 lr=['0.0100000'], tr/val_loss:  0.001829/  4.413188, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-638 lr=['0.0100000'], tr/val_loss:  0.001626/  4.404461, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-639 lr=['0.0100000'], tr/val_loss:  0.001603/  4.406223, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-640 lr=['0.0100000'], tr/val_loss:  0.000897/  4.387541, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-641 lr=['0.0100000'], tr/val_loss:  0.000886/  4.411464, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-642 lr=['0.0100000'], tr/val_loss:  0.000936/  4.391293, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-643 lr=['0.0100000'], tr/val_loss:  0.000871/  4.380882, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-644 lr=['0.0100000'], tr/val_loss:  0.000705/  4.389315, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-645 lr=['0.0100000'], tr/val_loss:  0.000844/  4.404430, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-646 lr=['0.0100000'], tr/val_loss:  0.000686/  4.424354, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-647 lr=['0.0100000'], tr/val_loss:  0.000788/  4.432351, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-648 lr=['0.0100000'], tr/val_loss:  0.000568/  4.446838, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-649 lr=['0.0100000'], tr/val_loss:  0.000543/  4.442975, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-650 lr=['0.0100000'], tr/val_loss:  0.000632/  4.415801, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-651 lr=['0.0100000'], tr/val_loss:  0.000530/  4.415536, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-652 lr=['0.0100000'], tr/val_loss:  0.000802/  4.418157, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-653 lr=['0.0100000'], tr/val_loss:  0.000823/  4.431931, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-654 lr=['0.0100000'], tr/val_loss:  0.001306/  4.419137, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-655 lr=['0.0100000'], tr/val_loss:  0.000918/  4.417333, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-656 lr=['0.0100000'], tr/val_loss:  0.001371/  4.398857, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-657 lr=['0.0100000'], tr/val_loss:  0.001664/  4.421439, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-658 lr=['0.0100000'], tr/val_loss:  0.002187/  4.426482, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-659 lr=['0.0100000'], tr/val_loss:  0.001034/  4.402891, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-660 lr=['0.0100000'], tr/val_loss:  0.001136/  4.391494, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-661 lr=['0.0100000'], tr/val_loss:  0.000676/  4.373006, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-662 lr=['0.0100000'], tr/val_loss:  0.001383/  4.394667, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-663 lr=['0.0100000'], tr/val_loss:  0.001592/  4.341881, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-664 lr=['0.0100000'], tr/val_loss:  0.000986/  4.348788, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-665 lr=['0.0100000'], tr/val_loss:  0.001867/  4.385716, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-666 lr=['0.0100000'], tr/val_loss:  0.000935/  4.369293, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-667 lr=['0.0100000'], tr/val_loss:  0.000985/  4.371454, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-668 lr=['0.0100000'], tr/val_loss:  0.000677/  4.391500, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-669 lr=['0.0100000'], tr/val_loss:  0.000863/  4.407207, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-670 lr=['0.0100000'], tr/val_loss:  0.001308/  4.463472, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-671 lr=['0.0100000'], tr/val_loss:  0.000856/  4.461370, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-672 lr=['0.0100000'], tr/val_loss:  0.000873/  4.460821, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-673 lr=['0.0100000'], tr/val_loss:  0.000836/  4.471477, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-674 lr=['0.0100000'], tr/val_loss:  0.001458/  4.475339, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-675 lr=['0.0100000'], tr/val_loss:  0.001621/  4.467366, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-676 lr=['0.0100000'], tr/val_loss:  0.001086/  4.462805, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-677 lr=['0.0100000'], tr/val_loss:  0.000673/  4.432447, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-678 lr=['0.0100000'], tr/val_loss:  0.000830/  4.454916, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-679 lr=['0.0100000'], tr/val_loss:  0.000784/  4.452491, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-680 lr=['0.0100000'], tr/val_loss:  0.000918/  4.450757, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-681 lr=['0.0100000'], tr/val_loss:  0.001217/  4.483717, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-682 lr=['0.0100000'], tr/val_loss:  0.000581/  4.468047, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-683 lr=['0.0100000'], tr/val_loss:  0.000618/  4.474435, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-684 lr=['0.0100000'], tr/val_loss:  0.000641/  4.487643, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-685 lr=['0.0100000'], tr/val_loss:  0.000517/  4.475345, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-686 lr=['0.0100000'], tr/val_loss:  0.000648/  4.466753, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-687 lr=['0.0100000'], tr/val_loss:  0.000668/  4.470850, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-688 lr=['0.0100000'], tr/val_loss:  0.000714/  4.474352, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-689 lr=['0.0100000'], tr/val_loss:  0.000672/  4.474925, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-690 lr=['0.0100000'], tr/val_loss:  0.000667/  4.489524, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-691 lr=['0.0100000'], tr/val_loss:  0.000854/  4.518459, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-692 lr=['0.0100000'], tr/val_loss:  0.000864/  4.508135, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-693 lr=['0.0100000'], tr/val_loss:  0.000840/  4.491204, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-694 lr=['0.0100000'], tr/val_loss:  0.000762/  4.506101, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-695 lr=['0.0100000'], tr/val_loss:  0.001008/  4.490481, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-696 lr=['0.0100000'], tr/val_loss:  0.000921/  4.480933, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-697 lr=['0.0100000'], tr/val_loss:  0.000805/  4.486160, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-698 lr=['0.0100000'], tr/val_loss:  0.000616/  4.460981, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-699 lr=['0.0100000'], tr/val_loss:  0.000654/  4.471486, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-700 lr=['0.0100000'], tr/val_loss:  0.000432/  4.466339, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-701 lr=['0.0100000'], tr/val_loss:  0.000561/  4.490378, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-702 lr=['0.0100000'], tr/val_loss:  0.000511/  4.492269, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-703 lr=['0.0100000'], tr/val_loss:  0.000653/  4.506625, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-704 lr=['0.0100000'], tr/val_loss:  0.000886/  4.534753, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-705 lr=['0.0100000'], tr/val_loss:  0.001775/  4.490681, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-706 lr=['0.0100000'], tr/val_loss:  0.001359/  4.496177, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-707 lr=['0.0100000'], tr/val_loss:  0.000973/  4.504790, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-708 lr=['0.0100000'], tr/val_loss:  0.000893/  4.511849, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-709 lr=['0.0100000'], tr/val_loss:  0.000925/  4.469797, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-710 lr=['0.0100000'], tr/val_loss:  0.001696/  4.473828, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-711 lr=['0.0100000'], tr/val_loss:  0.000778/  4.466404, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-712 lr=['0.0100000'], tr/val_loss:  0.000932/  4.512561, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-713 lr=['0.0100000'], tr/val_loss:  0.000957/  4.505928, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-714 lr=['0.0100000'], tr/val_loss:  0.000926/  4.521044, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-715 lr=['0.0100000'], tr/val_loss:  0.000829/  4.504901, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-716 lr=['0.0100000'], tr/val_loss:  0.000670/  4.526055, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-717 lr=['0.0100000'], tr/val_loss:  0.000798/  4.527212, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-718 lr=['0.0100000'], tr/val_loss:  0.001013/  4.525979, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-719 lr=['0.0100000'], tr/val_loss:  0.002271/  4.506050, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-720 lr=['0.0100000'], tr/val_loss:  0.006912/  4.557745, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-721 lr=['0.0100000'], tr/val_loss:  0.003038/  4.545914, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-722 lr=['0.0100000'], tr/val_loss:  0.003455/  4.538981, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-723 lr=['0.0100000'], tr/val_loss:  0.001816/  4.528682, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-724 lr=['0.0100000'], tr/val_loss:  0.001195/  4.519170, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-725 lr=['0.0100000'], tr/val_loss:  0.001132/  4.523383, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-726 lr=['0.0100000'], tr/val_loss:  0.001684/  4.547574, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-727 lr=['0.0100000'], tr/val_loss:  0.001439/  4.559304, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-728 lr=['0.0100000'], tr/val_loss:  0.001828/  4.532704, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-729 lr=['0.0100000'], tr/val_loss:  0.001285/  4.590322, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-730 lr=['0.0100000'], tr/val_loss:  0.001173/  4.553385, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-731 lr=['0.0100000'], tr/val_loss:  0.001174/  4.526224, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-732 lr=['0.0100000'], tr/val_loss:  0.001169/  4.517402, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-733 lr=['0.0100000'], tr/val_loss:  0.000948/  4.514929, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-734 lr=['0.0100000'], tr/val_loss:  0.000702/  4.519076, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-735 lr=['0.0100000'], tr/val_loss:  0.000727/  4.533031, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-736 lr=['0.0100000'], tr/val_loss:  0.001376/  4.533567, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-737 lr=['0.0100000'], tr/val_loss:  0.001240/  4.515150, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-738 lr=['0.0100000'], tr/val_loss:  0.000717/  4.532703, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-739 lr=['0.0100000'], tr/val_loss:  0.000523/  4.525200, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-740 lr=['0.0100000'], tr/val_loss:  0.000624/  4.547197, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-741 lr=['0.0100000'], tr/val_loss:  0.001601/  4.520105, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-742 lr=['0.0100000'], tr/val_loss:  0.001013/  4.569653, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-743 lr=['0.0100000'], tr/val_loss:  0.000783/  4.545279, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-744 lr=['0.0100000'], tr/val_loss:  0.000640/  4.533451, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-745 lr=['0.0100000'], tr/val_loss:  0.000645/  4.536141, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-746 lr=['0.0100000'], tr/val_loss:  0.000444/  4.532835, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-747 lr=['0.0100000'], tr/val_loss:  0.001613/  4.564172, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-748 lr=['0.0100000'], tr/val_loss:  0.005133/  4.533830, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-749 lr=['0.0100000'], tr/val_loss:  0.001112/  4.528443, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-750 lr=['0.0100000'], tr/val_loss:  0.000779/  4.561600, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-751 lr=['0.0100000'], tr/val_loss:  0.000688/  4.539981, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-752 lr=['0.0100000'], tr/val_loss:  0.000660/  4.541153, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-753 lr=['0.0100000'], tr/val_loss:  0.000724/  4.510294, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-754 lr=['0.0100000'], tr/val_loss:  0.001216/  4.506340, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-755 lr=['0.0100000'], tr/val_loss:  0.001877/  4.556861, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-756 lr=['0.0100000'], tr/val_loss:  0.001686/  4.540316, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-757 lr=['0.0100000'], tr/val_loss:  0.001769/  4.536076, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-758 lr=['0.0100000'], tr/val_loss:  0.002035/  4.572279, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-759 lr=['0.0100000'], tr/val_loss:  0.002101/  4.555061, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-760 lr=['0.0100000'], tr/val_loss:  0.001168/  4.537520, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-761 lr=['0.0100000'], tr/val_loss:  0.001141/  4.494956, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-762 lr=['0.0100000'], tr/val_loss:  0.001375/  4.518186, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-763 lr=['0.0100000'], tr/val_loss:  0.001061/  4.520673, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-764 lr=['0.0100000'], tr/val_loss:  0.000928/  4.496340, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-765 lr=['0.0100000'], tr/val_loss:  0.000734/  4.487756, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-766 lr=['0.0100000'], tr/val_loss:  0.000907/  4.541598, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-767 lr=['0.0100000'], tr/val_loss:  0.000525/  4.541633, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-768 lr=['0.0100000'], tr/val_loss:  0.001337/  4.540131, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-769 lr=['0.0100000'], tr/val_loss:  0.000930/  4.569538, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-770 lr=['0.0100000'], tr/val_loss:  0.001002/  4.544881, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-771 lr=['0.0100000'], tr/val_loss:  0.000934/  4.527682, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-772 lr=['0.0100000'], tr/val_loss:  0.000765/  4.550994, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-773 lr=['0.0100000'], tr/val_loss:  0.000518/  4.545114, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-774 lr=['0.0100000'], tr/val_loss:  0.000471/  4.554339, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-775 lr=['0.0100000'], tr/val_loss:  0.000608/  4.554013, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-776 lr=['0.0100000'], tr/val_loss:  0.000464/  4.543046, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-777 lr=['0.0100000'], tr/val_loss:  0.000490/  4.565498, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-778 lr=['0.0100000'], tr/val_loss:  0.000430/  4.580812, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-779 lr=['0.0100000'], tr/val_loss:  0.000411/  4.565797, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-780 lr=['0.0100000'], tr/val_loss:  0.000558/  4.558952, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-781 lr=['0.0100000'], tr/val_loss:  0.000393/  4.548015, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-782 lr=['0.0100000'], tr/val_loss:  0.000539/  4.553898, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-783 lr=['0.0100000'], tr/val_loss:  0.000750/  4.553977, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-784 lr=['0.0100000'], tr/val_loss:  0.000867/  4.569749, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-785 lr=['0.0100000'], tr/val_loss:  0.000738/  4.562766, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-786 lr=['0.0100000'], tr/val_loss:  0.000637/  4.570260, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-787 lr=['0.0100000'], tr/val_loss:  0.000745/  4.551603, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-788 lr=['0.0100000'], tr/val_loss:  0.000407/  4.562177, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-789 lr=['0.0100000'], tr/val_loss:  0.000376/  4.568027, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-790 lr=['0.0100000'], tr/val_loss:  0.000530/  4.570789, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-791 lr=['0.0100000'], tr/val_loss:  0.000391/  4.538454, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-792 lr=['0.0100000'], tr/val_loss:  0.000531/  4.539402, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-793 lr=['0.0100000'], tr/val_loss:  0.000377/  4.547808, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-794 lr=['0.0100000'], tr/val_loss:  0.000518/  4.543274, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-795 lr=['0.0100000'], tr/val_loss:  0.000371/  4.556540, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-796 lr=['0.0100000'], tr/val_loss:  0.000461/  4.559011, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-797 lr=['0.0100000'], tr/val_loss:  0.000561/  4.567493, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-798 lr=['0.0100000'], tr/val_loss:  0.000382/  4.564963, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-799 lr=['0.0100000'], tr/val_loss:  0.000482/  4.569441, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-800 lr=['0.0100000'], tr/val_loss:  0.000417/  4.554125, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-801 lr=['0.0100000'], tr/val_loss:  0.000769/  4.552553, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-802 lr=['0.0100000'], tr/val_loss:  0.000678/  4.542814, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-803 lr=['0.0100000'], tr/val_loss:  0.000599/  4.546193, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-804 lr=['0.0100000'], tr/val_loss:  0.000617/  4.561396, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-805 lr=['0.0100000'], tr/val_loss:  0.000417/  4.587489, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-806 lr=['0.0100000'], tr/val_loss:  0.000422/  4.557137, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-807 lr=['0.0100000'], tr/val_loss:  0.000683/  4.549045, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-808 lr=['0.0100000'], tr/val_loss:  0.000596/  4.559936, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-809 lr=['0.0100000'], tr/val_loss:  0.000400/  4.546143, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-810 lr=['0.0100000'], tr/val_loss:  0.000657/  4.544135, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-811 lr=['0.0100000'], tr/val_loss:  0.000437/  4.552717, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-812 lr=['0.0100000'], tr/val_loss:  0.000380/  4.557076, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-813 lr=['0.0100000'], tr/val_loss:  0.000380/  4.560874, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-814 lr=['0.0100000'], tr/val_loss:  0.000580/  4.545805, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-815 lr=['0.0100000'], tr/val_loss:  0.000382/  4.554591, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-816 lr=['0.0100000'], tr/val_loss:  0.000467/  4.532962, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-817 lr=['0.0100000'], tr/val_loss:  0.000534/  4.563737, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-818 lr=['0.0100000'], tr/val_loss:  0.000632/  4.547561, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-819 lr=['0.0100000'], tr/val_loss:  0.000807/  4.567965, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-820 lr=['0.0100000'], tr/val_loss:  0.000387/  4.568959, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-821 lr=['0.0100000'], tr/val_loss:  0.000370/  4.570430, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-822 lr=['0.0100000'], tr/val_loss:  0.000481/  4.576223, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-823 lr=['0.0100000'], tr/val_loss:  0.000319/  4.568488, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-824 lr=['0.0100000'], tr/val_loss:  0.000989/  4.557905, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-825 lr=['0.0100000'], tr/val_loss:  0.000443/  4.578289, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-826 lr=['0.0100000'], tr/val_loss:  0.000339/  4.578604, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-827 lr=['0.0100000'], tr/val_loss:  0.000499/  4.572378, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-828 lr=['0.0100000'], tr/val_loss:  0.000391/  4.568193, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-829 lr=['0.0100000'], tr/val_loss:  0.000375/  4.570838, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-830 lr=['0.0100000'], tr/val_loss:  0.000349/  4.578884, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-831 lr=['0.0100000'], tr/val_loss:  0.000330/  4.563759, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-832 lr=['0.0100000'], tr/val_loss:  0.000346/  4.560106, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-833 lr=['0.0100000'], tr/val_loss:  0.000430/  4.554651, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-834 lr=['0.0100000'], tr/val_loss:  0.000414/  4.561343, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-835 lr=['0.0100000'], tr/val_loss:  0.000453/  4.590024, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-836 lr=['0.0100000'], tr/val_loss:  0.000703/  4.599258, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-837 lr=['0.0100000'], tr/val_loss:  0.001218/  4.594504, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-838 lr=['0.0100000'], tr/val_loss:  0.000572/  4.588106, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-839 lr=['0.0100000'], tr/val_loss:  0.000418/  4.613494, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-840 lr=['0.0100000'], tr/val_loss:  0.000536/  4.624004, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-841 lr=['0.0100000'], tr/val_loss:  0.000441/  4.608491, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-842 lr=['0.0100000'], tr/val_loss:  0.000372/  4.629130, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-843 lr=['0.0100000'], tr/val_loss:  0.000921/  4.605588, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-844 lr=['0.0100000'], tr/val_loss:  0.000842/  4.609551, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-845 lr=['0.0100000'], tr/val_loss:  0.000803/  4.582609, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-846 lr=['0.0100000'], tr/val_loss:  0.001624/  4.594514, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-847 lr=['0.0100000'], tr/val_loss:  0.002248/  4.572586, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-848 lr=['0.0100000'], tr/val_loss:  0.000815/  4.591181, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-849 lr=['0.0100000'], tr/val_loss:  0.000681/  4.613143, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-850 lr=['0.0100000'], tr/val_loss:  0.000618/  4.596282, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-851 lr=['0.0100000'], tr/val_loss:  0.001681/  4.606123, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-852 lr=['0.0100000'], tr/val_loss:  0.001419/  4.584166, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-853 lr=['0.0100000'], tr/val_loss:  0.002510/  4.549263, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-854 lr=['0.0100000'], tr/val_loss:  0.003154/  4.582837, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-855 lr=['0.0100000'], tr/val_loss:  0.001358/  4.613613, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-856 lr=['0.0100000'], tr/val_loss:  0.002358/  4.570033, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-857 lr=['0.0100000'], tr/val_loss:  0.001230/  4.534044, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-858 lr=['0.0100000'], tr/val_loss:  0.001759/  4.550919, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-859 lr=['0.0100000'], tr/val_loss:  0.000683/  4.530718, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-860 lr=['0.0100000'], tr/val_loss:  0.000727/  4.576496, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-861 lr=['0.0100000'], tr/val_loss:  0.000811/  4.582420, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-862 lr=['0.0100000'], tr/val_loss:  0.000996/  4.591139, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-863 lr=['0.0100000'], tr/val_loss:  0.001088/  4.580180, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-864 lr=['0.0100000'], tr/val_loss:  0.000676/  4.580640, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-865 lr=['0.0100000'], tr/val_loss:  0.000680/  4.618784, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-866 lr=['0.0100000'], tr/val_loss:  0.000560/  4.575492, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-867 lr=['0.0100000'], tr/val_loss:  0.000518/  4.587082, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-868 lr=['0.0100000'], tr/val_loss:  0.000483/  4.595402, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-869 lr=['0.0100000'], tr/val_loss:  0.000596/  4.579320, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-870 lr=['0.0100000'], tr/val_loss:  0.000508/  4.591043, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-871 lr=['0.0100000'], tr/val_loss:  0.000466/  4.606415, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-872 lr=['0.0100000'], tr/val_loss:  0.000497/  4.599333, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-873 lr=['0.0100000'], tr/val_loss:  0.000559/  4.643465, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-874 lr=['0.0100000'], tr/val_loss:  0.000438/  4.611350, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-875 lr=['0.0100000'], tr/val_loss:  0.000424/  4.607426, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-876 lr=['0.0100000'], tr/val_loss:  0.000399/  4.600126, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-877 lr=['0.0100000'], tr/val_loss:  0.000390/  4.615647, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-878 lr=['0.0100000'], tr/val_loss:  0.000390/  4.603370, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-879 lr=['0.0100000'], tr/val_loss:  0.000379/  4.589824, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-880 lr=['0.0100000'], tr/val_loss:  0.000351/  4.590969, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-881 lr=['0.0100000'], tr/val_loss:  0.000469/  4.603718, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-882 lr=['0.0100000'], tr/val_loss:  0.000416/  4.610014, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-883 lr=['0.0100000'], tr/val_loss:  0.000453/  4.601436, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-884 lr=['0.0100000'], tr/val_loss:  0.000401/  4.611870, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-885 lr=['0.0100000'], tr/val_loss:  0.000363/  4.608514, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-886 lr=['0.0100000'], tr/val_loss:  0.000327/  4.611736, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-887 lr=['0.0100000'], tr/val_loss:  0.000315/  4.610269, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-888 lr=['0.0100000'], tr/val_loss:  0.000340/  4.603014, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    }
   ],
   "source": [
    "### my_snn control board (Gesture) ########################\n",
    "decay = 0.25 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "const2 = False # True # False\n",
    "\n",
    "unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "if const2 == True:\n",
    "    const2 = decay\n",
    "else:\n",
    "    const2 = 0.0\n",
    "\n",
    "wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "my_snn_system(  devices = \"1\",\n",
    "                single_step = True, # True # False # DFA_on이랑 같이 가라\n",
    "                unique_name = run_name,\n",
    "                my_seed = 42,\n",
    "                TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "                BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "                # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "                which_data = 'DVS_GESTURE_TONIC',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 1.0,   #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000.0, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "                lif_layer_sg_width = 3.0, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "\n",
    "                synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "                synapse_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "                # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "                cfg = [200, 200], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "                # cfg = ['M', 'M', 64], \n",
    "                # cfg = [64, 124, 64, 124],\n",
    "                # cfg = ['M','M',512], \n",
    "                # cfg = [512], \n",
    "                # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                # cfg = ['M','M',512],\n",
    "                # cfg = ['M',200],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = ['M','M',200,200],\n",
    "                # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = ['M',200,200],\n",
    "                # cfg = ['M','M',1024,512,256,128,64],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = [12], #fc\n",
    "                # cfg = [12, 'M', 48, 'M', 12], \n",
    "                # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [],        \n",
    "                \n",
    "                net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "                learning_rate = 0.01, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                epoch_num = 10000,\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "                BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                dvs_clipping = 5, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "                # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "                dvs_duration = 25_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "                # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "                # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "                DFA_on = True, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "                trace_on = True,   # True # False\n",
    "                OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용 # trace_on False면 의미없음.\n",
    "\n",
    "                exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                merge_polarities = True, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                denoise_on = True, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "                extra_train_dataset = 0, \n",
    "\n",
    "                num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "                chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "                pin_memory = True, # True # False \n",
    "\n",
    "                UDA_on = False,  # DECREPATED # uda\n",
    "                alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                bias = True, # True # False \n",
    "\n",
    "                last_lif = False,\n",
    "\n",
    "                temporal_filter = 5, \n",
    "                initial_pooling = 1,\n",
    "                ) \n",
    "\n",
    "# num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# num_workers = batch_size / num_GPU\n",
    "# num_workers = batch_size / num_CPU\n",
    "\n",
    "# sigmoid와 BN이 있어야 잘된다.\n",
    "# average pooling  \n",
    "# 이 낫다. \n",
    "\n",
    "# nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# # 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# # wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'bayes', # 'random', 'bayes'\n",
    "#     'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         # \"devices\": {\"values\": [\"1\"]},\n",
    "#         \"single_step\": {\"values\": [True]},\n",
    "#         # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "#         \"my_seed\": {\"values\": [42]},\n",
    "#         \"TIME\": {\"values\": [10]},\n",
    "#         \"BATCH\": {\"values\": [16]},\n",
    "#         \"IMAGE_SIZE\": {\"values\": [128]},\n",
    "#         \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "#         \"data_path\": {\"values\": ['/data2']},\n",
    "#         \"rate_coding\": {\"values\": [False]},\n",
    "#         \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "#         \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "#         \"lif_layer_v_threshold\": {\"values\": [0.25, 0.5, 0.75, 1.0]},\n",
    "#         \"lif_layer_v_reset\": {\"values\": [10000.0, 0.0]},\n",
    "#         \"lif_layer_sg_width\": {\"values\": [1.0,2.0,3.0,4.0,5.0]},\n",
    "\n",
    "#         \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "#         \"synapse_conv_stride\": {\"values\": [1]},\n",
    "#         \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "#         \"synapse_trace_const1\": {\"values\": [1]},\n",
    "#         \"synapse_trace_const2\": {\"values\": [0, 0.5]},\n",
    "\n",
    "#         \"pre_trained\": {\"values\": [False]},\n",
    "#         \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "#         \"cfg\": {\"values\": [['M','M',200,200]]},\n",
    "\n",
    "#         \"net_print\": {\"values\": [True]},\n",
    "\n",
    "#         \"pre_trained_path\": {\"values\": [\"net_save/save_now_net_weights_{unique_name}.pth\"]},\n",
    "#         \"learning_rate\": {\"values\": [0.001,0.01,0.1,0.0001]}, \n",
    "#         \"epoch_num\": {\"values\": [100]}, \n",
    "#         \"tdBN_on\": {\"values\": [False]},\n",
    "#         \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "#         \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"optimizer_what\": {\"values\": ['SGD']},\n",
    "#         \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "#         \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"dvs_clipping\": {\"values\": [5]}, \n",
    "\n",
    "#         \"dvs_duration\": {\"values\": [100_000]}, \n",
    "\n",
    "#         \"DFA_on\": {\"values\": [True, False]},\n",
    "\n",
    "#         \"trace_on\": {\"values\": [True]},\n",
    "#         \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "#         \"merge_polarities\": {\"values\": [False]},\n",
    "#         \"denoise_on\": {\"values\": [True, False]},\n",
    "\n",
    "#         \"extra_train_dataset\": {\"values\": [0]},\n",
    "\n",
    "#         \"num_workers\": {\"values\": [2]},\n",
    "#         \"chaching_on\": {\"values\": [True]},\n",
    "#         \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "#         \"UDA_on\": {\"values\": [False]},\n",
    "#         \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "#         \"bias\": {\"values\": [True]},\n",
    "\n",
    "#         \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "#         \"temporal_filter\": {\"values\": [1]},\n",
    "#         \"initial_pooling\": {\"values\": [1]},\n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "#     my_snn_system(  \n",
    "#         devices  =  \"0\",\n",
    "#         single_step  =  wandb.config.single_step,\n",
    "#         unique_name  =  unique_name_hyper,\n",
    "#         my_seed  =  wandb.config.my_seed,\n",
    "#         TIME  =  wandb.config.TIME,\n",
    "#         BATCH  =  wandb.config.BATCH,\n",
    "#         IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "#         which_data  =  wandb.config.which_data,\n",
    "#         data_path  =  wandb.config.data_path,\n",
    "#         rate_coding  =  wandb.config.rate_coding,\n",
    "#         lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "#         lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "#         lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "#         lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "#         lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "#         synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "#         synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "#         synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "#         synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "#         synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "#         pre_trained  =  wandb.config.pre_trained,\n",
    "#         convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "#         cfg  =  wandb.config.cfg,\n",
    "#         net_print  =  wandb.config.net_print,\n",
    "#         pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "#         learning_rate  =  wandb.config.learning_rate,\n",
    "#         epoch_num  =  wandb.config.epoch_num,\n",
    "#         tdBN_on  =  wandb.config.tdBN_on,\n",
    "#         BN_on  =  wandb.config.BN_on,\n",
    "#         surrogate  =  wandb.config.surrogate,\n",
    "#         BPTT_on  =  wandb.config.BPTT_on,\n",
    "#         optimizer_what  =  wandb.config.optimizer_what,\n",
    "#         scheduler_name  =  wandb.config.scheduler_name,\n",
    "#         ddp_on  =  wandb.config.ddp_on,\n",
    "#         dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "#         dvs_duration  =  wandb.config.dvs_duration,\n",
    "#         DFA_on  =  wandb.config.DFA_on,\n",
    "#         trace_on  =  wandb.config.trace_on,\n",
    "#         OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "#         exclude_class  =  wandb.config.exclude_class,\n",
    "#         merge_polarities  =  wandb.config.merge_polarities,\n",
    "#         denoise_on  =  wandb.config.denoise_on,\n",
    "#         extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "#         num_workers  =  wandb.config.num_workers,\n",
    "#         chaching_on  =  wandb.config.chaching_on,\n",
    "#         pin_memory  =  wandb.config.pin_memory,\n",
    "#         UDA_on  =  wandb.config.UDA_on,\n",
    "#         alpha_uda  =  wandb.config.alpha_uda,\n",
    "#         bias  =  wandb.config.bias,\n",
    "#         last_lif  =  wandb.config.last_lif,\n",
    "#         temporal_filter  =  wandb.config.temporal_filter,\n",
    "#         initial_pooling  =  wandb.config.initial_pooling,\n",
    "#                         ) \n",
    "#     # sigmoid와 BN이 있어야 잘된다.\n",
    "#     # average pooling\n",
    "#     # 이 낫다. \n",
    "    \n",
    "#     # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# # sweep_id = '6pj3lh8j'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
