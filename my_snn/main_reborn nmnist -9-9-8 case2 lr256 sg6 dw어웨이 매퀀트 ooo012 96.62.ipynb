{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36901/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "\n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    " \n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "\n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    if (outputs_one_time.detach().cpu()*(2**(-scale_exp[2][0]))).any() > 16383 or (outputs_one_time.detach().cpu()*(2**(-scale_exp[2][0]))).any() < -16384:\n",
    "                        print(f\"IN TR WARNING: outputs_one_time ACCUMULATED value is out of range at time {t} for 8bit quantization. outputs_one_time {outputs_one_time},  Max: {outputs_one_time.detach().cpu().max()}, Min: {outputs_one_time.detach().cpu().min()}\")\n",
    "\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                    optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                if (outputs.detach().cpu()*(2**(-scale_exp[2][0]))).any() > 16383 or (outputs.detach().cpu()*(2**(-scale_exp[2][0]))).any() < -16384:\n",
    "                                    print(f\"IN VAL WARNING: outputs value is out of range at time {t} for 8bit quantization. Outputs {outputs},  Max: {outputs.detach().cpu().max()}, Min: {outputs.detach().cpu().min()}\")\n",
    "\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                            if (outputs.detach().cpu()*(2**(-scale_exp[2][0]))*TIME).any() > 16383 or (outputs.detach().cpu()*(2**(-scale_exp[2][0]))*TIME).any() < -16384:\n",
    "                                print(f\"IN VAL WARNING: outputs ACCUMULATED value is out of range at time {t} for 8bit quantization. Outputs {outputs},  Max: {outputs.detach().cpu().max()}, Min: {outputs.detach().cpu().min()}\")\n",
    "\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250801_172650-smsknfzn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/smsknfzn' target=\"_blank\">restful-bee-14220</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/smsknfzn' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/smsknfzn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '4', 'single_step': True, 'unique_name': '20250801_172648_030', 'my_seed': 123123, 'TIME': 8, 'BATCH': 1, 'IMAGE_SIZE': 17, 'which_data': 'NMNIST_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0.0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000.0, 'lif_layer_sg_width': 6.0, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_20250730_043221_915.pth', 'learning_rate': 0.00390625, 'epoch_num': 300, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1.0, 'bias': False, 'last_lif': False, 'temporal_filter': 1, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]]} \n",
      "\n",
      "dataset_hash = a826ea872885eded99268693d52f91a7\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 60000 BATCH: 1 train_data_count: 60000\n",
      "len(test_loader): 10000 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 15, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 15, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=578, out_features=200, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (2): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.5, v_reset=10000.0, sg_width=6.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=8, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (5): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.5, v_reset=10000.0, sg_width=6.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=8, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=8, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 157,600\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  1.834716/  1.775214, val:  82.98%, val_best:  82.98%, tr:  98.83%, tr_best:  98.83%, epoch time: 2591.28 seconds, 43.19 minutes\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  1.626831/  1.579782, val:  90.29%, val_best:  90.29%, tr:  99.58%, tr_best:  99.58%, epoch time: 2605.20 seconds, 43.42 minutes\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  1.499726/  1.517176, val:  90.80%, val_best:  90.80%, tr:  99.66%, tr_best:  99.66%, epoch time: 2601.59 seconds, 43.36 minutes\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  1.416657/  1.454912, val:  90.45%, val_best:  90.80%, tr:  99.71%, tr_best:  99.71%, epoch time: 2602.50 seconds, 43.38 minutes\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  1.357697/  1.384635, val:  89.87%, val_best:  90.80%, tr:  99.72%, tr_best:  99.72%, epoch time: 2613.85 seconds, 43.56 minutes\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  1.315296/  1.423447, val:  92.01%, val_best:  92.01%, tr:  99.73%, tr_best:  99.73%, epoch time: 2682.75 seconds, 44.71 minutes\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  1.266155/  1.315053, val:  90.58%, val_best:  92.01%, tr:  99.76%, tr_best:  99.76%, epoch time: 2709.33 seconds, 45.16 minutes\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  1.227839/  1.305970, val:  90.00%, val_best:  92.01%, tr:  99.76%, tr_best:  99.76%, epoch time: 2631.60 seconds, 43.86 minutes\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  1.190992/  1.370365, val:  76.35%, val_best:  92.01%, tr:  99.77%, tr_best:  99.77%, epoch time: 2607.33 seconds, 43.46 minutes\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  1.184172/  1.281826, val:  88.34%, val_best:  92.01%, tr:  99.80%, tr_best:  99.80%, epoch time: 2606.18 seconds, 43.44 minutes\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  1.172797/  1.260367, val:  93.85%, val_best:  93.85%, tr:  99.83%, tr_best:  99.83%, epoch time: 2604.66 seconds, 43.41 minutes\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  1.156557/  1.192350, val:  95.60%, val_best:  95.60%, tr:  99.79%, tr_best:  99.83%, epoch time: 2604.04 seconds, 43.40 minutes\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  1.127840/  1.214370, val:  92.88%, val_best:  95.60%, tr:  99.81%, tr_best:  99.83%, epoch time: 2622.13 seconds, 43.70 minutes\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  1.123008/  1.184782, val:  91.22%, val_best:  95.60%, tr:  99.83%, tr_best:  99.83%, epoch time: 2616.42 seconds, 43.61 minutes\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  1.119840/  1.207461, val:  94.48%, val_best:  95.60%, tr:  99.81%, tr_best:  99.83%, epoch time: 2623.19 seconds, 43.72 minutes\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  1.097329/  1.195514, val:  86.93%, val_best:  95.60%, tr:  99.82%, tr_best:  99.83%, epoch time: 2637.31 seconds, 43.96 minutes\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  1.096661/  1.157821, val:  94.90%, val_best:  95.60%, tr:  99.81%, tr_best:  99.83%, epoch time: 2627.91 seconds, 43.80 minutes\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  1.088362/  1.206108, val:  93.75%, val_best:  95.60%, tr:  99.82%, tr_best:  99.83%, epoch time: 2633.72 seconds, 43.90 minutes\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  1.066995/  1.106069, val:  95.67%, val_best:  95.67%, tr:  99.84%, tr_best:  99.84%, epoch time: 2619.79 seconds, 43.66 minutes\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  1.040547/  1.093365, val:  92.71%, val_best:  95.67%, tr:  99.82%, tr_best:  99.84%, epoch time: 2611.65 seconds, 43.53 minutes\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.015713/  1.127059, val:  94.54%, val_best:  95.67%, tr:  99.83%, tr_best:  99.84%, epoch time: 2625.27 seconds, 43.75 minutes\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  1.007495/  1.079809, val:  94.54%, val_best:  95.67%, tr:  99.85%, tr_best:  99.85%, epoch time: 2626.65 seconds, 43.78 minutes\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  1.002269/  1.091786, val:  93.27%, val_best:  95.67%, tr:  99.85%, tr_best:  99.85%, epoch time: 2620.80 seconds, 43.68 minutes\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  0.984728/  1.067709, val:  91.29%, val_best:  95.67%, tr:  99.83%, tr_best:  99.85%, epoch time: 2605.30 seconds, 43.42 minutes\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  0.968076/  1.048194, val:  90.91%, val_best:  95.67%, tr:  99.83%, tr_best:  99.85%, epoch time: 2610.82 seconds, 43.51 minutes\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  0.943344/  1.001093, val:  95.71%, val_best:  95.71%, tr:  99.86%, tr_best:  99.86%, epoch time: 2613.50 seconds, 43.56 minutes\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  0.951180/  1.064710, val:  93.91%, val_best:  95.71%, tr:  99.83%, tr_best:  99.86%, epoch time: 2641.99 seconds, 44.03 minutes\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  0.943615/  1.010843, val:  94.12%, val_best:  95.71%, tr:  99.82%, tr_best:  99.86%, epoch time: 2646.04 seconds, 44.10 minutes\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  0.926917/  0.969000, val:  95.18%, val_best:  95.71%, tr:  99.84%, tr_best:  99.86%, epoch time: 2636.30 seconds, 43.94 minutes\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  0.925678/  0.998945, val:  95.13%, val_best:  95.71%, tr:  99.85%, tr_best:  99.86%, epoch time: 2674.09 seconds, 44.57 minutes\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  0.934336/  0.979376, val:  95.79%, val_best:  95.79%, tr:  99.86%, tr_best:  99.86%, epoch time: 2687.58 seconds, 44.79 minutes\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  0.897666/  0.991804, val:  92.90%, val_best:  95.79%, tr:  99.85%, tr_best:  99.86%, epoch time: 2720.15 seconds, 45.34 minutes\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  0.897213/  0.997979, val:  88.56%, val_best:  95.79%, tr:  99.85%, tr_best:  99.86%, epoch time: 2691.81 seconds, 44.86 minutes\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  0.894446/  0.979667, val:  95.49%, val_best:  95.79%, tr:  99.87%, tr_best:  99.87%, epoch time: 2699.69 seconds, 44.99 minutes\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  0.893321/  0.973803, val:  95.45%, val_best:  95.79%, tr:  99.87%, tr_best:  99.87%, epoch time: 2681.08 seconds, 44.68 minutes\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  0.885124/  0.987246, val:  92.96%, val_best:  95.79%, tr:  99.84%, tr_best:  99.87%, epoch time: 2695.63 seconds, 44.93 minutes\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  0.882441/  0.997665, val:  92.62%, val_best:  95.79%, tr:  99.86%, tr_best:  99.87%, epoch time: 2703.65 seconds, 45.06 minutes\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  0.874803/  0.926946, val:  95.90%, val_best:  95.90%, tr:  99.87%, tr_best:  99.87%, epoch time: 2691.17 seconds, 44.85 minutes\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  0.882550/  0.965493, val:  94.09%, val_best:  95.90%, tr:  99.83%, tr_best:  99.87%, epoch time: 2710.56 seconds, 45.18 minutes\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  0.865501/  0.885568, val:  96.17%, val_best:  96.17%, tr:  99.84%, tr_best:  99.87%, epoch time: 2702.42 seconds, 45.04 minutes\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  0.869746/  0.937901, val:  95.88%, val_best:  96.17%, tr:  99.87%, tr_best:  99.87%, epoch time: 2709.58 seconds, 45.16 minutes\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  0.855946/  0.924289, val:  93.95%, val_best:  96.17%, tr:  99.86%, tr_best:  99.87%, epoch time: 2705.47 seconds, 45.09 minutes\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  0.833727/  0.900235, val:  95.37%, val_best:  96.17%, tr:  99.86%, tr_best:  99.87%, epoch time: 2716.41 seconds, 45.27 minutes\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  0.835044/  0.941056, val:  92.06%, val_best:  96.17%, tr:  99.88%, tr_best:  99.88%, epoch time: 2702.87 seconds, 45.05 minutes\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  0.841198/  0.953715, val:  89.95%, val_best:  96.17%, tr:  99.86%, tr_best:  99.88%, epoch time: 2709.32 seconds, 45.16 minutes\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  0.837363/  0.939268, val:  94.37%, val_best:  96.17%, tr:  99.87%, tr_best:  99.88%, epoch time: 2717.68 seconds, 45.29 minutes\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  0.828497/  0.924521, val:  91.31%, val_best:  96.17%, tr:  99.89%, tr_best:  99.89%, epoch time: 2696.80 seconds, 44.95 minutes\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  0.824337/  0.930094, val:  93.40%, val_best:  96.17%, tr:  99.86%, tr_best:  99.89%, epoch time: 2700.84 seconds, 45.01 minutes\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  0.825376/  0.871453, val:  96.28%, val_best:  96.28%, tr:  99.87%, tr_best:  99.89%, epoch time: 2716.04 seconds, 45.27 minutes\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  0.818727/  0.925521, val:  90.54%, val_best:  96.28%, tr:  99.86%, tr_best:  99.89%, epoch time: 2717.40 seconds, 45.29 minutes\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  0.829584/  0.932629, val:  93.66%, val_best:  96.28%, tr:  99.88%, tr_best:  99.89%, epoch time: 2714.33 seconds, 45.24 minutes\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  0.807890/  0.853958, val:  95.49%, val_best:  96.28%, tr:  99.88%, tr_best:  99.89%, epoch time: 2698.25 seconds, 44.97 minutes\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  0.795314/  0.911439, val:  94.25%, val_best:  96.28%, tr:  99.85%, tr_best:  99.89%, epoch time: 2708.06 seconds, 45.13 minutes\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  0.796071/  0.859239, val:  94.34%, val_best:  96.28%, tr:  99.87%, tr_best:  99.89%, epoch time: 2689.47 seconds, 44.82 minutes\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  0.772467/  0.829159, val:  96.55%, val_best:  96.55%, tr:  99.89%, tr_best:  99.89%, epoch time: 2695.16 seconds, 44.92 minutes\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  0.773314/  0.883463, val:  94.32%, val_best:  96.55%, tr:  99.89%, tr_best:  99.89%, epoch time: 2703.64 seconds, 45.06 minutes\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  0.780047/  0.925237, val:  91.62%, val_best:  96.55%, tr:  99.88%, tr_best:  99.89%, epoch time: 2696.54 seconds, 44.94 minutes\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  0.772041/  0.859579, val:  94.47%, val_best:  96.55%, tr:  99.90%, tr_best:  99.90%, epoch time: 2690.12 seconds, 44.84 minutes\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  0.798973/  0.881300, val:  94.96%, val_best:  96.55%, tr:  99.88%, tr_best:  99.90%, epoch time: 2703.22 seconds, 45.05 minutes\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  0.789255/  0.903093, val:  95.30%, val_best:  96.55%, tr:  99.90%, tr_best:  99.90%, epoch time: 2692.86 seconds, 44.88 minutes\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  0.786332/  0.924003, val:  92.71%, val_best:  96.55%, tr:  99.87%, tr_best:  99.90%, epoch time: 2688.83 seconds, 44.81 minutes\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  0.781775/  0.903448, val:  91.23%, val_best:  96.55%, tr:  99.89%, tr_best:  99.90%, epoch time: 2691.35 seconds, 44.86 minutes\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  0.772990/  0.836069, val:  95.20%, val_best:  96.55%, tr:  99.91%, tr_best:  99.91%, epoch time: 2708.79 seconds, 45.15 minutes\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  0.776792/  0.826688, val:  94.01%, val_best:  96.55%, tr:  99.87%, tr_best:  99.91%, epoch time: 2699.20 seconds, 44.99 minutes\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  0.775321/  0.849339, val:  95.85%, val_best:  96.55%, tr:  99.86%, tr_best:  99.91%, epoch time: 2705.53 seconds, 45.09 minutes\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  0.767648/  0.835062, val:  96.05%, val_best:  96.55%, tr:  99.87%, tr_best:  99.91%, epoch time: 2712.77 seconds, 45.21 minutes\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  0.769327/  0.871680, val:  95.86%, val_best:  96.55%, tr:  99.91%, tr_best:  99.91%, epoch time: 2707.13 seconds, 45.12 minutes\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  0.761369/  0.809221, val:  95.89%, val_best:  96.55%, tr:  99.87%, tr_best:  99.91%, epoch time: 2673.92 seconds, 44.57 minutes\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  0.757235/  0.839907, val:  93.67%, val_best:  96.55%, tr:  99.85%, tr_best:  99.91%, epoch time: 2649.24 seconds, 44.15 minutes\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  0.768419/  0.922950, val:  87.95%, val_best:  96.55%, tr:  99.89%, tr_best:  99.91%, epoch time: 2658.67 seconds, 44.31 minutes\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  0.770034/  0.856087, val:  94.09%, val_best:  96.55%, tr:  99.88%, tr_best:  99.91%, epoch time: 2648.67 seconds, 44.14 minutes\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  0.774722/  0.847831, val:  95.28%, val_best:  96.55%, tr:  99.88%, tr_best:  99.91%, epoch time: 2645.88 seconds, 44.10 minutes\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  0.763737/  0.841361, val:  95.95%, val_best:  96.55%, tr:  99.86%, tr_best:  99.91%, epoch time: 2666.11 seconds, 44.44 minutes\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  0.764359/  0.864230, val:  94.95%, val_best:  96.55%, tr:  99.88%, tr_best:  99.91%, epoch time: 2636.84 seconds, 43.95 minutes\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  0.768069/  0.872318, val:  95.09%, val_best:  96.55%, tr:  99.89%, tr_best:  99.91%, epoch time: 2646.49 seconds, 44.11 minutes\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  0.777370/  0.841503, val:  95.69%, val_best:  96.55%, tr:  99.88%, tr_best:  99.91%, epoch time: 2654.89 seconds, 44.25 minutes\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  0.763299/  0.878738, val:  93.46%, val_best:  96.55%, tr:  99.86%, tr_best:  99.91%, epoch time: 2636.21 seconds, 43.94 minutes\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  0.757066/  0.847544, val:  94.77%, val_best:  96.55%, tr:  99.90%, tr_best:  99.91%, epoch time: 2661.72 seconds, 44.36 minutes\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  0.755689/  0.834951, val:  95.21%, val_best:  96.55%, tr:  99.89%, tr_best:  99.91%, epoch time: 2704.01 seconds, 45.07 minutes\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  0.770078/  0.869594, val:  93.97%, val_best:  96.55%, tr:  99.85%, tr_best:  99.91%, epoch time: 2703.88 seconds, 45.06 minutes\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  0.759407/  0.842505, val:  92.78%, val_best:  96.55%, tr:  99.89%, tr_best:  99.91%, epoch time: 2698.67 seconds, 44.98 minutes\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  0.744233/  0.809635, val:  95.88%, val_best:  96.55%, tr:  99.86%, tr_best:  99.91%, epoch time: 2709.88 seconds, 45.16 minutes\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  0.755023/  0.864661, val:  93.54%, val_best:  96.55%, tr:  99.89%, tr_best:  99.91%, epoch time: 2723.10 seconds, 45.38 minutes\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  0.748067/  0.813806, val:  95.45%, val_best:  96.55%, tr:  99.88%, tr_best:  99.91%, epoch time: 2696.83 seconds, 44.95 minutes\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  0.742846/  0.833102, val:  94.79%, val_best:  96.55%, tr:  99.90%, tr_best:  99.91%, epoch time: 2703.58 seconds, 45.06 minutes\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  0.749004/  0.842585, val:  96.39%, val_best:  96.55%, tr:  99.89%, tr_best:  99.91%, epoch time: 2706.06 seconds, 45.10 minutes\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  0.758707/  0.875058, val:  90.70%, val_best:  96.55%, tr:  99.87%, tr_best:  99.91%, epoch time: 2703.96 seconds, 45.07 minutes\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  0.747644/  0.862168, val:  94.58%, val_best:  96.55%, tr:  99.89%, tr_best:  99.91%, epoch time: 2714.58 seconds, 45.24 minutes\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  0.745570/  0.895201, val:  90.42%, val_best:  96.55%, tr:  99.88%, tr_best:  99.91%, epoch time: 2709.67 seconds, 45.16 minutes\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  0.757126/  0.813446, val:  94.20%, val_best:  96.55%, tr:  99.90%, tr_best:  99.91%, epoch time: 2701.33 seconds, 45.02 minutes\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  0.743906/  0.826513, val:  95.55%, val_best:  96.55%, tr:  99.88%, tr_best:  99.91%, epoch time: 2696.57 seconds, 44.94 minutes\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  0.750836/  0.896395, val:  91.90%, val_best:  96.55%, tr:  99.87%, tr_best:  99.91%, epoch time: 2704.37 seconds, 45.07 minutes\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  0.755788/  0.839845, val:  95.56%, val_best:  96.55%, tr:  99.89%, tr_best:  99.91%, epoch time: 2695.03 seconds, 44.92 minutes\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  0.743301/  0.906197, val:  88.77%, val_best:  96.55%, tr:  99.90%, tr_best:  99.91%, epoch time: 2702.25 seconds, 45.04 minutes\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  0.750062/  0.822843, val:  95.15%, val_best:  96.55%, tr:  99.89%, tr_best:  99.91%, epoch time: 2699.69 seconds, 44.99 minutes\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  0.753956/  0.821484, val:  95.52%, val_best:  96.55%, tr:  99.90%, tr_best:  99.91%, epoch time: 2689.40 seconds, 44.82 minutes\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  0.754689/  0.785231, val:  96.05%, val_best:  96.55%, tr:  99.90%, tr_best:  99.91%, epoch time: 2691.33 seconds, 44.86 minutes\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  0.755907/  0.868016, val:  94.53%, val_best:  96.55%, tr:  99.91%, tr_best:  99.91%, epoch time: 2703.26 seconds, 45.05 minutes\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  0.755747/  0.800659, val:  95.94%, val_best:  96.55%, tr:  99.89%, tr_best:  99.91%, epoch time: 2691.10 seconds, 44.85 minutes\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  0.746407/  0.804250, val:  95.63%, val_best:  96.55%, tr:  99.88%, tr_best:  99.91%, epoch time: 2704.52 seconds, 45.08 minutes\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  0.747997/  0.831356, val:  93.77%, val_best:  96.55%, tr:  99.91%, tr_best:  99.91%, epoch time: 2686.30 seconds, 44.77 minutes\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  0.748990/  0.844576, val:  95.19%, val_best:  96.55%, tr:  99.88%, tr_best:  99.91%, epoch time: 2684.00 seconds, 44.73 minutes\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  0.741138/  0.834395, val:  95.06%, val_best:  96.55%, tr:  99.87%, tr_best:  99.91%, epoch time: 2701.58 seconds, 45.03 minutes\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  0.730973/  0.827574, val:  90.63%, val_best:  96.55%, tr:  99.90%, tr_best:  99.91%, epoch time: 2706.20 seconds, 45.10 minutes\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  0.719439/  0.785728, val:  95.76%, val_best:  96.55%, tr:  99.90%, tr_best:  99.91%, epoch time: 2689.47 seconds, 44.82 minutes\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  0.734046/  0.817937, val:  93.92%, val_best:  96.55%, tr:  99.92%, tr_best:  99.92%, epoch time: 2701.81 seconds, 45.03 minutes\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  0.736977/  0.789328, val:  94.42%, val_best:  96.55%, tr:  99.89%, tr_best:  99.92%, epoch time: 2701.89 seconds, 45.03 minutes\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  0.728142/  0.786024, val:  95.75%, val_best:  96.55%, tr:  99.88%, tr_best:  99.92%, epoch time: 2701.44 seconds, 45.02 minutes\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  0.732663/  0.814765, val:  95.68%, val_best:  96.55%, tr:  99.90%, tr_best:  99.92%, epoch time: 2709.63 seconds, 45.16 minutes\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  0.740734/  0.812223, val:  95.59%, val_best:  96.55%, tr:  99.90%, tr_best:  99.92%, epoch time: 2706.10 seconds, 45.10 minutes\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  0.724182/  0.838176, val:  93.71%, val_best:  96.55%, tr:  99.89%, tr_best:  99.92%, epoch time: 2701.99 seconds, 45.03 minutes\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  0.731856/  0.810447, val:  95.55%, val_best:  96.55%, tr:  99.91%, tr_best:  99.92%, epoch time: 2711.35 seconds, 45.19 minutes\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  0.734934/  0.807308, val:  95.29%, val_best:  96.55%, tr:  99.91%, tr_best:  99.92%, epoch time: 2711.89 seconds, 45.20 minutes\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  0.737753/  0.801238, val:  96.16%, val_best:  96.55%, tr:  99.89%, tr_best:  99.92%, epoch time: 2703.17 seconds, 45.05 minutes\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  0.724066/  0.792450, val:  96.10%, val_best:  96.55%, tr:  99.91%, tr_best:  99.92%, epoch time: 2704.21 seconds, 45.07 minutes\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  0.730666/  0.800317, val:  95.96%, val_best:  96.55%, tr:  99.88%, tr_best:  99.92%, epoch time: 2720.31 seconds, 45.34 minutes\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  0.742008/  0.814003, val:  96.22%, val_best:  96.55%, tr:  99.89%, tr_best:  99.92%, epoch time: 2717.22 seconds, 45.29 minutes\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  0.743364/  0.805779, val:  95.55%, val_best:  96.55%, tr:  99.89%, tr_best:  99.92%, epoch time: 2711.47 seconds, 45.19 minutes\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  0.740187/  0.800737, val:  95.34%, val_best:  96.55%, tr:  99.91%, tr_best:  99.92%, epoch time: 2705.44 seconds, 45.09 minutes\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  0.733329/  0.780800, val:  96.59%, val_best:  96.59%, tr:  99.88%, tr_best:  99.92%, epoch time: 2718.10 seconds, 45.30 minutes\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  0.736264/  0.798676, val:  94.95%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2694.50 seconds, 44.91 minutes\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  0.729694/  0.817579, val:  92.23%, val_best:  96.59%, tr:  99.89%, tr_best:  99.92%, epoch time: 2692.55 seconds, 44.88 minutes\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  0.729319/  0.803088, val:  94.68%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2626.87 seconds, 43.78 minutes\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  0.746916/  0.833133, val:  93.31%, val_best:  96.59%, tr:  99.89%, tr_best:  99.92%, epoch time: 2536.90 seconds, 42.28 minutes\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  0.728570/  0.784215, val:  95.32%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2436.47 seconds, 40.61 minutes\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  0.733638/  0.832184, val:  93.00%, val_best:  96.59%, tr:  99.89%, tr_best:  99.92%, epoch time: 2656.27 seconds, 44.27 minutes\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  0.755117/  0.848866, val:  95.84%, val_best:  96.59%, tr:  99.88%, tr_best:  99.92%, epoch time: 2647.61 seconds, 44.13 minutes\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  0.749257/  0.813155, val:  95.86%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2660.00 seconds, 44.33 minutes\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  0.744267/  0.858071, val:  89.72%, val_best:  96.59%, tr:  99.88%, tr_best:  99.92%, epoch time: 2651.09 seconds, 44.18 minutes\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  0.738819/  0.796464, val:  94.36%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2663.83 seconds, 44.40 minutes\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  0.737114/  0.821432, val:  94.64%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2671.35 seconds, 44.52 minutes\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  0.739998/  0.827139, val:  93.52%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2665.92 seconds, 44.43 minutes\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  0.732256/  0.808342, val:  94.83%, val_best:  96.59%, tr:  99.88%, tr_best:  99.92%, epoch time: 2635.14 seconds, 43.92 minutes\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  0.736384/  0.809793, val:  95.35%, val_best:  96.59%, tr:  99.89%, tr_best:  99.92%, epoch time: 2642.46 seconds, 44.04 minutes\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  0.731066/  0.843924, val:  94.51%, val_best:  96.59%, tr:  99.89%, tr_best:  99.92%, epoch time: 2662.15 seconds, 44.37 minutes\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  0.737884/  0.833584, val:  93.75%, val_best:  96.59%, tr:  99.89%, tr_best:  99.92%, epoch time: 2655.31 seconds, 44.26 minutes\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  0.732680/  0.836264, val:  90.67%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2651.27 seconds, 44.19 minutes\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  0.735790/  0.791512, val:  95.81%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2651.40 seconds, 44.19 minutes\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  0.719998/  0.779348, val:  95.09%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2655.04 seconds, 44.25 minutes\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  0.713197/  0.783319, val:  94.09%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2672.82 seconds, 44.55 minutes\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  0.704505/  0.830130, val:  90.77%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2659.54 seconds, 44.33 minutes\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  0.710648/  0.819381, val:  94.03%, val_best:  96.59%, tr:  99.89%, tr_best:  99.92%, epoch time: 2660.68 seconds, 44.34 minutes\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  0.716300/  0.815884, val:  94.30%, val_best:  96.59%, tr:  99.88%, tr_best:  99.92%, epoch time: 2659.85 seconds, 44.33 minutes\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  0.720824/  0.833926, val:  94.19%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2655.08 seconds, 44.25 minutes\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  0.716854/  0.785339, val:  94.42%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2652.97 seconds, 44.22 minutes\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  0.721163/  0.815737, val:  93.15%, val_best:  96.59%, tr:  99.87%, tr_best:  99.92%, epoch time: 2659.16 seconds, 44.32 minutes\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  0.718180/  0.815829, val:  95.05%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2674.72 seconds, 44.58 minutes\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  0.713576/  0.812855, val:  92.86%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2660.64 seconds, 44.34 minutes\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  0.721138/  0.804030, val:  93.70%, val_best:  96.59%, tr:  99.89%, tr_best:  99.92%, epoch time: 2653.46 seconds, 44.22 minutes\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  0.721971/  0.839981, val:  89.58%, val_best:  96.59%, tr:  99.88%, tr_best:  99.92%, epoch time: 2653.89 seconds, 44.23 minutes\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  0.725596/  0.769924, val:  95.35%, val_best:  96.59%, tr:  99.89%, tr_best:  99.92%, epoch time: 2653.12 seconds, 44.22 minutes\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  0.717069/  0.830757, val:  90.64%, val_best:  96.59%, tr:  99.88%, tr_best:  99.92%, epoch time: 2653.48 seconds, 44.22 minutes\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  0.711387/  0.807309, val:  94.12%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2639.21 seconds, 43.99 minutes\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  0.713930/  0.783516, val:  95.79%, val_best:  96.59%, tr:  99.89%, tr_best:  99.92%, epoch time: 2636.41 seconds, 43.94 minutes\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  0.723298/  0.747171, val:  95.55%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2636.98 seconds, 43.95 minutes\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  0.722254/  0.835457, val:  92.67%, val_best:  96.59%, tr:  99.88%, tr_best:  99.92%, epoch time: 2640.38 seconds, 44.01 minutes\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  0.708318/  0.818462, val:  93.39%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2628.78 seconds, 43.81 minutes\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  0.714846/  0.777619, val:  96.04%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2629.73 seconds, 43.83 minutes\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  0.703217/  0.778898, val:  95.30%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2629.55 seconds, 43.83 minutes\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  0.707924/  0.796776, val:  94.24%, val_best:  96.59%, tr:  99.89%, tr_best:  99.92%, epoch time: 2635.81 seconds, 43.93 minutes\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  0.701697/  0.785563, val:  95.96%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2654.45 seconds, 44.24 minutes\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  0.697405/  0.805139, val:  95.37%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2653.68 seconds, 44.23 minutes\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  0.699788/  0.812658, val:  90.86%, val_best:  96.59%, tr:  99.87%, tr_best:  99.92%, epoch time: 2657.84 seconds, 44.30 minutes\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  0.699364/  0.799083, val:  94.31%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2656.75 seconds, 44.28 minutes\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  0.697054/  0.747792, val:  95.98%, val_best:  96.59%, tr:  99.89%, tr_best:  99.92%, epoch time: 2634.46 seconds, 43.91 minutes\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  0.695554/  0.773822, val:  95.58%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2634.63 seconds, 43.91 minutes\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  0.710555/  0.764275, val:  95.81%, val_best:  96.59%, tr:  99.89%, tr_best:  99.92%, epoch time: 2661.23 seconds, 44.35 minutes\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  0.701104/  0.770062, val:  93.86%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2671.73 seconds, 44.53 minutes\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  0.700262/  0.789535, val:  95.02%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2680.52 seconds, 44.68 minutes\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  0.707254/  0.757977, val:  95.88%, val_best:  96.59%, tr:  99.89%, tr_best:  99.92%, epoch time: 2694.71 seconds, 44.91 minutes\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  0.710658/  0.786369, val:  95.78%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2665.65 seconds, 44.43 minutes\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  0.716047/  0.792095, val:  95.18%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2666.85 seconds, 44.45 minutes\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  0.714556/  0.850087, val:  94.36%, val_best:  96.59%, tr:  99.89%, tr_best:  99.92%, epoch time: 2635.70 seconds, 43.93 minutes\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  0.710957/  0.773414, val:  95.92%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2672.76 seconds, 44.55 minutes\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  0.708229/  0.802730, val:  95.44%, val_best:  96.59%, tr:  99.92%, tr_best:  99.92%, epoch time: 2664.21 seconds, 44.40 minutes\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  0.704135/  0.788404, val:  96.09%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2683.97 seconds, 44.73 minutes\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  0.708405/  0.796509, val:  93.11%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2709.74 seconds, 45.16 minutes\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  0.705397/  0.785324, val:  95.23%, val_best:  96.59%, tr:  99.90%, tr_best:  99.92%, epoch time: 2663.02 seconds, 44.38 minutes\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  0.707788/  0.767174, val:  95.17%, val_best:  96.59%, tr:  99.91%, tr_best:  99.92%, epoch time: 2677.06 seconds, 44.62 minutes\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  0.706527/  0.806907, val:  95.23%, val_best:  96.59%, tr:  99.89%, tr_best:  99.92%, epoch time: 2666.63 seconds, 44.44 minutes\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  0.717830/  0.763427, val:  96.62%, val_best:  96.62%, tr:  99.89%, tr_best:  99.92%, epoch time: 2646.29 seconds, 44.10 minutes\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  0.699914/  0.761134, val:  95.40%, val_best:  96.62%, tr:  99.90%, tr_best:  99.92%, epoch time: 2627.89 seconds, 43.80 minutes\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  0.703708/  0.779942, val:  95.87%, val_best:  96.62%, tr:  99.91%, tr_best:  99.92%, epoch time: 2627.78 seconds, 43.80 minutes\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  0.689076/  0.790003, val:  93.72%, val_best:  96.62%, tr:  99.89%, tr_best:  99.92%, epoch time: 2647.61 seconds, 44.13 minutes\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  0.703817/  0.812983, val:  95.13%, val_best:  96.62%, tr:  99.89%, tr_best:  99.92%, epoch time: 2707.95 seconds, 45.13 minutes\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  0.715335/  0.812095, val:  95.55%, val_best:  96.62%, tr:  99.89%, tr_best:  99.92%, epoch time: 2732.04 seconds, 45.53 minutes\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  0.718760/  0.807815, val:  94.95%, val_best:  96.62%, tr:  99.91%, tr_best:  99.92%, epoch time: 2724.79 seconds, 45.41 minutes\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  0.720286/  0.792562, val:  95.69%, val_best:  96.62%, tr:  99.90%, tr_best:  99.92%, epoch time: 2523.86 seconds, 42.06 minutes\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  0.709370/  0.790962, val:  95.81%, val_best:  96.62%, tr:  99.90%, tr_best:  99.92%, epoch time: 2683.59 seconds, 44.73 minutes\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  0.714567/  0.847066, val:  93.71%, val_best:  96.62%, tr:  99.91%, tr_best:  99.92%, epoch time: 2677.37 seconds, 44.62 minutes\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  0.716823/  0.819009, val:  93.61%, val_best:  96.62%, tr:  99.92%, tr_best:  99.92%, epoch time: 2678.14 seconds, 44.64 minutes\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  0.728215/  0.758114, val:  95.87%, val_best:  96.62%, tr:  99.89%, tr_best:  99.92%, epoch time: 2671.89 seconds, 44.53 minutes\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  0.724754/  0.812217, val:  92.97%, val_best:  96.62%, tr:  99.91%, tr_best:  99.92%, epoch time: 2690.20 seconds, 44.84 minutes\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  0.729703/  0.813218, val:  95.30%, val_best:  96.62%, tr:  99.89%, tr_best:  99.92%, epoch time: 2668.40 seconds, 44.47 minutes\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  0.710460/  0.775010, val:  95.57%, val_best:  96.62%, tr:  99.91%, tr_best:  99.92%, epoch time: 2670.72 seconds, 44.51 minutes\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  0.712765/  0.788496, val:  95.18%, val_best:  96.62%, tr:  99.87%, tr_best:  99.92%, epoch time: 2671.29 seconds, 44.52 minutes\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  0.727968/  0.800801, val:  96.02%, val_best:  96.62%, tr:  99.88%, tr_best:  99.92%, epoch time: 2679.76 seconds, 44.66 minutes\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  0.721230/  0.805247, val:  95.20%, val_best:  96.62%, tr:  99.90%, tr_best:  99.92%, epoch time: 2685.46 seconds, 44.76 minutes\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  0.722512/  0.819521, val:  95.88%, val_best:  96.62%, tr:  99.89%, tr_best:  99.92%, epoch time: 2688.96 seconds, 44.82 minutes\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  0.713675/  0.812486, val:  94.80%, val_best:  96.62%, tr:  99.91%, tr_best:  99.92%, epoch time: 2691.67 seconds, 44.86 minutes\n",
      "epoch-200 lr=['0.0039062'], tr/val_loss:  0.705108/  0.801648, val:  94.55%, val_best:  96.62%, tr:  99.90%, tr_best:  99.92%, epoch time: 2685.79 seconds, 44.76 minutes\n",
      "epoch-201 lr=['0.0039062'], tr/val_loss:  0.702105/  0.787774, val:  96.08%, val_best:  96.62%, tr:  99.89%, tr_best:  99.92%, epoch time: 2687.06 seconds, 44.78 minutes\n",
      "epoch-202 lr=['0.0039062'], tr/val_loss:  0.698422/  0.769331, val:  95.73%, val_best:  96.62%, tr:  99.91%, tr_best:  99.92%, epoch time: 2686.13 seconds, 44.77 minutes\n",
      "epoch-203 lr=['0.0039062'], tr/val_loss:  0.700171/  0.781323, val:  95.99%, val_best:  96.62%, tr:  99.89%, tr_best:  99.92%, epoch time: 2690.78 seconds, 44.85 minutes\n",
      "epoch-204 lr=['0.0039062'], tr/val_loss:  0.696708/  0.813689, val:  94.39%, val_best:  96.62%, tr:  99.89%, tr_best:  99.92%, epoch time: 2691.09 seconds, 44.85 minutes\n",
      "epoch-205 lr=['0.0039062'], tr/val_loss:  0.705048/  0.770276, val:  96.24%, val_best:  96.62%, tr:  99.89%, tr_best:  99.92%, epoch time: 2701.72 seconds, 45.03 minutes\n",
      "epoch-206 lr=['0.0039062'], tr/val_loss:  0.701920/  0.806520, val:  93.17%, val_best:  96.62%, tr:  99.91%, tr_best:  99.92%, epoch time: 2692.09 seconds, 44.87 minutes\n",
      "epoch-207 lr=['0.0039062'], tr/val_loss:  0.690472/  0.847439, val:  90.45%, val_best:  96.62%, tr:  99.90%, tr_best:  99.92%, epoch time: 2694.39 seconds, 44.91 minutes\n",
      "epoch-208 lr=['0.0039062'], tr/val_loss:  0.691306/  0.790237, val:  94.74%, val_best:  96.62%, tr:  99.91%, tr_best:  99.92%, epoch time: 2692.49 seconds, 44.87 minutes\n",
      "epoch-209 lr=['0.0039062'], tr/val_loss:  0.699275/  0.767103, val:  96.56%, val_best:  96.62%, tr:  99.91%, tr_best:  99.92%, epoch time: 2698.28 seconds, 44.97 minutes\n",
      "epoch-210 lr=['0.0039062'], tr/val_loss:  0.696354/  0.792596, val:  95.23%, val_best:  96.62%, tr:  99.92%, tr_best:  99.92%, epoch time: 2697.81 seconds, 44.96 minutes\n",
      "epoch-211 lr=['0.0039062'], tr/val_loss:  0.710081/  0.781357, val:  96.09%, val_best:  96.62%, tr:  99.91%, tr_best:  99.92%, epoch time: 2694.20 seconds, 44.90 minutes\n",
      "epoch-212 lr=['0.0039062'], tr/val_loss:  0.699285/  0.727094, val:  96.18%, val_best:  96.62%, tr:  99.91%, tr_best:  99.92%, epoch time: 2694.46 seconds, 44.91 minutes\n",
      "epoch-213 lr=['0.0039062'], tr/val_loss:  0.679234/  0.789834, val:  92.74%, val_best:  96.62%, tr:  99.91%, tr_best:  99.92%, epoch time: 2685.53 seconds, 44.76 minutes\n",
      "epoch-214 lr=['0.0039062'], tr/val_loss:  0.668422/  0.751042, val:  96.37%, val_best:  96.62%, tr:  99.90%, tr_best:  99.92%, epoch time: 2686.99 seconds, 44.78 minutes\n"
     ]
    }
   ],
   "source": [
    "### my_snn control board (Gesture) ########################\n",
    "decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "unique_name = 'main'\n",
    "run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "my_snn_system(  devices = \"4\",\n",
    "                single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "                unique_name = run_name,\n",
    "                my_seed = 123123,\n",
    "                TIME = 8, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "                BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 17, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "                # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "                which_data = 'NMNIST_TONIC',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "                lif_layer_sg_width = 6.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "\n",
    "                synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "                synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "                # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "                cfg = [200, 200], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "                # cfg = ['M', 'M', 64], \n",
    "                # cfg = [64, 124, 64, 124],\n",
    "                # cfg = ['M','M',512], \n",
    "                # cfg = [512], \n",
    "                # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                # cfg = ['M','M',512],\n",
    "                # cfg = ['M',200],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = ['M','M',200,200],\n",
    "                # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = ['M',200,200],\n",
    "                # cfg = ['M','M',1024,512,256,128,64],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = [12], #fc\n",
    "                # cfg = [12, 'M', 48, 'M', 12], \n",
    "                # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [],        \n",
    "                \n",
    "                net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_weights_20250730_043221_915.pth\",\n",
    "                # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                learning_rate = 1/256, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                epoch_num = 300,\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "                BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                dvs_clipping = 1, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "                # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "                dvs_duration = 5_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "                # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "                # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "                DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "                trace_on = False,   # True # False\n",
    "                OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "                exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "                extra_train_dataset = 0, \n",
    "\n",
    "                num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "                chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "                pin_memory = True, # True # False \n",
    "\n",
    "                UDA_on = False,  # DECREPATED # uda\n",
    "                alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                bias = False, # True # False \n",
    "\n",
    "                last_lif = False, # True # False \n",
    "\n",
    "                temporal_filter = 1, \n",
    "                initial_pooling = 1,\n",
    "\n",
    "                temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "                quantize_bit_list=[8,8,8],\n",
    "                scale_exp=[[-9,-9],[-9,-9],[-8,-8]], \n",
    "# 1w -11~-9\n",
    "# 1b -11~ -7\n",
    "# 2w -10~-8\n",
    "# 2b -10~-8\n",
    "# 3w -10\n",
    "# 3b -10\n",
    "                ) \n",
    "\n",
    "# num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# num_workers = batch_size / num_GPU\n",
    "# num_workers = batch_size / num_CPU\n",
    "\n",
    "# sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# average pooling  \n",
    "# Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "\n",
    "# 46ÍπåÏßÄÌñàÏóàÍ∏¥ÌñàÏùå\n",
    "# och-42  lr=['0.0039062'], tr/val_loss:  0.791181/  0.859875, val:  94.82%, val_best:  96.05%, tr:  99.85%, tr_best:  99.88%, epoch time: 2653.91 seconds, 44.23 minutes\n",
    "# epoch-43  lr=['0.0039062'], tr/val_loss:  0.797550/  0.848301, val:  95.38%, val_best:  96.05%, tr:  99.87%, tr_best:  99.88%, epoch time: 2648.23 seconds, 44.14 minutes\n",
    "# epoch-44  lr=['0.0039062'], tr/val_loss:  0.801503/  0.850083, val:  95.41%, val_best:  96.05%, tr:  99.87%, tr_best:  99.88%, epoch time: 2644.07 seconds, 44.07 minutes\n",
    "# epoch-45  lr=['0.0039062'], tr/val_loss:  0.802839/  0.866149, val:  94.36%, val_best:  96.05%, tr:  99.87%, tr_best:  99.88%, epoch time: 2633.83 seconds, 43.90 minutes\n",
    "# Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# # Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# # wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "#     'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         # \"devices\": {\"values\": [\"1\"]},\n",
    "#         \"single_step\": {\"values\": [True]},\n",
    "#         # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "#         \"my_seed\": {\"values\": [42]},\n",
    "#         \"TIME\": {\"values\": [8]},\n",
    "#         \"BATCH\": {\"values\": [1]},\n",
    "#         \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "#         \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "#         \"data_path\": {\"values\": ['/data2']},\n",
    "#         \"rate_coding\": {\"values\": [False]},\n",
    "#         \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "#         \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "#         \"lif_layer_v_threshold\": {\"values\": [0.5]},\n",
    "#         \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "#         \"lif_layer_sg_width\": {\"values\": [4.0]},\n",
    "\n",
    "#         \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "#         \"synapse_conv_stride\": {\"values\": [1]},\n",
    "#         \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "#         \"synapse_trace_const1\": {\"values\": [1]},\n",
    "#         \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "#         \"pre_trained\": {\"values\": [False]},\n",
    "#         \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "#         \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "#         \"net_print\": {\"values\": [True]},\n",
    "\n",
    "#         \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "#         \"learning_rate\": {\"values\": [0.1,0.01,0.001,0.0001,0.00001]}, \n",
    "#         \"epoch_num\": {\"values\": [1]}, \n",
    "#         \"tdBN_on\": {\"values\": [False]},\n",
    "#         \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "#         \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"optimizer_what\": {\"values\": ['SGD']},\n",
    "#         \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "#         \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "#         \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "#         \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "#         \"trace_on\": {\"values\": [False]},\n",
    "#         \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "#         \"merge_polarities\": {\"values\": [True]},\n",
    "#         \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"extra_train_dataset\": {\"values\": [9]},\n",
    "\n",
    "#         \"num_workers\": {\"values\": [2]},\n",
    "#         \"chaching_on\": {\"values\": [True]},\n",
    "#         \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "#         \"UDA_on\": {\"values\": [False]},\n",
    "#         \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "#         \"bias\": {\"values\": [True]},\n",
    "\n",
    "#         \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "#         \"temporal_filter\": {\"values\": [5]},\n",
    "#         \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "#         \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "#         \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "#         \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "#         \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "#         \"scale_exp_1w\": {\"values\": [-11,-10,-9]},\n",
    "#         # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "#         \"scale_exp_2w\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "#         # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "#         \"scale_exp_3w\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "#         # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "#     my_snn_system(  \n",
    "#         devices  =  \"5\",\n",
    "#         single_step  =  wandb.config.single_step,\n",
    "#         unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "#         my_seed  =  wandb.config.my_seed,\n",
    "#         TIME  =  wandb.config.TIME,\n",
    "#         BATCH  =  wandb.config.BATCH,\n",
    "#         IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "#         which_data  =  wandb.config.which_data,\n",
    "#         data_path  =  wandb.config.data_path,\n",
    "#         rate_coding  =  wandb.config.rate_coding,\n",
    "#         lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "#         lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "#         lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "#         lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "#         lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "#         synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "#         synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "#         synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "#         synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "#         synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "#         pre_trained  =  wandb.config.pre_trained,\n",
    "#         convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "#         cfg  =  wandb.config.cfg,\n",
    "#         net_print  =  wandb.config.net_print,\n",
    "#         pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "#         learning_rate  =  wandb.config.learning_rate,\n",
    "#         epoch_num  =  wandb.config.epoch_num,\n",
    "#         tdBN_on  =  wandb.config.tdBN_on,\n",
    "#         BN_on  =  wandb.config.BN_on,\n",
    "#         surrogate  =  wandb.config.surrogate,\n",
    "#         BPTT_on  =  wandb.config.BPTT_on,\n",
    "#         optimizer_what  =  wandb.config.optimizer_what,\n",
    "#         scheduler_name  =  wandb.config.scheduler_name,\n",
    "#         ddp_on  =  wandb.config.ddp_on,\n",
    "#         dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "#         dvs_duration  =  wandb.config.dvs_duration,\n",
    "#         DFA_on  =  wandb.config.DFA_on,\n",
    "#         trace_on  =  wandb.config.trace_on,\n",
    "#         OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "#         exclude_class  =  wandb.config.exclude_class,\n",
    "#         merge_polarities  =  wandb.config.merge_polarities,\n",
    "#         denoise_on  =  wandb.config.denoise_on,\n",
    "#         extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "#         num_workers  =  wandb.config.num_workers,\n",
    "#         chaching_on  =  wandb.config.chaching_on,\n",
    "#         pin_memory  =  wandb.config.pin_memory,\n",
    "#         UDA_on  =  wandb.config.UDA_on,\n",
    "#         alpha_uda  =  wandb.config.alpha_uda,\n",
    "#         bias  =  wandb.config.bias,\n",
    "#         last_lif  =  wandb.config.last_lif,\n",
    "#         temporal_filter  =  wandb.config.temporal_filter,\n",
    "#         initial_pooling  =  wandb.config.initial_pooling,\n",
    "#         temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "#         quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "#         scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "#                         ) \n",
    "#     # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "#     # average pooling\n",
    "#     # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "#     # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# # sweep_id = 'v89awhtt'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
