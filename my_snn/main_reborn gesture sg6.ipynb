{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19379/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251105_150428-ymx7ady1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ymx7ady1' target=\"_blank\">expert-sponge-17446</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ymx7ady1' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ymx7ady1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251105_150426_096', 'my_seed': 29127, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0.0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000.0, 'lif_layer_sg_width': 6.0, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_main.pth', 'learning_rate': 0.001953125, 'epoch_num': 300, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1.0, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.5, v_reset=10000.0, sg_width=6.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.5, v_reset=10000.0, sg_width=6.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 471.0\n",
      "lif layer 1 self.abs_max_v: 471.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 523.0\n",
      "lif layer 1 self.abs_max_v: 667.5\n",
      "fc layer 2 self.abs_max_out: 202.0\n",
      "lif layer 2 self.abs_max_v: 202.0\n",
      "lif layer 1 self.abs_max_v: 683.0\n",
      "fc layer 1 self.abs_max_out: 731.0\n",
      "lif layer 1 self.abs_max_v: 966.5\n",
      "fc layer 2 self.abs_max_out: 447.0\n",
      "lif layer 2 self.abs_max_v: 498.0\n",
      "fc layer 1 self.abs_max_out: 789.0\n",
      "lif layer 1 self.abs_max_v: 1059.5\n",
      "lif layer 2 self.abs_max_v: 525.5\n",
      "lif layer 2 self.abs_max_v: 559.5\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "fc layer 1 self.abs_max_out: 1013.0\n",
      "fc layer 2 self.abs_max_out: 493.0\n",
      "fc layer 2 self.abs_max_out: 548.0\n",
      "lif layer 2 self.abs_max_v: 641.5\n",
      "fc layer 3 self.abs_max_out: 47.0\n",
      "lif layer 2 self.abs_max_v: 663.0\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "fc layer 1 self.abs_max_out: 1018.0\n",
      "fc layer 1 self.abs_max_out: 1046.0\n",
      "lif layer 1 self.abs_max_v: 1065.0\n",
      "lif layer 2 self.abs_max_v: 696.5\n",
      "fc layer 1 self.abs_max_out: 1293.0\n",
      "lif layer 1 self.abs_max_v: 1293.0\n",
      "fc layer 2 self.abs_max_out: 558.0\n",
      "lif layer 2 self.abs_max_v: 775.5\n",
      "fc layer 1 self.abs_max_out: 1578.0\n",
      "lif layer 1 self.abs_max_v: 1578.0\n",
      "fc layer 2 self.abs_max_out: 615.0\n",
      "lif layer 2 self.abs_max_v: 804.0\n",
      "fc layer 3 self.abs_max_out: 114.0\n",
      "lif layer 2 self.abs_max_v: 872.0\n",
      "fc layer 2 self.abs_max_out: 691.0\n",
      "lif layer 2 self.abs_max_v: 1127.0\n",
      "lif layer 2 self.abs_max_v: 1183.5\n",
      "fc layer 3 self.abs_max_out: 116.0\n",
      "fc layer 2 self.abs_max_out: 693.0\n",
      "fc layer 3 self.abs_max_out: 133.0\n",
      "fc layer 1 self.abs_max_out: 1965.0\n",
      "lif layer 1 self.abs_max_v: 1965.0\n",
      "fc layer 2 self.abs_max_out: 885.0\n",
      "fc layer 2 self.abs_max_out: 888.0\n",
      "fc layer 2 self.abs_max_out: 1245.0\n",
      "lif layer 2 self.abs_max_v: 1245.0\n",
      "fc layer 3 self.abs_max_out: 148.0\n",
      "fc layer 3 self.abs_max_out: 176.0\n",
      "fc layer 2 self.abs_max_out: 1270.0\n",
      "lif layer 2 self.abs_max_v: 1270.0\n",
      "fc layer 2 self.abs_max_out: 1284.0\n",
      "lif layer 2 self.abs_max_v: 1284.0\n",
      "fc layer 2 self.abs_max_out: 1300.0\n",
      "lif layer 2 self.abs_max_v: 1300.0\n",
      "fc layer 2 self.abs_max_out: 1412.0\n",
      "lif layer 2 self.abs_max_v: 1412.0\n",
      "fc layer 1 self.abs_max_out: 2127.0\n",
      "lif layer 1 self.abs_max_v: 2127.0\n",
      "lif layer 2 self.abs_max_v: 1507.5\n",
      "fc layer 3 self.abs_max_out: 189.0\n",
      "fc layer 1 self.abs_max_out: 2178.0\n",
      "lif layer 1 self.abs_max_v: 2178.0\n",
      "fc layer 3 self.abs_max_out: 202.0\n",
      "lif layer 2 self.abs_max_v: 1511.5\n",
      "lif layer 2 self.abs_max_v: 1610.0\n",
      "fc layer 3 self.abs_max_out: 290.0\n",
      "lif layer 2 self.abs_max_v: 1763.0\n",
      "fc layer 3 self.abs_max_out: 301.0\n",
      "fc layer 3 self.abs_max_out: 323.0\n",
      "lif layer 2 self.abs_max_v: 1767.0\n",
      "fc layer 2 self.abs_max_out: 1415.0\n",
      "fc layer 2 self.abs_max_out: 1428.0\n",
      "fc layer 2 self.abs_max_out: 1704.0\n",
      "fc layer 1 self.abs_max_out: 2306.0\n",
      "lif layer 1 self.abs_max_v: 2306.0\n",
      "fc layer 2 self.abs_max_out: 1717.0\n",
      "fc layer 2 self.abs_max_out: 1719.0\n",
      "fc layer 3 self.abs_max_out: 328.0\n",
      "fc layer 1 self.abs_max_out: 2354.0\n",
      "lif layer 1 self.abs_max_v: 2354.0\n",
      "fc layer 1 self.abs_max_out: 2451.0\n",
      "lif layer 1 self.abs_max_v: 2451.0\n",
      "fc layer 1 self.abs_max_out: 2482.0\n",
      "lif layer 1 self.abs_max_v: 2482.0\n",
      "fc layer 1 self.abs_max_out: 2532.0\n",
      "lif layer 1 self.abs_max_v: 2532.0\n",
      "fc layer 1 self.abs_max_out: 2833.0\n",
      "lif layer 1 self.abs_max_v: 2833.0\n",
      "fc layer 3 self.abs_max_out: 333.0\n"
     ]
    }
   ],
   "source": [
    "### my_snn control board (Gesture) ########################\n",
    "decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "unique_name = 'main'\n",
    "run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "my_snn_system(  devices = \"5\",\n",
    "                single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "                unique_name = run_name,\n",
    "                my_seed = 29127,\n",
    "                TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "                BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "                # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "                which_data = 'DVS_GESTURE_TONIC',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "                lif_layer_sg_width = 6.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "\n",
    "                synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "                synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "                # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "                cfg = [200, 200], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "                # cfg = ['M', 'M', 64], \n",
    "                # cfg = [64, 124, 64, 124],\n",
    "                # cfg = ['M','M',512], \n",
    "                # cfg = [512], \n",
    "                # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                # cfg = ['M','M',512],\n",
    "                # cfg = ['M',200],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = ['M','M',200,200],\n",
    "                # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = ['M',200,200],\n",
    "                # cfg = ['M','M',1024,512,256,128,64],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = [12], #fc\n",
    "                # cfg = [12, 'M', 48, 'M', 12], \n",
    "                # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [],        \n",
    "                \n",
    "                net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "                # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                epoch_num = 300,\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "                BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "                # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "                dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "                # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "                # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "                DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "                trace_on = False,   # True # False\n",
    "                OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "                exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "                extra_train_dataset = 9, \n",
    "\n",
    "                num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "                chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "                pin_memory = True, # True # False \n",
    "\n",
    "                UDA_on = False,  # DECREPATED # uda\n",
    "                alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                bias = False, # True # False \n",
    "\n",
    "                last_lif = False, # True # False \n",
    "\n",
    "                temporal_filter = 5, \n",
    "                initial_pooling = 1,\n",
    "\n",
    "                temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "                quantize_bit_list=[8,8,8],\n",
    "                scale_exp=[[-10,-10],[-10,-10],[-9,-9]], \n",
    "# 1w -11~-9\n",
    "# 1b -11~ -7\n",
    "# 2w -10~-8\n",
    "# 2b -10~-8\n",
    "# 3w -10\n",
    "# 3b -10\n",
    "                ) \n",
    "\n",
    "# num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# num_workers = batch_size / num_GPU\n",
    "# num_workers = batch_size / num_CPU\n",
    "\n",
    "# sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# average pooling  \n",
    "# Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: vz7o3mx3\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: alu834d7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 10854\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251022_172129-alu834d7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/alu834d7' target=\"_blank\">drawn-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/alu834d7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/alu834d7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251022_172136_458', 'my_seed': 10854, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 7, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=7, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=7, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 400.0\n",
      "lif layer 1 self.abs_max_v: 400.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 501.0\n",
      "lif layer 1 self.abs_max_v: 624.0\n",
      "fc layer 2 self.abs_max_out: 176.0\n",
      "lif layer 2 self.abs_max_v: 176.0\n",
      "fc layer 1 self.abs_max_out: 472.0\n",
      "lif layer 1 self.abs_max_v: 644.0\n",
      "fc layer 2 self.abs_max_out: 201.0\n",
      "lif layer 2 self.abs_max_v: 258.5\n",
      "lif layer 1 self.abs_max_v: 681.0\n",
      "fc layer 1 self.abs_max_out: 494.0\n",
      "lif layer 1 self.abs_max_v: 719.0\n",
      "fc layer 2 self.abs_max_out: 237.0\n",
      "lif layer 2 self.abs_max_v: 272.0\n",
      "fc layer 1 self.abs_max_out: 597.0\n",
      "lif layer 1 self.abs_max_v: 795.0\n",
      "fc layer 2 self.abs_max_out: 294.0\n",
      "lif layer 2 self.abs_max_v: 299.5\n",
      "lif layer 2 self.abs_max_v: 383.0\n",
      "fc layer 1 self.abs_max_out: 643.0\n",
      "fc layer 2 self.abs_max_out: 410.0\n",
      "lif layer 2 self.abs_max_v: 489.0\n",
      "fc layer 1 self.abs_max_out: 668.0\n",
      "lif layer 1 self.abs_max_v: 872.5\n",
      "fc layer 2 self.abs_max_out: 481.0\n",
      "lif layer 2 self.abs_max_v: 618.0\n",
      "fc layer 3 self.abs_max_out: 46.0\n",
      "fc layer 1 self.abs_max_out: 826.0\n",
      "lif layer 1 self.abs_max_v: 992.0\n",
      "fc layer 2 self.abs_max_out: 547.0\n",
      "lif layer 2 self.abs_max_v: 634.5\n",
      "fc layer 3 self.abs_max_out: 53.0\n",
      "lif layer 2 self.abs_max_v: 800.5\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "lif layer 1 self.abs_max_v: 1003.5\n",
      "fc layer 3 self.abs_max_out: 74.0\n",
      "fc layer 1 self.abs_max_out: 1047.0\n",
      "lif layer 1 self.abs_max_v: 1047.0\n",
      "lif layer 2 self.abs_max_v: 818.0\n",
      "fc layer 3 self.abs_max_out: 91.0\n",
      "fc layer 1 self.abs_max_out: 1102.0\n",
      "lif layer 1 self.abs_max_v: 1102.0\n",
      "lif layer 2 self.abs_max_v: 820.0\n",
      "fc layer 2 self.abs_max_out: 851.0\n",
      "lif layer 2 self.abs_max_v: 1075.0\n",
      "fc layer 3 self.abs_max_out: 133.0\n",
      "lif layer 1 self.abs_max_v: 1190.5\n",
      "fc layer 1 self.abs_max_out: 1157.0\n",
      "lif layer 1 self.abs_max_v: 1292.0\n",
      "lif layer 1 self.abs_max_v: 1421.5\n",
      "fc layer 3 self.abs_max_out: 140.0\n",
      "fc layer 1 self.abs_max_out: 1222.0\n",
      "fc layer 1 self.abs_max_out: 1515.0\n",
      "lif layer 1 self.abs_max_v: 1515.0\n",
      "fc layer 1 self.abs_max_out: 1695.0\n",
      "lif layer 1 self.abs_max_v: 1695.0\n",
      "fc layer 1 self.abs_max_out: 1732.0\n",
      "lif layer 1 self.abs_max_v: 1732.0\n",
      "fc layer 3 self.abs_max_out: 150.0\n",
      "fc layer 2 self.abs_max_out: 1111.0\n",
      "lif layer 2 self.abs_max_v: 1111.0\n",
      "fc layer 3 self.abs_max_out: 194.0\n",
      "fc layer 3 self.abs_max_out: 200.0\n",
      "lif layer 2 self.abs_max_v: 1161.5\n",
      "lif layer 2 self.abs_max_v: 1227.0\n",
      "fc layer 3 self.abs_max_out: 217.0\n",
      "lif layer 2 self.abs_max_v: 1331.0\n",
      "fc layer 1 self.abs_max_out: 1800.0\n",
      "lif layer 1 self.abs_max_v: 1800.0\n",
      "fc layer 3 self.abs_max_out: 233.0\n",
      "fc layer 1 self.abs_max_out: 2108.0\n",
      "lif layer 1 self.abs_max_v: 2108.0\n",
      "fc layer 3 self.abs_max_out: 259.0\n",
      "fc layer 2 self.abs_max_out: 1248.0\n",
      "lif layer 2 self.abs_max_v: 1470.0\n",
      "fc layer 2 self.abs_max_out: 1303.0\n",
      "fc layer 1 self.abs_max_out: 2163.0\n",
      "lif layer 1 self.abs_max_v: 2163.0\n",
      "fc layer 1 self.abs_max_out: 2182.0\n",
      "lif layer 1 self.abs_max_v: 2182.0\n",
      "lif layer 2 self.abs_max_v: 1526.5\n",
      "lif layer 2 self.abs_max_v: 1556.0\n",
      "fc layer 2 self.abs_max_out: 1305.0\n",
      "fc layer 1 self.abs_max_out: 2259.0\n",
      "lif layer 1 self.abs_max_v: 2259.0\n",
      "fc layer 3 self.abs_max_out: 291.0\n",
      "fc layer 2 self.abs_max_out: 1459.0\n",
      "lif layer 2 self.abs_max_v: 1561.0\n",
      "lif layer 2 self.abs_max_v: 1653.5\n",
      "lif layer 2 self.abs_max_v: 1869.5\n",
      "fc layer 3 self.abs_max_out: 314.0\n",
      "fc layer 1 self.abs_max_out: 2502.0\n",
      "lif layer 1 self.abs_max_v: 2502.0\n",
      "fc layer 2 self.abs_max_out: 1600.0\n",
      "fc layer 1 self.abs_max_out: 2519.0\n",
      "lif layer 1 self.abs_max_v: 2519.0\n",
      "fc layer 3 self.abs_max_out: 315.0\n",
      "fc layer 1 self.abs_max_out: 2649.0\n",
      "lif layer 1 self.abs_max_v: 2649.0\n",
      "fc layer 3 self.abs_max_out: 316.0\n",
      "fc layer 1 self.abs_max_out: 2794.0\n",
      "lif layer 1 self.abs_max_v: 2794.0\n",
      "fc layer 1 self.abs_max_out: 2962.0\n",
      "lif layer 1 self.abs_max_v: 2962.0\n",
      "fc layer 1 self.abs_max_out: 2982.0\n",
      "lif layer 1 self.abs_max_v: 2982.0\n",
      "fc layer 1 self.abs_max_out: 3163.0\n",
      "lif layer 1 self.abs_max_v: 3163.0\n",
      "fc layer 1 self.abs_max_out: 3236.0\n",
      "lif layer 1 self.abs_max_v: 3236.0\n",
      "fc layer 2 self.abs_max_out: 1628.0\n",
      "fc layer 2 self.abs_max_out: 1717.0\n",
      "fc layer 2 self.abs_max_out: 1789.0\n",
      "fc layer 2 self.abs_max_out: 1855.0\n",
      "fc layer 2 self.abs_max_out: 1912.0\n",
      "lif layer 2 self.abs_max_v: 1912.0\n",
      "fc layer 2 self.abs_max_out: 1965.0\n",
      "lif layer 2 self.abs_max_v: 1965.0\n",
      "fc layer 1 self.abs_max_out: 3470.0\n",
      "lif layer 1 self.abs_max_v: 3470.0\n",
      "fc layer 2 self.abs_max_out: 2135.0\n",
      "lif layer 2 self.abs_max_v: 2135.0\n",
      "fc layer 3 self.abs_max_out: 362.0\n",
      "fc layer 1 self.abs_max_out: 3553.0\n",
      "lif layer 1 self.abs_max_v: 3553.0\n",
      "fc layer 1 self.abs_max_out: 3793.0\n",
      "lif layer 1 self.abs_max_v: 3793.0\n",
      "fc layer 2 self.abs_max_out: 2164.0\n",
      "lif layer 2 self.abs_max_v: 2164.0\n",
      "fc layer 2 self.abs_max_out: 2265.0\n",
      "lif layer 2 self.abs_max_v: 2265.0\n",
      "fc layer 2 self.abs_max_out: 2324.0\n",
      "lif layer 2 self.abs_max_v: 2324.0\n",
      "fc layer 2 self.abs_max_out: 2467.0\n",
      "lif layer 2 self.abs_max_v: 2467.0\n",
      "fc layer 2 self.abs_max_out: 2535.0\n",
      "lif layer 2 self.abs_max_v: 2535.0\n",
      "fc layer 1 self.abs_max_out: 4004.0\n",
      "lif layer 1 self.abs_max_v: 4004.0\n",
      "fc layer 1 self.abs_max_out: 4412.0\n",
      "lif layer 1 self.abs_max_v: 4412.0\n",
      "fc layer 2 self.abs_max_out: 2618.0\n",
      "lif layer 2 self.abs_max_v: 2618.0\n",
      "fc layer 2 self.abs_max_out: 2626.0\n",
      "lif layer 2 self.abs_max_v: 2626.0\n",
      "fc layer 2 self.abs_max_out: 2694.0\n",
      "lif layer 2 self.abs_max_v: 2694.0\n",
      "fc layer 2 self.abs_max_out: 2748.0\n",
      "lif layer 2 self.abs_max_v: 2748.0\n",
      "fc layer 2 self.abs_max_out: 2760.0\n",
      "lif layer 2 self.abs_max_v: 2760.0\n",
      "fc layer 2 self.abs_max_out: 2834.0\n",
      "lif layer 2 self.abs_max_v: 2834.0\n",
      "fc layer 2 self.abs_max_out: 2873.0\n",
      "lif layer 2 self.abs_max_v: 2873.0\n",
      "fc layer 3 self.abs_max_out: 377.0\n",
      "fc layer 3 self.abs_max_out: 402.0\n",
      "fc layer 1 self.abs_max_out: 4428.0\n",
      "lif layer 1 self.abs_max_v: 4428.0\n",
      "fc layer 3 self.abs_max_out: 403.0\n",
      "fc layer 1 self.abs_max_out: 4739.0\n",
      "lif layer 1 self.abs_max_v: 4739.0\n",
      "fc layer 1 self.abs_max_out: 4754.0\n",
      "lif layer 1 self.abs_max_v: 4754.0\n",
      "lif layer 2 self.abs_max_v: 2884.0\n",
      "lif layer 2 self.abs_max_v: 3061.0\n",
      "fc layer 1 self.abs_max_out: 4846.0\n",
      "lif layer 1 self.abs_max_v: 4846.0\n",
      "fc layer 1 self.abs_max_out: 5037.0\n",
      "lif layer 1 self.abs_max_v: 5037.0\n",
      "fc layer 1 self.abs_max_out: 5704.0\n",
      "lif layer 1 self.abs_max_v: 5704.0\n",
      "fc layer 1 self.abs_max_out: 5862.0\n",
      "lif layer 1 self.abs_max_v: 5862.0\n",
      "fc layer 1 self.abs_max_out: 5968.0\n",
      "lif layer 1 self.abs_max_v: 5968.0\n",
      "fc layer 1 self.abs_max_out: 5995.0\n",
      "lif layer 1 self.abs_max_v: 5995.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.069212/  2.146406, val:  42.92%, val_best:  42.92%, tr:  94.52%, tr_best:  94.52%, epoch time: 269.60 seconds, 4.49 minutes\n",
      "total_backward_count 44360 real_backward_count 11005  24.808%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 6033.0\n",
      "lif layer 1 self.abs_max_v: 6033.0\n",
      "fc layer 3 self.abs_max_out: 414.0\n",
      "fc layer 2 self.abs_max_out: 2914.0\n",
      "lif layer 1 self.abs_max_v: 6875.5\n",
      "fc layer 2 self.abs_max_out: 2947.0\n",
      "lif layer 1 self.abs_max_v: 6982.5\n",
      "fc layer 1 self.abs_max_out: 6594.0\n",
      "lif layer 1 self.abs_max_v: 7384.0\n",
      "fc layer 1 self.abs_max_out: 6852.0\n",
      "lif layer 2 self.abs_max_v: 3080.0\n",
      "lif layer 2 self.abs_max_v: 3282.5\n",
      "lif layer 2 self.abs_max_v: 3465.5\n",
      "fc layer 2 self.abs_max_out: 2958.0\n",
      "fc layer 1 self.abs_max_out: 6904.0\n",
      "fc layer 2 self.abs_max_out: 2962.0\n",
      "lif layer 2 self.abs_max_v: 3591.5\n",
      "lif layer 2 self.abs_max_v: 3676.5\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.061685/  2.147651, val:  43.75%, val_best:  43.75%, tr:  99.26%, tr_best:  99.26%, epoch time: 265.41 seconds, 4.42 minutes\n",
      "total_backward_count 88720 real_backward_count 18596  20.960%\n",
      "fc layer 1 self.abs_max_out: 7068.0\n",
      "lif layer 1 self.abs_max_v: 7557.0\n",
      "fc layer 1 self.abs_max_out: 7296.0\n",
      "lif layer 1 self.abs_max_v: 7756.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.060298/  2.127561, val:  50.42%, val_best:  50.42%, tr:  99.53%, tr_best:  99.53%, epoch time: 229.97 seconds, 3.83 minutes\n",
      "total_backward_count 133080 real_backward_count 25667  19.287%\n",
      "fc layer 1 self.abs_max_out: 7418.0\n",
      "fc layer 2 self.abs_max_out: 3028.0\n",
      "lif layer 1 self.abs_max_v: 8360.0\n",
      "fc layer 1 self.abs_max_out: 7779.0\n",
      "lif layer 2 self.abs_max_v: 3774.0\n",
      "lif layer 1 self.abs_max_v: 8499.5\n",
      "lif layer 1 self.abs_max_v: 8522.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.044142/  2.105329, val:  57.92%, val_best:  57.92%, tr:  99.41%, tr_best:  99.53%, epoch time: 241.56 seconds, 4.03 minutes\n",
      "total_backward_count 177440 real_backward_count 32362  18.238%\n",
      "lif layer 1 self.abs_max_v: 8839.0\n",
      "fc layer 1 self.abs_max_out: 7960.0\n",
      "lif layer 2 self.abs_max_v: 3987.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.033656/  2.097239, val:  58.75%, val_best:  58.75%, tr:  99.84%, tr_best:  99.84%, epoch time: 260.82 seconds, 4.35 minutes\n",
      "total_backward_count 221800 real_backward_count 38766  17.478%\n",
      "fc layer 2 self.abs_max_out: 3135.0\n",
      "lif layer 1 self.abs_max_v: 9156.0\n",
      "fc layer 2 self.abs_max_out: 3141.0\n",
      "lif layer 1 self.abs_max_v: 9953.0\n",
      "fc layer 1 self.abs_max_out: 8226.0\n",
      "fc layer 2 self.abs_max_out: 3154.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.016307/  2.102618, val:  57.50%, val_best:  58.75%, tr:  99.66%, tr_best:  99.84%, epoch time: 262.27 seconds, 4.37 minutes\n",
      "total_backward_count 266160 real_backward_count 44913  16.874%\n",
      "lif layer 2 self.abs_max_v: 4018.5\n",
      "lif layer 2 self.abs_max_v: 4021.0\n",
      "lif layer 2 self.abs_max_v: 4023.5\n",
      "fc layer 1 self.abs_max_out: 8292.0\n",
      "lif layer 2 self.abs_max_v: 4068.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.026264/  2.097787, val:  60.00%, val_best:  60.00%, tr:  99.80%, tr_best:  99.84%, epoch time: 261.39 seconds, 4.36 minutes\n",
      "total_backward_count 310520 real_backward_count 50676  16.320%\n",
      "fc layer 2 self.abs_max_out: 3157.0\n",
      "fc layer 2 self.abs_max_out: 3206.0\n",
      "fc layer 2 self.abs_max_out: 3260.0\n",
      "fc layer 2 self.abs_max_out: 3432.0\n",
      "lif layer 2 self.abs_max_v: 4140.5\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  2.010661/  2.084737, val:  67.92%, val_best:  67.92%, tr:  99.89%, tr_best:  99.89%, epoch time: 262.63 seconds, 4.38 minutes\n",
      "total_backward_count 354880 real_backward_count 56351  15.879%\n",
      "lif layer 1 self.abs_max_v: 10665.5\n",
      "lif layer 1 self.abs_max_v: 10679.0\n",
      "lif layer 2 self.abs_max_v: 4182.0\n",
      "fc layer 1 self.abs_max_out: 8480.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.994337/  2.065619, val:  72.50%, val_best:  72.50%, tr:  99.93%, tr_best:  99.93%, epoch time: 261.80 seconds, 4.36 minutes\n",
      "total_backward_count 399240 real_backward_count 61720  15.459%\n",
      "fc layer 1 self.abs_max_out: 8617.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.984799/  2.053894, val:  84.58%, val_best:  84.58%, tr:  99.82%, tr_best:  99.93%, epoch time: 260.60 seconds, 4.34 minutes\n",
      "total_backward_count 443600 real_backward_count 66890  15.079%\n",
      "fc layer 1 self.abs_max_out: 8637.0\n",
      "lif layer 1 self.abs_max_v: 10750.5\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.983981/  2.056742, val:  81.67%, val_best:  84.58%, tr:  99.89%, tr_best:  99.93%, epoch time: 262.22 seconds, 4.37 minutes\n",
      "total_backward_count 487960 real_backward_count 71991  14.753%\n",
      "lif layer 2 self.abs_max_v: 4256.0\n",
      "lif layer 1 self.abs_max_v: 11022.5\n",
      "lif layer 2 self.abs_max_v: 4263.0\n",
      "lif layer 2 self.abs_max_v: 4350.0\n",
      "fc layer 1 self.abs_max_out: 8725.0\n",
      "fc layer 1 self.abs_max_out: 8730.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.981658/  2.066182, val:  80.83%, val_best:  84.58%, tr:  99.93%, tr_best:  99.93%, epoch time: 262.46 seconds, 4.37 minutes\n",
      "total_backward_count 532320 real_backward_count 76962  14.458%\n",
      "lif layer 2 self.abs_max_v: 4389.0\n",
      "fc layer 2 self.abs_max_out: 3444.0\n",
      "lif layer 2 self.abs_max_v: 4532.0\n",
      "lif layer 2 self.abs_max_v: 4611.0\n",
      "fc layer 1 self.abs_max_out: 8830.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.984917/  2.061547, val:  79.58%, val_best:  84.58%, tr:  99.93%, tr_best:  99.93%, epoch time: 260.10 seconds, 4.34 minutes\n",
      "total_backward_count 576680 real_backward_count 81801  14.185%\n",
      "fc layer 2 self.abs_max_out: 3517.0\n",
      "lif layer 2 self.abs_max_v: 4612.0\n",
      "lif layer 2 self.abs_max_v: 4637.0\n",
      "fc layer 1 self.abs_max_out: 8935.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.982815/  2.049331, val:  62.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.51 seconds, 4.34 minutes\n",
      "total_backward_count 621040 real_backward_count 86437  13.918%\n",
      "fc layer 2 self.abs_max_out: 3596.0\n",
      "fc layer 2 self.abs_max_out: 3667.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.979264/  2.054483, val:  72.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.84 seconds, 4.35 minutes\n",
      "total_backward_count 665400 real_backward_count 91000  13.676%\n",
      "fc layer 1 self.abs_max_out: 9044.0\n",
      "lif layer 1 self.abs_max_v: 11306.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.978807/  2.057090, val:  80.00%, val_best:  84.58%, tr:  99.98%, tr_best: 100.00%, epoch time: 276.77 seconds, 4.61 minutes\n",
      "total_backward_count 709760 real_backward_count 95357  13.435%\n",
      "fc layer 3 self.abs_max_out: 417.0\n",
      "fc layer 1 self.abs_max_out: 9100.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.973647/  2.032252, val:  75.00%, val_best:  84.58%, tr:  99.98%, tr_best: 100.00%, epoch time: 279.12 seconds, 4.65 minutes\n",
      "total_backward_count 754120 real_backward_count 99642  13.213%\n",
      "fc layer 1 self.abs_max_out: 9174.0\n",
      "lif layer 2 self.abs_max_v: 4701.0\n",
      "lif layer 1 self.abs_max_v: 11368.5\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.973416/  2.045620, val:  79.17%, val_best:  84.58%, tr:  99.95%, tr_best: 100.00%, epoch time: 280.31 seconds, 4.67 minutes\n",
      "total_backward_count 798480 real_backward_count 103807  13.001%\n",
      "lif layer 2 self.abs_max_v: 4771.5\n",
      "lif layer 2 self.abs_max_v: 4878.0\n",
      "lif layer 1 self.abs_max_v: 11898.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.975579/  2.051692, val:  82.50%, val_best:  84.58%, tr:  99.91%, tr_best: 100.00%, epoch time: 279.93 seconds, 4.67 minutes\n",
      "total_backward_count 842840 real_backward_count 107872  12.799%\n",
      "fc layer 2 self.abs_max_out: 3703.0\n",
      "lif layer 1 self.abs_max_v: 12073.0\n",
      "lif layer 2 self.abs_max_v: 4919.5\n",
      "fc layer 1 self.abs_max_out: 9195.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.968249/  2.038762, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.62 seconds, 4.54 minutes\n",
      "total_backward_count 887200 real_backward_count 111908  12.614%\n",
      "fc layer 2 self.abs_max_out: 3904.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.970619/  2.039092, val:  79.58%, val_best:  87.08%, tr:  99.95%, tr_best: 100.00%, epoch time: 254.82 seconds, 4.25 minutes\n",
      "total_backward_count 931560 real_backward_count 115777  12.428%\n",
      "lif layer 2 self.abs_max_v: 4983.5\n",
      "fc layer 1 self.abs_max_out: 9226.0\n",
      "lif layer 2 self.abs_max_v: 4987.0\n",
      "lif layer 2 self.abs_max_v: 5013.5\n",
      "lif layer 2 self.abs_max_v: 5206.5\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.962299/  2.035376, val:  82.92%, val_best:  87.08%, tr:  99.98%, tr_best: 100.00%, epoch time: 235.43 seconds, 3.92 minutes\n",
      "total_backward_count 975920 real_backward_count 119573  12.252%\n",
      "fc layer 1 self.abs_max_out: 9257.0\n",
      "fc layer 1 self.abs_max_out: 9271.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.963274/  2.037480, val:  82.50%, val_best:  87.08%, tr:  99.95%, tr_best: 100.00%, epoch time: 268.89 seconds, 4.48 minutes\n",
      "total_backward_count 1020280 real_backward_count 123398  12.095%\n",
      "lif layer 1 self.abs_max_v: 12249.0\n",
      "lif layer 1 self.abs_max_v: 13098.0\n",
      "fc layer 1 self.abs_max_out: 9320.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.958092/  2.026970, val:  81.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.63 seconds, 4.58 minutes\n",
      "total_backward_count 1064640 real_backward_count 126969  11.926%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.955341/  2.044875, val:  76.25%, val_best:  87.08%, tr:  99.98%, tr_best: 100.00%, epoch time: 273.59 seconds, 4.56 minutes\n",
      "total_backward_count 1109000 real_backward_count 130493  11.767%\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.953521/  2.027821, val:  84.58%, val_best:  87.08%, tr:  99.98%, tr_best: 100.00%, epoch time: 275.31 seconds, 4.59 minutes\n",
      "total_backward_count 1153360 real_backward_count 133939  11.613%\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.951864/  2.019907, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.86 seconds, 4.58 minutes\n",
      "total_backward_count 1197720 real_backward_count 137349  11.468%\n",
      "fc layer 1 self.abs_max_out: 9349.0\n",
      "fc layer 1 self.abs_max_out: 9385.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.947345/  2.023583, val:  83.33%, val_best:  87.08%, tr:  99.98%, tr_best: 100.00%, epoch time: 274.61 seconds, 4.58 minutes\n",
      "total_backward_count 1242080 real_backward_count 140613  11.321%\n",
      "fc layer 2 self.abs_max_out: 3910.0\n",
      "fc layer 1 self.abs_max_out: 9407.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.950752/  2.007944, val:  85.42%, val_best:  87.08%, tr:  99.95%, tr_best: 100.00%, epoch time: 275.22 seconds, 4.59 minutes\n",
      "total_backward_count 1286440 real_backward_count 143880  11.184%\n",
      "fc layer 1 self.abs_max_out: 9425.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.944533/  2.018765, val:  86.25%, val_best:  87.08%, tr:  99.95%, tr_best: 100.00%, epoch time: 272.47 seconds, 4.54 minutes\n",
      "total_backward_count 1330800 real_backward_count 147065  11.051%\n",
      "fc layer 1 self.abs_max_out: 9493.0\n",
      "fc layer 3 self.abs_max_out: 421.0\n",
      "fc layer 2 self.abs_max_out: 3975.0\n",
      "fc layer 3 self.abs_max_out: 422.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.944935/  2.019173, val:  82.08%, val_best:  87.08%, tr:  99.98%, tr_best: 100.00%, epoch time: 274.02 seconds, 4.57 minutes\n",
      "total_backward_count 1375160 real_backward_count 150150  10.919%\n",
      "fc layer 1 self.abs_max_out: 9550.0\n",
      "lif layer 1 self.abs_max_v: 13181.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.944416/  2.017661, val:  82.50%, val_best:  87.08%, tr:  99.98%, tr_best: 100.00%, epoch time: 275.66 seconds, 4.59 minutes\n",
      "total_backward_count 1419520 real_backward_count 153151  10.789%\n",
      "fc layer 2 self.abs_max_out: 3978.0\n",
      "fc layer 2 self.abs_max_out: 4040.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.943690/  2.013055, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.86 seconds, 4.56 minutes\n",
      "total_backward_count 1463880 real_backward_count 156156  10.667%\n",
      "fc layer 1 self.abs_max_out: 9609.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.950526/  2.029650, val:  87.92%, val_best:  87.92%, tr:  99.95%, tr_best: 100.00%, epoch time: 275.24 seconds, 4.59 minutes\n",
      "total_backward_count 1508240 real_backward_count 159032  10.544%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.942437/  2.014909, val:  80.42%, val_best:  87.92%, tr:  99.98%, tr_best: 100.00%, epoch time: 274.07 seconds, 4.57 minutes\n",
      "total_backward_count 1552600 real_backward_count 161864  10.425%\n",
      "fc layer 1 self.abs_max_out: 9612.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.929743/  2.002936, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.45 seconds, 4.57 minutes\n",
      "total_backward_count 1596960 real_backward_count 164735  10.316%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.925783/  1.998090, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.10 seconds, 4.55 minutes\n",
      "total_backward_count 1641320 real_backward_count 167584  10.210%\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.929269/  2.007554, val:  83.33%, val_best:  87.92%, tr:  99.98%, tr_best: 100.00%, epoch time: 273.68 seconds, 4.56 minutes\n",
      "total_backward_count 1685680 real_backward_count 170345  10.105%\n",
      "fc layer 1 self.abs_max_out: 9659.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.925448/  1.996951, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 274.44 seconds, 4.57 minutes\n",
      "total_backward_count 1730040 real_backward_count 172995   9.999%\n",
      "fc layer 1 self.abs_max_out: 9714.0\n",
      "lif layer 1 self.abs_max_v: 13536.5\n",
      "lif layer 2 self.abs_max_v: 5219.5\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.923467/  2.010465, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 273.86 seconds, 4.56 minutes\n",
      "total_backward_count 1774400 real_backward_count 175668   9.900%\n",
      "fc layer 1 self.abs_max_out: 9757.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.926525/  1.996305, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.48 seconds, 4.52 minutes\n",
      "total_backward_count 1818760 real_backward_count 178271   9.802%\n",
      "fc layer 2 self.abs_max_out: 4087.0\n",
      "lif layer 1 self.abs_max_v: 13576.0\n",
      "fc layer 1 self.abs_max_out: 9793.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.921282/  1.989082, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.80 seconds, 4.53 minutes\n",
      "total_backward_count 1863120 real_backward_count 180798   9.704%\n",
      "fc layer 1 self.abs_max_out: 9803.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.913898/  1.998790, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.52 seconds, 4.54 minutes\n",
      "total_backward_count 1907480 real_backward_count 183264   9.608%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.916781/  1.989833, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.94 seconds, 4.53 minutes\n",
      "total_backward_count 1951840 real_backward_count 185717   9.515%\n",
      "fc layer 1 self.abs_max_out: 9812.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.918451/  2.002745, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 272.02 seconds, 4.53 minutes\n",
      "total_backward_count 1996200 real_backward_count 188057   9.421%\n",
      "fc layer 3 self.abs_max_out: 460.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.920671/  1.995629, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.01 seconds, 4.52 minutes\n",
      "total_backward_count 2040560 real_backward_count 190507   9.336%\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.913099/  1.991633, val:  89.58%, val_best:  89.58%, tr:  99.98%, tr_best: 100.00%, epoch time: 272.19 seconds, 4.54 minutes\n",
      "total_backward_count 2084920 real_backward_count 192829   9.249%\n",
      "fc layer 1 self.abs_max_out: 9836.0\n",
      "fc layer 2 self.abs_max_out: 4102.0\n",
      "lif layer 2 self.abs_max_v: 5251.5\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.902035/  1.984252, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.57 seconds, 4.53 minutes\n",
      "total_backward_count 2129280 real_backward_count 195077   9.162%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.901751/  1.979710, val:  86.67%, val_best:  89.58%, tr:  99.98%, tr_best: 100.00%, epoch time: 271.19 seconds, 4.52 minutes\n",
      "total_backward_count 2173640 real_backward_count 197299   9.077%\n",
      "fc layer 1 self.abs_max_out: 9905.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.899454/  1.979719, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.63 seconds, 4.51 minutes\n",
      "total_backward_count 2218000 real_backward_count 199478   8.994%\n",
      "fc layer 1 self.abs_max_out: 9949.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.897642/  1.976229, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.03 seconds, 4.52 minutes\n",
      "total_backward_count 2262360 real_backward_count 201675   8.914%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.898428/  1.988763, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.87 seconds, 4.53 minutes\n",
      "total_backward_count 2306720 real_backward_count 203788   8.835%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.899024/  1.976485, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.99 seconds, 4.50 minutes\n",
      "total_backward_count 2351080 real_backward_count 205905   8.758%\n",
      "fc layer 3 self.abs_max_out: 462.0\n",
      "lif layer 1 self.abs_max_v: 13870.0\n",
      "lif layer 2 self.abs_max_v: 5308.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.889576/  1.960594, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.75 seconds, 4.50 minutes\n",
      "total_backward_count 2395440 real_backward_count 207984   8.682%\n",
      "fc layer 1 self.abs_max_out: 9955.0\n",
      "fc layer 2 self.abs_max_out: 4105.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.883098/  1.964694, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.64 seconds, 4.51 minutes\n",
      "total_backward_count 2439800 real_backward_count 209994   8.607%\n",
      "fc layer 2 self.abs_max_out: 4211.0\n",
      "fc layer 1 self.abs_max_out: 9973.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.871038/  1.964397, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.20 seconds, 4.52 minutes\n",
      "total_backward_count 2484160 real_backward_count 211961   8.533%\n",
      "fc layer 3 self.abs_max_out: 468.0\n",
      "lif layer 2 self.abs_max_v: 5338.5\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.875985/  1.966238, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.15 seconds, 4.50 minutes\n",
      "total_backward_count 2528520 real_backward_count 213959   8.462%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.868719/  1.966153, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.22 seconds, 4.50 minutes\n",
      "total_backward_count 2572880 real_backward_count 215899   8.391%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.871135/  1.947424, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.56 seconds, 4.51 minutes\n",
      "total_backward_count 2617240 real_backward_count 217846   8.324%\n",
      "fc layer 3 self.abs_max_out: 481.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.862796/  1.956052, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.05 seconds, 4.52 minutes\n",
      "total_backward_count 2661600 real_backward_count 219715   8.255%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.860375/  1.951732, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.36 seconds, 4.52 minutes\n",
      "total_backward_count 2705960 real_backward_count 221498   8.186%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.865787/  1.947690, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.34 seconds, 4.51 minutes\n",
      "total_backward_count 2750320 real_backward_count 223377   8.122%\n",
      "fc layer 3 self.abs_max_out: 487.0\n",
      "fc layer 2 self.abs_max_out: 4236.0\n",
      "fc layer 1 self.abs_max_out: 10064.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.853102/  1.942604, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.96 seconds, 4.52 minutes\n",
      "total_backward_count 2794680 real_backward_count 225163   8.057%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.857461/  1.950096, val:  82.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.01 seconds, 4.52 minutes\n",
      "total_backward_count 2839040 real_backward_count 226973   7.995%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.858947/  1.954660, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.40 seconds, 4.52 minutes\n",
      "total_backward_count 2883400 real_backward_count 228665   7.930%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.856649/  1.948275, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.39 seconds, 4.51 minutes\n",
      "total_backward_count 2927760 real_backward_count 230418   7.870%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.855167/  1.950780, val:  82.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.57 seconds, 4.53 minutes\n",
      "total_backward_count 2972120 real_backward_count 232175   7.812%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.851657/  1.948103, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.16 seconds, 4.52 minutes\n",
      "total_backward_count 3016480 real_backward_count 233930   7.755%\n",
      "fc layer 1 self.abs_max_out: 10087.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.845197/  1.941417, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.07 seconds, 4.50 minutes\n",
      "total_backward_count 3060840 real_backward_count 235560   7.696%\n",
      "fc layer 1 self.abs_max_out: 10093.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.847082/  1.940714, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.44 seconds, 4.51 minutes\n",
      "total_backward_count 3105200 real_backward_count 237311   7.642%\n",
      "fc layer 1 self.abs_max_out: 10136.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.846736/  1.937760, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.97 seconds, 4.48 minutes\n",
      "total_backward_count 3149560 real_backward_count 238985   7.588%\n",
      "fc layer 1 self.abs_max_out: 10162.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.844657/  1.941226, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 271.13 seconds, 4.52 minutes\n",
      "total_backward_count 3193920 real_backward_count 240576   7.532%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.839834/  1.940979, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.22 seconds, 4.32 minutes\n",
      "total_backward_count 3238280 real_backward_count 242148   7.478%\n",
      "fc layer 3 self.abs_max_out: 490.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.842177/  1.927280, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 235.41 seconds, 3.92 minutes\n",
      "total_backward_count 3282640 real_backward_count 243694   7.424%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.825340/  1.929207, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.74 seconds, 4.36 minutes\n",
      "total_backward_count 3327000 real_backward_count 245236   7.371%\n",
      "fc layer 1 self.abs_max_out: 10176.0\n",
      "fc layer 3 self.abs_max_out: 493.0\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.817903/  1.914914, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.89 seconds, 4.75 minutes\n",
      "total_backward_count 3371360 real_backward_count 246709   7.318%\n",
      "fc layer 1 self.abs_max_out: 10231.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.820215/  1.915367, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.59 seconds, 4.78 minutes\n",
      "total_backward_count 3415720 real_backward_count 248194   7.266%\n",
      "fc layer 3 self.abs_max_out: 509.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.815330/  1.899290, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.57 seconds, 4.78 minutes\n",
      "total_backward_count 3460080 real_backward_count 249608   7.214%\n",
      "fc layer 3 self.abs_max_out: 511.0\n",
      "fc layer 1 self.abs_max_out: 10308.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.812407/  1.914105, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.44 seconds, 4.76 minutes\n",
      "total_backward_count 3504440 real_backward_count 251060   7.164%\n",
      "fc layer 3 self.abs_max_out: 525.0\n",
      "fc layer 1 self.abs_max_out: 10341.0\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.803814/  1.905228, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.47 seconds, 4.77 minutes\n",
      "total_backward_count 3548800 real_backward_count 252470   7.114%\n",
      "fc layer 1 self.abs_max_out: 10351.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.800878/  1.907753, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.43 seconds, 4.76 minutes\n",
      "total_backward_count 3593160 real_backward_count 253853   7.065%\n",
      "fc layer 1 self.abs_max_out: 10359.0\n",
      "lif layer 2 self.abs_max_v: 5409.5\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.802758/  1.905688, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.88 seconds, 4.76 minutes\n",
      "total_backward_count 3637520 real_backward_count 255209   7.016%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.798366/  1.899711, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.05 seconds, 4.77 minutes\n",
      "total_backward_count 3681880 real_backward_count 256592   6.969%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.793688/  1.895545, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.23 seconds, 4.77 minutes\n",
      "total_backward_count 3726240 real_backward_count 257944   6.922%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.798438/  1.910328, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.52 seconds, 4.74 minutes\n",
      "total_backward_count 3770600 real_backward_count 259340   6.878%\n",
      "fc layer 1 self.abs_max_out: 10371.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.803116/  1.897124, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.75 seconds, 4.75 minutes\n",
      "total_backward_count 3814960 real_backward_count 260771   6.835%\n",
      "lif layer 2 self.abs_max_v: 5581.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.796189/  1.894983, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.90 seconds, 4.75 minutes\n",
      "total_backward_count 3859320 real_backward_count 262172   6.793%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.797892/  1.900824, val:  83.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.04 seconds, 4.77 minutes\n",
      "total_backward_count 3903680 real_backward_count 263506   6.750%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.797804/  1.899537, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.21 seconds, 4.77 minutes\n",
      "total_backward_count 3948040 real_backward_count 264845   6.708%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.791574/  1.894235, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.67 seconds, 4.76 minutes\n",
      "total_backward_count 3992400 real_backward_count 266175   6.667%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.785594/  1.898512, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.79 seconds, 4.78 minutes\n",
      "total_backward_count 4036760 real_backward_count 267503   6.627%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.786108/  1.880634, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.08 seconds, 4.77 minutes\n",
      "total_backward_count 4081120 real_backward_count 268805   6.587%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.777351/  1.887529, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.29 seconds, 4.75 minutes\n",
      "total_backward_count 4125480 real_backward_count 270123   6.548%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.776553/  1.879014, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.42 seconds, 4.76 minutes\n",
      "total_backward_count 4169840 real_backward_count 271342   6.507%\n",
      "fc layer 3 self.abs_max_out: 540.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.775568/  1.873571, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.47 seconds, 4.74 minutes\n",
      "total_backward_count 4214200 real_backward_count 272591   6.468%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.762818/  1.877012, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.36 seconds, 4.77 minutes\n",
      "total_backward_count 4258560 real_backward_count 273851   6.431%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.770228/  1.887349, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.03 seconds, 4.77 minutes\n",
      "total_backward_count 4302920 real_backward_count 275009   6.391%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.763335/  1.871835, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.06 seconds, 4.75 minutes\n",
      "total_backward_count 4347280 real_backward_count 276190   6.353%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.767720/  1.879091, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.26 seconds, 4.75 minutes\n",
      "total_backward_count 4391640 real_backward_count 277394   6.316%\n",
      "fc layer 3 self.abs_max_out: 564.0\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.769091/  1.886129, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.00 seconds, 4.75 minutes\n",
      "total_backward_count 4436000 real_backward_count 278524   6.279%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.775836/  1.885483, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.26 seconds, 4.75 minutes\n",
      "total_backward_count 4480360 real_backward_count 279649   6.242%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.764257/  1.876469, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.62 seconds, 4.74 minutes\n",
      "total_backward_count 4524720 real_backward_count 280734   6.204%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.760962/  1.880179, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.66 seconds, 4.76 minutes\n",
      "total_backward_count 4569080 real_backward_count 281942   6.171%\n",
      "lif layer 2 self.abs_max_v: 5630.5\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.758923/  1.863063, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.34 seconds, 4.74 minutes\n",
      "total_backward_count 4613440 real_backward_count 283083   6.136%\n",
      "lif layer 2 self.abs_max_v: 5670.0\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.755565/  1.869179, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.80 seconds, 4.76 minutes\n",
      "total_backward_count 4657800 real_backward_count 284204   6.102%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.754439/  1.860551, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.43 seconds, 4.77 minutes\n",
      "total_backward_count 4702160 real_backward_count 285312   6.068%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.749513/  1.862426, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.72 seconds, 4.78 minutes\n",
      "total_backward_count 4746520 real_backward_count 286396   6.034%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.761611/  1.865588, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.70 seconds, 4.74 minutes\n",
      "total_backward_count 4790880 real_backward_count 287530   6.002%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.754661/  1.868208, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.84 seconds, 4.75 minutes\n",
      "total_backward_count 4835240 real_backward_count 288559   5.968%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.753133/  1.868241, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.35 seconds, 4.74 minutes\n",
      "total_backward_count 4879600 real_backward_count 289597   5.935%\n",
      "lif layer 1 self.abs_max_v: 13998.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.748146/  1.852398, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.15 seconds, 4.75 minutes\n",
      "total_backward_count 4923960 real_backward_count 290619   5.902%\n",
      "fc layer 3 self.abs_max_out: 568.0\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.739675/  1.861647, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 283.89 seconds, 4.73 minutes\n",
      "total_backward_count 4968320 real_backward_count 291728   5.872%\n",
      "fc layer 3 self.abs_max_out: 580.0\n",
      "fc layer 3 self.abs_max_out: 596.0\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.745253/  1.862558, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.40 seconds, 4.76 minutes\n",
      "total_backward_count 5012680 real_backward_count 292748   5.840%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.752057/  1.865337, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.14 seconds, 4.77 minutes\n",
      "total_backward_count 5057040 real_backward_count 293775   5.809%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.744162/  1.856815, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 283.85 seconds, 4.73 minutes\n",
      "total_backward_count 5101400 real_backward_count 294797   5.779%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.745354/  1.864329, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.29 seconds, 4.75 minutes\n",
      "total_backward_count 5145760 real_backward_count 295795   5.748%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.743512/  1.854722, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.50 seconds, 4.74 minutes\n",
      "total_backward_count 5190120 real_backward_count 296702   5.717%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.740977/  1.857786, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.76 seconds, 4.75 minutes\n",
      "total_backward_count 5234480 real_backward_count 297697   5.687%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.746167/  1.865387, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.00 seconds, 4.75 minutes\n",
      "total_backward_count 5278840 real_backward_count 298646   5.657%\n",
      "fc layer 1 self.abs_max_out: 10422.0\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.736733/  1.850921, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.35 seconds, 4.76 minutes\n",
      "total_backward_count 5323200 real_backward_count 299648   5.629%\n",
      "fc layer 1 self.abs_max_out: 10456.0\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.735172/  1.851219, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.94 seconds, 4.77 minutes\n",
      "total_backward_count 5367560 real_backward_count 300637   5.601%\n",
      "fc layer 1 self.abs_max_out: 10472.0\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.736260/  1.859595, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.18 seconds, 4.77 minutes\n",
      "total_backward_count 5411920 real_backward_count 301666   5.574%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.730189/  1.843736, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.33 seconds, 4.76 minutes\n",
      "total_backward_count 5456280 real_backward_count 302622   5.546%\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.729057/  1.853026, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 282.33 seconds, 4.71 minutes\n",
      "total_backward_count 5500640 real_backward_count 303559   5.519%\n",
      "lif layer 1 self.abs_max_v: 14288.0\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.734596/  1.846326, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.28 seconds, 4.75 minutes\n",
      "total_backward_count 5545000 real_backward_count 304500   5.491%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.726153/  1.842755, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.15 seconds, 4.75 minutes\n",
      "total_backward_count 5589360 real_backward_count 305440   5.465%\n",
      "fc layer 1 self.abs_max_out: 10493.0\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.726548/  1.845716, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.39 seconds, 4.76 minutes\n",
      "total_backward_count 5633720 real_backward_count 306341   5.438%\n",
      "fc layer 1 self.abs_max_out: 10496.0\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.723829/  1.855622, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.10 seconds, 4.77 minutes\n",
      "total_backward_count 5678080 real_backward_count 307283   5.412%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.720537/  1.846541, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.27 seconds, 4.77 minutes\n",
      "total_backward_count 5722440 real_backward_count 308157   5.385%\n",
      "fc layer 3 self.abs_max_out: 602.0\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.721944/  1.835979, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.78 seconds, 4.76 minutes\n",
      "total_backward_count 5766800 real_backward_count 309003   5.358%\n",
      "fc layer 3 self.abs_max_out: 616.0\n",
      "fc layer 3 self.abs_max_out: 619.0\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.720796/  1.843539, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.72 seconds, 4.78 minutes\n",
      "total_backward_count 5811160 real_backward_count 309881   5.333%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.720306/  1.832986, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.69 seconds, 4.76 minutes\n",
      "total_backward_count 5855520 real_backward_count 310789   5.308%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.716364/  1.844569, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.09 seconds, 4.77 minutes\n",
      "total_backward_count 5899880 real_backward_count 311634   5.282%\n",
      "fc layer 1 self.abs_max_out: 10501.0\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.726736/  1.849278, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.35 seconds, 4.77 minutes\n",
      "total_backward_count 5944240 real_backward_count 312465   5.257%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.725878/  1.844512, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.22 seconds, 4.75 minutes\n",
      "total_backward_count 5988600 real_backward_count 313298   5.232%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.731807/  1.850580, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.47 seconds, 4.76 minutes\n",
      "total_backward_count 6032960 real_backward_count 314169   5.208%\n",
      "fc layer 1 self.abs_max_out: 10506.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.728317/  1.847808, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.04 seconds, 4.73 minutes\n",
      "total_backward_count 6077320 real_backward_count 315003   5.183%\n",
      "fc layer 1 self.abs_max_out: 10522.0\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.722496/  1.835980, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.33 seconds, 4.76 minutes\n",
      "total_backward_count 6121680 real_backward_count 315817   5.159%\n",
      "fc layer 1 self.abs_max_out: 10540.0\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.713377/  1.830602, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.34 seconds, 4.77 minutes\n",
      "total_backward_count 6166040 real_backward_count 316705   5.136%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.722489/  1.839964, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.83 seconds, 4.75 minutes\n",
      "total_backward_count 6210400 real_backward_count 317529   5.113%\n",
      "fc layer 3 self.abs_max_out: 623.0\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.722517/  1.841977, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.10 seconds, 4.77 minutes\n",
      "total_backward_count 6254760 real_backward_count 318350   5.090%\n",
      "fc layer 1 self.abs_max_out: 10553.0\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.718284/  1.849451, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.52 seconds, 4.76 minutes\n",
      "total_backward_count 6299120 real_backward_count 319204   5.067%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.720261/  1.843112, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.75 seconds, 4.78 minutes\n",
      "total_backward_count 6343480 real_backward_count 319989   5.044%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.722041/  1.845659, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.49 seconds, 4.77 minutes\n",
      "total_backward_count 6387840 real_backward_count 320774   5.022%\n",
      "fc layer 1 self.abs_max_out: 10562.0\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.719667/  1.850617, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.34 seconds, 4.76 minutes\n",
      "total_backward_count 6432200 real_backward_count 321529   4.999%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.714046/  1.830940, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.69 seconds, 4.74 minutes\n",
      "total_backward_count 6476560 real_backward_count 322318   4.977%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.711817/  1.836273, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.39 seconds, 4.76 minutes\n",
      "total_backward_count 6520920 real_backward_count 323174   4.956%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.707732/  1.831382, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.61 seconds, 4.76 minutes\n",
      "total_backward_count 6565280 real_backward_count 324002   4.935%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.712909/  1.839276, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.04 seconds, 4.75 minutes\n",
      "total_backward_count 6609640 real_backward_count 324789   4.914%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.722022/  1.845200, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.43 seconds, 4.76 minutes\n",
      "total_backward_count 6654000 real_backward_count 325585   4.893%\n",
      "fc layer 1 self.abs_max_out: 10623.0\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.719123/  1.833539, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.37 seconds, 4.76 minutes\n",
      "total_backward_count 6698360 real_backward_count 326395   4.873%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.714106/  1.841275, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.12 seconds, 4.75 minutes\n",
      "total_backward_count 6742720 real_backward_count 327157   4.852%\n",
      "fc layer 1 self.abs_max_out: 10634.0\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.715789/  1.832066, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.13 seconds, 4.75 minutes\n",
      "total_backward_count 6787080 real_backward_count 327907   4.831%\n",
      "fc layer 1 self.abs_max_out: 10659.0\n",
      "fc layer 3 self.abs_max_out: 624.0\n",
      "fc layer 3 self.abs_max_out: 629.0\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.710706/  1.832767, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.30 seconds, 4.74 minutes\n",
      "total_backward_count 6831440 real_backward_count 328649   4.811%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.708319/  1.829402, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.33 seconds, 4.76 minutes\n",
      "total_backward_count 6875800 real_backward_count 329418   4.791%\n",
      "fc layer 1 self.abs_max_out: 10668.0\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.704407/  1.828551, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.45 seconds, 4.74 minutes\n",
      "total_backward_count 6920160 real_backward_count 330172   4.771%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.704536/  1.836913, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.09 seconds, 4.73 minutes\n",
      "total_backward_count 6964520 real_backward_count 330899   4.751%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.705851/  1.834129, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.09 seconds, 4.75 minutes\n",
      "total_backward_count 7008880 real_backward_count 331640   4.732%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.703188/  1.823744, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.70 seconds, 4.75 minutes\n",
      "total_backward_count 7053240 real_backward_count 332359   4.712%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.695928/  1.813545, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.60 seconds, 4.74 minutes\n",
      "total_backward_count 7097600 real_backward_count 333078   4.693%\n",
      "fc layer 3 self.abs_max_out: 635.0\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.690668/  1.818261, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.42 seconds, 4.74 minutes\n",
      "total_backward_count 7141960 real_backward_count 333753   4.673%\n",
      "fc layer 3 self.abs_max_out: 640.0\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.692346/  1.819365, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.09 seconds, 4.75 minutes\n",
      "total_backward_count 7186320 real_backward_count 334486   4.654%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.694369/  1.825642, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.37 seconds, 4.76 minutes\n",
      "total_backward_count 7230680 real_backward_count 335208   4.636%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.695573/  1.828356, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.44 seconds, 4.77 minutes\n",
      "total_backward_count 7275040 real_backward_count 335917   4.617%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.696578/  1.828516, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 286.10 seconds, 4.77 minutes\n",
      "total_backward_count 7319400 real_backward_count 336571   4.598%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.694248/  1.825225, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.97 seconds, 4.77 minutes\n",
      "total_backward_count 7363760 real_backward_count 337271   4.580%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.694846/  1.824975, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.98 seconds, 4.75 minutes\n",
      "total_backward_count 7408120 real_backward_count 338070   4.564%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.695406/  1.831472, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.20 seconds, 4.75 minutes\n",
      "total_backward_count 7452480 real_backward_count 338788   4.546%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.690676/  1.817565, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.01 seconds, 4.73 minutes\n",
      "total_backward_count 7496840 real_backward_count 339450   4.528%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.697670/  1.822795, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.87 seconds, 4.75 minutes\n",
      "total_backward_count 7541200 real_backward_count 340175   4.511%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.699866/  1.823933, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.28 seconds, 4.75 minutes\n",
      "total_backward_count 7585560 real_backward_count 340862   4.494%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.689868/  1.818589, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 287.78 seconds, 4.80 minutes\n",
      "total_backward_count 7629920 real_backward_count 341513   4.476%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.688305/  1.816915, val:  85.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.78 seconds, 4.70 minutes\n",
      "total_backward_count 7674280 real_backward_count 342162   4.459%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.685549/  1.807654, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.45 seconds, 4.74 minutes\n",
      "total_backward_count 7718640 real_backward_count 342821   4.441%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.686105/  1.815858, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.08 seconds, 4.75 minutes\n",
      "total_backward_count 7763000 real_backward_count 343502   4.425%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.692213/  1.818316, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.93 seconds, 4.75 minutes\n",
      "total_backward_count 7807360 real_backward_count 344179   4.408%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.693151/  1.819310, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 283.24 seconds, 4.72 minutes\n",
      "total_backward_count 7851720 real_backward_count 344851   4.392%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.691119/  1.818380, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.56 seconds, 4.74 minutes\n",
      "total_backward_count 7896080 real_backward_count 345525   4.376%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.695482/  1.820545, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.51 seconds, 4.74 minutes\n",
      "total_backward_count 7940440 real_backward_count 346170   4.360%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.698155/  1.820506, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 283.85 seconds, 4.73 minutes\n",
      "total_backward_count 7984800 real_backward_count 346790   4.343%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.694911/  1.829432, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 283.20 seconds, 4.72 minutes\n",
      "total_backward_count 8029160 real_backward_count 347387   4.327%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.694032/  1.832296, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.28 seconds, 4.75 minutes\n",
      "total_backward_count 8073520 real_backward_count 347998   4.310%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.684223/  1.817328, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.72 seconds, 4.75 minutes\n",
      "total_backward_count 8117880 real_backward_count 348614   4.294%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.679759/  1.813718, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.67 seconds, 4.74 minutes\n",
      "total_backward_count 8162240 real_backward_count 349228   4.279%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.679506/  1.823603, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.69 seconds, 4.76 minutes\n",
      "total_backward_count 8206600 real_backward_count 349838   4.263%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.686542/  1.827070, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.62 seconds, 4.76 minutes\n",
      "total_backward_count 8250960 real_backward_count 350479   4.248%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.686341/  1.812514, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.54 seconds, 4.74 minutes\n",
      "total_backward_count 8295320 real_backward_count 351143   4.233%\n",
      "fc layer 3 self.abs_max_out: 658.0\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.681472/  1.811364, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 283.86 seconds, 4.73 minutes\n",
      "total_backward_count 8339680 real_backward_count 351733   4.218%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.676087/  1.806913, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.74 seconds, 4.75 minutes\n",
      "total_backward_count 8384040 real_backward_count 352342   4.203%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.668552/  1.800690, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 283.91 seconds, 4.73 minutes\n",
      "total_backward_count 8428400 real_backward_count 352919   4.187%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.670860/  1.806060, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.43 seconds, 4.74 minutes\n",
      "total_backward_count 8472760 real_backward_count 353526   4.173%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.673998/  1.811386, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.88 seconds, 4.75 minutes\n",
      "total_backward_count 8517120 real_backward_count 354132   4.158%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.682372/  1.816643, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.20 seconds, 4.75 minutes\n",
      "total_backward_count 8561480 real_backward_count 354721   4.143%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.683778/  1.826638, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.06 seconds, 4.75 minutes\n",
      "total_backward_count 8605840 real_backward_count 355313   4.129%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.681011/  1.811743, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.65 seconds, 4.76 minutes\n",
      "total_backward_count 8650200 real_backward_count 355938   4.115%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.683008/  1.816224, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.11 seconds, 4.75 minutes\n",
      "total_backward_count 8694560 real_backward_count 356537   4.101%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.683403/  1.808698, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 285.15 seconds, 4.75 minutes\n",
      "total_backward_count 8738920 real_backward_count 357183   4.087%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.680100/  1.815781, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.96 seconds, 4.75 minutes\n",
      "total_backward_count 8783280 real_backward_count 357754   4.073%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.678199/  1.811579, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 281.83 seconds, 4.70 minutes\n",
      "total_backward_count 8827640 real_backward_count 358308   4.059%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.682957/  1.813115, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 282.84 seconds, 4.71 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cfd5ff457643a6bae46140a1d7b6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.68296</td></tr><tr><td>val_acc_best</td><td>0.92083</td></tr><tr><td>val_acc_now</td><td>0.9</td></tr><tr><td>val_loss</td><td>1.81311</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drawn-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/alu834d7' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/alu834d7</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251022_172129-alu834d7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f11bylam with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 15054\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251023_085228-f11bylam</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f11bylam' target=\"_blank\">distinctive-sweep-10</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f11bylam' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f11bylam</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251023_085237_467', 'my_seed': 15054, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 7, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=7, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=7, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 454.0\n",
      "lif layer 1 self.abs_max_v: 454.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 583.5\n",
      "fc layer 2 self.abs_max_out: 200.0\n",
      "lif layer 2 self.abs_max_v: 200.0\n",
      "fc layer 1 self.abs_max_out: 550.0\n",
      "lif layer 1 self.abs_max_v: 680.5\n",
      "fc layer 2 self.abs_max_out: 202.0\n",
      "lif layer 2 self.abs_max_v: 227.0\n",
      "fc layer 1 self.abs_max_out: 562.0\n",
      "lif layer 1 self.abs_max_v: 809.0\n",
      "fc layer 2 self.abs_max_out: 217.0\n",
      "lif layer 2 self.abs_max_v: 249.0\n",
      "lif layer 1 self.abs_max_v: 811.5\n",
      "fc layer 2 self.abs_max_out: 286.0\n",
      "lif layer 2 self.abs_max_v: 299.5\n",
      "fc layer 1 self.abs_max_out: 602.0\n",
      "lif layer 1 self.abs_max_v: 1008.0\n",
      "fc layer 2 self.abs_max_out: 399.0\n",
      "lif layer 2 self.abs_max_v: 514.5\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "fc layer 1 self.abs_max_out: 731.0\n",
      "lif layer 1 self.abs_max_v: 1008.5\n",
      "fc layer 2 self.abs_max_out: 403.0\n",
      "fc layer 2 self.abs_max_out: 475.0\n",
      "lif layer 2 self.abs_max_v: 660.5\n",
      "fc layer 3 self.abs_max_out: 81.0\n",
      "fc layer 1 self.abs_max_out: 806.0\n",
      "lif layer 1 self.abs_max_v: 1042.0\n",
      "fc layer 1 self.abs_max_out: 959.0\n",
      "fc layer 1 self.abs_max_out: 1046.0\n",
      "lif layer 1 self.abs_max_v: 1046.0\n",
      "fc layer 2 self.abs_max_out: 681.0\n",
      "lif layer 2 self.abs_max_v: 870.0\n",
      "fc layer 3 self.abs_max_out: 86.0\n",
      "fc layer 1 self.abs_max_out: 1200.0\n",
      "lif layer 1 self.abs_max_v: 1200.0\n",
      "lif layer 1 self.abs_max_v: 1217.0\n",
      "fc layer 1 self.abs_max_out: 1333.0\n",
      "lif layer 1 self.abs_max_v: 1333.0\n",
      "fc layer 2 self.abs_max_out: 792.0\n",
      "lif layer 2 self.abs_max_v: 955.5\n",
      "fc layer 3 self.abs_max_out: 102.0\n",
      "lif layer 2 self.abs_max_v: 1088.5\n",
      "fc layer 3 self.abs_max_out: 129.0\n",
      "fc layer 3 self.abs_max_out: 163.0\n",
      "fc layer 1 self.abs_max_out: 1448.0\n",
      "lif layer 1 self.abs_max_v: 1448.0\n",
      "fc layer 2 self.abs_max_out: 861.0\n",
      "fc layer 1 self.abs_max_out: 1680.0\n",
      "lif layer 1 self.abs_max_v: 1680.0\n",
      "lif layer 2 self.abs_max_v: 1140.5\n",
      "fc layer 1 self.abs_max_out: 1779.0\n",
      "lif layer 1 self.abs_max_v: 1779.0\n",
      "lif layer 2 self.abs_max_v: 1164.0\n",
      "lif layer 2 self.abs_max_v: 1255.0\n",
      "fc layer 1 self.abs_max_out: 1956.0\n",
      "lif layer 1 self.abs_max_v: 1956.0\n",
      "fc layer 2 self.abs_max_out: 963.0\n",
      "lif layer 2 self.abs_max_v: 1280.0\n",
      "fc layer 3 self.abs_max_out: 176.0\n",
      "fc layer 3 self.abs_max_out: 193.0\n",
      "fc layer 3 self.abs_max_out: 196.0\n",
      "fc layer 2 self.abs_max_out: 976.0\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "lif layer 2 self.abs_max_v: 1330.0\n",
      "fc layer 2 self.abs_max_out: 1077.0\n",
      "lif layer 2 self.abs_max_v: 1401.5\n",
      "lif layer 2 self.abs_max_v: 1437.0\n",
      "lif layer 2 self.abs_max_v: 1512.5\n",
      "lif layer 2 self.abs_max_v: 1655.5\n",
      "fc layer 1 self.abs_max_out: 1980.0\n",
      "lif layer 1 self.abs_max_v: 1980.0\n",
      "fc layer 2 self.abs_max_out: 1084.0\n",
      "fc layer 2 self.abs_max_out: 1094.0\n",
      "fc layer 2 self.abs_max_out: 1242.0\n",
      "fc layer 2 self.abs_max_out: 1395.0\n",
      "fc layer 3 self.abs_max_out: 334.0\n",
      "fc layer 2 self.abs_max_out: 1408.0\n",
      "lif layer 2 self.abs_max_v: 1726.5\n",
      "lif layer 2 self.abs_max_v: 1767.0\n",
      "lif layer 2 self.abs_max_v: 1821.5\n",
      "fc layer 1 self.abs_max_out: 2235.0\n",
      "lif layer 1 self.abs_max_v: 2235.0\n",
      "lif layer 2 self.abs_max_v: 2012.0\n",
      "fc layer 2 self.abs_max_out: 1415.0\n",
      "fc layer 1 self.abs_max_out: 2397.0\n",
      "lif layer 1 self.abs_max_v: 2397.0\n",
      "fc layer 1 self.abs_max_out: 2439.0\n",
      "lif layer 1 self.abs_max_v: 2439.0\n",
      "fc layer 1 self.abs_max_out: 2586.0\n",
      "lif layer 1 self.abs_max_v: 2586.0\n",
      "fc layer 2 self.abs_max_out: 1578.0\n",
      "fc layer 1 self.abs_max_out: 2680.0\n",
      "lif layer 1 self.abs_max_v: 2680.0\n",
      "fc layer 2 self.abs_max_out: 1585.0\n",
      "fc layer 1 self.abs_max_out: 2700.0\n",
      "lif layer 1 self.abs_max_v: 2700.0\n",
      "lif layer 2 self.abs_max_v: 2106.5\n",
      "fc layer 2 self.abs_max_out: 1610.0\n",
      "fc layer 1 self.abs_max_out: 2734.0\n",
      "lif layer 1 self.abs_max_v: 2734.0\n",
      "fc layer 1 self.abs_max_out: 2762.0\n",
      "lif layer 1 self.abs_max_v: 2762.0\n",
      "fc layer 1 self.abs_max_out: 2863.0\n",
      "lif layer 1 self.abs_max_v: 2863.0\n",
      "fc layer 1 self.abs_max_out: 3272.0\n",
      "lif layer 1 self.abs_max_v: 3272.0\n",
      "fc layer 1 self.abs_max_out: 3416.0\n",
      "lif layer 1 self.abs_max_v: 3416.0\n",
      "fc layer 2 self.abs_max_out: 1656.0\n",
      "fc layer 1 self.abs_max_out: 3441.0\n",
      "lif layer 1 self.abs_max_v: 3441.0\n",
      "fc layer 3 self.abs_max_out: 349.0\n",
      "fc layer 1 self.abs_max_out: 3546.0\n",
      "lif layer 1 self.abs_max_v: 3546.0\n",
      "fc layer 2 self.abs_max_out: 1697.0\n",
      "fc layer 1 self.abs_max_out: 3574.0\n",
      "lif layer 1 self.abs_max_v: 3574.0\n",
      "fc layer 1 self.abs_max_out: 3703.0\n",
      "lif layer 1 self.abs_max_v: 3703.0\n",
      "lif layer 1 self.abs_max_v: 3768.0\n",
      "lif layer 1 self.abs_max_v: 3942.0\n",
      "lif layer 2 self.abs_max_v: 2198.5\n",
      "fc layer 1 self.abs_max_out: 3885.0\n",
      "lif layer 2 self.abs_max_v: 2270.0\n",
      "lif layer 2 self.abs_max_v: 2339.5\n",
      "fc layer 3 self.abs_max_out: 358.0\n",
      "fc layer 3 self.abs_max_out: 370.0\n",
      "fc layer 2 self.abs_max_out: 1700.0\n",
      "fc layer 2 self.abs_max_out: 1724.0\n",
      "fc layer 3 self.abs_max_out: 378.0\n",
      "fc layer 3 self.abs_max_out: 383.0\n",
      "fc layer 2 self.abs_max_out: 1907.0\n",
      "fc layer 2 self.abs_max_out: 2007.0\n",
      "fc layer 2 self.abs_max_out: 2023.0\n",
      "fc layer 1 self.abs_max_out: 4306.0\n",
      "lif layer 1 self.abs_max_v: 4306.0\n",
      "lif layer 2 self.abs_max_v: 2433.0\n",
      "fc layer 2 self.abs_max_out: 2095.0\n",
      "fc layer 1 self.abs_max_out: 4395.0\n",
      "lif layer 1 self.abs_max_v: 4395.0\n",
      "fc layer 2 self.abs_max_out: 2132.0\n",
      "fc layer 2 self.abs_max_out: 2137.0\n",
      "fc layer 2 self.abs_max_out: 2175.0\n",
      "fc layer 2 self.abs_max_out: 2290.0\n",
      "fc layer 3 self.abs_max_out: 384.0\n",
      "fc layer 2 self.abs_max_out: 2319.0\n",
      "fc layer 3 self.abs_max_out: 416.0\n",
      "fc layer 2 self.abs_max_out: 2332.0\n",
      "fc layer 1 self.abs_max_out: 4640.0\n",
      "lif layer 1 self.abs_max_v: 4640.0\n",
      "fc layer 2 self.abs_max_out: 2365.0\n",
      "fc layer 2 self.abs_max_out: 2410.0\n",
      "fc layer 2 self.abs_max_out: 2535.0\n",
      "lif layer 2 self.abs_max_v: 2535.0\n",
      "fc layer 1 self.abs_max_out: 4998.0\n",
      "lif layer 1 self.abs_max_v: 4998.0\n",
      "fc layer 1 self.abs_max_out: 4999.0\n",
      "lif layer 1 self.abs_max_v: 4999.0\n",
      "lif layer 2 self.abs_max_v: 2571.0\n",
      "fc layer 1 self.abs_max_out: 5258.0\n",
      "lif layer 1 self.abs_max_v: 5258.0\n",
      "fc layer 1 self.abs_max_out: 5288.0\n",
      "lif layer 1 self.abs_max_v: 5288.0\n",
      "fc layer 1 self.abs_max_out: 5693.0\n",
      "lif layer 1 self.abs_max_v: 5693.0\n",
      "fc layer 1 self.abs_max_out: 5792.0\n",
      "lif layer 1 self.abs_max_v: 5792.0\n",
      "fc layer 1 self.abs_max_out: 5939.0\n",
      "lif layer 1 self.abs_max_v: 5939.0\n",
      "fc layer 2 self.abs_max_out: 2796.0\n",
      "lif layer 2 self.abs_max_v: 2796.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.079224/  2.157757, val:  48.33%, val_best:  48.33%, tr:  94.12%, tr_best:  94.12%, epoch time: 285.56 seconds, 4.76 minutes\n",
      "total_backward_count 44360 real_backward_count 11453  25.818%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 2843.0\n",
      "lif layer 2 self.abs_max_v: 2843.0\n",
      "fc layer 1 self.abs_max_out: 5982.0\n",
      "lif layer 1 self.abs_max_v: 5982.0\n",
      "fc layer 1 self.abs_max_out: 6036.0\n",
      "lif layer 1 self.abs_max_v: 6036.0\n",
      "lif layer 2 self.abs_max_v: 2859.5\n",
      "lif layer 2 self.abs_max_v: 2936.0\n",
      "fc layer 1 self.abs_max_out: 6261.0\n",
      "lif layer 1 self.abs_max_v: 6261.0\n",
      "lif layer 2 self.abs_max_v: 3004.0\n",
      "fc layer 1 self.abs_max_out: 6365.0\n",
      "lif layer 1 self.abs_max_v: 6365.0\n",
      "fc layer 1 self.abs_max_out: 6496.0\n",
      "lif layer 1 self.abs_max_v: 6496.0\n",
      "fc layer 1 self.abs_max_out: 6576.0\n",
      "lif layer 1 self.abs_max_v: 6576.0\n",
      "lif layer 1 self.abs_max_v: 6646.5\n",
      "lif layer 2 self.abs_max_v: 3030.0\n",
      "lif layer 2 self.abs_max_v: 3045.5\n",
      "fc layer 1 self.abs_max_out: 6721.0\n",
      "lif layer 1 self.abs_max_v: 6721.0\n",
      "lif layer 2 self.abs_max_v: 3084.5\n",
      "fc layer 1 self.abs_max_out: 7070.0\n",
      "lif layer 1 self.abs_max_v: 7070.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.070019/  2.128281, val:  54.17%, val_best:  54.17%, tr:  99.28%, tr_best:  99.28%, epoch time: 284.34 seconds, 4.74 minutes\n",
      "total_backward_count 88720 real_backward_count 19159  21.595%\n",
      "lif layer 2 self.abs_max_v: 3280.5\n",
      "lif layer 2 self.abs_max_v: 3334.0\n",
      "lif layer 2 self.abs_max_v: 3348.5\n",
      "lif layer 2 self.abs_max_v: 3352.0\n",
      "lif layer 2 self.abs_max_v: 3454.5\n",
      "fc layer 1 self.abs_max_out: 7244.0\n",
      "lif layer 1 self.abs_max_v: 7244.0\n",
      "lif layer 2 self.abs_max_v: 3507.5\n",
      "lif layer 2 self.abs_max_v: 3522.5\n",
      "lif layer 2 self.abs_max_v: 3537.5\n",
      "fc layer 2 self.abs_max_out: 2877.0\n",
      "lif layer 1 self.abs_max_v: 7748.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.043465/  2.123887, val:  53.75%, val_best:  54.17%, tr:  99.64%, tr_best:  99.64%, epoch time: 283.59 seconds, 4.73 minutes\n",
      "total_backward_count 133080 real_backward_count 26241  19.718%\n",
      "lif layer 2 self.abs_max_v: 3683.5\n",
      "fc layer 1 self.abs_max_out: 7303.0\n",
      "fc layer 1 self.abs_max_out: 7385.0\n",
      "fc layer 2 self.abs_max_out: 3127.0\n",
      "fc layer 1 self.abs_max_out: 7473.0\n",
      "fc layer 1 self.abs_max_out: 8185.0\n",
      "lif layer 1 self.abs_max_v: 8185.0\n",
      "lif layer 2 self.abs_max_v: 3766.0\n",
      "lif layer 2 self.abs_max_v: 3909.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.033440/  2.108396, val:  52.92%, val_best:  54.17%, tr:  99.59%, tr_best:  99.64%, epoch time: 283.99 seconds, 4.73 minutes\n",
      "total_backward_count 177440 real_backward_count 32999  18.597%\n",
      "fc layer 1 self.abs_max_out: 8444.0\n",
      "lif layer 1 self.abs_max_v: 8444.0\n",
      "lif layer 2 self.abs_max_v: 3999.5\n",
      "lif layer 2 self.abs_max_v: 4081.5\n",
      "lif layer 2 self.abs_max_v: 4198.0\n",
      "lif layer 2 self.abs_max_v: 4296.5\n",
      "lif layer 2 self.abs_max_v: 4438.5\n",
      "lif layer 2 self.abs_max_v: 4521.5\n",
      "lif layer 1 self.abs_max_v: 8609.0\n",
      "lif layer 1 self.abs_max_v: 8625.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.030740/  2.119294, val:  56.67%, val_best:  56.67%, tr:  99.71%, tr_best:  99.71%, epoch time: 283.47 seconds, 4.72 minutes\n",
      "total_backward_count 221800 real_backward_count 39485  17.802%\n",
      "fc layer 1 self.abs_max_out: 8646.0\n",
      "lif layer 1 self.abs_max_v: 8646.0\n",
      "lif layer 1 self.abs_max_v: 9146.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.027871/  2.097703, val:  59.58%, val_best:  59.58%, tr:  99.86%, tr_best:  99.86%, epoch time: 284.57 seconds, 4.74 minutes\n",
      "total_backward_count 266160 real_backward_count 45834  17.220%\n",
      "fc layer 1 self.abs_max_out: 8954.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.023331/  2.089040, val:  62.50%, val_best:  62.50%, tr:  99.68%, tr_best:  99.86%, epoch time: 284.46 seconds, 4.74 minutes\n",
      "total_backward_count 310520 real_backward_count 51821  16.688%\n",
      "lif layer 2 self.abs_max_v: 4660.5\n",
      "fc layer 1 self.abs_max_out: 9264.0\n",
      "lif layer 1 self.abs_max_v: 9264.0\n",
      "lif layer 2 self.abs_max_v: 4727.0\n",
      "fc layer 2 self.abs_max_out: 3187.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  2.020921/  2.098114, val:  73.33%, val_best:  73.33%, tr:  99.86%, tr_best:  99.86%, epoch time: 283.84 seconds, 4.73 minutes\n",
      "total_backward_count 354880 real_backward_count 57515  16.207%\n",
      "fc layer 1 self.abs_max_out: 9372.0\n",
      "lif layer 1 self.abs_max_v: 9372.0\n",
      "lif layer 2 self.abs_max_v: 4881.0\n",
      "lif layer 2 self.abs_max_v: 5043.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  2.025025/  2.076350, val:  72.50%, val_best:  73.33%, tr:  99.89%, tr_best:  99.89%, epoch time: 283.39 seconds, 4.72 minutes\n",
      "total_backward_count 399240 real_backward_count 63325  15.861%\n",
      "fc layer 1 self.abs_max_out: 9451.0\n",
      "lif layer 1 self.abs_max_v: 9451.0\n",
      "lif layer 2 self.abs_max_v: 5072.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  2.021060/  2.077519, val:  84.58%, val_best:  84.58%, tr:  99.82%, tr_best:  99.89%, epoch time: 283.71 seconds, 4.73 minutes\n",
      "total_backward_count 443600 real_backward_count 68854  15.522%\n",
      "lif layer 2 self.abs_max_v: 5163.5\n",
      "fc layer 1 self.abs_max_out: 9556.0\n",
      "lif layer 1 self.abs_max_v: 9556.0\n",
      "lif layer 1 self.abs_max_v: 9945.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  2.014215/  2.079459, val:  62.92%, val_best:  84.58%, tr:  99.93%, tr_best:  99.93%, epoch time: 282.07 seconds, 4.70 minutes\n",
      "total_backward_count 487960 real_backward_count 74125  15.191%\n",
      "lif layer 2 self.abs_max_v: 5221.0\n",
      "fc layer 1 self.abs_max_out: 9662.0\n",
      "lif layer 2 self.abs_max_v: 5307.5\n",
      "lif layer 1 self.abs_max_v: 10165.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  2.011410/  2.087066, val:  65.42%, val_best:  84.58%, tr:  99.86%, tr_best:  99.93%, epoch time: 283.96 seconds, 4.73 minutes\n",
      "total_backward_count 532320 real_backward_count 79166  14.872%\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  2.011545/  2.081533, val:  74.17%, val_best:  84.58%, tr:  99.84%, tr_best:  99.93%, epoch time: 284.03 seconds, 4.73 minutes\n",
      "total_backward_count 576680 real_backward_count 84056  14.576%\n",
      "fc layer 1 self.abs_max_out: 9705.0\n",
      "lif layer 2 self.abs_max_v: 5449.5\n",
      "lif layer 1 self.abs_max_v: 10517.5\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  2.010876/  2.074739, val:  72.08%, val_best:  84.58%, tr:  99.84%, tr_best:  99.93%, epoch time: 267.79 seconds, 4.46 minutes\n",
      "total_backward_count 621040 real_backward_count 88935  14.320%\n",
      "fc layer 1 self.abs_max_out: 9812.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  2.002262/  2.052467, val:  83.75%, val_best:  84.58%, tr:  99.91%, tr_best:  99.93%, epoch time: 257.18 seconds, 4.29 minutes\n",
      "total_backward_count 665400 real_backward_count 93620  14.070%\n",
      "fc layer 2 self.abs_max_out: 3208.0\n",
      "fc layer 3 self.abs_max_out: 417.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  2.001763/  2.063672, val:  82.08%, val_best:  84.58%, tr:  99.95%, tr_best:  99.95%, epoch time: 257.95 seconds, 4.30 minutes\n",
      "total_backward_count 709760 real_backward_count 98172  13.832%\n",
      "fc layer 2 self.abs_max_out: 3271.0\n",
      "fc layer 1 self.abs_max_out: 9910.0\n",
      "fc layer 2 self.abs_max_out: 3479.0\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "lif layer 2 self.abs_max_v: 5453.5\n",
      "lif layer 1 self.abs_max_v: 10671.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  2.000257/  2.076952, val:  67.50%, val_best:  84.58%, tr:  99.89%, tr_best:  99.95%, epoch time: 258.21 seconds, 4.30 minutes\n",
      "total_backward_count 754120 real_backward_count 102632  13.610%\n",
      "lif layer 2 self.abs_max_v: 5641.0\n",
      "fc layer 1 self.abs_max_out: 9936.0\n",
      "lif layer 1 self.abs_max_v: 10900.5\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  2.002989/  2.062344, val:  82.92%, val_best:  84.58%, tr:  99.93%, tr_best:  99.95%, epoch time: 258.12 seconds, 4.30 minutes\n",
      "total_backward_count 798480 real_backward_count 106995  13.400%\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  2.002150/  2.073522, val:  77.08%, val_best:  84.58%, tr:  99.95%, tr_best:  99.95%, epoch time: 259.11 seconds, 4.32 minutes\n",
      "total_backward_count 842840 real_backward_count 111222  13.196%\n",
      "fc layer 1 self.abs_max_out: 9948.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  2.001153/  2.058815, val:  79.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.71 seconds, 4.31 minutes\n",
      "total_backward_count 887200 real_backward_count 115450  13.013%\n",
      "fc layer 1 self.abs_max_out: 10036.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.994744/  2.063249, val:  83.33%, val_best:  84.58%, tr:  99.98%, tr_best: 100.00%, epoch time: 258.35 seconds, 4.31 minutes\n",
      "total_backward_count 931560 real_backward_count 119468  12.825%\n",
      "fc layer 1 self.abs_max_out: 10100.0\n",
      "lif layer 1 self.abs_max_v: 11345.5\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.992350/  2.052141, val:  69.58%, val_best:  84.58%, tr:  99.95%, tr_best: 100.00%, epoch time: 255.20 seconds, 4.25 minutes\n",
      "total_backward_count 975920 real_backward_count 123381  12.643%\n",
      "fc layer 1 self.abs_max_out: 10241.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.980566/  2.056516, val:  66.25%, val_best:  84.58%, tr:  99.98%, tr_best: 100.00%, epoch time: 257.87 seconds, 4.30 minutes\n",
      "total_backward_count 1020280 real_backward_count 127248  12.472%\n",
      "fc layer 1 self.abs_max_out: 10247.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.976610/  2.035017, val:  86.25%, val_best:  86.25%, tr:  99.98%, tr_best: 100.00%, epoch time: 258.18 seconds, 4.30 minutes\n",
      "total_backward_count 1064640 real_backward_count 130964  12.301%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.984219/  2.051304, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.02 seconds, 4.30 minutes\n",
      "total_backward_count 1109000 real_backward_count 134619  12.139%\n",
      "fc layer 2 self.abs_max_out: 3540.0\n",
      "fc layer 1 self.abs_max_out: 10362.0\n",
      "fc layer 2 self.abs_max_out: 3563.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.979961/  2.041641, val:  86.67%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 257.89 seconds, 4.30 minutes\n",
      "total_backward_count 1153360 real_backward_count 138219  11.984%\n",
      "fc layer 1 self.abs_max_out: 10412.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.972674/  2.029992, val:  85.83%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 258.66 seconds, 4.31 minutes\n",
      "total_backward_count 1197720 real_backward_count 141616  11.824%\n",
      "fc layer 1 self.abs_max_out: 10444.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.964584/  2.031986, val:  85.00%, val_best:  86.67%, tr:  99.95%, tr_best: 100.00%, epoch time: 259.02 seconds, 4.32 minutes\n",
      "total_backward_count 1242080 real_backward_count 145073  11.680%\n",
      "fc layer 1 self.abs_max_out: 10448.0\n",
      "lif layer 1 self.abs_max_v: 11399.5\n",
      "fc layer 3 self.abs_max_out: 448.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.963895/  2.027125, val:  79.58%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 258.25 seconds, 4.30 minutes\n",
      "total_backward_count 1286440 real_backward_count 148433  11.538%\n",
      "fc layer 1 self.abs_max_out: 10569.0\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.955615/  2.046311, val:  84.58%, val_best:  86.67%, tr:  99.93%, tr_best: 100.00%, epoch time: 257.60 seconds, 4.29 minutes\n",
      "total_backward_count 1330800 real_backward_count 151703  11.399%\n",
      "lif layer 1 self.abs_max_v: 11727.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.959012/  2.021286, val:  72.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.01 seconds, 4.30 minutes\n",
      "total_backward_count 1375160 real_backward_count 154920  11.266%\n",
      "lif layer 1 self.abs_max_v: 11819.5\n",
      "fc layer 2 self.abs_max_out: 3662.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.950439/  2.023000, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.55 seconds, 4.31 minutes\n",
      "total_backward_count 1419520 real_backward_count 158024  11.132%\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.958059/  2.021054, val:  87.50%, val_best:  87.50%, tr:  99.95%, tr_best: 100.00%, epoch time: 258.40 seconds, 4.31 minutes\n",
      "total_backward_count 1463880 real_backward_count 161164  11.009%\n",
      "fc layer 1 self.abs_max_out: 10577.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.950059/  2.015890, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.06 seconds, 4.32 minutes\n",
      "total_backward_count 1508240 real_backward_count 164193  10.886%\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.942542/  2.025244, val:  80.00%, val_best:  87.50%, tr:  99.98%, tr_best: 100.00%, epoch time: 260.04 seconds, 4.33 minutes\n",
      "total_backward_count 1552600 real_backward_count 167186  10.768%\n",
      "fc layer 3 self.abs_max_out: 484.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.939158/  1.994111, val:  78.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.66 seconds, 4.34 minutes\n",
      "total_backward_count 1596960 real_backward_count 170080  10.650%\n",
      "fc layer 1 self.abs_max_out: 10578.0\n",
      "lif layer 1 self.abs_max_v: 11987.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.935651/  1.989624, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.22 seconds, 4.34 minutes\n",
      "total_backward_count 1641320 real_backward_count 173054  10.544%\n",
      "fc layer 1 self.abs_max_out: 10588.0\n",
      "lif layer 1 self.abs_max_v: 12044.5\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.931294/  2.004565, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.25 seconds, 4.34 minutes\n",
      "total_backward_count 1685680 real_backward_count 175819  10.430%\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.931358/  2.009970, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.66 seconds, 4.33 minutes\n",
      "total_backward_count 1730040 real_backward_count 178601  10.324%\n",
      "lif layer 1 self.abs_max_v: 12056.5\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.932343/  2.015971, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.24 seconds, 4.34 minutes\n",
      "total_backward_count 1774400 real_backward_count 181339  10.220%\n",
      "lif layer 1 self.abs_max_v: 12162.5\n",
      "fc layer 1 self.abs_max_out: 10624.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.936712/  2.003562, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.15 seconds, 4.35 minutes\n",
      "total_backward_count 1818760 real_backward_count 183939  10.113%\n",
      "lif layer 1 self.abs_max_v: 12312.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.926495/  1.996486, val:  87.08%, val_best:  88.33%, tr:  99.98%, tr_best: 100.00%, epoch time: 259.88 seconds, 4.33 minutes\n",
      "total_backward_count 1863120 real_backward_count 186504  10.010%\n",
      "fc layer 1 self.abs_max_out: 10655.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.913244/  1.997635, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.94 seconds, 4.33 minutes\n",
      "total_backward_count 1907480 real_backward_count 189142   9.916%\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.909864/  2.000235, val:  80.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.09 seconds, 4.33 minutes\n",
      "total_backward_count 1951840 real_backward_count 191787   9.826%\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.910977/  1.979154, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.40 seconds, 4.36 minutes\n",
      "total_backward_count 1996200 real_backward_count 194207   9.729%\n",
      "fc layer 1 self.abs_max_out: 10725.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.905355/  1.994376, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.22 seconds, 4.32 minutes\n",
      "total_backward_count 2040560 real_backward_count 196678   9.638%\n",
      "lif layer 1 self.abs_max_v: 12357.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.910298/  1.993258, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.71 seconds, 4.33 minutes\n",
      "total_backward_count 2084920 real_backward_count 199022   9.546%\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.903697/  1.987761, val:  74.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.05 seconds, 4.33 minutes\n",
      "total_backward_count 2129280 real_backward_count 201286   9.453%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.908763/  1.993230, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.80 seconds, 4.33 minutes\n",
      "total_backward_count 2173640 real_backward_count 203612   9.367%\n",
      "fc layer 2 self.abs_max_out: 3663.0\n",
      "fc layer 2 self.abs_max_out: 3862.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.902198/  1.985633, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.74 seconds, 4.33 minutes\n",
      "total_backward_count 2218000 real_backward_count 205832   9.280%\n",
      "fc layer 1 self.abs_max_out: 10727.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.896822/  1.981236, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.95 seconds, 4.33 minutes\n",
      "total_backward_count 2262360 real_backward_count 208010   9.194%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.891987/  1.963415, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.73 seconds, 4.35 minutes\n",
      "total_backward_count 2306720 real_backward_count 210125   9.109%\n",
      "fc layer 3 self.abs_max_out: 485.0\n",
      "fc layer 3 self.abs_max_out: 500.0\n",
      "lif layer 1 self.abs_max_v: 12406.5\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.888522/  1.972932, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.46 seconds, 4.34 minutes\n",
      "total_backward_count 2351080 real_backward_count 212251   9.028%\n",
      "fc layer 3 self.abs_max_out: 506.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.891245/  1.973446, val:  82.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.82 seconds, 4.33 minutes\n",
      "total_backward_count 2395440 real_backward_count 214464   8.953%\n",
      "lif layer 1 self.abs_max_v: 12454.5\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.882968/  1.970281, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.59 seconds, 4.34 minutes\n",
      "total_backward_count 2439800 real_backward_count 216531   8.875%\n",
      "fc layer 3 self.abs_max_out: 507.0\n",
      "fc layer 1 self.abs_max_out: 10811.0\n",
      "fc layer 3 self.abs_max_out: 540.0\n",
      "lif layer 1 self.abs_max_v: 12689.5\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.884759/  1.971970, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.55 seconds, 4.33 minutes\n",
      "total_backward_count 2484160 real_backward_count 218675   8.803%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.882309/  1.969177, val:  86.25%, val_best:  90.42%, tr:  99.98%, tr_best: 100.00%, epoch time: 259.67 seconds, 4.33 minutes\n",
      "total_backward_count 2528520 real_backward_count 220746   8.730%\n",
      "fc layer 1 self.abs_max_out: 10961.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.883233/  1.958552, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.10 seconds, 4.33 minutes\n",
      "total_backward_count 2572880 real_backward_count 222812   8.660%\n",
      "fc layer 1 self.abs_max_out: 10986.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.880627/  1.960579, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.00 seconds, 4.35 minutes\n",
      "total_backward_count 2617240 real_backward_count 224812   8.590%\n",
      "fc layer 1 self.abs_max_out: 10991.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.876039/  1.958038, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.86 seconds, 4.35 minutes\n",
      "total_backward_count 2661600 real_backward_count 226783   8.521%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.869759/  1.961904, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.07 seconds, 4.33 minutes\n",
      "total_backward_count 2705960 real_backward_count 228799   8.455%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.880154/  1.958561, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.52 seconds, 4.33 minutes\n",
      "total_backward_count 2750320 real_backward_count 230721   8.389%\n",
      "fc layer 1 self.abs_max_out: 10999.0\n",
      "lif layer 1 self.abs_max_v: 12796.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.879154/  1.960870, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.89 seconds, 4.35 minutes\n",
      "total_backward_count 2794680 real_backward_count 232621   8.324%\n",
      "fc layer 1 self.abs_max_out: 11049.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.873605/  1.969429, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.85 seconds, 4.35 minutes\n",
      "total_backward_count 2839040 real_backward_count 234410   8.257%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.878459/  1.962523, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.72 seconds, 4.35 minutes\n",
      "total_backward_count 2883400 real_backward_count 236310   8.196%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.871480/  1.960205, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.67 seconds, 4.34 minutes\n",
      "total_backward_count 2927760 real_backward_count 238129   8.133%\n",
      "lif layer 1 self.abs_max_v: 12950.5\n",
      "fc layer 1 self.abs_max_out: 11064.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.864265/  1.951216, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.23 seconds, 4.34 minutes\n",
      "total_backward_count 2972120 real_backward_count 239926   8.073%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.866765/  1.965954, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.57 seconds, 4.34 minutes\n",
      "total_backward_count 3016480 real_backward_count 241771   8.015%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.872953/  1.956508, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.10 seconds, 4.33 minutes\n",
      "total_backward_count 3060840 real_backward_count 243499   7.955%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.867191/  1.967155, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.16 seconds, 4.35 minutes\n",
      "total_backward_count 3105200 real_backward_count 245123   7.894%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.863799/  1.951923, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 256.58 seconds, 4.28 minutes\n",
      "total_backward_count 3149560 real_backward_count 246862   7.838%\n",
      "lif layer 1 self.abs_max_v: 12970.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.863217/  1.958624, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.50 seconds, 4.33 minutes\n",
      "total_backward_count 3193920 real_backward_count 248585   7.783%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.861450/  1.951470, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.41 seconds, 4.34 minutes\n",
      "total_backward_count 3238280 real_backward_count 250243   7.728%\n",
      "fc layer 3 self.abs_max_out: 541.0\n",
      "fc layer 3 self.abs_max_out: 557.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.863095/  1.957510, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.84 seconds, 4.35 minutes\n",
      "total_backward_count 3282640 real_backward_count 251953   7.675%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.863387/  1.946204, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.86 seconds, 4.33 minutes\n",
      "total_backward_count 3327000 real_backward_count 253542   7.621%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.858274/  1.945316, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.90 seconds, 4.35 minutes\n",
      "total_backward_count 3371360 real_backward_count 255123   7.567%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.857786/  1.965194, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.73 seconds, 4.35 minutes\n",
      "total_backward_count 3415720 real_backward_count 256694   7.515%\n",
      "lif layer 1 self.abs_max_v: 12978.5\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.855141/  1.935601, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.51 seconds, 4.36 minutes\n",
      "total_backward_count 3460080 real_backward_count 258326   7.466%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.842420/  1.941082, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.11 seconds, 4.34 minutes\n",
      "total_backward_count 3504440 real_backward_count 259844   7.415%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.842428/  1.925348, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.07 seconds, 4.35 minutes\n",
      "total_backward_count 3548800 real_backward_count 261268   7.362%\n",
      "lif layer 1 self.abs_max_v: 13061.0\n",
      "fc layer 3 self.abs_max_out: 571.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.830610/  1.933600, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.11 seconds, 4.35 minutes\n",
      "total_backward_count 3593160 real_backward_count 262789   7.314%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.832815/  1.938062, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.67 seconds, 4.34 minutes\n",
      "total_backward_count 3637520 real_backward_count 264327   7.267%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.844012/  1.940559, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.60 seconds, 4.36 minutes\n",
      "total_backward_count 3681880 real_backward_count 265809   7.219%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.840473/  1.928273, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.54 seconds, 4.34 minutes\n",
      "total_backward_count 3726240 real_backward_count 267259   7.172%\n",
      "fc layer 3 self.abs_max_out: 573.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.831665/  1.929430, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.88 seconds, 4.33 minutes\n",
      "total_backward_count 3770600 real_backward_count 268680   7.126%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.823906/  1.934136, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.73 seconds, 4.36 minutes\n",
      "total_backward_count 3814960 real_backward_count 270142   7.081%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.825325/  1.927571, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.47 seconds, 4.36 minutes\n",
      "total_backward_count 3859320 real_backward_count 271484   7.035%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.818464/  1.926773, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.95 seconds, 4.33 minutes\n",
      "total_backward_count 3903680 real_backward_count 272850   6.990%\n",
      "fc layer 1 self.abs_max_out: 11069.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.822792/  1.921193, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.81 seconds, 4.36 minutes\n",
      "total_backward_count 3948040 real_backward_count 274239   6.946%\n",
      "fc layer 1 self.abs_max_out: 11093.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.826427/  1.928546, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.40 seconds, 4.32 minutes\n",
      "total_backward_count 3992400 real_backward_count 275584   6.903%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.826848/  1.935015, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.33 seconds, 4.36 minutes\n",
      "total_backward_count 4036760 real_backward_count 276909   6.860%\n",
      "lif layer 1 self.abs_max_v: 13137.0\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.822729/  1.916057, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.88 seconds, 4.35 minutes\n",
      "total_backward_count 4081120 real_backward_count 278229   6.817%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.813931/  1.919161, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.88 seconds, 4.35 minutes\n",
      "total_backward_count 4125480 real_backward_count 279541   6.776%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.809942/  1.910697, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.77 seconds, 4.35 minutes\n",
      "total_backward_count 4169840 real_backward_count 280849   6.735%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.803339/  1.911567, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.25 seconds, 4.34 minutes\n",
      "total_backward_count 4214200 real_backward_count 282137   6.695%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.809708/  1.913848, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.44 seconds, 4.34 minutes\n",
      "total_backward_count 4258560 real_backward_count 283396   6.655%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.813858/  1.912388, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.26 seconds, 4.34 minutes\n",
      "total_backward_count 4302920 real_backward_count 284683   6.616%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.806909/  1.912441, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.91 seconds, 4.35 minutes\n",
      "total_backward_count 4347280 real_backward_count 285960   6.578%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.812238/  1.913957, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.40 seconds, 4.34 minutes\n",
      "total_backward_count 4391640 real_backward_count 287128   6.538%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.801587/  1.906786, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.66 seconds, 4.36 minutes\n",
      "total_backward_count 4436000 real_backward_count 288328   6.500%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.798370/  1.911491, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.08 seconds, 4.35 minutes\n",
      "total_backward_count 4480360 real_backward_count 289410   6.460%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.803668/  1.914206, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.17 seconds, 4.34 minutes\n",
      "total_backward_count 4524720 real_backward_count 290598   6.422%\n",
      "fc layer 1 self.abs_max_out: 11094.0\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.795695/  1.897164, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.60 seconds, 4.34 minutes\n",
      "total_backward_count 4569080 real_backward_count 291767   6.386%\n",
      "fc layer 1 self.abs_max_out: 11156.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.792339/  1.910264, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.92 seconds, 4.33 minutes\n",
      "total_backward_count 4613440 real_backward_count 292970   6.350%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.788869/  1.897439, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.81 seconds, 4.35 minutes\n",
      "total_backward_count 4657800 real_backward_count 294206   6.316%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.789108/  1.892390, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.90 seconds, 4.33 minutes\n",
      "total_backward_count 4702160 real_backward_count 295352   6.281%\n",
      "fc layer 3 self.abs_max_out: 580.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.782333/  1.888243, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.67 seconds, 4.33 minutes\n",
      "total_backward_count 4746520 real_backward_count 296466   6.246%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.789214/  1.899186, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.17 seconds, 4.34 minutes\n",
      "total_backward_count 4790880 real_backward_count 297589   6.212%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.798892/  1.891589, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.21 seconds, 4.35 minutes\n",
      "total_backward_count 4835240 real_backward_count 298682   6.177%\n",
      "fc layer 3 self.abs_max_out: 603.0\n",
      "fc layer 1 self.abs_max_out: 11184.0\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.792787/  1.896831, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.30 seconds, 4.36 minutes\n",
      "total_backward_count 4879600 real_backward_count 299820   6.144%\n",
      "fc layer 1 self.abs_max_out: 11187.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.801783/  1.899606, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.77 seconds, 4.35 minutes\n",
      "total_backward_count 4923960 real_backward_count 300923   6.111%\n",
      "lif layer 1 self.abs_max_v: 13149.5\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.796624/  1.906296, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.92 seconds, 4.33 minutes\n",
      "total_backward_count 4968320 real_backward_count 302044   6.079%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.795753/  1.900808, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.24 seconds, 4.35 minutes\n",
      "total_backward_count 5012680 real_backward_count 303157   6.048%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.792857/  1.889981, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.09 seconds, 4.35 minutes\n",
      "total_backward_count 5057040 real_backward_count 304168   6.015%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.795599/  1.901670, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.72 seconds, 4.35 minutes\n",
      "total_backward_count 5101400 real_backward_count 305195   5.983%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.796588/  1.906343, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.62 seconds, 4.38 minutes\n",
      "total_backward_count 5145760 real_backward_count 306372   5.954%\n",
      "fc layer 1 self.abs_max_out: 11224.0\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.797331/  1.894497, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.46 seconds, 4.32 minutes\n",
      "total_backward_count 5190120 real_backward_count 307412   5.923%\n",
      "fc layer 1 self.abs_max_out: 11226.0\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.793229/  1.893776, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.48 seconds, 4.32 minutes\n",
      "total_backward_count 5234480 real_backward_count 308461   5.893%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.792968/  1.901353, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.96 seconds, 4.30 minutes\n",
      "total_backward_count 5278840 real_backward_count 309501   5.863%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.786971/  1.885756, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 255.22 seconds, 4.25 minutes\n",
      "total_backward_count 5323200 real_backward_count 310454   5.832%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.772298/  1.882611, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.37 seconds, 4.29 minutes\n",
      "total_backward_count 5367560 real_backward_count 311539   5.804%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.776039/  1.871359, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.42 seconds, 4.31 minutes\n",
      "total_backward_count 5411920 real_backward_count 312548   5.775%\n",
      "fc layer 1 self.abs_max_out: 11243.0\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.778756/  1.891732, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.94 seconds, 4.30 minutes\n",
      "total_backward_count 5456280 real_backward_count 313441   5.745%\n",
      "fc layer 1 self.abs_max_out: 11247.0\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.781058/  1.890323, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.16 seconds, 4.32 minutes\n",
      "total_backward_count 5500640 real_backward_count 314359   5.715%\n",
      "lif layer 1 self.abs_max_v: 13166.0\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.782487/  1.886492, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.58 seconds, 4.31 minutes\n",
      "total_backward_count 5545000 real_backward_count 315415   5.688%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.771083/  1.888900, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.20 seconds, 4.29 minutes\n",
      "total_backward_count 5589360 real_backward_count 316391   5.661%\n",
      "lif layer 1 self.abs_max_v: 13172.5\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.785835/  1.894795, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.88 seconds, 4.30 minutes\n",
      "total_backward_count 5633720 real_backward_count 317389   5.634%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.785685/  1.897387, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.51 seconds, 4.29 minutes\n",
      "total_backward_count 5678080 real_backward_count 318375   5.607%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.788379/  1.895618, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.32 seconds, 4.31 minutes\n",
      "total_backward_count 5722440 real_backward_count 319333   5.580%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.786896/  1.893300, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.69 seconds, 4.31 minutes\n",
      "total_backward_count 5766800 real_backward_count 320204   5.553%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.785606/  1.895336, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.56 seconds, 4.31 minutes\n",
      "total_backward_count 5811160 real_backward_count 321207   5.527%\n",
      "fc layer 1 self.abs_max_out: 11254.0\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.777698/  1.883647, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.81 seconds, 4.30 minutes\n",
      "total_backward_count 5855520 real_backward_count 322167   5.502%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.774726/  1.898028, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.49 seconds, 4.31 minutes\n",
      "total_backward_count 5899880 real_backward_count 323029   5.475%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.784884/  1.884317, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.79 seconds, 4.30 minutes\n",
      "total_backward_count 5944240 real_backward_count 323874   5.449%\n",
      "fc layer 1 self.abs_max_out: 11292.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.774706/  1.884954, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.46 seconds, 4.29 minutes\n",
      "total_backward_count 5988600 real_backward_count 324769   5.423%\n",
      "fc layer 1 self.abs_max_out: 11296.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.767097/  1.866628, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.88 seconds, 4.31 minutes\n",
      "total_backward_count 6032960 real_backward_count 325619   5.397%\n",
      "fc layer 3 self.abs_max_out: 605.0\n",
      "fc layer 3 self.abs_max_out: 606.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.762763/  1.869305, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.21 seconds, 4.30 minutes\n",
      "total_backward_count 6077320 real_backward_count 326584   5.374%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.760792/  1.871188, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.45 seconds, 4.29 minutes\n",
      "total_backward_count 6121680 real_backward_count 327389   5.348%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.757195/  1.870359, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.38 seconds, 4.31 minutes\n",
      "total_backward_count 6166040 real_backward_count 328215   5.323%\n",
      "fc layer 3 self.abs_max_out: 612.0\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.752106/  1.866099, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.83 seconds, 4.31 minutes\n",
      "total_backward_count 6210400 real_backward_count 329062   5.299%\n",
      "lif layer 1 self.abs_max_v: 13291.5\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.759513/  1.883681, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.72 seconds, 4.31 minutes\n",
      "total_backward_count 6254760 real_backward_count 329887   5.274%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.768394/  1.878761, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.85 seconds, 4.30 minutes\n",
      "total_backward_count 6299120 real_backward_count 330695   5.250%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.771107/  1.887131, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.68 seconds, 4.31 minutes\n",
      "total_backward_count 6343480 real_backward_count 331495   5.226%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.769714/  1.881415, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.51 seconds, 4.31 minutes\n",
      "total_backward_count 6387840 real_backward_count 332317   5.202%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.766102/  1.873943, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.37 seconds, 4.29 minutes\n",
      "total_backward_count 6432200 real_backward_count 333090   5.178%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.758843/  1.866777, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.10 seconds, 4.32 minutes\n",
      "total_backward_count 6476560 real_backward_count 333944   5.156%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.753891/  1.864882, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 256.94 seconds, 4.28 minutes\n",
      "total_backward_count 6520920 real_backward_count 334739   5.133%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.753068/  1.869237, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.47 seconds, 4.32 minutes\n",
      "total_backward_count 6565280 real_backward_count 335578   5.111%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.755785/  1.868339, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.66 seconds, 4.31 minutes\n",
      "total_backward_count 6609640 real_backward_count 336322   5.088%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.760997/  1.866021, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.01 seconds, 4.30 minutes\n",
      "total_backward_count 6654000 real_backward_count 337120   5.066%\n",
      "lif layer 1 self.abs_max_v: 13860.0\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.756004/  1.857437, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.72 seconds, 4.31 minutes\n",
      "total_backward_count 6698360 real_backward_count 337878   5.044%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.753046/  1.864780, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.35 seconds, 4.31 minutes\n",
      "total_backward_count 6742720 real_backward_count 338672   5.023%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.756890/  1.864246, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.54 seconds, 4.31 minutes\n",
      "total_backward_count 6787080 real_backward_count 339457   5.002%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.757452/  1.871595, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.07 seconds, 4.30 minutes\n",
      "total_backward_count 6831440 real_backward_count 340217   4.980%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.757003/  1.870616, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.38 seconds, 4.31 minutes\n",
      "total_backward_count 6875800 real_backward_count 341003   4.959%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.753006/  1.872420, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.34 seconds, 4.31 minutes\n",
      "total_backward_count 6920160 real_backward_count 341776   4.939%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.751556/  1.856804, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.29 seconds, 4.29 minutes\n",
      "total_backward_count 6964520 real_backward_count 342521   4.918%\n",
      "fc layer 3 self.abs_max_out: 618.0\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.743501/  1.856331, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.81 seconds, 4.30 minutes\n",
      "total_backward_count 7008880 real_backward_count 343293   4.898%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.748162/  1.871861, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.67 seconds, 4.31 minutes\n",
      "total_backward_count 7053240 real_backward_count 344037   4.878%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.755565/  1.869814, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.87 seconds, 4.31 minutes\n",
      "total_backward_count 7097600 real_backward_count 344751   4.857%\n",
      "fc layer 3 self.abs_max_out: 638.0\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.753014/  1.860026, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.49 seconds, 4.31 minutes\n",
      "total_backward_count 7141960 real_backward_count 345453   4.837%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.751773/  1.869201, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.34 seconds, 4.32 minutes\n",
      "total_backward_count 7186320 real_backward_count 346231   4.818%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.749608/  1.863357, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.98 seconds, 4.30 minutes\n",
      "total_backward_count 7230680 real_backward_count 346949   4.798%\n",
      "fc layer 3 self.abs_max_out: 643.0\n",
      "fc layer 3 self.abs_max_out: 662.0\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.738629/  1.859645, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.39 seconds, 4.31 minutes\n",
      "total_backward_count 7275040 real_backward_count 347682   4.779%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.742747/  1.862153, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.82 seconds, 4.31 minutes\n",
      "total_backward_count 7319400 real_backward_count 348407   4.760%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.745926/  1.862697, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.19 seconds, 4.29 minutes\n",
      "total_backward_count 7363760 real_backward_count 349167   4.742%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.743323/  1.852221, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.86 seconds, 4.31 minutes\n",
      "total_backward_count 7408120 real_backward_count 349863   4.723%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.739938/  1.849558, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 253.69 seconds, 4.23 minutes\n",
      "total_backward_count 7452480 real_backward_count 350525   4.703%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.744594/  1.863827, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.92 seconds, 4.30 minutes\n",
      "total_backward_count 7496840 real_backward_count 351227   4.685%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.745922/  1.864401, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.62 seconds, 4.31 minutes\n",
      "total_backward_count 7541200 real_backward_count 351884   4.666%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.747775/  1.867496, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.90 seconds, 4.30 minutes\n",
      "total_backward_count 7585560 real_backward_count 352541   4.648%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.750196/  1.858386, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.05 seconds, 4.30 minutes\n",
      "total_backward_count 7629920 real_backward_count 353207   4.629%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.745253/  1.858119, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.05 seconds, 4.30 minutes\n",
      "total_backward_count 7674280 real_backward_count 353910   4.612%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.742877/  1.862926, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.86 seconds, 4.30 minutes\n",
      "total_backward_count 7718640 real_backward_count 354638   4.595%\n",
      "fc layer 1 self.abs_max_out: 11317.0\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.742643/  1.861979, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.58 seconds, 4.31 minutes\n",
      "total_backward_count 7763000 real_backward_count 355323   4.577%\n",
      "fc layer 1 self.abs_max_out: 11319.0\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.740904/  1.866339, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.76 seconds, 4.30 minutes\n",
      "total_backward_count 7807360 real_backward_count 356018   4.560%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.740835/  1.862390, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 258.25 seconds, 4.30 minutes\n",
      "total_backward_count 7851720 real_backward_count 356722   4.543%\n",
      "fc layer 2 self.abs_max_out: 3896.0\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.736925/  1.849723, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.27 seconds, 4.34 minutes\n",
      "total_backward_count 7896080 real_backward_count 357442   4.527%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.739436/  1.857120, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 280.65 seconds, 4.68 minutes\n",
      "total_backward_count 7940440 real_backward_count 358143   4.510%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.738183/  1.857420, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.07 seconds, 4.85 minutes\n",
      "total_backward_count 7984800 real_backward_count 358885   4.495%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.739554/  1.854801, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.34 seconds, 4.86 minutes\n",
      "total_backward_count 8029160 real_backward_count 359529   4.478%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.730182/  1.853040, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.51 seconds, 4.84 minutes\n",
      "total_backward_count 8073520 real_backward_count 360179   4.461%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.734095/  1.853662, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.72 seconds, 4.88 minutes\n",
      "total_backward_count 8117880 real_backward_count 360872   4.445%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.729640/  1.844293, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.47 seconds, 4.82 minutes\n",
      "total_backward_count 8162240 real_backward_count 361510   4.429%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.735557/  1.859696, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.27 seconds, 4.82 minutes\n",
      "total_backward_count 8206600 real_backward_count 362138   4.413%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.740101/  1.867828, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.86 seconds, 4.83 minutes\n",
      "total_backward_count 8250960 real_backward_count 362858   4.398%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.733209/  1.854119, val:  86.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.42 seconds, 4.84 minutes\n",
      "total_backward_count 8295320 real_backward_count 363473   4.382%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.727710/  1.856725, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 288.30 seconds, 4.80 minutes\n",
      "total_backward_count 8339680 real_backward_count 364039   4.365%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.727875/  1.851000, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.57 seconds, 4.83 minutes\n",
      "total_backward_count 8384040 real_backward_count 364691   4.350%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.734074/  1.854864, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 288.65 seconds, 4.81 minutes\n",
      "total_backward_count 8428400 real_backward_count 365242   4.333%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.728142/  1.847898, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.10 seconds, 4.84 minutes\n",
      "total_backward_count 8472760 real_backward_count 365844   4.318%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.725646/  1.850183, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.17 seconds, 4.85 minutes\n",
      "total_backward_count 8517120 real_backward_count 366482   4.303%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.723249/  1.847671, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.07 seconds, 4.85 minutes\n",
      "total_backward_count 8561480 real_backward_count 367117   4.288%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.726310/  1.853436, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.82 seconds, 4.83 minutes\n",
      "total_backward_count 8605840 real_backward_count 367787   4.274%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.728565/  1.847429, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.73 seconds, 4.85 minutes\n",
      "total_backward_count 8650200 real_backward_count 368435   4.259%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.721021/  1.838442, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.32 seconds, 4.86 minutes\n",
      "total_backward_count 8694560 real_backward_count 369077   4.245%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.724922/  1.848966, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 288.88 seconds, 4.81 minutes\n",
      "total_backward_count 8738920 real_backward_count 369630   4.230%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.722823/  1.843058, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.50 seconds, 4.86 minutes\n",
      "total_backward_count 8783280 real_backward_count 370255   4.215%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.717357/  1.833127, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 294.18 seconds, 4.90 minutes\n",
      "total_backward_count 8827640 real_backward_count 370779   4.200%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.713840/  1.832065, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.56 seconds, 4.86 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f049f07db340e6886d1e237f5ea476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.71384</td></tr><tr><td>val_acc_best</td><td>0.91667</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>1.83207</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">distinctive-sweep-10</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f11bylam' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/f11bylam</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251023_085228-f11bylam/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sfnmtp9d with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 2738\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251023_233519-sfnmtp9d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sfnmtp9d' target=\"_blank\">avid-sweep-15</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sfnmtp9d' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sfnmtp9d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251023_233528_734', 'my_seed': 2738, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 7, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=7, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=7, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 537.0\n",
      "lif layer 1 self.abs_max_v: 537.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 71.0\n",
      "lif layer 2 self.abs_max_v: 71.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 693.5\n",
      "fc layer 2 self.abs_max_out: 223.0\n",
      "lif layer 2 self.abs_max_v: 249.5\n",
      "lif layer 1 self.abs_max_v: 734.0\n",
      "lif layer 1 self.abs_max_v: 754.0\n",
      "fc layer 2 self.abs_max_out: 373.0\n",
      "lif layer 2 self.abs_max_v: 476.5\n",
      "fc layer 1 self.abs_max_out: 550.0\n",
      "lif layer 1 self.abs_max_v: 772.5\n",
      "lif layer 1 self.abs_max_v: 822.5\n",
      "fc layer 1 self.abs_max_out: 622.0\n",
      "lif layer 1 self.abs_max_v: 943.0\n",
      "fc layer 1 self.abs_max_out: 676.0\n",
      "fc layer 2 self.abs_max_out: 383.0\n",
      "lif layer 2 self.abs_max_v: 506.0\n",
      "fc layer 1 self.abs_max_out: 1004.0\n",
      "lif layer 1 self.abs_max_v: 1154.0\n",
      "fc layer 1 self.abs_max_out: 1044.0\n",
      "fc layer 2 self.abs_max_out: 482.0\n",
      "lif layer 2 self.abs_max_v: 527.0\n",
      "fc layer 3 self.abs_max_out: 35.0\n",
      "lif layer 2 self.abs_max_v: 569.5\n",
      "fc layer 2 self.abs_max_out: 541.0\n",
      "lif layer 2 self.abs_max_v: 648.0\n",
      "fc layer 3 self.abs_max_out: 55.0\n",
      "fc layer 1 self.abs_max_out: 1050.0\n",
      "lif layer 2 self.abs_max_v: 756.0\n",
      "fc layer 3 self.abs_max_out: 105.0\n",
      "fc layer 1 self.abs_max_out: 1140.0\n",
      "fc layer 2 self.abs_max_out: 636.0\n",
      "lif layer 2 self.abs_max_v: 829.5\n",
      "fc layer 1 self.abs_max_out: 1255.0\n",
      "lif layer 1 self.abs_max_v: 1255.0\n",
      "fc layer 1 self.abs_max_out: 1374.0\n",
      "lif layer 1 self.abs_max_v: 1374.0\n",
      "fc layer 1 self.abs_max_out: 1475.0\n",
      "lif layer 1 self.abs_max_v: 1475.0\n",
      "lif layer 2 self.abs_max_v: 848.5\n",
      "fc layer 2 self.abs_max_out: 738.0\n",
      "fc layer 3 self.abs_max_out: 129.0\n",
      "lif layer 2 self.abs_max_v: 881.0\n",
      "lif layer 2 self.abs_max_v: 944.5\n",
      "lif layer 2 self.abs_max_v: 991.0\n",
      "fc layer 3 self.abs_max_out: 143.0\n",
      "lif layer 2 self.abs_max_v: 1024.5\n",
      "fc layer 2 self.abs_max_out: 942.0\n",
      "lif layer 2 self.abs_max_v: 1239.5\n",
      "fc layer 3 self.abs_max_out: 165.0\n",
      "lif layer 1 self.abs_max_v: 1619.0\n",
      "fc layer 2 self.abs_max_out: 986.0\n",
      "fc layer 2 self.abs_max_out: 1021.0\n",
      "fc layer 3 self.abs_max_out: 192.0\n",
      "fc layer 1 self.abs_max_out: 1564.0\n",
      "fc layer 1 self.abs_max_out: 1589.0\n",
      "fc layer 3 self.abs_max_out: 230.0\n",
      "fc layer 1 self.abs_max_out: 1948.0\n",
      "lif layer 1 self.abs_max_v: 1948.0\n",
      "fc layer 3 self.abs_max_out: 264.0\n",
      "fc layer 2 self.abs_max_out: 1049.0\n",
      "fc layer 2 self.abs_max_out: 1112.0\n",
      "lif layer 2 self.abs_max_v: 1283.5\n",
      "lif layer 2 self.abs_max_v: 1409.0\n",
      "fc layer 2 self.abs_max_out: 1272.0\n",
      "fc layer 3 self.abs_max_out: 364.0\n",
      "fc layer 1 self.abs_max_out: 2229.0\n",
      "lif layer 1 self.abs_max_v: 2229.0\n",
      "fc layer 2 self.abs_max_out: 1281.0\n",
      "lif layer 2 self.abs_max_v: 1465.5\n",
      "lif layer 2 self.abs_max_v: 1546.0\n",
      "fc layer 2 self.abs_max_out: 1313.0\n",
      "lif layer 2 self.abs_max_v: 1615.5\n",
      "fc layer 2 self.abs_max_out: 1411.0\n",
      "fc layer 1 self.abs_max_out: 2398.0\n",
      "lif layer 1 self.abs_max_v: 2398.0\n",
      "fc layer 1 self.abs_max_out: 2574.0\n",
      "lif layer 1 self.abs_max_v: 2574.0\n",
      "lif layer 2 self.abs_max_v: 1734.5\n",
      "fc layer 2 self.abs_max_out: 1428.0\n",
      "fc layer 2 self.abs_max_out: 1430.0\n",
      "fc layer 2 self.abs_max_out: 1450.0\n",
      "fc layer 2 self.abs_max_out: 1480.0\n",
      "fc layer 1 self.abs_max_out: 2594.0\n",
      "lif layer 1 self.abs_max_v: 2594.0\n",
      "fc layer 2 self.abs_max_out: 1531.0\n",
      "fc layer 2 self.abs_max_out: 1533.0\n",
      "fc layer 2 self.abs_max_out: 1568.0\n",
      "fc layer 2 self.abs_max_out: 1661.0\n",
      "lif layer 2 self.abs_max_v: 1766.0\n",
      "fc layer 1 self.abs_max_out: 2712.0\n",
      "lif layer 1 self.abs_max_v: 2712.0\n",
      "lif layer 2 self.abs_max_v: 1806.0\n",
      "lif layer 2 self.abs_max_v: 1825.0\n",
      "lif layer 2 self.abs_max_v: 1888.0\n",
      "fc layer 2 self.abs_max_out: 1790.0\n",
      "fc layer 1 self.abs_max_out: 2743.0\n",
      "lif layer 1 self.abs_max_v: 2743.0\n",
      "fc layer 1 self.abs_max_out: 2750.0\n",
      "lif layer 1 self.abs_max_v: 2750.0\n",
      "fc layer 1 self.abs_max_out: 3206.0\n",
      "lif layer 1 self.abs_max_v: 3206.0\n",
      "lif layer 1 self.abs_max_v: 3283.0\n",
      "fc layer 2 self.abs_max_out: 1816.0\n",
      "fc layer 2 self.abs_max_out: 1826.0\n",
      "fc layer 2 self.abs_max_out: 1830.0\n",
      "fc layer 2 self.abs_max_out: 2006.0\n",
      "lif layer 2 self.abs_max_v: 2006.0\n",
      "fc layer 1 self.abs_max_out: 3376.0\n",
      "lif layer 1 self.abs_max_v: 3376.0\n",
      "fc layer 1 self.abs_max_out: 3492.0\n",
      "lif layer 1 self.abs_max_v: 3492.0\n",
      "fc layer 3 self.abs_max_out: 372.0\n",
      "fc layer 1 self.abs_max_out: 3596.0\n",
      "lif layer 1 self.abs_max_v: 3596.0\n",
      "fc layer 1 self.abs_max_out: 3656.0\n",
      "lif layer 1 self.abs_max_v: 3656.0\n",
      "fc layer 1 self.abs_max_out: 3700.0\n",
      "lif layer 1 self.abs_max_v: 3700.0\n",
      "fc layer 1 self.abs_max_out: 3814.0\n",
      "lif layer 1 self.abs_max_v: 3814.0\n",
      "fc layer 1 self.abs_max_out: 3860.0\n",
      "lif layer 1 self.abs_max_v: 3860.0\n",
      "fc layer 1 self.abs_max_out: 3978.0\n",
      "lif layer 1 self.abs_max_v: 3978.0\n",
      "fc layer 1 self.abs_max_out: 4147.0\n",
      "lif layer 1 self.abs_max_v: 4147.0\n",
      "fc layer 2 self.abs_max_out: 2056.0\n",
      "lif layer 2 self.abs_max_v: 2056.0\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "fc layer 2 self.abs_max_out: 2147.0\n",
      "lif layer 2 self.abs_max_v: 2147.0\n",
      "fc layer 2 self.abs_max_out: 2234.0\n",
      "lif layer 2 self.abs_max_v: 2234.0\n",
      "lif layer 1 self.abs_max_v: 4240.0\n",
      "lif layer 1 self.abs_max_v: 4293.0\n",
      "lif layer 1 self.abs_max_v: 4312.5\n",
      "lif layer 1 self.abs_max_v: 4504.5\n",
      "lif layer 1 self.abs_max_v: 4702.5\n",
      "lif layer 2 self.abs_max_v: 2334.0\n",
      "lif layer 1 self.abs_max_v: 4913.0\n",
      "fc layer 1 self.abs_max_out: 4154.0\n",
      "fc layer 1 self.abs_max_out: 4322.0\n",
      "fc layer 3 self.abs_max_out: 423.0\n",
      "lif layer 2 self.abs_max_v: 2424.5\n",
      "lif layer 2 self.abs_max_v: 2512.5\n",
      "lif layer 1 self.abs_max_v: 5100.5\n",
      "lif layer 2 self.abs_max_v: 2676.5\n",
      "lif layer 2 self.abs_max_v: 2684.5\n",
      "fc layer 2 self.abs_max_out: 2338.0\n",
      "fc layer 1 self.abs_max_out: 4504.0\n",
      "fc layer 1 self.abs_max_out: 4590.0\n",
      "fc layer 1 self.abs_max_out: 4598.0\n",
      "lif layer 1 self.abs_max_v: 5238.5\n",
      "lif layer 1 self.abs_max_v: 5522.5\n",
      "fc layer 1 self.abs_max_out: 4628.0\n",
      "fc layer 1 self.abs_max_out: 4662.0\n",
      "fc layer 1 self.abs_max_out: 5008.0\n",
      "fc layer 1 self.abs_max_out: 5038.0\n",
      "fc layer 1 self.abs_max_out: 5052.0\n",
      "fc layer 1 self.abs_max_out: 5114.0\n",
      "fc layer 1 self.abs_max_out: 5154.0\n",
      "fc layer 1 self.abs_max_out: 5352.0\n",
      "fc layer 2 self.abs_max_out: 2358.0\n",
      "fc layer 1 self.abs_max_out: 5435.0\n",
      "fc layer 1 self.abs_max_out: 5544.0\n",
      "lif layer 1 self.abs_max_v: 5544.0\n",
      "lif layer 2 self.abs_max_v: 2709.5\n",
      "lif layer 2 self.abs_max_v: 2747.5\n",
      "fc layer 1 self.abs_max_out: 5663.0\n",
      "lif layer 1 self.abs_max_v: 5663.0\n",
      "lif layer 1 self.abs_max_v: 5745.0\n",
      "fc layer 2 self.abs_max_out: 2444.0\n",
      "fc layer 1 self.abs_max_out: 6024.0\n",
      "lif layer 1 self.abs_max_v: 6024.0\n",
      "lif layer 1 self.abs_max_v: 6034.0\n",
      "lif layer 1 self.abs_max_v: 6575.5\n",
      "lif layer 1 self.abs_max_v: 6728.0\n",
      "fc layer 1 self.abs_max_out: 6080.0\n",
      "fc layer 2 self.abs_max_out: 2498.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.088110/  2.161653, val:  46.25%, val_best:  46.25%, tr:  93.49%, tr_best:  93.49%, epoch time: 293.37 seconds, 4.89 minutes\n",
      "total_backward_count 44360 real_backward_count 11663  26.292%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 2965.5\n",
      "lif layer 2 self.abs_max_v: 2995.0\n",
      "lif layer 2 self.abs_max_v: 3031.0\n",
      "lif layer 1 self.abs_max_v: 6849.5\n",
      "lif layer 2 self.abs_max_v: 3046.5\n",
      "lif layer 2 self.abs_max_v: 3235.0\n",
      "fc layer 1 self.abs_max_out: 6350.0\n",
      "fc layer 2 self.abs_max_out: 2692.0\n",
      "fc layer 2 self.abs_max_out: 2728.0\n",
      "fc layer 1 self.abs_max_out: 6651.0\n",
      "lif layer 1 self.abs_max_v: 7021.5\n",
      "lif layer 1 self.abs_max_v: 7281.0\n",
      "fc layer 1 self.abs_max_out: 7304.0\n",
      "lif layer 1 self.abs_max_v: 7304.0\n",
      "lif layer 1 self.abs_max_v: 7473.0\n",
      "lif layer 1 self.abs_max_v: 8007.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.080456/  2.146342, val:  38.75%, val_best:  46.25%, tr:  98.99%, tr_best:  98.99%, epoch time: 292.27 seconds, 4.87 minutes\n",
      "total_backward_count 88720 real_backward_count 19729  22.237%\n",
      "lif layer 2 self.abs_max_v: 3333.5\n",
      "lif layer 2 self.abs_max_v: 3450.0\n",
      "lif layer 2 self.abs_max_v: 3553.0\n",
      "fc layer 1 self.abs_max_out: 7678.0\n",
      "fc layer 2 self.abs_max_out: 2781.0\n",
      "fc layer 2 self.abs_max_out: 2948.0\n",
      "fc layer 2 self.abs_max_out: 3005.0\n",
      "lif layer 1 self.abs_max_v: 8052.5\n",
      "lif layer 1 self.abs_max_v: 8917.0\n",
      "fc layer 1 self.abs_max_out: 7771.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.076835/  2.139056, val:  60.83%, val_best:  60.83%, tr:  99.44%, tr_best:  99.44%, epoch time: 291.93 seconds, 4.87 minutes\n",
      "total_backward_count 133080 real_backward_count 26986  20.278%\n",
      "fc layer 1 self.abs_max_out: 8120.0\n",
      "lif layer 2 self.abs_max_v: 3643.5\n",
      "lif layer 2 self.abs_max_v: 3704.5\n",
      "lif layer 2 self.abs_max_v: 3891.5\n",
      "lif layer 1 self.abs_max_v: 9040.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.067778/  2.135366, val:  60.00%, val_best:  60.83%, tr:  99.62%, tr_best:  99.62%, epoch time: 291.79 seconds, 4.86 minutes\n",
      "total_backward_count 177440 real_backward_count 33959  19.138%\n",
      "fc layer 1 self.abs_max_out: 8457.0\n",
      "lif layer 1 self.abs_max_v: 9153.0\n",
      "lif layer 1 self.abs_max_v: 9234.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.062904/  2.135956, val:  45.00%, val_best:  60.83%, tr:  99.64%, tr_best:  99.64%, epoch time: 291.36 seconds, 4.86 minutes\n",
      "total_backward_count 221800 real_backward_count 40675  18.339%\n",
      "lif layer 1 self.abs_max_v: 9327.0\n",
      "fc layer 2 self.abs_max_out: 3017.0\n",
      "lif layer 2 self.abs_max_v: 3898.5\n",
      "fc layer 2 self.abs_max_out: 3067.0\n",
      "lif layer 1 self.abs_max_v: 9463.5\n",
      "fc layer 2 self.abs_max_out: 3116.0\n",
      "fc layer 1 self.abs_max_out: 8768.0\n",
      "lif layer 1 self.abs_max_v: 9492.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.067625/  2.140810, val:  52.50%, val_best:  60.83%, tr:  99.71%, tr_best:  99.71%, epoch time: 290.52 seconds, 4.84 minutes\n",
      "total_backward_count 266160 real_backward_count 47108  17.699%\n",
      "fc layer 1 self.abs_max_out: 8812.0\n",
      "fc layer 2 self.abs_max_out: 3133.0\n",
      "lif layer 1 self.abs_max_v: 9695.5\n",
      "lif layer 1 self.abs_max_v: 9793.0\n",
      "lif layer 1 self.abs_max_v: 10133.5\n",
      "lif layer 2 self.abs_max_v: 4042.5\n",
      "fc layer 2 self.abs_max_out: 3164.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.053851/  2.127026, val:  49.58%, val_best:  60.83%, tr:  99.77%, tr_best:  99.77%, epoch time: 291.21 seconds, 4.85 minutes\n",
      "total_backward_count 310520 real_backward_count 53360  17.184%\n",
      "fc layer 1 self.abs_max_out: 9060.0\n",
      "lif layer 2 self.abs_max_v: 4068.5\n",
      "lif layer 1 self.abs_max_v: 10188.5\n",
      "lif layer 1 self.abs_max_v: 10299.5\n",
      "lif layer 1 self.abs_max_v: 10592.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  2.044250/  2.115806, val:  65.42%, val_best:  65.42%, tr:  99.68%, tr_best:  99.77%, epoch time: 290.72 seconds, 4.85 minutes\n",
      "total_backward_count 354880 real_backward_count 59451  16.752%\n",
      "fc layer 2 self.abs_max_out: 3229.0\n",
      "fc layer 2 self.abs_max_out: 3240.0\n",
      "fc layer 2 self.abs_max_out: 3259.0\n",
      "fc layer 2 self.abs_max_out: 3278.0\n",
      "lif layer 1 self.abs_max_v: 10618.0\n",
      "lif layer 1 self.abs_max_v: 10846.0\n",
      "lif layer 1 self.abs_max_v: 11131.0\n",
      "fc layer 2 self.abs_max_out: 3289.0\n",
      "fc layer 1 self.abs_max_out: 9302.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  2.035178/  2.097293, val:  66.67%, val_best:  66.67%, tr:  99.86%, tr_best:  99.86%, epoch time: 291.06 seconds, 4.85 minutes\n",
      "total_backward_count 399240 real_backward_count 65173  16.324%\n",
      "fc layer 2 self.abs_max_out: 3311.0\n",
      "lif layer 2 self.abs_max_v: 4155.0\n",
      "lif layer 2 self.abs_max_v: 4197.0\n",
      "fc layer 1 self.abs_max_out: 9403.0\n",
      "fc layer 2 self.abs_max_out: 3659.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  2.025814/  2.095822, val:  84.58%, val_best:  84.58%, tr:  99.98%, tr_best:  99.98%, epoch time: 292.03 seconds, 4.87 minutes\n",
      "total_backward_count 443600 real_backward_count 70642  15.925%\n",
      "fc layer 1 self.abs_max_out: 9413.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  2.018446/  2.090108, val:  65.83%, val_best:  84.58%, tr:  99.89%, tr_best:  99.98%, epoch time: 291.88 seconds, 4.86 minutes\n",
      "total_backward_count 487960 real_backward_count 75736  15.521%\n",
      "lif layer 1 self.abs_max_v: 11285.5\n",
      "fc layer 1 self.abs_max_out: 9474.0\n",
      "lif layer 2 self.abs_max_v: 4271.0\n",
      "lif layer 2 self.abs_max_v: 4316.0\n",
      "lif layer 2 self.abs_max_v: 4573.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  2.012902/  2.081386, val:  61.25%, val_best:  84.58%, tr:  99.95%, tr_best:  99.98%, epoch time: 291.25 seconds, 4.85 minutes\n",
      "total_backward_count 532320 real_backward_count 80729  15.166%\n",
      "fc layer 1 self.abs_max_out: 9664.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  2.007115/  2.080549, val:  69.58%, val_best:  84.58%, tr:  99.93%, tr_best:  99.98%, epoch time: 291.90 seconds, 4.86 minutes\n",
      "total_backward_count 576680 real_backward_count 85509  14.828%\n",
      "fc layer 1 self.abs_max_out: 9706.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  2.008360/  2.077951, val:  73.75%, val_best:  84.58%, tr:  99.91%, tr_best:  99.98%, epoch time: 291.60 seconds, 4.86 minutes\n",
      "total_backward_count 621040 real_backward_count 90295  14.539%\n",
      "fc layer 1 self.abs_max_out: 9752.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  2.009587/  2.078054, val:  66.67%, val_best:  84.58%, tr:  99.98%, tr_best:  99.98%, epoch time: 289.63 seconds, 4.83 minutes\n",
      "total_backward_count 665400 real_backward_count 94833  14.252%\n",
      "fc layer 1 self.abs_max_out: 9816.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  2.000863/  2.059216, val:  81.67%, val_best:  84.58%, tr:  99.95%, tr_best:  99.98%, epoch time: 290.84 seconds, 4.85 minutes\n",
      "total_backward_count 709760 real_backward_count 99387  14.003%\n",
      "fc layer 2 self.abs_max_out: 3665.0\n",
      "fc layer 2 self.abs_max_out: 3707.0\n",
      "fc layer 1 self.abs_max_out: 9865.0\n",
      "fc layer 2 self.abs_max_out: 3812.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.998882/  2.065818, val:  82.92%, val_best:  84.58%, tr:  99.95%, tr_best:  99.98%, epoch time: 288.49 seconds, 4.81 minutes\n",
      "total_backward_count 754120 real_backward_count 103694  13.750%\n",
      "fc layer 1 self.abs_max_out: 9973.0\n",
      "lif layer 1 self.abs_max_v: 11469.5\n",
      "lif layer 1 self.abs_max_v: 11844.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  2.004311/  2.063692, val:  72.50%, val_best:  84.58%, tr:  99.95%, tr_best:  99.98%, epoch time: 290.82 seconds, 4.85 minutes\n",
      "total_backward_count 798480 real_backward_count 108025  13.529%\n",
      "fc layer 1 self.abs_max_out: 9982.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.998343/  2.064669, val:  72.92%, val_best:  84.58%, tr:  99.93%, tr_best:  99.98%, epoch time: 291.82 seconds, 4.86 minutes\n",
      "total_backward_count 842840 real_backward_count 112302  13.324%\n",
      "fc layer 1 self.abs_max_out: 10053.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.994697/  2.070592, val:  77.92%, val_best:  84.58%, tr:  99.95%, tr_best:  99.98%, epoch time: 290.73 seconds, 4.85 minutes\n",
      "total_backward_count 887200 real_backward_count 116434  13.124%\n",
      "lif layer 1 self.abs_max_v: 11948.0\n",
      "fc layer 1 self.abs_max_out: 10104.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.993397/  2.073215, val:  76.25%, val_best:  84.58%, tr:  99.98%, tr_best:  99.98%, epoch time: 289.68 seconds, 4.83 minutes\n",
      "total_backward_count 931560 real_backward_count 120352  12.919%\n",
      "fc layer 1 self.abs_max_out: 10137.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.997306/  2.067768, val:  82.92%, val_best:  84.58%, tr:  99.98%, tr_best:  99.98%, epoch time: 292.21 seconds, 4.87 minutes\n",
      "total_backward_count 975920 real_backward_count 124248  12.731%\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.992760/  2.063326, val:  77.50%, val_best:  84.58%, tr:  99.98%, tr_best:  99.98%, epoch time: 292.67 seconds, 4.88 minutes\n",
      "total_backward_count 1020280 real_backward_count 127999  12.545%\n",
      "fc layer 1 self.abs_max_out: 10177.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.987634/  2.057330, val:  86.25%, val_best:  86.25%, tr:  99.91%, tr_best:  99.98%, epoch time: 291.31 seconds, 4.86 minutes\n",
      "total_backward_count 1064640 real_backward_count 131593  12.360%\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.981607/  2.054620, val:  84.58%, val_best:  86.25%, tr:  99.98%, tr_best:  99.98%, epoch time: 292.61 seconds, 4.88 minutes\n",
      "total_backward_count 1109000 real_backward_count 135161  12.188%\n",
      "fc layer 2 self.abs_max_out: 3821.0\n",
      "lif layer 1 self.abs_max_v: 11969.5\n",
      "fc layer 1 self.abs_max_out: 10211.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.987316/  2.068974, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.01 seconds, 4.85 minutes\n",
      "total_backward_count 1153360 real_backward_count 138621  12.019%\n",
      "fc layer 1 self.abs_max_out: 10275.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.979818/  2.057605, val:  77.08%, val_best:  86.25%, tr:  99.95%, tr_best: 100.00%, epoch time: 290.15 seconds, 4.84 minutes\n",
      "total_backward_count 1197720 real_backward_count 142038  11.859%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.972766/  2.044119, val:  86.67%, val_best:  86.67%, tr:  99.95%, tr_best: 100.00%, epoch time: 291.13 seconds, 4.85 minutes\n",
      "total_backward_count 1242080 real_backward_count 145251  11.694%\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.969283/  2.044764, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.56 seconds, 4.84 minutes\n",
      "total_backward_count 1286440 real_backward_count 148558  11.548%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.962604/  2.022850, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.48 seconds, 4.84 minutes\n",
      "total_backward_count 1330800 real_backward_count 151741  11.402%\n",
      "fc layer 1 self.abs_max_out: 10324.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.956419/  2.027245, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.68 seconds, 4.83 minutes\n",
      "total_backward_count 1375160 real_backward_count 154785  11.256%\n",
      "fc layer 1 self.abs_max_out: 10430.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.959876/  2.024150, val:  84.58%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 291.40 seconds, 4.86 minutes\n",
      "total_backward_count 1419520 real_backward_count 157808  11.117%\n",
      "lif layer 1 self.abs_max_v: 12011.5\n",
      "fc layer 1 self.abs_max_out: 10506.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.955831/  2.033344, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.55 seconds, 4.84 minutes\n",
      "total_backward_count 1463880 real_backward_count 160743  10.981%\n",
      "fc layer 1 self.abs_max_out: 10529.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.949973/  2.022063, val:  83.75%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 290.57 seconds, 4.84 minutes\n",
      "total_backward_count 1508240 real_backward_count 163599  10.847%\n",
      "fc layer 1 self.abs_max_out: 10530.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.948281/  2.023365, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.88 seconds, 4.86 minutes\n",
      "total_backward_count 1552600 real_backward_count 166483  10.723%\n",
      "fc layer 1 self.abs_max_out: 10563.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.937246/  2.011024, val:  82.92%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 290.23 seconds, 4.84 minutes\n",
      "total_backward_count 1596960 real_backward_count 169260  10.599%\n",
      "fc layer 1 self.abs_max_out: 10594.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.930452/  2.015247, val:  85.42%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 290.04 seconds, 4.83 minutes\n",
      "total_backward_count 1641320 real_backward_count 172041  10.482%\n",
      "lif layer 1 self.abs_max_v: 12112.0\n",
      "fc layer 1 self.abs_max_out: 10647.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.922893/  2.007631, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.19 seconds, 4.85 minutes\n",
      "total_backward_count 1685680 real_backward_count 174781  10.369%\n",
      "fc layer 1 self.abs_max_out: 10692.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.918556/  2.003035, val:  85.00%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 288.54 seconds, 4.81 minutes\n",
      "total_backward_count 1730040 real_backward_count 177392  10.254%\n",
      "lif layer 1 self.abs_max_v: 12174.0\n",
      "fc layer 1 self.abs_max_out: 10703.0\n",
      "fc layer 2 self.abs_max_out: 3843.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.921609/  1.999377, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.72 seconds, 4.83 minutes\n",
      "total_backward_count 1774400 real_backward_count 179857  10.136%\n",
      "fc layer 1 self.abs_max_out: 10712.0\n",
      "lif layer 1 self.abs_max_v: 12183.5\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.925479/  2.023191, val:  80.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.33 seconds, 4.86 minutes\n",
      "total_backward_count 1818760 real_backward_count 182297  10.023%\n",
      "fc layer 1 self.abs_max_out: 10762.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.933888/  2.003542, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.90 seconds, 4.83 minutes\n",
      "total_backward_count 1863120 real_backward_count 184799   9.919%\n",
      "lif layer 1 self.abs_max_v: 12189.5\n",
      "fc layer 1 self.abs_max_out: 10835.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.922139/  1.999430, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.14 seconds, 4.84 minutes\n",
      "total_backward_count 1907480 real_backward_count 187288   9.819%\n",
      "lif layer 1 self.abs_max_v: 12310.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.914961/  1.982847, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.06 seconds, 4.83 minutes\n",
      "total_backward_count 1951840 real_backward_count 189584   9.713%\n",
      "fc layer 3 self.abs_max_out: 429.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.909657/  1.996129, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.55 seconds, 4.84 minutes\n",
      "total_backward_count 1996200 real_backward_count 191931   9.615%\n",
      "fc layer 3 self.abs_max_out: 446.0\n",
      "fc layer 1 self.abs_max_out: 10845.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.906081/  1.990023, val:  85.42%, val_best:  87.92%, tr:  99.98%, tr_best: 100.00%, epoch time: 288.84 seconds, 4.81 minutes\n",
      "total_backward_count 2040560 real_backward_count 194333   9.524%\n",
      "fc layer 3 self.abs_max_out: 461.0\n",
      "fc layer 1 self.abs_max_out: 10846.0\n",
      "lif layer 1 self.abs_max_v: 12493.5\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.903960/  2.007966, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.91 seconds, 4.85 minutes\n",
      "total_backward_count 2084920 real_backward_count 196553   9.427%\n",
      "fc layer 1 self.abs_max_out: 10902.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.903310/  1.977617, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.35 seconds, 4.87 minutes\n",
      "total_backward_count 2129280 real_backward_count 198742   9.334%\n",
      "fc layer 1 self.abs_max_out: 10927.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.891701/  1.978231, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.47 seconds, 4.86 minutes\n",
      "total_backward_count 2173640 real_backward_count 200837   9.240%\n",
      "fc layer 1 self.abs_max_out: 10932.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.891894/  1.986645, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.80 seconds, 4.83 minutes\n",
      "total_backward_count 2218000 real_backward_count 202962   9.151%\n",
      "fc layer 1 self.abs_max_out: 10970.0\n",
      "fc layer 3 self.abs_max_out: 485.0\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.883953/  1.977630, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.81 seconds, 4.83 minutes\n",
      "total_backward_count 2262360 real_backward_count 205006   9.062%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.889356/  1.974467, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.12 seconds, 4.84 minutes\n",
      "total_backward_count 2306720 real_backward_count 206979   8.973%\n",
      "fc layer 1 self.abs_max_out: 10976.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.890865/  1.980134, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.45 seconds, 4.86 minutes\n",
      "total_backward_count 2351080 real_backward_count 208955   8.888%\n",
      "fc layer 1 self.abs_max_out: 10998.0\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.899529/  1.981419, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.48 seconds, 4.84 minutes\n",
      "total_backward_count 2395440 real_backward_count 210994   8.808%\n",
      "fc layer 1 self.abs_max_out: 11015.0\n",
      "fc layer 2 self.abs_max_out: 3867.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.886481/  1.977252, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.38 seconds, 4.82 minutes\n",
      "total_backward_count 2439800 real_backward_count 212989   8.730%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.882979/  1.965553, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.80 seconds, 4.83 minutes\n",
      "total_backward_count 2484160 real_backward_count 214908   8.651%\n",
      "lif layer 1 self.abs_max_v: 12728.5\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.879946/  1.965261, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.44 seconds, 4.82 minutes\n",
      "total_backward_count 2528520 real_backward_count 216760   8.573%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.873719/  1.952623, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.98 seconds, 4.85 minutes\n",
      "total_backward_count 2572880 real_backward_count 218636   8.498%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.874195/  1.977133, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.07 seconds, 4.83 minutes\n",
      "total_backward_count 2617240 real_backward_count 220434   8.422%\n",
      "fc layer 1 self.abs_max_out: 11033.0\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.878007/  1.978318, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.02 seconds, 4.85 minutes\n",
      "total_backward_count 2661600 real_backward_count 222181   8.348%\n",
      "fc layer 1 self.abs_max_out: 11060.0\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.875545/  1.967380, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.73 seconds, 4.83 minutes\n",
      "total_backward_count 2705960 real_backward_count 223939   8.276%\n",
      "fc layer 1 self.abs_max_out: 11065.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.866869/  1.959583, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.59 seconds, 4.83 minutes\n",
      "total_backward_count 2750320 real_backward_count 225626   8.204%\n",
      "fc layer 1 self.abs_max_out: 11070.0\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.861499/  1.945743, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 288.75 seconds, 4.81 minutes\n",
      "total_backward_count 2794680 real_backward_count 227301   8.133%\n",
      "fc layer 1 self.abs_max_out: 11072.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.864192/  1.967413, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 288.91 seconds, 4.82 minutes\n",
      "total_backward_count 2839040 real_backward_count 228921   8.063%\n",
      "fc layer 1 self.abs_max_out: 11078.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.864801/  1.956696, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.53 seconds, 4.84 minutes\n",
      "total_backward_count 2883400 real_backward_count 230550   7.996%\n",
      "fc layer 1 self.abs_max_out: 11082.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.866321/  1.957224, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 288.23 seconds, 4.80 minutes\n",
      "total_backward_count 2927760 real_backward_count 232163   7.930%\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.863315/  1.957521, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.86 seconds, 4.83 minutes\n",
      "total_backward_count 2972120 real_backward_count 233725   7.864%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.860180/  1.957432, val:  80.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.31 seconds, 4.84 minutes\n",
      "total_backward_count 3016480 real_backward_count 235299   7.800%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.850955/  1.943056, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.36 seconds, 4.82 minutes\n",
      "total_backward_count 3060840 real_backward_count 236801   7.736%\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.841503/  1.937562, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.57 seconds, 4.83 minutes\n",
      "total_backward_count 3105200 real_backward_count 238317   7.675%\n",
      "fc layer 1 self.abs_max_out: 11085.0\n",
      "lif layer 1 self.abs_max_v: 12797.0\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.837805/  1.934929, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.78 seconds, 4.83 minutes\n",
      "total_backward_count 3149560 real_backward_count 239818   7.614%\n",
      "fc layer 1 self.abs_max_out: 11100.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.838372/  1.937779, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 288.19 seconds, 4.80 minutes\n",
      "total_backward_count 3193920 real_backward_count 241289   7.555%\n",
      "fc layer 1 self.abs_max_out: 11121.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.839841/  1.934742, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.75 seconds, 4.83 minutes\n",
      "total_backward_count 3238280 real_backward_count 242754   7.496%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.840182/  1.936132, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 288.97 seconds, 4.82 minutes\n",
      "total_backward_count 3282640 real_backward_count 244144   7.437%\n",
      "fc layer 1 self.abs_max_out: 11127.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.822851/  1.923973, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 288.78 seconds, 4.81 minutes\n",
      "total_backward_count 3327000 real_backward_count 245520   7.380%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.827595/  1.943315, val:  85.42%, val_best:  90.42%, tr:  99.98%, tr_best: 100.00%, epoch time: 290.24 seconds, 4.84 minutes\n",
      "total_backward_count 3371360 real_backward_count 246931   7.324%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.830612/  1.932965, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.05 seconds, 4.85 minutes\n",
      "total_backward_count 3415720 real_backward_count 248319   7.270%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.827417/  1.922954, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.43 seconds, 4.86 minutes\n",
      "total_backward_count 3460080 real_backward_count 249650   7.215%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.823074/  1.929181, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.95 seconds, 4.85 minutes\n",
      "total_backward_count 3504440 real_backward_count 251001   7.162%\n",
      "fc layer 1 self.abs_max_out: 11139.0\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.826477/  1.922697, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.20 seconds, 4.84 minutes\n",
      "total_backward_count 3548800 real_backward_count 252296   7.109%\n",
      "fc layer 1 self.abs_max_out: 11147.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.827839/  1.934194, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.01 seconds, 4.83 minutes\n",
      "total_backward_count 3593160 real_backward_count 253622   7.058%\n",
      "fc layer 3 self.abs_max_out: 514.0\n",
      "fc layer 1 self.abs_max_out: 11156.0\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.822029/  1.923411, val:  83.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.19 seconds, 4.85 minutes\n",
      "total_backward_count 3637520 real_backward_count 254968   7.009%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.821710/  1.927960, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.77 seconds, 4.83 minutes\n",
      "total_backward_count 3681880 real_backward_count 256294   6.961%\n",
      "fc layer 1 self.abs_max_out: 11166.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.829534/  1.928333, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.78 seconds, 4.85 minutes\n",
      "total_backward_count 3726240 real_backward_count 257558   6.912%\n",
      "fc layer 1 self.abs_max_out: 11180.0\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.819048/  1.917152, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.70 seconds, 4.84 minutes\n",
      "total_backward_count 3770600 real_backward_count 258802   6.864%\n",
      "fc layer 1 self.abs_max_out: 11184.0\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.808142/  1.908340, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.46 seconds, 4.84 minutes\n",
      "total_backward_count 3814960 real_backward_count 260074   6.817%\n",
      "fc layer 1 self.abs_max_out: 11189.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.804389/  1.918327, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.35 seconds, 4.82 minutes\n",
      "total_backward_count 3859320 real_backward_count 261277   6.770%\n",
      "fc layer 1 self.abs_max_out: 11196.0\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.809111/  1.914443, val:  86.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.18 seconds, 4.87 minutes\n",
      "total_backward_count 3903680 real_backward_count 262540   6.725%\n",
      "fc layer 1 self.abs_max_out: 11222.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.814694/  1.916812, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.09 seconds, 4.85 minutes\n",
      "total_backward_count 3948040 real_backward_count 263729   6.680%\n",
      "fc layer 1 self.abs_max_out: 11255.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.812276/  1.911243, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.96 seconds, 4.88 minutes\n",
      "total_backward_count 3992400 real_backward_count 264893   6.635%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.808147/  1.902625, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.52 seconds, 4.88 minutes\n",
      "total_backward_count 4036760 real_backward_count 266041   6.590%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.800462/  1.905936, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.54 seconds, 4.84 minutes\n",
      "total_backward_count 4081120 real_backward_count 267185   6.547%\n",
      "fc layer 3 self.abs_max_out: 529.0\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.794678/  1.898841, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.87 seconds, 4.85 minutes\n",
      "total_backward_count 4125480 real_backward_count 268340   6.504%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.788230/  1.901932, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.08 seconds, 4.87 minutes\n",
      "total_backward_count 4169840 real_backward_count 269471   6.462%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.792562/  1.903088, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.19 seconds, 4.85 minutes\n",
      "total_backward_count 4214200 real_backward_count 270532   6.420%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.798211/  1.907528, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.08 seconds, 4.85 minutes\n",
      "total_backward_count 4258560 real_backward_count 271693   6.380%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.798865/  1.892622, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.25 seconds, 4.85 minutes\n",
      "total_backward_count 4302920 real_backward_count 272826   6.340%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.782943/  1.906817, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.87 seconds, 4.83 minutes\n",
      "total_backward_count 4347280 real_backward_count 273890   6.300%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.790015/  1.893660, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.24 seconds, 4.84 minutes\n",
      "total_backward_count 4391640 real_backward_count 274936   6.260%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.784321/  1.900469, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.80 seconds, 4.86 minutes\n",
      "total_backward_count 4436000 real_backward_count 275976   6.221%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.784590/  1.897620, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.92 seconds, 4.87 minutes\n",
      "total_backward_count 4480360 real_backward_count 276966   6.182%\n",
      "fc layer 1 self.abs_max_out: 11268.0\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.783252/  1.894419, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.99 seconds, 4.87 minutes\n",
      "total_backward_count 4524720 real_backward_count 278044   6.145%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.783296/  1.894757, val:  85.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.66 seconds, 4.84 minutes\n",
      "total_backward_count 4569080 real_backward_count 279130   6.109%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.785998/  1.903763, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.37 seconds, 4.84 minutes\n",
      "total_backward_count 4613440 real_backward_count 280140   6.072%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.784873/  1.892052, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.17 seconds, 4.84 minutes\n",
      "total_backward_count 4657800 real_backward_count 281139   6.036%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.781590/  1.898425, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.63 seconds, 4.84 minutes\n",
      "total_backward_count 4702160 real_backward_count 282176   6.001%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.775521/  1.884831, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.33 seconds, 4.84 minutes\n",
      "total_backward_count 4746520 real_backward_count 283246   5.967%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.762139/  1.878705, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.56 seconds, 4.88 minutes\n",
      "total_backward_count 4790880 real_backward_count 284219   5.933%\n",
      "fc layer 3 self.abs_max_out: 530.0\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.757514/  1.866887, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.25 seconds, 4.84 minutes\n",
      "total_backward_count 4835240 real_backward_count 285253   5.899%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.749810/  1.878665, val:  85.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.93 seconds, 4.83 minutes\n",
      "total_backward_count 4879600 real_backward_count 286203   5.865%\n",
      "fc layer 1 self.abs_max_out: 11279.0\n",
      "fc layer 3 self.abs_max_out: 533.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.756749/  1.874718, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.27 seconds, 4.84 minutes\n",
      "total_backward_count 4923960 real_backward_count 287195   5.833%\n",
      "fc layer 1 self.abs_max_out: 11294.0\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.750207/  1.867257, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.77 seconds, 4.85 minutes\n",
      "total_backward_count 4968320 real_backward_count 288087   5.798%\n",
      "fc layer 1 self.abs_max_out: 11312.0\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.747452/  1.867453, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.02 seconds, 4.85 minutes\n",
      "total_backward_count 5012680 real_backward_count 289063   5.767%\n",
      "fc layer 1 self.abs_max_out: 11315.0\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.740634/  1.857941, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.18 seconds, 4.85 minutes\n",
      "total_backward_count 5057040 real_backward_count 290040   5.735%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.739312/  1.856006, val:  85.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.66 seconds, 4.84 minutes\n",
      "total_backward_count 5101400 real_backward_count 290962   5.704%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.735077/  1.854411, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.07 seconds, 4.87 minutes\n",
      "total_backward_count 5145760 real_backward_count 291876   5.672%\n",
      "fc layer 1 self.abs_max_out: 11317.0\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.737529/  1.864866, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.75 seconds, 4.86 minutes\n",
      "total_backward_count 5190120 real_backward_count 292784   5.641%\n",
      "fc layer 3 self.abs_max_out: 564.0\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.739782/  1.863764, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.15 seconds, 4.85 minutes\n",
      "total_backward_count 5234480 real_backward_count 293609   5.609%\n",
      "fc layer 1 self.abs_max_out: 11343.0\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.747570/  1.857040, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.31 seconds, 4.82 minutes\n",
      "total_backward_count 5278840 real_backward_count 294445   5.578%\n",
      "fc layer 1 self.abs_max_out: 11344.0\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.742169/  1.855212, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.39 seconds, 4.87 minutes\n",
      "total_backward_count 5323200 real_backward_count 295348   5.548%\n",
      "fc layer 1 self.abs_max_out: 11345.0\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.729106/  1.864496, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.82 seconds, 4.86 minutes\n",
      "total_backward_count 5367560 real_backward_count 296220   5.519%\n",
      "fc layer 1 self.abs_max_out: 11361.0\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.736622/  1.866808, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.10 seconds, 4.88 minutes\n",
      "total_backward_count 5411920 real_backward_count 297087   5.489%\n",
      "fc layer 1 self.abs_max_out: 11364.0\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.743887/  1.863781, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.20 seconds, 4.85 minutes\n",
      "total_backward_count 5456280 real_backward_count 297973   5.461%\n",
      "fc layer 1 self.abs_max_out: 11366.0\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.745718/  1.858616, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.01 seconds, 4.88 minutes\n",
      "total_backward_count 5500640 real_backward_count 298828   5.433%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.738321/  1.854914, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.49 seconds, 4.87 minutes\n",
      "total_backward_count 5545000 real_backward_count 299707   5.405%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.731467/  1.855876, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.52 seconds, 4.86 minutes\n",
      "total_backward_count 5589360 real_backward_count 300482   5.376%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.731110/  1.853901, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.91 seconds, 4.87 minutes\n",
      "total_backward_count 5633720 real_backward_count 301306   5.348%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.728922/  1.854896, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.67 seconds, 4.84 minutes\n",
      "total_backward_count 5678080 real_backward_count 302106   5.321%\n",
      "fc layer 1 self.abs_max_out: 11372.0\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.732134/  1.852755, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.70 seconds, 4.83 minutes\n",
      "total_backward_count 5722440 real_backward_count 302904   5.293%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.731067/  1.854116, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.33 seconds, 4.87 minutes\n",
      "total_backward_count 5766800 real_backward_count 303710   5.267%\n",
      "fc layer 1 self.abs_max_out: 11388.0\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.730093/  1.843214, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.58 seconds, 4.88 minutes\n",
      "total_backward_count 5811160 real_backward_count 304472   5.239%\n",
      "fc layer 3 self.abs_max_out: 616.0\n",
      "fc layer 1 self.abs_max_out: 11390.0\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.717190/  1.840132, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.33 seconds, 4.84 minutes\n",
      "total_backward_count 5855520 real_backward_count 305270   5.213%\n",
      "fc layer 1 self.abs_max_out: 11394.0\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.716105/  1.845185, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.55 seconds, 4.88 minutes\n",
      "total_backward_count 5899880 real_backward_count 306094   5.188%\n",
      "fc layer 1 self.abs_max_out: 11401.0\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.720216/  1.857831, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.75 seconds, 4.90 minutes\n",
      "total_backward_count 5944240 real_backward_count 306896   5.163%\n",
      "fc layer 1 self.abs_max_out: 11402.0\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.722292/  1.847737, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.54 seconds, 4.86 minutes\n",
      "total_backward_count 5988600 real_backward_count 307651   5.137%\n",
      "fc layer 1 self.abs_max_out: 11405.0\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.722366/  1.844921, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.69 seconds, 4.89 minutes\n",
      "total_backward_count 6032960 real_backward_count 308413   5.112%\n",
      "fc layer 1 self.abs_max_out: 11416.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.721931/  1.853243, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.49 seconds, 4.89 minutes\n",
      "total_backward_count 6077320 real_backward_count 309145   5.087%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.722874/  1.851706, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.96 seconds, 4.87 minutes\n",
      "total_backward_count 6121680 real_backward_count 309922   5.063%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.729283/  1.858145, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 294.60 seconds, 4.91 minutes\n",
      "total_backward_count 6166040 real_backward_count 310637   5.038%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.731184/  1.852983, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.85 seconds, 4.86 minutes\n",
      "total_backward_count 6210400 real_backward_count 311441   5.015%\n",
      "fc layer 1 self.abs_max_out: 11422.0\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.728890/  1.846599, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.37 seconds, 4.84 minutes\n",
      "total_backward_count 6254760 real_backward_count 312211   4.992%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.725074/  1.852570, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 295.53 seconds, 4.93 minutes\n",
      "total_backward_count 6299120 real_backward_count 312895   4.967%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.731152/  1.850289, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.89 seconds, 4.86 minutes\n",
      "total_backward_count 6343480 real_backward_count 313636   4.944%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.722496/  1.839346, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 294.46 seconds, 4.91 minutes\n",
      "total_backward_count 6387840 real_backward_count 314314   4.921%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.717453/  1.833965, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 294.23 seconds, 4.90 minutes\n",
      "total_backward_count 6432200 real_backward_count 314994   4.897%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.708221/  1.843170, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.72 seconds, 4.88 minutes\n",
      "total_backward_count 6476560 real_backward_count 315621   4.873%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.715722/  1.841664, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.36 seconds, 4.87 minutes\n",
      "total_backward_count 6520920 real_backward_count 316304   4.851%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.715383/  1.850832, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 296.31 seconds, 4.94 minutes\n",
      "total_backward_count 6565280 real_backward_count 316984   4.828%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.720017/  1.839820, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.98 seconds, 4.88 minutes\n",
      "total_backward_count 6609640 real_backward_count 317695   4.807%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.710716/  1.836049, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.87 seconds, 4.90 minutes\n",
      "total_backward_count 6654000 real_backward_count 318425   4.785%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.711767/  1.841353, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.88 seconds, 4.90 minutes\n",
      "total_backward_count 6698360 real_backward_count 319113   4.764%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.705812/  1.825734, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 295.06 seconds, 4.92 minutes\n",
      "total_backward_count 6742720 real_backward_count 319762   4.742%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.713639/  1.833307, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.54 seconds, 4.86 minutes\n",
      "total_backward_count 6787080 real_backward_count 320435   4.721%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.716229/  1.839219, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 294.80 seconds, 4.91 minutes\n",
      "total_backward_count 6831440 real_backward_count 321104   4.700%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.715775/  1.835766, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.12 seconds, 4.89 minutes\n",
      "total_backward_count 6875800 real_backward_count 321736   4.679%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.707842/  1.835220, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 296.07 seconds, 4.93 minutes\n",
      "total_backward_count 6920160 real_backward_count 322320   4.658%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.709294/  1.840804, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 295.18 seconds, 4.92 minutes\n",
      "total_backward_count 6964520 real_backward_count 322987   4.638%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.711943/  1.839033, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.04 seconds, 4.88 minutes\n",
      "total_backward_count 7008880 real_backward_count 323581   4.617%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.707250/  1.839082, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.53 seconds, 4.88 minutes\n",
      "total_backward_count 7053240 real_backward_count 324195   4.596%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.711895/  1.843162, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.68 seconds, 4.88 minutes\n",
      "total_backward_count 7097600 real_backward_count 324779   4.576%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.718964/  1.846882, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.21 seconds, 4.89 minutes\n",
      "total_backward_count 7141960 real_backward_count 325388   4.556%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.719370/  1.839587, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.92 seconds, 4.90 minutes\n",
      "total_backward_count 7186320 real_backward_count 326066   4.537%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.708751/  1.838290, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.68 seconds, 4.88 minutes\n",
      "total_backward_count 7230680 real_backward_count 326707   4.518%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.706863/  1.836602, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.53 seconds, 4.83 minutes\n",
      "total_backward_count 7275040 real_backward_count 327339   4.499%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.703061/  1.833996, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 288.87 seconds, 4.81 minutes\n",
      "total_backward_count 7319400 real_backward_count 327955   4.481%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.701893/  1.839254, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.32 seconds, 4.86 minutes\n",
      "total_backward_count 7363760 real_backward_count 328564   4.462%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.702379/  1.827508, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.94 seconds, 4.83 minutes\n",
      "total_backward_count 7408120 real_backward_count 329167   4.443%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.697504/  1.828090, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.45 seconds, 4.84 minutes\n",
      "total_backward_count 7452480 real_backward_count 329786   4.425%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.697300/  1.826993, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.45 seconds, 4.86 minutes\n",
      "total_backward_count 7496840 real_backward_count 330401   4.407%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.693448/  1.820127, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.64 seconds, 4.86 minutes\n",
      "total_backward_count 7541200 real_backward_count 331022   4.390%\n",
      "fc layer 3 self.abs_max_out: 631.0\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.686099/  1.810286, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.37 seconds, 4.84 minutes\n",
      "total_backward_count 7585560 real_backward_count 331621   4.372%\n",
      "lif layer 1 self.abs_max_v: 12892.5\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.688787/  1.820408, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.09 seconds, 4.87 minutes\n",
      "total_backward_count 7629920 real_backward_count 332195   4.354%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.692598/  1.830924, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.68 seconds, 4.88 minutes\n",
      "total_backward_count 7674280 real_backward_count 332771   4.336%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.702767/  1.825607, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.51 seconds, 4.86 minutes\n",
      "total_backward_count 7718640 real_backward_count 333384   4.319%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.697334/  1.830789, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.31 seconds, 4.86 minutes\n",
      "total_backward_count 7763000 real_backward_count 333976   4.302%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.699627/  1.829158, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.57 seconds, 4.84 minutes\n",
      "total_backward_count 7807360 real_backward_count 334602   4.286%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.706135/  1.822915, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 289.52 seconds, 4.83 minutes\n",
      "total_backward_count 7851720 real_backward_count 335193   4.269%\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.699515/  1.834858, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.85 seconds, 4.88 minutes\n",
      "total_backward_count 7896080 real_backward_count 335800   4.253%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.704052/  1.832576, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.26 seconds, 4.87 minutes\n",
      "total_backward_count 7940440 real_backward_count 336388   4.236%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.701458/  1.831650, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.99 seconds, 4.90 minutes\n",
      "total_backward_count 7984800 real_backward_count 336947   4.220%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.697104/  1.830034, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.76 seconds, 4.86 minutes\n",
      "total_backward_count 8029160 real_backward_count 337473   4.203%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.699469/  1.834734, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.66 seconds, 4.86 minutes\n",
      "total_backward_count 8073520 real_backward_count 337985   4.186%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.700191/  1.838267, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.01 seconds, 4.85 minutes\n",
      "total_backward_count 8117880 real_backward_count 338491   4.170%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.698981/  1.826995, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.85 seconds, 4.90 minutes\n",
      "total_backward_count 8162240 real_backward_count 339043   4.154%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.690913/  1.824844, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.92 seconds, 4.90 minutes\n",
      "total_backward_count 8206600 real_backward_count 339555   4.138%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.687588/  1.818366, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.52 seconds, 4.89 minutes\n",
      "total_backward_count 8250960 real_backward_count 340110   4.122%\n",
      "lif layer 1 self.abs_max_v: 12943.0\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.690261/  1.826886, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.21 seconds, 4.87 minutes\n",
      "total_backward_count 8295320 real_backward_count 340673   4.107%\n",
      "lif layer 1 self.abs_max_v: 12983.0\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.688324/  1.822501, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.65 seconds, 4.89 minutes\n",
      "total_backward_count 8339680 real_backward_count 341213   4.091%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.686069/  1.819941, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.71 seconds, 4.85 minutes\n",
      "total_backward_count 8384040 real_backward_count 341766   4.076%\n",
      "fc layer 3 self.abs_max_out: 635.0\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.687443/  1.822261, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 290.77 seconds, 4.85 minutes\n",
      "total_backward_count 8428400 real_backward_count 342363   4.062%\n",
      "fc layer 3 self.abs_max_out: 643.0\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.688683/  1.823187, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.24 seconds, 4.89 minutes\n",
      "total_backward_count 8472760 real_backward_count 342952   4.048%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.692213/  1.829298, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 292.22 seconds, 4.87 minutes\n",
      "total_backward_count 8517120 real_backward_count 343492   4.033%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.696774/  1.830346, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 293.09 seconds, 4.88 minutes\n",
      "total_backward_count 8561480 real_backward_count 344081   4.019%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.697189/  1.825191, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.11 seconds, 4.85 minutes\n",
      "total_backward_count 8605840 real_backward_count 344562   4.004%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.692787/  1.821719, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.99 seconds, 4.87 minutes\n",
      "total_backward_count 8650200 real_backward_count 345114   3.990%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.690557/  1.817770, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 291.98 seconds, 4.87 minutes\n",
      "total_backward_count 8694560 real_backward_count 345665   3.976%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.686837/  1.805233, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.87 seconds, 4.75 minutes\n",
      "total_backward_count 8738920 real_backward_count 346180   3.961%\n",
      "fc layer 3 self.abs_max_out: 648.0\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.678696/  1.814438, val:  88.33%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 283.12 seconds, 4.72 minutes\n",
      "total_backward_count 8783280 real_backward_count 346684   3.947%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.674508/  1.807073, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 282.84 seconds, 4.71 minutes\n",
      "total_backward_count 8827640 real_backward_count 347171   3.933%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.677229/  1.817906, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 284.31 seconds, 4.74 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b8ddcf9ae9459c9d2851b61095b71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.67723</td></tr><tr><td>val_acc_best</td><td>0.925</td></tr><tr><td>val_acc_now</td><td>0.90833</td></tr><tr><td>val_loss</td><td>1.81791</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">avid-sweep-15</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sfnmtp9d' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/sfnmtp9d</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251023_233519-sfnmtp9d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2xyu6g76 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 17158\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251024_154735-2xyu6g76</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2xyu6g76' target=\"_blank\">fiery-sweep-20</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/vz7o3mx3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2xyu6g76' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/2xyu6g76</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251024_154743_016', 'my_seed': 17158, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 7, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=7, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=7, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 418.0\n",
      "lif layer 1 self.abs_max_v: 418.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 462.0\n",
      "lif layer 1 self.abs_max_v: 555.5\n",
      "fc layer 2 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "lif layer 2 self.abs_max_v: 103.0\n",
      "lif layer 1 self.abs_max_v: 595.5\n",
      "fc layer 2 self.abs_max_out: 138.0\n",
      "lif layer 2 self.abs_max_v: 148.5\n",
      "fc layer 2 self.abs_max_out: 142.0\n",
      "lif layer 2 self.abs_max_v: 164.0\n",
      "lif layer 2 self.abs_max_v: 203.5\n",
      "fc layer 1 self.abs_max_out: 473.0\n",
      "lif layer 1 self.abs_max_v: 637.5\n",
      "fc layer 1 self.abs_max_out: 534.0\n",
      "lif layer 1 self.abs_max_v: 711.0\n",
      "fc layer 1 self.abs_max_out: 554.0\n",
      "fc layer 2 self.abs_max_out: 203.0\n",
      "lif layer 2 self.abs_max_v: 245.0\n",
      "fc layer 1 self.abs_max_out: 712.0\n",
      "lif layer 1 self.abs_max_v: 712.0\n",
      "lif layer 1 self.abs_max_v: 864.5\n",
      "fc layer 1 self.abs_max_out: 725.0\n",
      "lif layer 1 self.abs_max_v: 942.0\n",
      "lif layer 2 self.abs_max_v: 271.0\n",
      "fc layer 2 self.abs_max_out: 253.0\n",
      "fc layer 2 self.abs_max_out: 267.0\n",
      "lif layer 2 self.abs_max_v: 307.5\n",
      "lif layer 2 self.abs_max_v: 342.5\n",
      "fc layer 1 self.abs_max_out: 809.0\n",
      "fc layer 2 self.abs_max_out: 368.0\n",
      "lif layer 2 self.abs_max_v: 423.5\n",
      "lif layer 2 self.abs_max_v: 529.0\n",
      "fc layer 3 self.abs_max_out: 34.0\n",
      "fc layer 1 self.abs_max_out: 941.0\n",
      "fc layer 2 self.abs_max_out: 401.0\n",
      "fc layer 1 self.abs_max_out: 1156.0\n",
      "lif layer 1 self.abs_max_v: 1156.0\n",
      "fc layer 2 self.abs_max_out: 420.0\n",
      "lif layer 2 self.abs_max_v: 607.0\n",
      "fc layer 3 self.abs_max_out: 58.0\n",
      "fc layer 1 self.abs_max_out: 1672.0\n",
      "lif layer 1 self.abs_max_v: 1672.0\n",
      "fc layer 2 self.abs_max_out: 477.0\n",
      "lif layer 2 self.abs_max_v: 651.5\n",
      "fc layer 3 self.abs_max_out: 70.0\n",
      "fc layer 2 self.abs_max_out: 617.0\n",
      "lif layer 2 self.abs_max_v: 897.0\n",
      "lif layer 2 self.abs_max_v: 996.5\n",
      "fc layer 3 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 1039.5\n",
      "fc layer 3 self.abs_max_out: 79.0\n",
      "fc layer 3 self.abs_max_out: 83.0\n",
      "fc layer 3 self.abs_max_out: 115.0\n",
      "fc layer 2 self.abs_max_out: 669.0\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 2 self.abs_max_out: 798.0\n",
      "lif layer 2 self.abs_max_v: 1125.5\n",
      "lif layer 2 self.abs_max_v: 1233.5\n",
      "fc layer 1 self.abs_max_out: 1715.0\n",
      "lif layer 1 self.abs_max_v: 1715.0\n",
      "fc layer 2 self.abs_max_out: 951.0\n",
      "fc layer 3 self.abs_max_out: 143.0\n",
      "fc layer 3 self.abs_max_out: 171.0\n",
      "lif layer 2 self.abs_max_v: 1330.5\n",
      "fc layer 3 self.abs_max_out: 182.0\n",
      "fc layer 2 self.abs_max_out: 1028.0\n",
      "fc layer 1 self.abs_max_out: 2005.0\n",
      "lif layer 1 self.abs_max_v: 2005.0\n",
      "fc layer 1 self.abs_max_out: 2169.0\n",
      "lif layer 1 self.abs_max_v: 2169.0\n",
      "fc layer 3 self.abs_max_out: 194.0\n",
      "fc layer 2 self.abs_max_out: 1075.0\n",
      "fc layer 2 self.abs_max_out: 1141.0\n",
      "fc layer 3 self.abs_max_out: 235.0\n",
      "fc layer 3 self.abs_max_out: 339.0\n",
      "lif layer 2 self.abs_max_v: 1406.0\n",
      "fc layer 1 self.abs_max_out: 2277.0\n",
      "lif layer 1 self.abs_max_v: 2277.0\n",
      "lif layer 2 self.abs_max_v: 1419.0\n",
      "lif layer 2 self.abs_max_v: 1428.0\n",
      "fc layer 2 self.abs_max_out: 1179.0\n",
      "lif layer 2 self.abs_max_v: 1495.0\n",
      "fc layer 2 self.abs_max_out: 1265.0\n",
      "lif layer 2 self.abs_max_v: 1508.5\n",
      "fc layer 2 self.abs_max_out: 1296.0\n",
      "lif layer 2 self.abs_max_v: 1527.0\n",
      "lif layer 2 self.abs_max_v: 1711.5\n",
      "fc layer 2 self.abs_max_out: 1403.0\n",
      "fc layer 2 self.abs_max_out: 1452.0\n",
      "fc layer 2 self.abs_max_out: 1569.0\n",
      "fc layer 1 self.abs_max_out: 2324.0\n",
      "lif layer 1 self.abs_max_v: 2324.0\n",
      "fc layer 1 self.abs_max_out: 2363.0\n",
      "lif layer 1 self.abs_max_v: 2363.0\n",
      "fc layer 1 self.abs_max_out: 2582.0\n",
      "lif layer 1 self.abs_max_v: 2582.0\n",
      "fc layer 2 self.abs_max_out: 1589.0\n",
      "fc layer 1 self.abs_max_out: 2702.0\n",
      "lif layer 1 self.abs_max_v: 2702.0\n",
      "fc layer 2 self.abs_max_out: 1618.0\n",
      "fc layer 1 self.abs_max_out: 2840.0\n",
      "lif layer 1 self.abs_max_v: 2840.0\n",
      "fc layer 1 self.abs_max_out: 2976.0\n",
      "lif layer 1 self.abs_max_v: 2976.0\n",
      "fc layer 2 self.abs_max_out: 1662.0\n",
      "fc layer 2 self.abs_max_out: 1704.0\n",
      "fc layer 3 self.abs_max_out: 356.0\n",
      "lif layer 2 self.abs_max_v: 1739.0\n",
      "fc layer 3 self.abs_max_out: 377.0\n",
      "lif layer 2 self.abs_max_v: 1757.0\n",
      "fc layer 1 self.abs_max_out: 3017.0\n",
      "lif layer 1 self.abs_max_v: 3017.0\n",
      "fc layer 3 self.abs_max_out: 381.0\n",
      "fc layer 2 self.abs_max_out: 1744.0\n",
      "fc layer 1 self.abs_max_out: 3078.0\n",
      "lif layer 1 self.abs_max_v: 3078.0\n",
      "fc layer 1 self.abs_max_out: 3237.0\n",
      "lif layer 1 self.abs_max_v: 3237.0\n",
      "fc layer 1 self.abs_max_out: 3477.0\n",
      "lif layer 1 self.abs_max_v: 3477.0\n",
      "fc layer 2 self.abs_max_out: 1815.0\n",
      "lif layer 2 self.abs_max_v: 1815.0\n",
      "fc layer 1 self.abs_max_out: 3489.0\n",
      "lif layer 1 self.abs_max_v: 3489.0\n",
      "fc layer 1 self.abs_max_out: 3559.0\n",
      "lif layer 1 self.abs_max_v: 3559.0\n",
      "fc layer 2 self.abs_max_out: 1993.0\n",
      "lif layer 2 self.abs_max_v: 1993.0\n",
      "fc layer 1 self.abs_max_out: 3758.0\n",
      "lif layer 1 self.abs_max_v: 3758.0\n",
      "fc layer 1 self.abs_max_out: 3814.0\n",
      "lif layer 1 self.abs_max_v: 3814.0\n",
      "fc layer 1 self.abs_max_out: 3992.0\n",
      "lif layer 1 self.abs_max_v: 3992.0\n",
      "lif layer 2 self.abs_max_v: 2080.0\n",
      "fc layer 1 self.abs_max_out: 4050.0\n",
      "lif layer 1 self.abs_max_v: 4050.0\n",
      "fc layer 1 self.abs_max_out: 4068.0\n",
      "lif layer 1 self.abs_max_v: 4068.0\n",
      "fc layer 2 self.abs_max_out: 2003.0\n",
      "fc layer 2 self.abs_max_out: 2004.0\n",
      "fc layer 2 self.abs_max_out: 2034.0\n",
      "fc layer 1 self.abs_max_out: 4356.0\n",
      "lif layer 1 self.abs_max_v: 4356.0\n",
      "fc layer 1 self.abs_max_out: 4370.0\n",
      "lif layer 1 self.abs_max_v: 4370.0\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "fc layer 2 self.abs_max_out: 2035.0\n",
      "fc layer 2 self.abs_max_out: 2075.0\n",
      "fc layer 2 self.abs_max_out: 2087.0\n",
      "lif layer 2 self.abs_max_v: 2087.0\n",
      "fc layer 2 self.abs_max_out: 2210.0\n",
      "lif layer 2 self.abs_max_v: 2210.0\n",
      "fc layer 2 self.abs_max_out: 2238.0\n",
      "lif layer 2 self.abs_max_v: 2238.0\n",
      "fc layer 2 self.abs_max_out: 2245.0\n",
      "lif layer 2 self.abs_max_v: 2245.0\n",
      "fc layer 2 self.abs_max_out: 2440.0\n",
      "lif layer 2 self.abs_max_v: 2440.0\n",
      "fc layer 1 self.abs_max_out: 4451.0\n",
      "lif layer 1 self.abs_max_v: 4451.0\n",
      "fc layer 2 self.abs_max_out: 2482.0\n",
      "lif layer 2 self.abs_max_v: 2482.0\n",
      "fc layer 2 self.abs_max_out: 2625.0\n",
      "lif layer 2 self.abs_max_v: 2625.0\n",
      "fc layer 2 self.abs_max_out: 2637.0\n",
      "lif layer 2 self.abs_max_v: 2637.0\n",
      "fc layer 2 self.abs_max_out: 2665.0\n",
      "lif layer 2 self.abs_max_v: 2665.0\n",
      "fc layer 1 self.abs_max_out: 4677.0\n",
      "lif layer 1 self.abs_max_v: 4677.0\n",
      "fc layer 1 self.abs_max_out: 4909.0\n",
      "lif layer 1 self.abs_max_v: 4909.0\n",
      "fc layer 1 self.abs_max_out: 5070.0\n",
      "lif layer 1 self.abs_max_v: 5070.0\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "fc layer 3 self.abs_max_out: 413.0\n",
      "fc layer 1 self.abs_max_out: 5184.0\n",
      "lif layer 1 self.abs_max_v: 5184.0\n",
      "fc layer 1 self.abs_max_out: 5337.0\n",
      "lif layer 1 self.abs_max_v: 5337.0\n",
      "fc layer 1 self.abs_max_out: 5581.0\n",
      "lif layer 1 self.abs_max_v: 5581.0\n",
      "lif layer 2 self.abs_max_v: 2695.0\n",
      "lif layer 2 self.abs_max_v: 2716.0\n",
      "fc layer 1 self.abs_max_out: 5662.0\n",
      "lif layer 1 self.abs_max_v: 5662.0\n",
      "fc layer 1 self.abs_max_out: 5702.0\n",
      "lif layer 1 self.abs_max_v: 5702.0\n",
      "fc layer 1 self.abs_max_out: 5718.0\n",
      "lif layer 1 self.abs_max_v: 5718.0\n",
      "fc layer 1 self.abs_max_out: 5720.0\n",
      "lif layer 1 self.abs_max_v: 5720.0\n",
      "fc layer 1 self.abs_max_out: 6325.0\n",
      "lif layer 1 self.abs_max_v: 6325.0\n",
      "lif layer 2 self.abs_max_v: 2814.0\n",
      "lif layer 2 self.abs_max_v: 2892.0\n",
      "fc layer 2 self.abs_max_out: 2670.0\n",
      "fc layer 1 self.abs_max_out: 6369.0\n",
      "lif layer 1 self.abs_max_v: 6369.0\n",
      "fc layer 1 self.abs_max_out: 6631.0\n",
      "lif layer 1 self.abs_max_v: 6631.0\n",
      "lif layer 2 self.abs_max_v: 2954.0\n",
      "fc layer 2 self.abs_max_out: 2678.0\n",
      "lif layer 1 self.abs_max_v: 6918.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.053401/  2.112299, val:  52.92%, val_best:  52.92%, tr:  94.57%, tr_best:  94.57%, epoch time: 283.31 seconds, 4.72 minutes\n",
      "total_backward_count 44360 real_backward_count 11114  25.054%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 3023.0\n",
      "lif layer 2 self.abs_max_v: 3252.5\n",
      "lif layer 2 self.abs_max_v: 3302.5\n",
      "lif layer 1 self.abs_max_v: 7383.0\n",
      "lif layer 1 self.abs_max_v: 7906.5\n",
      "fc layer 1 self.abs_max_out: 6770.0\n",
      "fc layer 1 self.abs_max_out: 7292.0\n",
      "fc layer 2 self.abs_max_out: 2777.0\n",
      "fc layer 2 self.abs_max_out: 2825.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.038861/  2.119046, val:  44.58%, val_best:  52.92%, tr:  99.32%, tr_best:  99.32%, epoch time: 284.67 seconds, 4.74 minutes\n",
      "total_backward_count 88720 real_backward_count 18838  21.233%\n",
      "fc layer 2 self.abs_max_out: 2886.0\n",
      "fc layer 1 self.abs_max_out: 7591.0\n",
      "lif layer 2 self.abs_max_v: 3352.0\n",
      "fc layer 2 self.abs_max_out: 2908.0\n",
      "fc layer 2 self.abs_max_out: 3012.0\n",
      "lif layer 2 self.abs_max_v: 3475.0\n",
      "fc layer 2 self.abs_max_out: 3160.0\n",
      "lif layer 1 self.abs_max_v: 8192.0\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.035725/  2.120315, val:  51.67%, val_best:  52.92%, tr:  99.14%, tr_best:  99.32%, epoch time: 282.81 seconds, 4.71 minutes\n",
      "total_backward_count 133080 real_backward_count 25867  19.437%\n",
      "fc layer 1 self.abs_max_out: 8051.0\n",
      "lif layer 2 self.abs_max_v: 3549.5\n",
      "lif layer 2 self.abs_max_v: 3592.5\n",
      "lif layer 2 self.abs_max_v: 3671.0\n",
      "lif layer 2 self.abs_max_v: 3751.0\n",
      "lif layer 1 self.abs_max_v: 8582.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.021334/  2.103089, val:  40.00%, val_best:  52.92%, tr:  99.48%, tr_best:  99.48%, epoch time: 284.27 seconds, 4.74 minutes\n",
      "total_backward_count 177440 real_backward_count 32756  18.460%\n",
      "fc layer 1 self.abs_max_out: 8375.0\n",
      "lif layer 1 self.abs_max_v: 8772.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.006233/  2.091118, val:  53.75%, val_best:  53.75%, tr:  99.57%, tr_best:  99.57%, epoch time: 284.68 seconds, 4.74 minutes\n",
      "total_backward_count 221800 real_backward_count 39236  17.690%\n",
      "lif layer 2 self.abs_max_v: 3787.0\n",
      "lif layer 2 self.abs_max_v: 3820.0\n",
      "lif layer 2 self.abs_max_v: 3930.0\n",
      "lif layer 2 self.abs_max_v: 4042.0\n",
      "fc layer 2 self.abs_max_out: 3194.0\n",
      "fc layer 1 self.abs_max_out: 8588.0\n",
      "fc layer 2 self.abs_max_out: 3277.0\n",
      "lif layer 2 self.abs_max_v: 4261.0\n",
      "lif layer 1 self.abs_max_v: 9324.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  1.998361/  2.057616, val:  68.33%, val_best:  68.33%, tr:  99.77%, tr_best:  99.77%, epoch time: 282.60 seconds, 4.71 minutes\n",
      "total_backward_count 266160 real_backward_count 45313  17.025%\n",
      "fc layer 1 self.abs_max_out: 8624.0\n",
      "lif layer 1 self.abs_max_v: 9855.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.983206/  2.061158, val:  67.08%, val_best:  68.33%, tr:  99.84%, tr_best:  99.84%, epoch time: 282.27 seconds, 4.70 minutes\n",
      "total_backward_count 310520 real_backward_count 51253  16.506%\n",
      "fc layer 1 self.abs_max_out: 8742.0\n",
      "lif layer 1 self.abs_max_v: 10554.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.983208/  2.057284, val:  74.58%, val_best:  74.58%, tr:  99.86%, tr_best:  99.86%, epoch time: 282.58 seconds, 4.71 minutes\n",
      "total_backward_count 354880 real_backward_count 57007  16.064%\n",
      "lif layer 2 self.abs_max_v: 4421.5\n",
      "lif layer 2 self.abs_max_v: 4461.0\n",
      "lif layer 2 self.abs_max_v: 4471.5\n",
      "fc layer 1 self.abs_max_out: 8926.0\n",
      "lif layer 2 self.abs_max_v: 4492.0\n",
      "lif layer 2 self.abs_max_v: 4534.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.985991/  2.053661, val:  66.67%, val_best:  74.58%, tr:  99.89%, tr_best:  99.89%, epoch time: 282.54 seconds, 4.71 minutes\n",
      "total_backward_count 399240 real_backward_count 62515  15.659%\n",
      "lif layer 2 self.abs_max_v: 4701.0\n",
      "fc layer 1 self.abs_max_out: 9071.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.974215/  2.040245, val:  65.83%, val_best:  74.58%, tr:  99.93%, tr_best:  99.93%, epoch time: 283.37 seconds, 4.72 minutes\n",
      "total_backward_count 443600 real_backward_count 68012  15.332%\n",
      "fc layer 1 self.abs_max_out: 9110.0\n",
      "lif layer 1 self.abs_max_v: 10978.5\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.972140/  2.048695, val:  74.58%, val_best:  74.58%, tr:  99.89%, tr_best:  99.93%, epoch time: 282.89 seconds, 4.71 minutes\n",
      "total_backward_count 487960 real_backward_count 73171  14.995%\n"
     ]
    }
   ],
   "source": [
    "# # sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# # Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# # wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'random', # 'random', 'bayes', 'grid'\n",
    "#     'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         # \"devices\": {\"values\": [\"1\"]},\n",
    "#         \"single_step\": {\"values\": [True]},\n",
    "#         # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "#         \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "#         # \"my_seed\": {\"values\": [42]},\n",
    "#         \"TIME\": {\"values\": [10]},\n",
    "#         \"BATCH\": {\"values\": [1]},\n",
    "#         \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "#         \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "#         \"data_path\": {\"values\": ['/data2']},\n",
    "#         \"rate_coding\": {\"values\": [False]},\n",
    "#         \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "#         \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "#         \"lif_layer_v_threshold\": {\"values\": [0.5]},\n",
    "#         \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "#         \"lif_layer_sg_width\": {\"values\": [7.0]},\n",
    "#         # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "#         \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "#         \"synapse_conv_stride\": {\"values\": [1]},\n",
    "#         \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "#         \"synapse_trace_const1\": {\"values\": [1]},\n",
    "#         \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "#         \"pre_trained\": {\"values\": [False]},\n",
    "#         \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "#         \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "#         \"net_print\": {\"values\": [True]},\n",
    "\n",
    "#         \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "#         \"learning_rate\": {\"values\": [1/512]}, \n",
    "#         \"epoch_num\": {\"values\": [200]}, \n",
    "#         \"tdBN_on\": {\"values\": [False]},\n",
    "#         \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "#         \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"optimizer_what\": {\"values\": ['SGD']},\n",
    "#         \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "#         \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "#         \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "#         \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "#         \"trace_on\": {\"values\": [False]},\n",
    "#         \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "#         \"merge_polarities\": {\"values\": [True]},\n",
    "#         \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"extra_train_dataset\": {\"values\": [9]},\n",
    "\n",
    "#         \"num_workers\": {\"values\": [2]},\n",
    "#         \"chaching_on\": {\"values\": [True]},\n",
    "#         \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "#         \"UDA_on\": {\"values\": [False]},\n",
    "#         \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "#         \"bias\": {\"values\": [False]},\n",
    "\n",
    "#         \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "#         \"temporal_filter\": {\"values\": [5]},\n",
    "#         \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "#         \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "#         \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "#         \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "#         \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "#         \"scale_exp_1w\": {\"values\": [-10]},\n",
    "#         # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "#         \"scale_exp_2w\": {\"values\": [-10]},\n",
    "#         # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "#         \"scale_exp_3w\": {\"values\": [-9]},\n",
    "#         # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "#     my_snn_system(  \n",
    "#         devices  =  \"5\",\n",
    "#         single_step  =  wandb.config.single_step,\n",
    "#         unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "#         my_seed  =  wandb.config.my_seed,\n",
    "#         TIME  =  wandb.config.TIME,\n",
    "#         BATCH  =  wandb.config.BATCH,\n",
    "#         IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "#         which_data  =  wandb.config.which_data,\n",
    "#         data_path  =  wandb.config.data_path,\n",
    "#         rate_coding  =  wandb.config.rate_coding,\n",
    "#         lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "#         lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "#         lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "#         lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "#         lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "#         synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "#         synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "#         synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "#         synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "#         synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "#         pre_trained  =  wandb.config.pre_trained,\n",
    "#         convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "#         cfg  =  wandb.config.cfg,\n",
    "#         net_print  =  wandb.config.net_print,\n",
    "#         pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "#         learning_rate  =  wandb.config.learning_rate,\n",
    "#         epoch_num  =  wandb.config.epoch_num,\n",
    "#         tdBN_on  =  wandb.config.tdBN_on,\n",
    "#         BN_on  =  wandb.config.BN_on,\n",
    "#         surrogate  =  wandb.config.surrogate,\n",
    "#         BPTT_on  =  wandb.config.BPTT_on,\n",
    "#         optimizer_what  =  wandb.config.optimizer_what,\n",
    "#         scheduler_name  =  wandb.config.scheduler_name,\n",
    "#         ddp_on  =  wandb.config.ddp_on,\n",
    "#         dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "#         dvs_duration  =  wandb.config.dvs_duration,\n",
    "#         DFA_on  =  wandb.config.DFA_on,\n",
    "#         trace_on  =  wandb.config.trace_on,\n",
    "#         OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "#         exclude_class  =  wandb.config.exclude_class,\n",
    "#         merge_polarities  =  wandb.config.merge_polarities,\n",
    "#         denoise_on  =  wandb.config.denoise_on,\n",
    "#         extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "#         num_workers  =  wandb.config.num_workers,\n",
    "#         chaching_on  =  wandb.config.chaching_on,\n",
    "#         pin_memory  =  wandb.config.pin_memory,\n",
    "#         UDA_on  =  wandb.config.UDA_on,\n",
    "#         alpha_uda  =  wandb.config.alpha_uda,\n",
    "#         bias  =  wandb.config.bias,\n",
    "#         last_lif  =  wandb.config.last_lif,\n",
    "#         temporal_filter  =  wandb.config.temporal_filter,\n",
    "#         initial_pooling  =  wandb.config.initial_pooling,\n",
    "#         temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "#         quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "#         scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "#                         ) \n",
    "#     # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "#     # average pooling\n",
    "#     # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "#     # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# sweep_id = 'vz7o3mx3'\n",
    "# # sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
