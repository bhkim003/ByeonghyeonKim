{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21066/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' 레퍼런스\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loader에서 train dataset을 몇개 더 쓸건지 \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "                    ):\n",
    "    ## 함수 내 모든 로컬 변수 저장 ########################################################\n",
    "    hyperparameters = locals()\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFA랑 single_step공존하게해라'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb 세팅 ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader 가져오기 ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        net.load_state_dict(torch.load(pre_trained_path))\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter logging해줌\n",
    "    ############################################################\n",
    "\n",
    "\n",
    "    ## criterion ########################################## # loss 구해주는 친구\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #     criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "        ####### iterator : input_loading & tqdm을 통한 progress_bar 생성###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train 모드로 바꿔줘야함\n",
    "\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch 크기 ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # 차원 전처리\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    slice_bucket.append(slice_concat)\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs 데이터 시각화 코드 (확인 필요할 시 써라)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            ## gradient 초기화 #######################################\n",
    "            optimizer.zero_grad()\n",
    "            ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # net에 넣어줄때는 batch가 젤 앞 차원으로 와야함. # dataparallel때매\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first input도 ottt trace 적용하기 위한 코드 (validation 시에는 필요X) ##########################\n",
    "                if OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight 업데이트!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "                optimizer.step() # full step time update\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # ottt꺼 쓸때\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net 그림 출력해보기 #################################################################\n",
    "            # print('시각화')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch 어긋남 방지 ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval 모드로 바꿔줘야함 \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel 차원은 그대로 두고, Height, Width 차원에 대해서만 pooling 적용\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, 1).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                slice_bucket.append(slice_concat)\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network 연산 시작 ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb 키면 state_dict아닌거는 저장 안됨\n",
    "                    # network save\n",
    "                    # torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            wandb.log({\"iter_acc\": iter_acc})\n",
    "            wandb.log({\"tr_acc\": tr_acc})\n",
    "            wandb.log({\"val_acc_now\": val_acc_now})\n",
    "            wandb.log({\"val_acc_best\": val_acc_best})\n",
    "            wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "            wandb.log({\"val_loss\": val_loss}) \n",
    "            wandb.log({\"tr_epoch_loss\": tr_epoch_loss})   \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.25 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "# const2 = False # True # False\n",
    "\n",
    "# unique_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "# run_name = 'main' ## 이거 설정하면 새로운 경로에 모두 save\n",
    "\n",
    "# if const2 == True:\n",
    "#     const2 = decay\n",
    "# else:\n",
    "#     const2 = 0.0\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"4\",\n",
    "#                 single_step = True, # True # False # DFA_on이랑 같이 가라\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 42,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # 제작하는 dvs에서 TIME넘거나 적으면 자르거나 PADDING함\n",
    "#                 BATCH = 16, # batch norm 할거면 2이상으로 해야함   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 128, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 할거면 time 10으로 해라\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'아직\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.720291189014991,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000, # 10000이상은 hardreset (내 LIF쓰기는 함 ㅇㅇ)\n",
    "#                 lif_layer_sg_width = 3.555718888923306, # 2.570969004857107 # sigmoid류에서는 alpha값 4.0, rectangle류에서는 width값 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # 현재 trace구할 때 현재 spike에 곱해지는 상수. 걍 1로 두셈.\n",
    "#                 synapse_trace_const2 = const2, # 현재 trace구할 때 직전 trace에 곱해지는 상수. lif_layer_v_decay와 같게 할 것을 추천\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # conv에서 10000 이상은 depth-wise separable (BPTT만 지원), 20000이상은 depth-wise (BPTT만 지원)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # 끝에 linear classifier 하나 자동으로 붙습니다\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # True로 하길 추천\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 10000,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # True이면 BPTT, False이면 OTTT  # depthwise, separable은 BPTT만 가능\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 2, #일반적으로 1 또는 2 # 100ms때는 5 # 숫자만큼 크면 spike 아니면 걍 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 아니면 time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # 있는 데이터들 #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # 한 숫자가 1us인듯 (spikingjelly코드에서)\n",
    "#                 # 한 장에 50 timestep만 생산함. 싫으면 my_snn/trying/spikingjelly_dvsgesture의__init__.py 를 참고해봐\n",
    "#                 # nmnist 5_000us, gesture는 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = False, # True # False # single_step이랑 같이 켜야 됨.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # 맨 처음 input에 trace 적용\n",
    "\n",
    "#                 exclude_class = True, # True # False # gesture에서 10번째 클래스 제외\n",
    "\n",
    "#                 merge_polarities = False, # True # False # tonic dvs dataset 에서 polarities 합치기\n",
    "#                 denoise_on = True, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = 0, \n",
    "\n",
    "#                 num_workers = 2, # local wsl에서는 2가 맞고, 서버에서는 4가 좋더라.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = True, # True # False \n",
    "\n",
    "#                 last_lif = False,\n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 8,\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoid와 BN이 있어야 잘된다.\n",
    "# # average pooling  \n",
    "# # 이 낫다. \n",
    "\n",
    "# # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e6g62uay with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_025316-e6g62uay</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e6g62uay' target=\"_blank\">dazzling-sweep-2</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e6g62uay' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e6g62uay</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.5, v_reset=0, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.5, v_reset=0, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.305348/  2.302960, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.283269/  2.223199, val:  23.33%, val_best:  23.33%, tr:  13.48%, tr_best:  13.48%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.025592/  1.882778, val:  37.50%, val_best:  37.50%, tr:  28.40%, tr_best:  28.40%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  1.620456/  1.584238, val:  50.83%, val_best:  50.83%, tr:  49.95%, tr_best:  49.95%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  1.362183/  1.430084, val:  54.58%, val_best:  54.58%, tr:  58.63%, tr_best:  58.63%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  1.210566/  1.301338, val:  66.25%, val_best:  66.25%, tr:  63.02%, tr_best:  63.02%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  1.124862/  1.229525, val:  64.17%, val_best:  66.25%, tr:  63.94%, tr_best:  63.94%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  1.060717/  1.178628, val:  64.58%, val_best:  66.25%, tr:  65.17%, tr_best:  65.17%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  1.006247/  1.132886, val:  66.25%, val_best:  66.25%, tr:  66.60%, tr_best:  66.60%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.970463/  1.151511, val:  64.58%, val_best:  66.25%, tr:  69.87%, tr_best:  69.87%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.940374/  1.121869, val:  63.33%, val_best:  66.25%, tr:  69.66%, tr_best:  69.87%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.905770/  1.074561, val:  66.67%, val_best:  66.67%, tr:  71.09%, tr_best:  71.09%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.885401/  1.044572, val:  70.83%, val_best:  70.83%, tr:  70.89%, tr_best:  71.09%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.866776/  1.051892, val:  65.42%, val_best:  70.83%, tr:  71.71%, tr_best:  71.71%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.820690/  1.070498, val:  64.58%, val_best:  70.83%, tr:  73.44%, tr_best:  73.44%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.812914/  1.049236, val:  67.08%, val_best:  70.83%, tr:  73.54%, tr_best:  73.54%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.800088/  0.994410, val:  72.08%, val_best:  72.08%, tr:  72.93%, tr_best:  73.54%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.762446/  0.990229, val:  71.25%, val_best:  72.08%, tr:  78.04%, tr_best:  78.04%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.745289/  1.059523, val:  67.08%, val_best:  72.08%, tr:  77.83%, tr_best:  78.04%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.751603/  1.035790, val:  69.17%, val_best:  72.08%, tr:  74.46%, tr_best:  78.04%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.710373/  1.009000, val:  75.42%, val_best:  75.42%, tr:  78.14%, tr_best:  78.14%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.697237/  1.009437, val:  72.50%, val_best:  75.42%, tr:  79.67%, tr_best:  79.67%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.699640/  0.994121, val:  74.58%, val_best:  75.42%, tr:  78.04%, tr_best:  79.67%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.665865/  1.013219, val:  77.08%, val_best:  77.08%, tr:  82.33%, tr_best:  82.33%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.644058/  1.013989, val:  74.17%, val_best:  77.08%, tr:  82.53%, tr_best:  82.53%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.626064/  0.994891, val:  78.75%, val_best:  78.75%, tr:  82.84%, tr_best:  82.84%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.609641/  0.993612, val:  76.67%, val_best:  78.75%, tr:  83.66%, tr_best:  83.66%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.585621/  1.020301, val:  77.92%, val_best:  78.75%, tr:  88.36%, tr_best:  88.36%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.582828/  0.989736, val:  78.75%, val_best:  78.75%, tr:  86.93%, tr_best:  88.36%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.554386/  1.017971, val:  77.08%, val_best:  78.75%, tr:  89.79%, tr_best:  89.79%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.534262/  1.016918, val:  78.33%, val_best:  78.75%, tr:  91.01%, tr_best:  91.01%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.532718/  1.006425, val:  80.00%, val_best:  80.00%, tr:  88.56%, tr_best:  91.01%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.510861/  1.006366, val:  82.92%, val_best:  82.92%, tr:  90.60%, tr_best:  91.01%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.499276/  1.005545, val:  81.67%, val_best:  82.92%, tr:  92.13%, tr_best:  92.13%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.472977/  1.045578, val:  75.83%, val_best:  82.92%, tr:  93.67%, tr_best:  93.67%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.466613/  1.046138, val:  79.17%, val_best:  82.92%, tr:  92.65%, tr_best:  93.67%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.429129/  1.008549, val:  81.67%, val_best:  82.92%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.424704/  1.041794, val:  81.67%, val_best:  82.92%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.407545/  0.991063, val:  84.58%, val_best:  84.58%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.385163/  1.015426, val:  82.50%, val_best:  84.58%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.368515/  1.040492, val:  80.83%, val_best:  84.58%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.359079/  1.030577, val:  85.00%, val_best:  85.00%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.333997/  1.028254, val:  83.75%, val_best:  85.00%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.322813/  1.007056, val:  84.17%, val_best:  85.00%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.313967/  1.019967, val:  84.58%, val_best:  85.00%, tr:  97.65%, tr_best:  98.37%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.296699/  1.086769, val:  80.42%, val_best:  85.00%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.285435/  1.022237, val:  87.08%, val_best:  87.08%, tr:  98.06%, tr_best:  98.57%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.275845/  1.063978, val:  82.92%, val_best:  87.08%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.276487/  1.027279, val:  84.58%, val_best:  87.08%, tr:  98.47%, tr_best:  99.49%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.257304/  1.072156, val:  82.08%, val_best:  87.08%, tr:  99.18%, tr_best:  99.49%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.237785/  1.049542, val:  85.83%, val_best:  87.08%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.236817/  1.076383, val:  84.17%, val_best:  87.08%, tr:  98.98%, tr_best:  99.49%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.217921/  1.084160, val:  86.25%, val_best:  87.08%, tr:  99.18%, tr_best:  99.49%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.214221/  1.087714, val:  85.83%, val_best:  87.08%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.199579/  1.079040, val:  87.08%, val_best:  87.08%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.193455/  1.106150, val:  87.08%, val_best:  87.08%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.186360/  1.148375, val:  85.83%, val_best:  87.08%, tr:  99.28%, tr_best:  99.49%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.185955/  1.077002, val:  85.42%, val_best:  87.08%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.166129/  1.101072, val:  86.67%, val_best:  87.08%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.165001/  1.147840, val:  84.58%, val_best:  87.08%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.164851/  1.102435, val:  87.08%, val_best:  87.08%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.152171/  1.120543, val:  86.25%, val_best:  87.08%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.151005/  1.140768, val:  85.42%, val_best:  87.08%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.147430/  1.120879, val:  86.25%, val_best:  87.08%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.133068/  1.131798, val:  87.08%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.131443/  1.182821, val:  85.83%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.130155/  1.164110, val:  86.25%, val_best:  87.08%, tr:  99.59%, tr_best:  99.90%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.126138/  1.181781, val:  87.50%, val_best:  87.50%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.124911/  1.165106, val:  85.42%, val_best:  87.50%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.110115/  1.164879, val:  87.92%, val_best:  87.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.103404/  1.208907, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.114517/  1.197132, val:  87.08%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.106254/  1.233755, val:  87.08%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.117679/  1.196620, val:  86.67%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.106366/  1.211532, val:  87.92%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.099061/  1.235922, val:  87.08%, val_best:  87.92%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.097178/  1.249743, val:  85.00%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.092446/  1.229335, val:  86.25%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.087062/  1.273348, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.084485/  1.225634, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.080611/  1.235262, val:  87.08%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.075561/  1.240718, val:  87.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.072790/  1.299615, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.076978/  1.302543, val:  87.08%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.067335/  1.295514, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.064875/  1.291511, val:  87.08%, val_best:  87.92%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.061877/  1.293163, val:  87.50%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.058837/  1.326874, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.060734/  1.304085, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.054499/  1.335892, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.052488/  1.331931, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.050911/  1.357076, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.053437/  1.340601, val:  87.08%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.051277/  1.348390, val:  87.08%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.046024/  1.368998, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.046415/  1.392410, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.044036/  1.406403, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.044476/  1.380908, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.040266/  1.375937, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.043808/  1.410514, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb8139ab9e345c38e1a5b131aef5d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▄▅▅▅▅▇▃▅▇▆█▆▇██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▅▆▆▆▆▇▆▇▇▇▇█▇▇████▇███████████████████</td></tr><tr><td>tr_acc</td><td>▁▂▅▅▆▆▆▆▆▆▇▇▇▇▇█████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▅▆▆▆▆▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▅▆▆▆▆▇▆▇▇▇▇█▇▇████▇███████████████████</td></tr><tr><td>val_loss</td><td>█▆▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.04381</td></tr><tr><td>val_acc_best</td><td>0.87917</td></tr><tr><td>val_acc_now</td><td>0.86667</td></tr><tr><td>val_loss</td><td>1.41051</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dazzling-sweep-2</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e6g62uay' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/e6g62uay</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_025316-e6g62uay/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nzjg0wpv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_032849-nzjg0wpv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nzjg0wpv' target=\"_blank\">vague-sweep-4</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nzjg0wpv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nzjg0wpv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=10000, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  1.993089/  2.077308, val:  47.50%, val_best:  47.50%, tr:  29.93%, tr_best:  29.93%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.729912/  2.450214, val:  45.42%, val_best:  47.50%, tr:  50.66%, tr_best:  50.66%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.195255/  2.650289, val:  41.67%, val_best:  47.50%, tr:  53.22%, tr_best:  53.22%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.580948/  2.062505, val:  57.50%, val_best:  57.50%, tr:  60.78%, tr_best:  60.78%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.887977/  2.115123, val:  62.08%, val_best:  62.08%, tr:  60.16%, tr_best:  60.78%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.830626/  2.167210, val:  51.25%, val_best:  62.08%, tr:  62.41%, tr_best:  62.41%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  1.494498/  2.103987, val:  60.00%, val_best:  62.08%, tr:  66.19%, tr_best:  66.19%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  1.438142/  2.638152, val:  53.33%, val_best:  62.08%, tr:  69.87%, tr_best:  69.87%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  1.187940/  1.784571, val:  67.92%, val_best:  67.92%, tr:  77.73%, tr_best:  77.73%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  1.263865/  2.173640, val:  67.08%, val_best:  67.92%, tr:  78.96%, tr_best:  78.96%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.971107/  1.265269, val:  79.17%, val_best:  79.17%, tr:  83.66%, tr_best:  83.66%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.862596/  2.102960, val:  70.42%, val_best:  79.17%, tr:  86.52%, tr_best:  86.52%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.804541/  1.385620, val:  80.83%, val_best:  80.83%, tr:  89.07%, tr_best:  89.07%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.729751/  1.511503, val:  77.08%, val_best:  80.83%, tr:  88.66%, tr_best:  89.07%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.668118/  1.869763, val:  76.25%, val_best:  80.83%, tr:  89.79%, tr_best:  89.79%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.675399/  1.802435, val:  74.17%, val_best:  80.83%, tr:  90.60%, tr_best:  90.60%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.762228/  1.685688, val:  80.83%, val_best:  80.83%, tr:  89.99%, tr_best:  90.60%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.553695/  1.439157, val:  84.17%, val_best:  84.17%, tr:  92.95%, tr_best:  92.95%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.499110/  1.737423, val:  78.33%, val_best:  84.17%, tr:  93.67%, tr_best:  93.67%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.444912/  1.485611, val:  79.17%, val_best:  84.17%, tr:  94.59%, tr_best:  94.59%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.458390/  1.627277, val:  82.92%, val_best:  84.17%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.481984/  1.967510, val:  74.17%, val_best:  84.17%, tr:  94.38%, tr_best:  94.89%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.438956/  1.713994, val:  78.75%, val_best:  84.17%, tr:  95.20%, tr_best:  95.20%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.422619/  1.707115, val:  83.33%, val_best:  84.17%, tr:  95.40%, tr_best:  95.40%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.441470/  1.872453, val:  83.33%, val_best:  84.17%, tr:  94.59%, tr_best:  95.40%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.335802/  2.015971, val:  80.42%, val_best:  84.17%, tr:  96.32%, tr_best:  96.32%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.315764/  1.531366, val:  85.83%, val_best:  85.83%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.251734/  1.667628, val:  82.92%, val_best:  85.83%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.257178/  1.659917, val:  84.17%, val_best:  85.83%, tr:  97.75%, tr_best:  97.85%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.259678/  1.749896, val:  83.33%, val_best:  85.83%, tr:  97.75%, tr_best:  97.85%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.185129/  1.796594, val:  81.67%, val_best:  85.83%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.181139/  1.794804, val:  83.33%, val_best:  85.83%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.209217/  1.744469, val:  85.42%, val_best:  85.83%, tr:  98.37%, tr_best:  99.28%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.483822/  2.489495, val:  77.50%, val_best:  85.83%, tr:  93.87%, tr_best:  99.28%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.235020/  2.038628, val:  81.25%, val_best:  85.83%, tr:  98.57%, tr_best:  99.28%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.174602/  1.872519, val:  84.58%, val_best:  85.83%, tr:  98.88%, tr_best:  99.28%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.118469/  1.796408, val:  85.42%, val_best:  85.83%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.120452/  1.829005, val:  85.83%, val_best:  85.83%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.153315/  2.017647, val:  80.42%, val_best:  85.83%, tr:  98.57%, tr_best:  99.80%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.186851/  1.937703, val:  87.08%, val_best:  87.08%, tr:  98.37%, tr_best:  99.80%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.100870/  1.900369, val:  85.83%, val_best:  87.08%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.110354/  1.907427, val:  85.83%, val_best:  87.08%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.119142/  2.048068, val:  83.33%, val_best:  87.08%, tr:  99.18%, tr_best:  99.80%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.100043/  1.976630, val:  83.75%, val_best:  87.08%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.097941/  2.055266, val:  82.92%, val_best:  87.08%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.069074/  1.929617, val:  86.25%, val_best:  87.08%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.071025/  1.932837, val:  88.33%, val_best:  88.33%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.100117/  1.960969, val:  86.67%, val_best:  88.33%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.109444/  2.109787, val:  85.00%, val_best:  88.33%, tr:  99.28%, tr_best:  99.90%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.071891/  2.174250, val:  83.75%, val_best:  88.33%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.097520/  2.076175, val:  87.08%, val_best:  88.33%, tr:  99.39%, tr_best:  99.90%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.056143/  2.131566, val:  87.08%, val_best:  88.33%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.056279/  2.146989, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.049699/  2.146847, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.048815/  2.198425, val:  86.67%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.048515/  2.216054, val:  86.67%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.041732/  2.278817, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.043836/  2.172535, val:  86.67%, val_best:  88.33%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.035061/  2.260784, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.034393/  2.310506, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.030630/  2.256774, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.034913/  2.288978, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.029090/  2.323960, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.030803/  2.331391, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.025464/  2.350976, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.028327/  2.352233, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.028342/  2.344906, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.020782/  2.397384, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.020058/  2.369154, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.019939/  2.382640, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.017831/  2.409299, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.020773/  2.378721, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.019804/  2.433657, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.016361/  2.454263, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.016825/  2.478756, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.014517/  2.470396, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.014511/  2.493803, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.016084/  2.500706, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.022090/  2.546584, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.015852/  2.452391, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.013785/  2.475210, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.013749/  2.481958, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.012195/  2.495928, val:  88.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.010931/  2.545046, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.012535/  2.542833, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.010138/  2.554619, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.010284/  2.560677, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.009360/  2.591159, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.009230/  2.603265, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.010951/  2.620486, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.016054/  2.556149, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.010837/  2.614930, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.009837/  2.611146, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.009956/  2.583970, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.008811/  2.651639, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.008006/  2.613361, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.007779/  2.662695, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.008462/  2.661485, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.006123/  2.649540, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.006981/  2.661954, val:  86.67%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d5ef9979424e9198c1726c46fb8419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▃▆▆▃▇▇█▇█████████▇████████████████████</td></tr><tr><td>summary_val_acc</td><td>▂▁▄▃▅▇▆▇▇▆▇█▇█▇██▇▇█▇███████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▆▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>▇█▇▆▅▄▃▃▂▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▃▃▄▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▂▁▄▃▅▇▆▇▇▆▇█▇█▇██▇▇█▇███████████████████</td></tr><tr><td>val_loss</td><td>▅█▅█▅▁▄▁▂▄▄▂▃▃▄▃▄▅▅▄▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00698</td></tr><tr><td>val_acc_best</td><td>0.8875</td></tr><tr><td>val_acc_now</td><td>0.86667</td></tr><tr><td>val_loss</td><td>2.66195</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vague-sweep-4</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nzjg0wpv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nzjg0wpv</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_032849-nzjg0wpv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gspnfcpl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_040427-gspnfcpl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gspnfcpl' target=\"_blank\">earthy-sweep-6</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gspnfcpl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gspnfcpl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.325540/  2.309361, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.09%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  2.323422/  2.309338, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:   9.50%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  2.319913/  2.316170, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:   9.50%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  2.322621/  2.318868, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.50%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  2.323630/  2.316624, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:   9.50%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  2.314643/  2.312522, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:   9.50%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  2.322383/  2.317189, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  2.323500/  2.311199, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.91%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  2.318820/  2.316814, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:   9.91%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  2.325358/  2.328855, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:   9.91%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  2.318062/  2.313319, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:   9.91%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  2.321499/  2.314362, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:   9.91%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  2.318739/  2.304731, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:   9.91%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  2.323381/  2.312831, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  10.52%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  2.318632/  2.314031, val:  10.00%, val_best:  10.00%, tr:  11.24%, tr_best:  11.24%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  2.334141/  2.312476, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  2.325246/  2.309235, val:  10.00%, val_best:  10.00%, tr:  10.73%, tr_best:  11.24%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  2.313498/  2.304715, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  11.24%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  2.317416/  2.315703, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  11.24%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  2.328925/  2.309338, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  11.24%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  2.325712/  2.311423, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  2.317473/  2.314049, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  11.24%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  2.321084/  2.316247, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  11.24%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  2.323519/  2.311760, val:  10.00%, val_best:  10.00%, tr:   7.97%, tr_best:  11.24%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  2.315132/  2.309812, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  11.24%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  2.321132/  2.308563, val:  10.00%, val_best:  10.00%, tr:   7.56%, tr_best:  11.24%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  2.324926/  2.309820, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  11.24%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  2.315943/  2.313346, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  11.24%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  2.320931/  2.314629, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  11.24%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  2.316856/  2.309039, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  11.24%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  2.317568/  2.312462, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  11.24%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  2.322536/  2.308460, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  11.24%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  2.317184/  2.325424, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  11.24%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  2.323347/  2.308666, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  11.24%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  2.324209/  2.309332, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  11.24%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  2.317179/  2.315681, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  11.24%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  2.326358/  2.314513, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  11.24%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  2.318460/  2.316844, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  11.24%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  2.317593/  2.308844, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  11.24%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  2.322765/  2.307623, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  11.24%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  2.325134/  2.308829, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  11.24%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  2.314856/  2.311299, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  11.24%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  2.277522/  2.138597, val:  21.67%, val_best:  21.67%, tr:  14.81%, tr_best:  14.81%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  1.764215/  1.546227, val:  54.17%, val_best:  54.17%, tr:  36.87%, tr_best:  36.87%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  1.216397/  1.314505, val:  55.83%, val_best:  55.83%, tr:  59.45%, tr_best:  59.45%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  1.016427/  1.363510, val:  55.83%, val_best:  55.83%, tr:  63.33%, tr_best:  63.33%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.972866/  1.294479, val:  55.83%, val_best:  55.83%, tr:  64.04%, tr_best:  64.04%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.888813/  1.102557, val:  65.42%, val_best:  65.42%, tr:  67.21%, tr_best:  67.21%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.861634/  1.079863, val:  65.00%, val_best:  65.42%, tr:  70.17%, tr_best:  70.17%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.756666/  1.123880, val:  70.00%, val_best:  70.00%, tr:  71.71%, tr_best:  71.71%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.670536/  1.085299, val:  70.42%, val_best:  70.42%, tr:  79.57%, tr_best:  79.57%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.642411/  1.264083, val:  66.25%, val_best:  70.42%, tr:  79.26%, tr_best:  79.57%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.563546/  1.161767, val:  75.42%, val_best:  75.42%, tr:  86.41%, tr_best:  86.41%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.449829/  1.172783, val:  76.67%, val_best:  76.67%, tr:  92.85%, tr_best:  92.85%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.392793/  1.284715, val:  79.58%, val_best:  79.58%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.381598/  1.093253, val:  80.42%, val_best:  80.42%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.318521/  1.237158, val:  79.17%, val_best:  80.42%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.314829/  1.016331, val:  84.17%, val_best:  84.17%, tr:  95.20%, tr_best:  95.20%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.238078/  1.013111, val:  87.92%, val_best:  87.92%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.276902/  1.061188, val:  81.67%, val_best:  87.92%, tr:  96.42%, tr_best:  96.73%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.193154/  1.157949, val:  80.00%, val_best:  87.92%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.227784/  1.059412, val:  82.08%, val_best:  87.92%, tr:  95.40%, tr_best:  98.26%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.179970/  1.108622, val:  84.58%, val_best:  87.92%, tr:  98.16%, tr_best:  98.26%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.100765/  1.087476, val:  84.58%, val_best:  87.92%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.079933/  1.071972, val:  88.33%, val_best:  88.33%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.051791/  1.103275, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.068498/  1.197131, val:  84.58%, val_best:  88.33%, tr:  99.28%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.041280/  1.198850, val:  86.25%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.088030/  1.188217, val:  86.67%, val_best:  88.33%, tr:  98.98%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.081092/  1.247387, val:  85.83%, val_best:  88.33%, tr:  99.18%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.077170/  1.223753, val:  85.00%, val_best:  88.33%, tr:  98.77%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.062262/  1.215459, val:  86.25%, val_best:  88.33%, tr:  99.39%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.031109/  1.311997, val:  88.75%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.025842/  1.300782, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.093587/  1.256246, val:  85.00%, val_best:  88.75%, tr:  98.16%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.041842/  1.285981, val:  84.17%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.029539/  1.374832, val:  82.08%, val_best:  88.75%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.025087/  1.462474, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.043390/  1.329365, val:  85.42%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.065333/  1.308024, val:  84.17%, val_best:  88.75%, tr:  99.08%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.072300/  1.336883, val:  85.42%, val_best:  88.75%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.034065/  1.328092, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.028617/  1.392071, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.015001/  1.340954, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.006791/  1.422867, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.003704/  1.455345, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.002022/  1.431790, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.001534/  1.428908, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.001422/  1.436013, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.001274/  1.440970, val:  82.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.001122/  1.444920, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.001053/  1.443941, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000988/  1.446333, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.000832/  1.444845, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000780/  1.437078, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000710/  1.437858, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000674/  1.446310, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000669/  1.466974, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000644/  1.477974, val:  83.33%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000620/  1.478524, val:  83.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446b4054cc104fec9a1dd7f34fd11da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▃▁▁▂▁▁▁▁▁▁▁▂▂▂▁▂▂▄▇▅▆███████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▅▆▆▇▇█▇▇██████████▇███</td></tr><tr><td>tr_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▅▆▆▇██████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██████████████████▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▅▆▆▇▇█████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▅▆▆▇▇█▇▇██████████▇███</td></tr><tr><td>val_loss</td><td>█████████████████▇▃▁▂▂▂▁▁▁▁▂▂▃▂▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00062</td></tr><tr><td>val_acc_best</td><td>0.8875</td></tr><tr><td>val_acc_now</td><td>0.8375</td></tr><tr><td>val_loss</td><td>1.47852</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earthy-sweep-6</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gspnfcpl' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gspnfcpl</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_040427-gspnfcpl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ope1e2rt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_043903-ope1e2rt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ope1e2rt' target=\"_blank\">absurd-sweep-8</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ope1e2rt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ope1e2rt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=0, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  1.526269/  1.556458, val:  49.58%, val_best:  49.58%, tr:  48.42%, tr_best:  48.42%\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.139216/  1.269154, val:  60.83%, val_best:  60.83%, tr:  58.32%, tr_best:  58.32%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.014264/  1.202661, val:  58.33%, val_best:  60.83%, tr:  63.94%, tr_best:  63.94%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  0.882306/  1.316880, val:  54.58%, val_best:  60.83%, tr:  68.13%, tr_best:  68.13%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  0.876206/  1.122699, val:  66.25%, val_best:  66.25%, tr:  66.60%, tr_best:  68.13%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  0.743325/  1.066134, val:  66.25%, val_best:  66.25%, tr:  72.01%, tr_best:  72.01%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.718035/  1.112340, val:  67.08%, val_best:  67.08%, tr:  74.36%, tr_best:  74.36%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.703441/  1.017985, val:  79.58%, val_best:  79.58%, tr:  77.83%, tr_best:  77.83%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.577943/  1.091015, val:  73.33%, val_best:  79.58%, tr:  83.35%, tr_best:  83.35%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.504861/  1.059601, val:  79.17%, val_best:  79.58%, tr:  87.54%, tr_best:  87.54%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.472743/  0.947803, val:  83.33%, val_best:  83.33%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.372281/  1.186748, val:  79.17%, val_best:  83.33%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.342711/  0.957317, val:  84.17%, val_best:  84.17%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.294468/  0.928234, val:  85.42%, val_best:  85.42%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.251151/  1.314922, val:  79.58%, val_best:  85.42%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.279916/  1.006261, val:  84.17%, val_best:  85.42%, tr:  95.30%, tr_best:  96.42%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.202619/  1.071818, val:  83.75%, val_best:  85.42%, tr:  97.24%, tr_best:  97.24%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.182492/  1.041913, val:  87.50%, val_best:  87.50%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.144059/  1.122663, val:  82.08%, val_best:  87.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.127926/  1.079296, val:  84.17%, val_best:  87.50%, tr:  98.37%, tr_best:  98.98%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.119451/  1.104017, val:  85.00%, val_best:  87.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.133497/  1.169786, val:  79.58%, val_best:  87.50%, tr:  98.37%, tr_best:  98.98%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.108530/  1.132643, val:  83.75%, val_best:  87.50%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.136418/  1.024020, val:  85.42%, val_best:  87.50%, tr:  97.96%, tr_best:  99.08%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.069642/  1.005154, val:  86.67%, val_best:  87.50%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.045574/  1.098330, val:  86.67%, val_best:  87.50%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.031732/  1.060121, val:  87.92%, val_best:  87.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.024350/  1.143095, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.023456/  1.066219, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.015673/  1.124810, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.014229/  1.112759, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.017955/  1.057539, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.031168/  1.186865, val:  87.92%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.044767/  1.277637, val:  83.75%, val_best:  89.17%, tr:  99.49%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.048629/  1.210930, val:  87.92%, val_best:  89.17%, tr:  99.18%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.031460/  1.183444, val:  89.17%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.029969/  1.290003, val:  85.00%, val_best:  89.17%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.026512/  1.261636, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.038796/  1.315712, val:  85.42%, val_best:  89.17%, tr:  99.59%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.054663/  1.183051, val:  86.25%, val_best:  89.17%, tr:  99.28%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.018096/  1.171476, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.041490/  1.216439, val:  87.92%, val_best:  89.17%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.093661/  1.301069, val:  85.00%, val_best:  89.17%, tr:  98.77%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.065969/  1.325477, val:  83.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.085011/  1.370457, val:  83.75%, val_best:  89.17%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.078595/  1.158151, val:  85.42%, val_best:  89.17%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.031392/  1.109428, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.013553/  1.169338, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.006933/  1.243975, val:  82.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.005076/  1.175945, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.005148/  1.154943, val:  86.25%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.002679/  1.145081, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.002416/  1.204638, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.001117/  1.199851, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.000925/  1.223167, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.000713/  1.211436, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.000633/  1.210863, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.000549/  1.206105, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.000515/  1.210784, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.000490/  1.212724, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.000472/  1.216333, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.000451/  1.217247, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.000435/  1.221769, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.000414/  1.224691, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.000403/  1.227379, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.000386/  1.226591, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.000365/  1.228100, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.000353/  1.229369, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.000340/  1.232037, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.000328/  1.237938, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.000319/  1.237968, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.000313/  1.245556, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.000295/  1.248993, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.000297/  1.248363, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.000286/  1.248981, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.000279/  1.252537, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.000275/  1.259191, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.000269/  1.258229, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.000258/  1.266262, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.000255/  1.266321, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.000247/  1.266984, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.000246/  1.270273, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.000241/  1.274834, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.000237/  1.275948, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.000227/  1.272283, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.000227/  1.268024, val:  88.75%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.000218/  1.267045, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.000214/  1.268943, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.000215/  1.274530, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.000212/  1.282916, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.000215/  1.282274, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.000216/  1.286041, val:  88.33%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000205/  1.287693, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.000198/  1.288489, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.000192/  1.277791, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.000188/  1.277841, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000182/  1.280989, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000196/  1.274177, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000179/  1.273451, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000173/  1.273145, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a88612ff544053ac64d59520eb0b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▆▅▅▅▇▇▇████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▆▆▇▆█▇▆█████▇▇▇▇█████████████████████</td></tr><tr><td>tr_acc</td><td>▁▃▃▅▆▇██████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▆▆▇▇█████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▆▆▇▆█▇▆█████▇▇▇▇█████████████████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▅▂▂▃▂▂▃▄▄▅▄▅▆▃▄▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00017</td></tr><tr><td>val_acc_best</td><td>0.89167</td></tr><tr><td>val_acc_now</td><td>0.86667</td></tr><tr><td>val_loss</td><td>1.27314</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">absurd-sweep-8</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ope1e2rt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ope1e2rt</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_043903-ope1e2rt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 62l8gfhw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_051248-62l8gfhw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/62l8gfhw' target=\"_blank\">sleek-sweep-10</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/62l8gfhw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/62l8gfhw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=4, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.305372/  2.302864, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.305082/  2.302657, val:  10.00%, val_best:  10.00%, tr:   8.27%, tr_best:  10.01%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.305119/  2.303084, val:  10.42%, val_best:  10.42%, tr:   8.17%, tr_best:  10.01%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.297898/  2.281978, val:  21.25%, val_best:  21.25%, tr:  12.16%, tr_best:  12.16%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  2.246681/  2.206324, val:  26.25%, val_best:  26.25%, tr:  13.99%, tr_best:  13.99%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  2.118773/  2.059411, val:  43.75%, val_best:  43.75%, tr:  30.95%, tr_best:  30.95%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  1.897571/  1.839160, val:  47.50%, val_best:  47.50%, tr:  42.80%, tr_best:  42.80%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  1.685561/  1.692306, val:  53.75%, val_best:  53.75%, tr:  51.07%, tr_best:  51.07%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  1.533111/  1.577728, val:  60.00%, val_best:  60.00%, tr:  59.55%, tr_best:  59.55%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  1.407374/  1.514533, val:  58.75%, val_best:  60.00%, tr:  63.53%, tr_best:  63.53%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  1.306319/  1.427309, val:  60.83%, val_best:  60.83%, tr:  64.15%, tr_best:  64.15%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  1.224795/  1.358869, val:  62.50%, val_best:  62.50%, tr:  64.96%, tr_best:  64.96%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  1.162650/  1.312099, val:  64.17%, val_best:  64.17%, tr:  66.39%, tr_best:  66.39%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  1.118308/  1.285101, val:  63.75%, val_best:  64.17%, tr:  67.82%, tr_best:  67.82%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  1.066995/  1.274234, val:  62.50%, val_best:  64.17%, tr:  68.74%, tr_best:  68.74%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  1.037699/  1.253597, val:  65.83%, val_best:  65.83%, tr:  71.30%, tr_best:  71.30%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.988725/  1.227091, val:  73.75%, val_best:  73.75%, tr:  74.77%, tr_best:  74.77%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.948544/  1.213789, val:  76.25%, val_best:  76.25%, tr:  81.72%, tr_best:  81.72%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.918776/  1.235916, val:  75.00%, val_best:  76.25%, tr:  82.02%, tr_best:  82.02%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.889099/  1.226539, val:  73.33%, val_best:  76.25%, tr:  79.37%, tr_best:  82.02%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.848645/  1.201902, val:  78.33%, val_best:  78.33%, tr:  85.39%, tr_best:  85.39%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.812567/  1.166852, val:  75.42%, val_best:  78.33%, tr:  86.31%, tr_best:  86.31%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.794005/  1.161113, val:  80.42%, val_best:  80.42%, tr:  86.82%, tr_best:  86.82%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.749873/  1.174030, val:  82.50%, val_best:  82.50%, tr:  90.09%, tr_best:  90.09%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.714863/  1.163029, val:  79.17%, val_best:  82.50%, tr:  91.42%, tr_best:  91.42%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.694550/  1.141033, val:  83.33%, val_best:  83.33%, tr:  92.03%, tr_best:  92.03%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.668338/  1.150780, val:  81.67%, val_best:  83.33%, tr:  92.85%, tr_best:  92.85%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.640238/  1.182589, val:  82.08%, val_best:  83.33%, tr:  93.36%, tr_best:  93.36%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.627965/  1.136018, val:  84.58%, val_best:  84.58%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.596166/  1.156790, val:  82.92%, val_best:  84.58%, tr:  93.67%, tr_best:  93.77%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.585503/  1.156169, val:  82.92%, val_best:  84.58%, tr:  94.99%, tr_best:  94.99%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.574289/  1.162388, val:  83.33%, val_best:  84.58%, tr:  94.59%, tr_best:  94.99%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.545182/  1.151830, val:  82.08%, val_best:  84.58%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.532766/  1.150114, val:  82.92%, val_best:  84.58%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.515676/  1.165242, val:  82.50%, val_best:  84.58%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.498276/  1.171257, val:  81.25%, val_best:  84.58%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.482462/  1.181008, val:  82.92%, val_best:  84.58%, tr:  96.94%, tr_best:  96.94%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.472935/  1.170564, val:  83.75%, val_best:  84.58%, tr:  96.73%, tr_best:  96.94%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.455467/  1.155678, val:  83.75%, val_best:  84.58%, tr:  96.32%, tr_best:  96.94%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.442854/  1.156319, val:  85.00%, val_best:  85.00%, tr:  97.55%, tr_best:  97.55%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.426566/  1.166896, val:  83.33%, val_best:  85.00%, tr:  97.04%, tr_best:  97.55%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.417638/  1.167061, val:  86.67%, val_best:  86.67%, tr:  97.14%, tr_best:  97.55%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.406889/  1.183165, val:  85.42%, val_best:  86.67%, tr:  97.45%, tr_best:  97.55%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.394267/  1.194796, val:  86.25%, val_best:  86.67%, tr:  97.65%, tr_best:  97.65%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.379140/  1.188075, val:  84.17%, val_best:  86.67%, tr:  97.96%, tr_best:  97.96%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.368313/  1.181667, val:  84.17%, val_best:  86.67%, tr:  97.65%, tr_best:  97.96%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.356540/  1.200944, val:  83.75%, val_best:  86.67%, tr:  97.85%, tr_best:  97.96%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.347111/  1.194761, val:  86.25%, val_best:  86.67%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.339177/  1.204675, val:  84.17%, val_best:  86.67%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.331586/  1.202665, val:  85.83%, val_best:  86.67%, tr:  98.26%, tr_best:  98.37%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.322606/  1.203280, val:  86.25%, val_best:  86.67%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.312685/  1.210598, val:  85.83%, val_best:  86.67%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.305202/  1.231709, val:  85.83%, val_best:  86.67%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.301522/  1.218172, val:  85.83%, val_best:  86.67%, tr:  98.67%, tr_best:  98.67%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.282468/  1.239412, val:  87.50%, val_best:  87.50%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.272406/  1.240809, val:  86.67%, val_best:  87.50%, tr:  98.88%, tr_best:  98.88%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.265455/  1.258316, val:  85.42%, val_best:  87.50%, tr:  98.77%, tr_best:  98.88%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.260707/  1.241255, val:  87.50%, val_best:  87.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.249864/  1.243305, val:  86.25%, val_best:  87.50%, tr:  98.88%, tr_best:  98.98%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.240594/  1.256953, val:  86.67%, val_best:  87.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.246814/  1.260365, val:  86.67%, val_best:  87.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.236916/  1.270949, val:  87.50%, val_best:  87.50%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.234842/  1.293277, val:  86.25%, val_best:  87.50%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.222449/  1.281220, val:  87.92%, val_best:  87.92%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.207075/  1.316371, val:  86.67%, val_best:  87.92%, tr:  99.28%, tr_best:  99.39%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.203895/  1.296565, val:  88.75%, val_best:  88.75%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.202451/  1.309478, val:  87.50%, val_best:  88.75%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.203507/  1.315302, val:  86.67%, val_best:  88.75%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.197483/  1.307776, val:  87.50%, val_best:  88.75%, tr:  99.39%, tr_best:  99.49%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.193189/  1.341623, val:  86.25%, val_best:  88.75%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.186514/  1.350051, val:  87.08%, val_best:  88.75%, tr:  99.49%, tr_best:  99.49%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.183165/  1.361495, val:  87.50%, val_best:  88.75%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.172781/  1.368684, val:  86.67%, val_best:  88.75%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.177018/  1.345154, val:  87.50%, val_best:  88.75%, tr:  99.49%, tr_best:  99.80%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.166027/  1.346669, val:  86.67%, val_best:  88.75%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.163359/  1.376185, val:  86.67%, val_best:  88.75%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.156916/  1.399923, val:  86.67%, val_best:  88.75%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.156231/  1.401568, val:  86.67%, val_best:  88.75%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.151994/  1.383623, val:  87.50%, val_best:  88.75%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.152017/  1.404256, val:  87.50%, val_best:  88.75%, tr:  99.69%, tr_best:  99.80%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.145661/  1.405884, val:  87.08%, val_best:  88.75%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.140150/  1.416792, val:  87.08%, val_best:  88.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.134043/  1.433110, val:  86.25%, val_best:  88.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.140539/  1.444030, val:  87.92%, val_best:  88.75%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.131514/  1.443385, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.129848/  1.451238, val:  86.25%, val_best:  88.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.122735/  1.453545, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.120040/  1.467417, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.123081/  1.465131, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.116294/  1.489342, val:  87.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.111408/  1.480450, val:  87.08%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.106831/  1.467473, val:  86.25%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.104314/  1.497277, val:  87.50%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.105173/  1.484898, val:  87.50%, val_best:  88.75%, tr:  99.80%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.100320/  1.504547, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.099076/  1.510852, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.094041/  1.524639, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.094806/  1.511969, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.090624/  1.523838, val:  87.92%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.092187/  1.536783, val:  87.08%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9366b3d0eb6e476885ef5a53b829fad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▃▅▆▆▇▅▆▇███▇██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▂▅▅▆▆▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▁▁▄▅▅▆▇▆▇▇▇████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>███▆▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▂▅▅▆▆▇▇▇▇█████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▂▅▅▆▆▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>val_loss</td><td>██▇▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.09219</td></tr><tr><td>val_acc_best</td><td>0.8875</td></tr><tr><td>val_acc_now</td><td>0.87083</td></tr><tr><td>val_loss</td><td>1.53678</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sleek-sweep-10</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/62l8gfhw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/62l8gfhw</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_051248-62l8gfhw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gbvrg87w with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_054754-gbvrg87w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gbvrg87w' target=\"_blank\">deft-sweep-12</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gbvrg87w' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gbvrg87w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 1, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = da7e4b41c275f954fb82e708e43d2b96\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=1, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.305593/  2.302397, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:   9.70%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.246095/  2.052552, val:  31.67%, val_best:  31.67%, tr:  14.81%, tr_best:  14.81%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  1.683559/  1.551112, val:  41.25%, val_best:  41.25%, tr:  44.74%, tr_best:  44.74%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  1.352428/  1.438926, val:  47.50%, val_best:  47.50%, tr:  56.28%, tr_best:  56.28%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  1.241741/  1.389992, val:  51.67%, val_best:  51.67%, tr:  58.43%, tr_best:  58.43%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  1.159588/  1.301018, val:  56.67%, val_best:  56.67%, tr:  59.86%, tr_best:  59.86%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  1.112239/  1.243883, val:  55.83%, val_best:  56.67%, tr:  62.51%, tr_best:  62.51%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  1.071507/  1.218207, val:  55.42%, val_best:  56.67%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  1.025334/  1.217258, val:  59.17%, val_best:  59.17%, tr:  64.56%, tr_best:  64.56%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  0.999309/  1.196404, val:  59.17%, val_best:  59.17%, tr:  68.85%, tr_best:  68.85%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  0.985670/  1.192949, val:  57.92%, val_best:  59.17%, tr:  66.19%, tr_best:  68.85%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.954632/  1.170288, val:  62.92%, val_best:  62.92%, tr:  68.54%, tr_best:  68.85%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.933174/  1.156130, val:  61.67%, val_best:  62.92%, tr:  71.40%, tr_best:  71.40%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.921128/  1.175394, val:  61.25%, val_best:  62.92%, tr:  71.60%, tr_best:  71.60%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.875659/  1.248307, val:  58.33%, val_best:  62.92%, tr:  72.32%, tr_best:  72.32%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.863151/  1.136109, val:  70.00%, val_best:  70.00%, tr:  75.28%, tr_best:  75.28%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.862350/  1.136610, val:  63.33%, val_best:  70.00%, tr:  72.22%, tr_best:  75.28%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.827393/  1.117911, val:  67.50%, val_best:  70.00%, tr:  76.61%, tr_best:  76.61%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.802334/  1.162893, val:  64.17%, val_best:  70.00%, tr:  76.92%, tr_best:  76.92%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.811603/  1.179113, val:  63.33%, val_best:  70.00%, tr:  76.51%, tr_best:  76.92%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.776992/  1.157954, val:  65.00%, val_best:  70.00%, tr:  79.78%, tr_best:  79.78%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.766321/  1.138117, val:  66.67%, val_best:  70.00%, tr:  80.08%, tr_best:  80.08%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.747001/  1.127515, val:  72.92%, val_best:  72.92%, tr:  80.69%, tr_best:  80.69%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.716244/  1.134734, val:  68.75%, val_best:  72.92%, tr:  83.04%, tr_best:  83.04%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.698887/  1.176207, val:  70.42%, val_best:  72.92%, tr:  85.50%, tr_best:  85.50%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.680062/  1.131267, val:  75.42%, val_best:  75.42%, tr:  84.47%, tr_best:  85.50%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.669867/  1.134001, val:  74.58%, val_best:  75.42%, tr:  86.93%, tr_best:  86.93%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.651067/  1.156371, val:  73.75%, val_best:  75.42%, tr:  87.33%, tr_best:  87.33%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.653142/  1.152967, val:  73.75%, val_best:  75.42%, tr:  88.36%, tr_best:  88.36%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.603478/  1.158612, val:  75.00%, val_best:  75.42%, tr:  91.11%, tr_best:  91.11%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.586558/  1.201191, val:  73.33%, val_best:  75.42%, tr:  91.83%, tr_best:  91.83%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.610895/  1.186234, val:  73.75%, val_best:  75.42%, tr:  88.15%, tr_best:  91.83%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.578013/  1.185857, val:  77.50%, val_best:  77.50%, tr:  90.60%, tr_best:  91.83%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.564622/  1.184147, val:  72.50%, val_best:  77.50%, tr:  92.95%, tr_best:  92.95%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.540289/  1.229822, val:  69.17%, val_best:  77.50%, tr:  93.16%, tr_best:  93.16%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.545175/  1.216838, val:  72.92%, val_best:  77.50%, tr:  93.16%, tr_best:  93.16%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.508492/  1.241327, val:  71.67%, val_best:  77.50%, tr:  93.97%, tr_best:  93.97%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.500894/  1.224139, val:  70.83%, val_best:  77.50%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.489436/  1.211455, val:  75.00%, val_best:  77.50%, tr:  95.61%, tr_best:  95.61%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.477565/  1.251599, val:  74.17%, val_best:  77.50%, tr:  96.02%, tr_best:  96.02%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.469387/  1.276238, val:  75.00%, val_best:  77.50%, tr:  95.20%, tr_best:  96.02%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.453562/  1.253469, val:  75.42%, val_best:  77.50%, tr:  96.63%, tr_best:  96.63%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.445886/  1.284688, val:  75.00%, val_best:  77.50%, tr:  96.73%, tr_best:  96.73%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.432268/  1.318462, val:  73.33%, val_best:  77.50%, tr:  96.94%, tr_best:  96.94%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.417831/  1.287113, val:  74.58%, val_best:  77.50%, tr:  96.42%, tr_best:  96.94%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.403225/  1.350138, val:  73.75%, val_best:  77.50%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.394952/  1.299908, val:  75.83%, val_best:  77.50%, tr:  98.16%, tr_best:  98.16%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.386958/  1.339730, val:  75.00%, val_best:  77.50%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.378512/  1.346148, val:  76.67%, val_best:  77.50%, tr:  98.16%, tr_best:  98.26%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.375681/  1.362485, val:  73.33%, val_best:  77.50%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.362645/  1.373495, val:  73.75%, val_best:  77.50%, tr:  98.06%, tr_best:  98.37%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.355690/  1.364734, val:  75.83%, val_best:  77.50%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.336601/  1.396067, val:  76.67%, val_best:  77.50%, tr:  98.77%, tr_best:  98.77%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.331910/  1.392170, val:  76.67%, val_best:  77.50%, tr:  98.98%, tr_best:  98.98%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.324370/  1.421457, val:  75.00%, val_best:  77.50%, tr:  98.77%, tr_best:  98.98%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.311707/  1.409466, val:  77.92%, val_best:  77.92%, tr:  98.88%, tr_best:  98.98%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.310677/  1.450256, val:  75.83%, val_best:  77.92%, tr:  98.88%, tr_best:  98.98%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.302107/  1.446675, val:  77.08%, val_best:  77.92%, tr:  99.08%, tr_best:  99.08%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.295850/  1.485601, val:  76.67%, val_best:  77.92%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.283544/  1.468610, val:  76.67%, val_best:  77.92%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.277314/  1.486914, val:  77.50%, val_best:  77.92%, tr:  99.18%, tr_best:  99.39%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.272460/  1.517341, val:  76.25%, val_best:  77.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.272540/  1.530102, val:  75.83%, val_best:  77.92%, tr:  99.59%, tr_best:  99.80%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.258670/  1.506254, val:  77.08%, val_best:  77.92%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.254312/  1.543554, val:  78.33%, val_best:  78.33%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.252962/  1.547873, val:  77.92%, val_best:  78.33%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.242295/  1.556520, val:  79.17%, val_best:  79.17%, tr:  99.69%, tr_best:  99.90%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.237135/  1.560091, val:  77.92%, val_best:  79.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.232867/  1.623475, val:  76.67%, val_best:  79.17%, tr:  99.80%, tr_best:  99.90%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.225804/  1.604775, val:  77.08%, val_best:  79.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.220046/  1.601068, val:  76.67%, val_best:  79.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.216733/  1.633218, val:  77.92%, val_best:  79.17%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.205316/  1.645595, val:  76.25%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.205802/  1.618270, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.198610/  1.641940, val:  78.75%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.200948/  1.646805, val:  80.42%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.191177/  1.691654, val:  78.75%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.188200/  1.699150, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.191251/  1.723879, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.180192/  1.718365, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.172828/  1.721243, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.170854/  1.743666, val:  78.75%, val_best:  80.42%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.169552/  1.749319, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.161991/  1.763229, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.161420/  1.760161, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.157237/  1.824304, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.153584/  1.766244, val:  79.58%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.153281/  1.825151, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.151017/  1.831870, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.137021/  1.841398, val:  80.00%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.143629/  1.859086, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.143271/  1.874506, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.134339/  1.870013, val:  78.75%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.133256/  1.859683, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.133323/  1.919500, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.126354/  1.930069, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.123465/  1.921808, val:  78.33%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.121709/  1.935119, val:  77.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.117868/  1.921096, val:  77.92%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.116006/  1.943420, val:  79.17%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab13c81e704454194d2488fd530eb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▆▅▅▅▆▆▃▆██▇▇▆██████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▄▅▆▆▆▆▇▆▇▇▇▇█▇▇▇▇▇▇▇█▇█████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▅▅▆▆▆▆▆▆▇▇▇▇▇█████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▄▅▆▆▆▆▇▇▇▇█████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▄▅▆▆▆▆▇▆▇▇▇▇█▇▇▇▇▇▇▇█▇█████████████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▁▁▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.11601</td></tr><tr><td>val_acc_best</td><td>0.80417</td></tr><tr><td>val_acc_now</td><td>0.79167</td></tr><tr><td>val_loss</td><td>1.94342</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deft-sweep-12</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gbvrg87w' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/gbvrg87w</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_054754-gbvrg87w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 01fuo5rc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_062240-01fuo5rc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/01fuo5rc' target=\"_blank\">dainty-sweep-14</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/01fuo5rc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/01fuo5rc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = da7e4b41c275f954fb82e708e43d2b96\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0010000'], tr/val_loss:  2.305372/  2.302864, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0010000'], tr/val_loss:  2.305171/  2.303511, val:  10.42%, val_best:  10.42%, tr:   8.07%, tr_best:  10.01%\n",
      "epoch-2   lr=['0.0010000'], tr/val_loss:  2.273021/  2.202029, val:  27.50%, val_best:  27.50%, tr:  16.55%, tr_best:  16.55%\n",
      "epoch-3   lr=['0.0010000'], tr/val_loss:  2.046102/  1.892613, val:  42.50%, val_best:  42.50%, tr:  30.13%, tr_best:  30.13%\n",
      "epoch-4   lr=['0.0010000'], tr/val_loss:  1.723305/  1.667272, val:  50.00%, val_best:  50.00%, tr:  47.70%, tr_best:  47.70%\n",
      "epoch-5   lr=['0.0010000'], tr/val_loss:  1.504637/  1.497215, val:  57.50%, val_best:  57.50%, tr:  56.69%, tr_best:  56.69%\n",
      "epoch-6   lr=['0.0010000'], tr/val_loss:  1.354928/  1.395778, val:  55.42%, val_best:  57.50%, tr:  62.72%, tr_best:  62.72%\n",
      "epoch-7   lr=['0.0010000'], tr/val_loss:  1.231523/  1.320029, val:  56.25%, val_best:  57.50%, tr:  63.43%, tr_best:  63.43%\n",
      "epoch-8   lr=['0.0010000'], tr/val_loss:  1.139765/  1.293623, val:  60.83%, val_best:  60.83%, tr:  67.21%, tr_best:  67.21%\n",
      "epoch-9   lr=['0.0010000'], tr/val_loss:  1.072244/  1.297422, val:  58.33%, val_best:  60.83%, tr:  68.95%, tr_best:  68.95%\n",
      "epoch-10  lr=['0.0010000'], tr/val_loss:  1.017425/  1.267374, val:  58.33%, val_best:  60.83%, tr:  70.28%, tr_best:  70.28%\n",
      "epoch-11  lr=['0.0010000'], tr/val_loss:  0.968784/  1.246980, val:  61.25%, val_best:  61.25%, tr:  71.71%, tr_best:  71.71%\n",
      "epoch-12  lr=['0.0010000'], tr/val_loss:  0.926915/  1.230487, val:  66.25%, val_best:  66.25%, tr:  76.61%, tr_best:  76.61%\n",
      "epoch-13  lr=['0.0010000'], tr/val_loss:  0.878486/  1.248526, val:  61.25%, val_best:  66.25%, tr:  81.00%, tr_best:  81.00%\n",
      "epoch-14  lr=['0.0010000'], tr/val_loss:  0.827080/  1.260443, val:  60.83%, val_best:  66.25%, tr:  81.41%, tr_best:  81.41%\n",
      "epoch-15  lr=['0.0010000'], tr/val_loss:  0.794327/  1.254289, val:  72.08%, val_best:  72.08%, tr:  83.76%, tr_best:  83.76%\n",
      "epoch-16  lr=['0.0010000'], tr/val_loss:  0.765466/  1.222960, val:  75.42%, val_best:  75.42%, tr:  84.07%, tr_best:  84.07%\n",
      "epoch-17  lr=['0.0010000'], tr/val_loss:  0.702537/  1.236357, val:  73.75%, val_best:  75.42%, tr:  90.70%, tr_best:  90.70%\n",
      "epoch-18  lr=['0.0010000'], tr/val_loss:  0.660772/  1.265279, val:  73.75%, val_best:  75.42%, tr:  91.73%, tr_best:  91.73%\n",
      "epoch-19  lr=['0.0010000'], tr/val_loss:  0.634625/  1.273125, val:  72.08%, val_best:  75.42%, tr:  91.52%, tr_best:  91.73%\n",
      "epoch-20  lr=['0.0010000'], tr/val_loss:  0.576659/  1.281623, val:  74.58%, val_best:  75.42%, tr:  94.59%, tr_best:  94.59%\n",
      "epoch-21  lr=['0.0010000'], tr/val_loss:  0.536512/  1.273785, val:  75.42%, val_best:  75.42%, tr:  95.10%, tr_best:  95.10%\n",
      "epoch-22  lr=['0.0010000'], tr/val_loss:  0.492123/  1.274833, val:  78.33%, val_best:  78.33%, tr:  96.83%, tr_best:  96.83%\n",
      "epoch-23  lr=['0.0010000'], tr/val_loss:  0.456269/  1.294283, val:  80.83%, val_best:  80.83%, tr:  97.85%, tr_best:  97.85%\n",
      "epoch-24  lr=['0.0010000'], tr/val_loss:  0.414591/  1.320658, val:  78.75%, val_best:  80.83%, tr:  98.06%, tr_best:  98.06%\n",
      "epoch-25  lr=['0.0010000'], tr/val_loss:  0.394275/  1.315406, val:  82.50%, val_best:  82.50%, tr:  98.26%, tr_best:  98.26%\n",
      "epoch-26  lr=['0.0010000'], tr/val_loss:  0.352436/  1.337317, val:  80.00%, val_best:  82.50%, tr:  99.18%, tr_best:  99.18%\n",
      "epoch-27  lr=['0.0010000'], tr/val_loss:  0.325525/  1.367764, val:  81.67%, val_best:  82.50%, tr:  99.28%, tr_best:  99.28%\n",
      "epoch-28  lr=['0.0010000'], tr/val_loss:  0.310672/  1.375713, val:  82.08%, val_best:  82.50%, tr:  99.39%, tr_best:  99.39%\n",
      "epoch-29  lr=['0.0010000'], tr/val_loss:  0.276250/  1.388858, val:  84.58%, val_best:  84.58%, tr:  99.28%, tr_best:  99.39%\n",
      "epoch-30  lr=['0.0010000'], tr/val_loss:  0.255941/  1.441412, val:  79.58%, val_best:  84.58%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-31  lr=['0.0010000'], tr/val_loss:  0.248484/  1.429895, val:  82.92%, val_best:  84.58%, tr:  99.39%, tr_best:  99.59%\n",
      "epoch-32  lr=['0.0010000'], tr/val_loss:  0.224446/  1.425540, val:  82.08%, val_best:  84.58%, tr:  99.49%, tr_best:  99.59%\n",
      "epoch-33  lr=['0.0010000'], tr/val_loss:  0.210441/  1.452849, val:  81.25%, val_best:  84.58%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-34  lr=['0.0010000'], tr/val_loss:  0.191336/  1.483439, val:  82.92%, val_best:  84.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-35  lr=['0.0010000'], tr/val_loss:  0.179051/  1.511060, val:  81.25%, val_best:  84.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-36  lr=['0.0010000'], tr/val_loss:  0.167502/  1.502788, val:  81.25%, val_best:  84.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-37  lr=['0.0010000'], tr/val_loss:  0.153570/  1.532701, val:  82.50%, val_best:  84.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-38  lr=['0.0010000'], tr/val_loss:  0.144537/  1.554639, val:  80.83%, val_best:  84.58%, tr:  99.90%, tr_best:  99.90%\n",
      "epoch-39  lr=['0.0010000'], tr/val_loss:  0.131574/  1.561192, val:  81.67%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0010000'], tr/val_loss:  0.122168/  1.579988, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0010000'], tr/val_loss:  0.118034/  1.614827, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0010000'], tr/val_loss:  0.108855/  1.615538, val:  79.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0010000'], tr/val_loss:  0.098607/  1.641662, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0010000'], tr/val_loss:  0.093868/  1.638160, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0010000'], tr/val_loss:  0.087644/  1.655712, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0010000'], tr/val_loss:  0.081131/  1.667293, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0010000'], tr/val_loss:  0.075196/  1.661458, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0010000'], tr/val_loss:  0.072298/  1.707655, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0010000'], tr/val_loss:  0.066066/  1.716441, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0010000'], tr/val_loss:  0.064218/  1.729827, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0010000'], tr/val_loss:  0.060178/  1.746876, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0010000'], tr/val_loss:  0.058089/  1.750319, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0010000'], tr/val_loss:  0.056529/  1.761873, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0010000'], tr/val_loss:  0.050228/  1.785931, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0010000'], tr/val_loss:  0.047265/  1.769400, val:  84.17%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0010000'], tr/val_loss:  0.045208/  1.791461, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0010000'], tr/val_loss:  0.045185/  1.812879, val:  82.50%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0010000'], tr/val_loss:  0.043337/  1.801536, val:  83.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0010000'], tr/val_loss:  0.038785/  1.817251, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0010000'], tr/val_loss:  0.038282/  1.820799, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0010000'], tr/val_loss:  0.037241/  1.810924, val:  83.75%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0010000'], tr/val_loss:  0.035995/  1.872706, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0010000'], tr/val_loss:  0.032029/  1.854985, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0010000'], tr/val_loss:  0.030832/  1.865777, val:  82.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0010000'], tr/val_loss:  0.029527/  1.886415, val:  82.92%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0010000'], tr/val_loss:  0.030521/  1.887257, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0010000'], tr/val_loss:  0.029200/  1.887220, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0010000'], tr/val_loss:  0.028187/  1.915308, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0010000'], tr/val_loss:  0.026585/  1.920783, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0010000'], tr/val_loss:  0.025160/  1.912215, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0010000'], tr/val_loss:  0.025235/  1.922214, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0010000'], tr/val_loss:  0.023698/  1.928454, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0010000'], tr/val_loss:  0.022256/  1.922905, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0010000'], tr/val_loss:  0.024191/  1.946823, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0010000'], tr/val_loss:  0.021788/  1.959661, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0010000'], tr/val_loss:  0.020349/  1.972342, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0010000'], tr/val_loss:  0.021198/  1.960942, val:  83.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0010000'], tr/val_loss:  0.019596/  1.953406, val:  83.33%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0010000'], tr/val_loss:  0.019136/  1.959338, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0010000'], tr/val_loss:  0.019356/  1.988890, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0010000'], tr/val_loss:  0.017682/  1.960181, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0010000'], tr/val_loss:  0.016990/  1.980626, val:  82.08%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0010000'], tr/val_loss:  0.017129/  1.980078, val:  81.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0010000'], tr/val_loss:  0.016476/  1.994014, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0010000'], tr/val_loss:  0.016062/  1.989144, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0010000'], tr/val_loss:  0.015212/  2.001637, val:  82.50%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0010000'], tr/val_loss:  0.014986/  1.982387, val:  82.92%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0010000'], tr/val_loss:  0.014160/  2.007508, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0010000'], tr/val_loss:  0.014233/  2.004868, val:  84.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0010000'], tr/val_loss:  0.013883/  2.011741, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0010000'], tr/val_loss:  0.014288/  2.022694, val:  84.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0010000'], tr/val_loss:  0.013296/  2.025343, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0010000'], tr/val_loss:  0.012832/  2.023076, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0010000'], tr/val_loss:  0.012505/  2.029706, val:  83.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0010000'], tr/val_loss:  0.011760/  2.023128, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0010000'], tr/val_loss:  0.011463/  2.040562, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0010000'], tr/val_loss:  0.011804/  2.057787, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0010000'], tr/val_loss:  0.011585/  2.054214, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0010000'], tr/val_loss:  0.011014/  2.061185, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253cc8ec56a643a184fda620209bad14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▅▅▅▇▇▅████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▃▅▅▆▆▆▇▇▇▇██████▇██████████████████████</td></tr><tr><td>tr_acc</td><td>▁▂▄▅▆▆▇▇▇███████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▆▅▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▅▅▆▆▆▇▇▇██████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▅▅▆▆▆▇▇▇▇██████▇██████████████████████</td></tr><tr><td>val_loss</td><td>█▇▄▂▁▁▁▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.01101</td></tr><tr><td>val_acc_best</td><td>0.85417</td></tr><tr><td>val_acc_now</td><td>0.82083</td></tr><tr><td>val_loss</td><td>2.06118</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dainty-sweep-14</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/01fuo5rc' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/01fuo5rc</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_062240-01fuo5rc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y4coc1lv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_065744-y4coc1lv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y4coc1lv' target=\"_blank\">flowing-sweep-16</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y4coc1lv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y4coc1lv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.25, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.25, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.303544/  2.290382, val:  17.50%, val_best:  17.50%, tr:  10.52%, tr_best:  10.52%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.254311/  2.217973, val:  23.75%, val_best:  23.75%, tr:  20.74%, tr_best:  20.74%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.153469/  2.096168, val:  35.83%, val_best:  35.83%, tr:  24.72%, tr_best:  24.72%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  1.989761/  1.929291, val:  41.25%, val_best:  41.25%, tr:  38.51%, tr_best:  38.51%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  1.804906/  1.779524, val:  45.00%, val_best:  45.00%, tr:  45.15%, tr_best:  45.15%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  1.649124/  1.660428, val:  47.50%, val_best:  47.50%, tr:  51.38%, tr_best:  51.38%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  1.546971/  1.575440, val:  48.33%, val_best:  48.33%, tr:  55.06%, tr_best:  55.06%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  1.451601/  1.514770, val:  52.08%, val_best:  52.08%, tr:  57.92%, tr_best:  57.92%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  1.391894/  1.463951, val:  53.75%, val_best:  53.75%, tr:  59.45%, tr_best:  59.45%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  1.343941/  1.435674, val:  57.08%, val_best:  57.08%, tr:  61.18%, tr_best:  61.18%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  1.297481/  1.410488, val:  54.58%, val_best:  57.08%, tr:  62.92%, tr_best:  62.92%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  1.275265/  1.390474, val:  52.92%, val_best:  57.08%, tr:  62.00%, tr_best:  62.92%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  1.255944/  1.373888, val:  52.08%, val_best:  57.08%, tr:  63.33%, tr_best:  63.33%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  1.232096/  1.364572, val:  56.67%, val_best:  57.08%, tr:  64.35%, tr_best:  64.35%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  1.208675/  1.352005, val:  57.92%, val_best:  57.92%, tr:  63.33%, tr_best:  64.35%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  1.192577/  1.340599, val:  57.08%, val_best:  57.92%, tr:  64.86%, tr_best:  64.86%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  1.163586/  1.328228, val:  57.50%, val_best:  57.92%, tr:  64.45%, tr_best:  64.86%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  1.153859/  1.314931, val:  55.42%, val_best:  57.92%, tr:  66.80%, tr_best:  66.80%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  1.142522/  1.307535, val:  60.42%, val_best:  60.42%, tr:  67.01%, tr_best:  67.01%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  1.125571/  1.299480, val:  59.17%, val_best:  60.42%, tr:  63.84%, tr_best:  67.01%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  1.109598/  1.291215, val:  60.00%, val_best:  60.42%, tr:  67.11%, tr_best:  67.11%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  1.098453/  1.285253, val:  59.17%, val_best:  60.42%, tr:  67.62%, tr_best:  67.62%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  1.086214/  1.269407, val:  59.58%, val_best:  60.42%, tr:  66.19%, tr_best:  67.62%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  1.068101/  1.263831, val:  60.00%, val_best:  60.42%, tr:  67.82%, tr_best:  67.82%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  1.061739/  1.258632, val:  60.00%, val_best:  60.42%, tr:  70.79%, tr_best:  70.79%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  1.049061/  1.250110, val:  60.83%, val_best:  60.83%, tr:  70.89%, tr_best:  70.89%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  1.038061/  1.231101, val:  61.67%, val_best:  61.67%, tr:  68.85%, tr_best:  70.89%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  1.025334/  1.225006, val:  62.08%, val_best:  62.08%, tr:  70.07%, tr_best:  70.89%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  1.030710/  1.215262, val:  63.33%, val_best:  63.33%, tr:  69.77%, tr_best:  70.89%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  1.013331/  1.212204, val:  62.50%, val_best:  63.33%, tr:  69.36%, tr_best:  70.89%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  1.003125/  1.204876, val:  63.75%, val_best:  63.75%, tr:  68.13%, tr_best:  70.89%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  0.998413/  1.205224, val:  59.58%, val_best:  63.75%, tr:  69.46%, tr_best:  70.89%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  0.991272/  1.197895, val:  63.33%, val_best:  63.75%, tr:  72.01%, tr_best:  72.01%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  0.989199/  1.185966, val:  65.83%, val_best:  65.83%, tr:  71.30%, tr_best:  72.01%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  0.972924/  1.178914, val:  65.83%, val_best:  65.83%, tr:  70.79%, tr_best:  72.01%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  0.964087/  1.178807, val:  66.67%, val_best:  66.67%, tr:  71.30%, tr_best:  72.01%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  0.961894/  1.174626, val:  65.00%, val_best:  66.67%, tr:  71.09%, tr_best:  72.01%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  0.945159/  1.164815, val:  62.08%, val_best:  66.67%, tr:  71.60%, tr_best:  72.01%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  0.946310/  1.157754, val:  65.00%, val_best:  66.67%, tr:  72.32%, tr_best:  72.32%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  0.940480/  1.154101, val:  66.25%, val_best:  66.67%, tr:  72.73%, tr_best:  72.73%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  0.926713/  1.152545, val:  63.75%, val_best:  66.67%, tr:  70.99%, tr_best:  72.73%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  0.932381/  1.149319, val:  67.08%, val_best:  67.08%, tr:  73.75%, tr_best:  73.75%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  0.914889/  1.142536, val:  67.92%, val_best:  67.92%, tr:  74.46%, tr_best:  74.46%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  0.911900/  1.146631, val:  65.83%, val_best:  67.92%, tr:  76.00%, tr_best:  76.00%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  0.907895/  1.139643, val:  67.08%, val_best:  67.92%, tr:  72.83%, tr_best:  76.00%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  0.898646/  1.138406, val:  64.58%, val_best:  67.92%, tr:  75.18%, tr_best:  76.00%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  0.895750/  1.135970, val:  67.08%, val_best:  67.92%, tr:  76.10%, tr_best:  76.10%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  0.896931/  1.132163, val:  66.25%, val_best:  67.92%, tr:  73.75%, tr_best:  76.10%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  0.895059/  1.128010, val:  66.25%, val_best:  67.92%, tr:  76.92%, tr_best:  76.92%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  0.878228/  1.121773, val:  66.25%, val_best:  67.92%, tr:  76.71%, tr_best:  76.92%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  0.876359/  1.121535, val:  64.58%, val_best:  67.92%, tr:  76.30%, tr_best:  76.92%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  0.877091/  1.118321, val:  67.08%, val_best:  67.92%, tr:  76.51%, tr_best:  76.92%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  0.870881/  1.114302, val:  68.75%, val_best:  68.75%, tr:  76.92%, tr_best:  76.92%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  0.862811/  1.106716, val:  70.42%, val_best:  70.42%, tr:  78.14%, tr_best:  78.14%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  0.857496/  1.112192, val:  70.83%, val_best:  70.83%, tr:  78.96%, tr_best:  78.96%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  0.855321/  1.111416, val:  68.33%, val_best:  70.83%, tr:  79.06%, tr_best:  79.06%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  0.845753/  1.098473, val:  70.83%, val_best:  70.83%, tr:  80.59%, tr_best:  80.59%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  0.843591/  1.104952, val:  70.00%, val_best:  70.83%, tr:  81.31%, tr_best:  81.31%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  0.836091/  1.109520, val:  69.17%, val_best:  70.83%, tr:  77.83%, tr_best:  81.31%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  0.832862/  1.100479, val:  68.75%, val_best:  70.83%, tr:  80.29%, tr_best:  81.31%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  0.829607/  1.100380, val:  67.92%, val_best:  70.83%, tr:  77.32%, tr_best:  81.31%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  0.830360/  1.108624, val:  70.00%, val_best:  70.83%, tr:  80.69%, tr_best:  81.31%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  0.827275/  1.101149, val:  72.08%, val_best:  72.08%, tr:  80.59%, tr_best:  81.31%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  0.815927/  1.093375, val:  73.33%, val_best:  73.33%, tr:  82.94%, tr_best:  82.94%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  0.809064/  1.094754, val:  72.08%, val_best:  73.33%, tr:  83.86%, tr_best:  83.86%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  0.811429/  1.092151, val:  72.50%, val_best:  73.33%, tr:  82.64%, tr_best:  83.86%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  0.805064/  1.088356, val:  70.42%, val_best:  73.33%, tr:  82.53%, tr_best:  83.86%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  0.802401/  1.082135, val:  73.33%, val_best:  73.33%, tr:  84.27%, tr_best:  84.27%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  0.797762/  1.091707, val:  72.92%, val_best:  73.33%, tr:  81.82%, tr_best:  84.27%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  0.790932/  1.097471, val:  71.25%, val_best:  73.33%, tr:  83.66%, tr_best:  84.27%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  0.786608/  1.083577, val:  76.67%, val_best:  76.67%, tr:  85.39%, tr_best:  85.39%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  0.783089/  1.087143, val:  70.42%, val_best:  76.67%, tr:  83.45%, tr_best:  85.39%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  0.774674/  1.090880, val:  72.08%, val_best:  76.67%, tr:  82.74%, tr_best:  85.39%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  0.779043/  1.081562, val:  73.75%, val_best:  76.67%, tr:  84.37%, tr_best:  85.39%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  0.771307/  1.081934, val:  72.08%, val_best:  76.67%, tr:  84.68%, tr_best:  85.39%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  0.773711/  1.077628, val:  74.17%, val_best:  76.67%, tr:  84.58%, tr_best:  85.39%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  0.759549/  1.080603, val:  70.00%, val_best:  76.67%, tr:  84.37%, tr_best:  85.39%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  0.768167/  1.082799, val:  75.00%, val_best:  76.67%, tr:  86.93%, tr_best:  86.93%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  0.756046/  1.073790, val:  73.75%, val_best:  76.67%, tr:  86.62%, tr_best:  86.93%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  0.759838/  1.073039, val:  75.42%, val_best:  76.67%, tr:  85.70%, tr_best:  86.93%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  0.747177/  1.071403, val:  73.33%, val_best:  76.67%, tr:  85.60%, tr_best:  86.93%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  0.747521/  1.074082, val:  73.75%, val_best:  76.67%, tr:  86.41%, tr_best:  86.93%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  0.744115/  1.077374, val:  74.17%, val_best:  76.67%, tr:  87.64%, tr_best:  87.64%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  0.742977/  1.068582, val:  74.58%, val_best:  76.67%, tr:  88.56%, tr_best:  88.56%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  0.743220/  1.065579, val:  77.08%, val_best:  77.08%, tr:  88.15%, tr_best:  88.56%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  0.739371/  1.073996, val:  72.08%, val_best:  77.08%, tr:  87.54%, tr_best:  88.56%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  0.729347/  1.071068, val:  75.00%, val_best:  77.08%, tr:  87.74%, tr_best:  88.56%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  0.727854/  1.054450, val:  75.42%, val_best:  77.08%, tr:  88.66%, tr_best:  88.66%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  0.728421/  1.065241, val:  69.58%, val_best:  77.08%, tr:  87.44%, tr_best:  88.66%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  0.716114/  1.060688, val:  74.58%, val_best:  77.08%, tr:  87.33%, tr_best:  88.66%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  0.720931/  1.057048, val:  75.83%, val_best:  77.08%, tr:  88.15%, tr_best:  88.66%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  0.707476/  1.055682, val:  75.83%, val_best:  77.08%, tr:  88.97%, tr_best:  88.97%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  0.711648/  1.057180, val:  75.00%, val_best:  77.08%, tr:  88.56%, tr_best:  88.97%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  0.714073/  1.054672, val:  76.67%, val_best:  77.08%, tr:  89.89%, tr_best:  89.89%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  0.708757/  1.050912, val:  75.83%, val_best:  77.08%, tr:  89.48%, tr_best:  89.89%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  0.698801/  1.046603, val:  75.83%, val_best:  77.08%, tr:  89.07%, tr_best:  89.89%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  0.693739/  1.054466, val:  76.67%, val_best:  77.08%, tr:  91.62%, tr_best:  91.62%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  0.691339/  1.048962, val:  75.00%, val_best:  77.08%, tr:  89.89%, tr_best:  91.62%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  0.693022/  1.046683, val:  79.58%, val_best:  79.58%, tr:  90.09%, tr_best:  91.62%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  0.690056/  1.050996, val:  77.92%, val_best:  79.58%, tr:  89.99%, tr_best:  91.62%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc27b88b7564d92ba917582a4114e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▅▁▃▅▅▆▄▅▆▇▇▅▅▇▆▇▅▇▆▆▆▇▇█▇▇▇▇▆▇▇▆▇▇▇▆▇█▇</td></tr><tr><td>summary_val_acc</td><td>▁▃▄▅▆▅▆▅▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇██▇██████████</td></tr><tr><td>tr_acc</td><td>▁▂▄▅▅▆▆▆▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇██▇██████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▆▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▃▄▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>val_acc_now</td><td>▁▃▄▅▆▅▆▅▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇██▇██████████</td></tr><tr><td>val_loss</td><td>█▇▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.8999</td></tr><tr><td>tr_epoch_loss</td><td>0.69006</td></tr><tr><td>val_acc_best</td><td>0.79583</td></tr><tr><td>val_acc_now</td><td>0.77917</td></tr><tr><td>val_loss</td><td>1.051</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">flowing-sweep-16</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y4coc1lv' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y4coc1lv</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_065744-y4coc1lv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ctgpc9tu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_073215-ctgpc9tu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ctgpc9tu' target=\"_blank\">snowy-sweep-18</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ctgpc9tu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ctgpc9tu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = da7e4b41c275f954fb82e708e43d2b96\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.5, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.5, v_reset=10000, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.304075/  2.305134, val:   9.58%, val_best:   9.58%, tr:   9.81%, tr_best:   9.81%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.302774/  2.295109, val:   6.67%, val_best:   9.58%, tr:   7.66%, tr_best:   9.81%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.273611/  2.252442, val:  22.50%, val_best:  22.50%, tr:  14.81%, tr_best:  14.81%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.185123/  2.122541, val:  33.75%, val_best:  33.75%, tr:  23.90%, tr_best:  23.90%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  2.005558/  1.931197, val:  41.25%, val_best:  41.25%, tr:  34.73%, tr_best:  34.73%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  1.795485/  1.757167, val:  35.83%, val_best:  41.25%, tr:  43.92%, tr_best:  43.92%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  1.640457/  1.634789, val:  38.75%, val_best:  41.25%, tr:  48.01%, tr_best:  48.01%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  1.519931/  1.558601, val:  39.17%, val_best:  41.25%, tr:  50.56%, tr_best:  50.56%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  1.449045/  1.502487, val:  46.67%, val_best:  46.67%, tr:  54.65%, tr_best:  54.65%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  1.387466/  1.466713, val:  48.75%, val_best:  48.75%, tr:  54.65%, tr_best:  54.65%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  1.335847/  1.433705, val:  49.17%, val_best:  49.17%, tr:  57.41%, tr_best:  57.41%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  1.303313/  1.406220, val:  50.00%, val_best:  50.00%, tr:  58.73%, tr_best:  58.73%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  1.275475/  1.383969, val:  50.83%, val_best:  50.83%, tr:  60.37%, tr_best:  60.37%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  1.245612/  1.369249, val:  52.92%, val_best:  52.92%, tr:  61.80%, tr_best:  61.80%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  1.213121/  1.355452, val:  53.33%, val_best:  53.33%, tr:  60.98%, tr_best:  61.80%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  1.187306/  1.335470, val:  54.58%, val_best:  54.58%, tr:  62.51%, tr_best:  62.51%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  1.155597/  1.311815, val:  55.83%, val_best:  55.83%, tr:  62.00%, tr_best:  62.51%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  1.139837/  1.302603, val:  54.58%, val_best:  55.83%, tr:  64.96%, tr_best:  64.96%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  1.120094/  1.294267, val:  55.42%, val_best:  55.83%, tr:  64.76%, tr_best:  64.96%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  1.103915/  1.278160, val:  57.92%, val_best:  57.92%, tr:  62.61%, tr_best:  64.96%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  1.082676/  1.271552, val:  55.83%, val_best:  57.92%, tr:  65.47%, tr_best:  65.47%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  1.066131/  1.256906, val:  55.42%, val_best:  57.92%, tr:  65.17%, tr_best:  65.47%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  1.050711/  1.248075, val:  57.92%, val_best:  57.92%, tr:  65.47%, tr_best:  65.47%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  1.030046/  1.240109, val:  56.25%, val_best:  57.92%, tr:  67.01%, tr_best:  67.01%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  1.023417/  1.242120, val:  56.67%, val_best:  57.92%, tr:  68.85%, tr_best:  68.85%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  1.004524/  1.233989, val:  61.25%, val_best:  61.25%, tr:  69.36%, tr_best:  69.36%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  0.996245/  1.218002, val:  62.08%, val_best:  62.08%, tr:  67.62%, tr_best:  69.36%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  0.984813/  1.216586, val:  59.17%, val_best:  62.08%, tr:  70.17%, tr_best:  70.17%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  0.990616/  1.209703, val:  60.00%, val_best:  62.08%, tr:  69.46%, tr_best:  70.17%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  0.967563/  1.209746, val:  57.92%, val_best:  62.08%, tr:  71.50%, tr_best:  71.50%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  0.954221/  1.212928, val:  59.58%, val_best:  62.08%, tr:  71.09%, tr_best:  71.50%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  0.952291/  1.207140, val:  62.08%, val_best:  62.08%, tr:  70.48%, tr_best:  71.50%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  0.941979/  1.205897, val:  61.25%, val_best:  62.08%, tr:  73.03%, tr_best:  73.03%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  0.944084/  1.190435, val:  67.50%, val_best:  67.50%, tr:  71.30%, tr_best:  73.03%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  0.927594/  1.195266, val:  64.17%, val_best:  67.50%, tr:  72.83%, tr_best:  73.03%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  0.917273/  1.198028, val:  63.75%, val_best:  67.50%, tr:  72.42%, tr_best:  73.03%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  0.917298/  1.193781, val:  66.67%, val_best:  67.50%, tr:  75.08%, tr_best:  75.08%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  0.896778/  1.187497, val:  62.08%, val_best:  67.50%, tr:  74.26%, tr_best:  75.08%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  0.895581/  1.189846, val:  65.83%, val_best:  67.50%, tr:  76.40%, tr_best:  76.40%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  0.889302/  1.190131, val:  64.17%, val_best:  67.50%, tr:  75.18%, tr_best:  76.40%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  0.873161/  1.195397, val:  63.75%, val_best:  67.50%, tr:  74.67%, tr_best:  76.40%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  0.877290/  1.187985, val:  66.67%, val_best:  67.50%, tr:  78.65%, tr_best:  78.65%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  0.856092/  1.186029, val:  64.17%, val_best:  67.50%, tr:  77.12%, tr_best:  78.65%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  0.852362/  1.180336, val:  65.42%, val_best:  67.50%, tr:  77.43%, tr_best:  78.65%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  0.845208/  1.183404, val:  64.17%, val_best:  67.50%, tr:  77.83%, tr_best:  78.65%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  0.832336/  1.190623, val:  63.75%, val_best:  67.50%, tr:  78.65%, tr_best:  78.65%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  0.827616/  1.186334, val:  68.33%, val_best:  68.33%, tr:  79.88%, tr_best:  79.88%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  0.832476/  1.183115, val:  66.67%, val_best:  68.33%, tr:  77.83%, tr_best:  79.88%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  0.818971/  1.187672, val:  67.08%, val_best:  68.33%, tr:  81.21%, tr_best:  81.21%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  0.808045/  1.179074, val:  65.83%, val_best:  68.33%, tr:  80.29%, tr_best:  81.21%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  0.804516/  1.177608, val:  66.25%, val_best:  68.33%, tr:  79.98%, tr_best:  81.21%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  0.800655/  1.174769, val:  69.58%, val_best:  69.58%, tr:  81.31%, tr_best:  81.31%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  0.795818/  1.175884, val:  68.75%, val_best:  69.58%, tr:  82.12%, tr_best:  82.12%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  0.788884/  1.171201, val:  70.00%, val_best:  70.00%, tr:  82.12%, tr_best:  82.12%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  0.777500/  1.179578, val:  71.25%, val_best:  71.25%, tr:  83.66%, tr_best:  83.66%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  0.773666/  1.179267, val:  70.83%, val_best:  71.25%, tr:  83.25%, tr_best:  83.66%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  0.758545/  1.173587, val:  70.00%, val_best:  71.25%, tr:  84.88%, tr_best:  84.88%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  0.753722/  1.183262, val:  70.00%, val_best:  71.25%, tr:  84.27%, tr_best:  84.88%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  0.746558/  1.182454, val:  68.33%, val_best:  71.25%, tr:  83.45%, tr_best:  84.88%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  0.747370/  1.176844, val:  69.58%, val_best:  71.25%, tr:  83.35%, tr_best:  84.88%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  0.737128/  1.170485, val:  72.08%, val_best:  72.08%, tr:  83.55%, tr_best:  84.88%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  0.740527/  1.180838, val:  72.08%, val_best:  72.08%, tr:  85.50%, tr_best:  85.50%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  0.730837/  1.177249, val:  73.33%, val_best:  73.33%, tr:  85.90%, tr_best:  85.90%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  0.722190/  1.163575, val:  73.33%, val_best:  73.33%, tr:  87.84%, tr_best:  87.84%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  0.715113/  1.178033, val:  72.08%, val_best:  73.33%, tr:  87.44%, tr_best:  87.84%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  0.715183/  1.179643, val:  73.75%, val_best:  73.75%, tr:  87.64%, tr_best:  87.84%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  0.704300/  1.180803, val:  74.17%, val_best:  74.17%, tr:  88.15%, tr_best:  88.15%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  0.703452/  1.182167, val:  75.42%, val_best:  75.42%, tr:  89.07%, tr_best:  89.07%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  0.696162/  1.191898, val:  72.50%, val_best:  75.42%, tr:  87.44%, tr_best:  89.07%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  0.687905/  1.197002, val:  71.67%, val_best:  75.42%, tr:  89.48%, tr_best:  89.48%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  0.681457/  1.186334, val:  75.83%, val_best:  75.83%, tr:  89.99%, tr_best:  89.99%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  0.674810/  1.187554, val:  71.67%, val_best:  75.83%, tr:  89.68%, tr_best:  89.99%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  0.665700/  1.202005, val:  69.17%, val_best:  75.83%, tr:  89.89%, tr_best:  89.99%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  0.669383/  1.191395, val:  74.58%, val_best:  75.83%, tr:  89.99%, tr_best:  89.99%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  0.655404/  1.203272, val:  76.67%, val_best:  76.67%, tr:  90.91%, tr_best:  90.91%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  0.658777/  1.190064, val:  75.83%, val_best:  76.67%, tr:  90.81%, tr_best:  90.91%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  0.640840/  1.204713, val:  72.50%, val_best:  76.67%, tr:  91.52%, tr_best:  91.52%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  0.651442/  1.207488, val:  75.83%, val_best:  76.67%, tr:  92.13%, tr_best:  92.13%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  0.633733/  1.190730, val:  77.50%, val_best:  77.50%, tr:  92.03%, tr_best:  92.13%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  0.636502/  1.200042, val:  76.25%, val_best:  77.50%, tr:  91.73%, tr_best:  92.13%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  0.623286/  1.203308, val:  75.00%, val_best:  77.50%, tr:  92.03%, tr_best:  92.13%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  0.619038/  1.227484, val:  74.17%, val_best:  77.50%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  0.615092/  1.226065, val:  74.17%, val_best:  77.50%, tr:  92.75%, tr_best:  93.77%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  0.611855/  1.213583, val:  74.58%, val_best:  77.50%, tr:  93.67%, tr_best:  93.77%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  0.601608/  1.216577, val:  76.25%, val_best:  77.50%, tr:  93.77%, tr_best:  93.77%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  0.601220/  1.241808, val:  73.33%, val_best:  77.50%, tr:  94.28%, tr_best:  94.28%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  0.595266/  1.227183, val:  74.17%, val_best:  77.50%, tr:  93.05%, tr_best:  94.28%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  0.593058/  1.220662, val:  76.67%, val_best:  77.50%, tr:  93.46%, tr_best:  94.28%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  0.586469/  1.223166, val:  72.92%, val_best:  77.50%, tr:  94.08%, tr_best:  94.28%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  0.576433/  1.235273, val:  72.92%, val_best:  77.50%, tr:  94.48%, tr_best:  94.48%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  0.577954/  1.223866, val:  76.25%, val_best:  77.50%, tr:  94.28%, tr_best:  94.48%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  0.572023/  1.237433, val:  75.42%, val_best:  77.50%, tr:  94.28%, tr_best:  94.48%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  0.571896/  1.238402, val:  76.67%, val_best:  77.50%, tr:  94.69%, tr_best:  94.69%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  0.567813/  1.228342, val:  75.42%, val_best:  77.50%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  0.568825/  1.231277, val:  76.67%, val_best:  77.50%, tr:  94.59%, tr_best:  94.89%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  0.553333/  1.244196, val:  75.42%, val_best:  77.50%, tr:  94.89%, tr_best:  94.89%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  0.546633/  1.255057, val:  73.75%, val_best:  77.50%, tr:  95.20%, tr_best:  95.20%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  0.544771/  1.243518, val:  75.83%, val_best:  77.50%, tr:  95.51%, tr_best:  95.51%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  0.537681/  1.242633, val:  77.50%, val_best:  77.50%, tr:  95.91%, tr_best:  95.91%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  0.540860/  1.276376, val:  76.25%, val_best:  77.50%, tr:  95.61%, tr_best:  95.91%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e11aec08aaa4deb8f747e7fbe0a8d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▂▁▂▅▅▆▅▄▆▆▇▆▅▇▇█▆▇▇█▇▇██▇███▇██▇▇█▆▇▇██</td></tr><tr><td>summary_val_acc</td><td>▁▂▄▄▅▅▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇████▇██████████</td></tr><tr><td>tr_acc</td><td>▁▁▃▄▅▅▅▆▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>tr_epoch_loss</td><td>██▇▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▄▄▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▄▄▅▅▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇████▇██████████</td></tr><tr><td>val_loss</td><td>██▆▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.95608</td></tr><tr><td>tr_epoch_loss</td><td>0.54086</td></tr><tr><td>val_acc_best</td><td>0.775</td></tr><tr><td>val_acc_now</td><td>0.7625</td></tr><tr><td>val_loss</td><td>1.27638</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">snowy-sweep-18</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ctgpc9tu' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ctgpc9tu</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_073215-ctgpc9tu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y0i7xbi9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_080637-y0i7xbi9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y0i7xbi9' target=\"_blank\">rosy-sweep-20</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y0i7xbi9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y0i7xbi9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 34.000645/ 39.181633, val:  26.67%, val_best:  26.67%, tr:  23.60%, tr_best:  23.60%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss: 28.545355/ 34.966118, val:  38.33%, val_best:  38.33%, tr:  40.14%, tr_best:  40.14%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss: 24.015331/ 30.796803, val:  39.58%, val_best:  39.58%, tr:  46.78%, tr_best:  46.78%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss: 20.669415/ 19.019461, val:  47.50%, val_best:  47.50%, tr:  49.85%, tr_best:  49.85%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss: 17.744156/ 15.279341, val:  59.17%, val_best:  59.17%, tr:  56.28%, tr_best:  56.28%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss: 17.094187/ 25.701818, val:  50.42%, val_best:  59.17%, tr:  53.22%, tr_best:  56.28%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss: 12.745494/ 18.179510, val:  55.00%, val_best:  59.17%, tr:  62.31%, tr_best:  62.31%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss: 11.051240/ 21.607903, val:  49.58%, val_best:  59.17%, tr:  63.74%, tr_best:  63.74%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss: 10.093130/ 17.248966, val:  61.67%, val_best:  61.67%, tr:  66.60%, tr_best:  66.60%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss: 13.657437/ 19.762167, val:  59.17%, val_best:  61.67%, tr:  68.85%, tr_best:  68.85%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  9.265431/ 11.542340, val:  71.67%, val_best:  71.67%, tr:  74.16%, tr_best:  74.16%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  7.381561/ 18.043585, val:  64.58%, val_best:  71.67%, tr:  82.02%, tr_best:  82.02%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  7.990768/ 10.941708, val:  81.25%, val_best:  81.25%, tr:  78.04%, tr_best:  82.02%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  7.621445/ 16.798609, val:  67.50%, val_best:  81.25%, tr:  82.43%, tr_best:  82.43%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  5.307785/ 18.317169, val:  68.33%, val_best:  81.25%, tr:  87.64%, tr_best:  87.64%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  4.563572/ 11.635014, val:  80.00%, val_best:  81.25%, tr:  90.19%, tr_best:  90.19%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  3.347014/ 13.870158, val:  78.75%, val_best:  81.25%, tr:  93.16%, tr_best:  93.16%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  3.178666/ 16.727928, val:  72.92%, val_best:  81.25%, tr:  93.16%, tr_best:  93.16%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  3.837943/ 14.028827, val:  80.00%, val_best:  81.25%, tr:  92.65%, tr_best:  93.16%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  2.147559/ 11.326509, val:  84.58%, val_best:  84.58%, tr:  96.12%, tr_best:  96.12%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  1.691574/ 12.685224, val:  82.08%, val_best:  84.58%, tr:  96.42%, tr_best:  96.42%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  1.917924/ 11.578724, val:  84.17%, val_best:  84.58%, tr:  97.14%, tr_best:  97.14%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  1.400454/ 11.968324, val:  83.75%, val_best:  84.58%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  1.436915/ 13.292150, val:  84.17%, val_best:  84.58%, tr:  97.34%, tr_best:  97.34%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  1.437983/ 11.898404, val:  84.58%, val_best:  84.58%, tr:  97.75%, tr_best:  97.75%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  0.801727/ 11.222243, val:  87.08%, val_best:  87.08%, tr:  98.57%, tr_best:  98.57%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  0.625660/ 11.674397, val:  84.58%, val_best:  87.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  0.719230/ 11.080687, val:  85.83%, val_best:  87.08%, tr:  98.88%, tr_best:  99.69%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  0.480344/ 11.709771, val:  85.00%, val_best:  87.08%, tr:  99.69%, tr_best:  99.69%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  0.396533/ 11.035799, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  0.288696/ 11.709640, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  0.218360/ 11.782644, val:  86.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  0.177158/ 11.071030, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  0.220814/ 11.555608, val:  87.92%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  0.245698/ 11.846012, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  0.260439/ 12.388301, val:  86.67%, val_best:  88.33%, tr:  99.69%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  0.137035/ 12.255873, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  0.162934/ 11.643162, val:  85.00%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  0.087571/ 11.804025, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  0.056864/ 11.438545, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  0.054436/ 11.719439, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  0.045369/ 11.904553, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  0.053801/ 11.776848, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  0.048404/ 11.925447, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  0.044221/ 12.185743, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  0.039046/ 12.421947, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  0.058781/ 12.492640, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  0.057807/ 12.808304, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  0.028432/ 12.579282, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  0.040080/ 12.562896, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  0.041467/ 13.043063, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  0.019737/ 12.884932, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  0.036588/ 12.933284, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  0.015895/ 12.493544, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  0.024046/ 12.522445, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  0.025962/ 12.814814, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  0.018439/ 12.991118, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  0.010733/ 13.029244, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  0.014315/ 12.994414, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  0.014894/ 13.186354, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  0.024399/ 12.946535, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  0.030162/ 13.127616, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  0.004744/ 13.154572, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  0.006099/ 12.757708, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  0.024623/ 12.717848, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  0.017557/ 12.537423, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  0.010016/ 12.662233, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  0.015016/ 12.621832, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  0.015781/ 13.091785, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  0.011055/ 12.860716, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  0.006227/ 12.892200, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  0.001556/ 12.864432, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  0.004082/ 12.487185, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  0.023661/ 13.063489, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  0.013795/ 12.896195, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  0.013920/ 12.554408, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  0.012389/ 12.555168, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  0.006500/ 12.854958, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  0.009773/ 12.831723, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  0.012014/ 12.664521, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  0.000708/ 13.104853, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  0.002607/ 13.152079, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  0.000655/ 13.031589, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  0.003796/ 13.181775, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  0.000433/ 13.016789, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  0.000025/ 13.035204, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  0.000023/ 13.047019, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  0.000025/ 13.063256, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  0.000022/ 13.059104, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  0.000024/ 13.076265, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  0.000876/ 13.131979, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  0.000432/ 12.890025, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  0.000012/ 12.881615, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  0.000010/ 12.884402, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  0.000010/ 12.860640, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  0.000009/ 12.860249, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  0.000008/ 12.882967, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  0.000009/ 12.895772, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  0.000007/ 12.878970, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  0.000007/ 12.889324, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8cb0cf088bd4c28ba38078bedd670b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▁▃▅▇▆▆████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▂▅▄▅▇▆▆█████████████▇███▇█▇████████████</td></tr><tr><td>tr_acc</td><td>▁▃▄▅▅▆▇▇████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▆▅▃▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▂▅▅▅▇▇▇████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▂▅▄▅▇▆▆█████████████▇███▇█▇████████████</td></tr><tr><td>val_loss</td><td>█▆▂▄▃▁▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1e-05</td></tr><tr><td>val_acc_best</td><td>0.88333</td></tr><tr><td>val_acc_now</td><td>0.84583</td></tr><tr><td>val_loss</td><td>12.88932</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rosy-sweep-20</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y0i7xbi9' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/y0i7xbi9</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_080637-y0i7xbi9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ep1lhqzm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_084100-ep1lhqzm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ep1lhqzm' target=\"_blank\">peach-sweep-22</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ep1lhqzm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ep1lhqzm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.01, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = da7e4b41c275f954fb82e708e43d2b96\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0100000'], tr/val_loss:  2.001470/  2.257308, val:  45.00%, val_best:  45.00%, tr:  26.86%, tr_best:  26.86%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0100000'], tr/val_loss:  1.667987/  1.691191, val:  47.08%, val_best:  47.08%, tr:  50.15%, tr_best:  50.15%\n",
      "epoch-2   lr=['0.0100000'], tr/val_loss:  1.591356/  1.868037, val:  47.50%, val_best:  47.50%, tr:  54.75%, tr_best:  54.75%\n",
      "epoch-3   lr=['0.0100000'], tr/val_loss:  1.267205/  1.890495, val:  56.67%, val_best:  56.67%, tr:  64.66%, tr_best:  64.66%\n",
      "epoch-4   lr=['0.0100000'], tr/val_loss:  1.095830/  1.356280, val:  58.75%, val_best:  58.75%, tr:  65.58%, tr_best:  65.58%\n",
      "epoch-5   lr=['0.0100000'], tr/val_loss:  1.030860/  1.570948, val:  57.50%, val_best:  58.75%, tr:  71.60%, tr_best:  71.60%\n",
      "epoch-6   lr=['0.0100000'], tr/val_loss:  0.933839/  1.608227, val:  66.25%, val_best:  66.25%, tr:  74.67%, tr_best:  74.67%\n",
      "epoch-7   lr=['0.0100000'], tr/val_loss:  0.834529/  1.412265, val:  71.67%, val_best:  71.67%, tr:  78.86%, tr_best:  78.86%\n",
      "epoch-8   lr=['0.0100000'], tr/val_loss:  0.710174/  1.380695, val:  75.83%, val_best:  75.83%, tr:  83.04%, tr_best:  83.04%\n",
      "epoch-9   lr=['0.0100000'], tr/val_loss:  0.531756/  1.357329, val:  80.83%, val_best:  80.83%, tr:  88.97%, tr_best:  88.97%\n",
      "epoch-10  lr=['0.0100000'], tr/val_loss:  0.381375/  1.559089, val:  75.42%, val_best:  80.83%, tr:  95.71%, tr_best:  95.71%\n",
      "epoch-11  lr=['0.0100000'], tr/val_loss:  0.345970/  1.519682, val:  77.92%, val_best:  80.83%, tr:  96.53%, tr_best:  96.53%\n",
      "epoch-12  lr=['0.0100000'], tr/val_loss:  0.230003/  1.636412, val:  76.67%, val_best:  80.83%, tr:  98.37%, tr_best:  98.37%\n",
      "epoch-13  lr=['0.0100000'], tr/val_loss:  0.177159/  1.581541, val:  81.67%, val_best:  81.67%, tr:  99.59%, tr_best:  99.59%\n",
      "epoch-14  lr=['0.0100000'], tr/val_loss:  0.115202/  1.599647, val:  82.92%, val_best:  82.92%, tr:  99.80%, tr_best:  99.80%\n",
      "epoch-15  lr=['0.0100000'], tr/val_loss:  0.072781/  1.691393, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-16  lr=['0.0100000'], tr/val_loss:  0.052467/  1.806845, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-17  lr=['0.0100000'], tr/val_loss:  0.044639/  1.808170, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-18  lr=['0.0100000'], tr/val_loss:  0.037766/  1.799722, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-19  lr=['0.0100000'], tr/val_loss:  0.030818/  1.854892, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-20  lr=['0.0100000'], tr/val_loss:  0.020687/  1.913365, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-21  lr=['0.0100000'], tr/val_loss:  0.014869/  1.956048, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-22  lr=['0.0100000'], tr/val_loss:  0.014445/  1.939314, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-23  lr=['0.0100000'], tr/val_loss:  0.012509/  2.062922, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-24  lr=['0.0100000'], tr/val_loss:  0.010880/  1.962738, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-25  lr=['0.0100000'], tr/val_loss:  0.009152/  1.993014, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-26  lr=['0.0100000'], tr/val_loss:  0.007263/  2.066167, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-27  lr=['0.0100000'], tr/val_loss:  0.006858/  2.070962, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-28  lr=['0.0100000'], tr/val_loss:  0.005514/  2.074809, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-29  lr=['0.0100000'], tr/val_loss:  0.005649/  2.072617, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-30  lr=['0.0100000'], tr/val_loss:  0.004977/  2.105814, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-31  lr=['0.0100000'], tr/val_loss:  0.004498/  2.102088, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-32  lr=['0.0100000'], tr/val_loss:  0.003808/  2.064240, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-33  lr=['0.0100000'], tr/val_loss:  0.003834/  2.089633, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-34  lr=['0.0100000'], tr/val_loss:  0.003287/  2.145413, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-35  lr=['0.0100000'], tr/val_loss:  0.002563/  2.116575, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-36  lr=['0.0100000'], tr/val_loss:  0.003159/  2.125634, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-37  lr=['0.0100000'], tr/val_loss:  0.002421/  2.141577, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-38  lr=['0.0100000'], tr/val_loss:  0.002649/  2.154372, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-39  lr=['0.0100000'], tr/val_loss:  0.002260/  2.156445, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-40  lr=['0.0100000'], tr/val_loss:  0.001845/  2.167202, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-41  lr=['0.0100000'], tr/val_loss:  0.001840/  2.163111, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-42  lr=['0.0100000'], tr/val_loss:  0.001716/  2.156846, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-43  lr=['0.0100000'], tr/val_loss:  0.001716/  2.152601, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-44  lr=['0.0100000'], tr/val_loss:  0.001802/  2.203198, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-45  lr=['0.0100000'], tr/val_loss:  0.001934/  2.219203, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-46  lr=['0.0100000'], tr/val_loss:  0.001803/  2.230278, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-47  lr=['0.0100000'], tr/val_loss:  0.001789/  2.226397, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-48  lr=['0.0100000'], tr/val_loss:  0.001789/  2.203904, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-49  lr=['0.0100000'], tr/val_loss:  0.001352/  2.190713, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-50  lr=['0.0100000'], tr/val_loss:  0.001416/  2.196919, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-51  lr=['0.0100000'], tr/val_loss:  0.001253/  2.200881, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-52  lr=['0.0100000'], tr/val_loss:  0.001539/  2.249986, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-53  lr=['0.0100000'], tr/val_loss:  0.001831/  2.226555, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-54  lr=['0.0100000'], tr/val_loss:  0.001588/  2.219959, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-55  lr=['0.0100000'], tr/val_loss:  0.001083/  2.240463, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-56  lr=['0.0100000'], tr/val_loss:  0.000971/  2.256697, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-57  lr=['0.0100000'], tr/val_loss:  0.000927/  2.252692, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-58  lr=['0.0100000'], tr/val_loss:  0.001780/  2.266821, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-59  lr=['0.0100000'], tr/val_loss:  0.001286/  2.239765, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-60  lr=['0.0100000'], tr/val_loss:  0.001283/  2.266905, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-61  lr=['0.0100000'], tr/val_loss:  0.001166/  2.268907, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-62  lr=['0.0100000'], tr/val_loss:  0.000944/  2.258005, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-63  lr=['0.0100000'], tr/val_loss:  0.000906/  2.227855, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-64  lr=['0.0100000'], tr/val_loss:  0.000935/  2.235687, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-65  lr=['0.0100000'], tr/val_loss:  0.001062/  2.247414, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-66  lr=['0.0100000'], tr/val_loss:  0.000926/  2.259806, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-67  lr=['0.0100000'], tr/val_loss:  0.000895/  2.246317, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-68  lr=['0.0100000'], tr/val_loss:  0.000858/  2.279805, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-69  lr=['0.0100000'], tr/val_loss:  0.000845/  2.288796, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-70  lr=['0.0100000'], tr/val_loss:  0.000903/  2.290701, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-71  lr=['0.0100000'], tr/val_loss:  0.001173/  2.269172, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-72  lr=['0.0100000'], tr/val_loss:  0.000948/  2.288630, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-73  lr=['0.0100000'], tr/val_loss:  0.001113/  2.316528, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-74  lr=['0.0100000'], tr/val_loss:  0.001051/  2.298387, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-75  lr=['0.0100000'], tr/val_loss:  0.000836/  2.295471, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-76  lr=['0.0100000'], tr/val_loss:  0.000761/  2.303533, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-77  lr=['0.0100000'], tr/val_loss:  0.000800/  2.299956, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-78  lr=['0.0100000'], tr/val_loss:  0.000701/  2.299031, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-79  lr=['0.0100000'], tr/val_loss:  0.000865/  2.312276, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-80  lr=['0.0100000'], tr/val_loss:  0.000788/  2.310848, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-81  lr=['0.0100000'], tr/val_loss:  0.000708/  2.326144, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-82  lr=['0.0100000'], tr/val_loss:  0.000878/  2.322860, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-83  lr=['0.0100000'], tr/val_loss:  0.001429/  2.307467, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-84  lr=['0.0100000'], tr/val_loss:  0.000748/  2.336928, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-85  lr=['0.0100000'], tr/val_loss:  0.000607/  2.323219, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-86  lr=['0.0100000'], tr/val_loss:  0.000595/  2.329854, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-87  lr=['0.0100000'], tr/val_loss:  0.000762/  2.334443, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-88  lr=['0.0100000'], tr/val_loss:  0.000680/  2.341704, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-89  lr=['0.0100000'], tr/val_loss:  0.000665/  2.378318, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-90  lr=['0.0100000'], tr/val_loss:  0.000658/  2.388427, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-91  lr=['0.0100000'], tr/val_loss:  0.000561/  2.363529, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-92  lr=['0.0100000'], tr/val_loss:  0.000805/  2.367362, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-93  lr=['0.0100000'], tr/val_loss:  0.000947/  2.410172, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-94  lr=['0.0100000'], tr/val_loss:  0.001228/  2.368362, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-95  lr=['0.0100000'], tr/val_loss:  0.001570/  2.382701, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-96  lr=['0.0100000'], tr/val_loss:  0.000880/  2.406424, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-97  lr=['0.0100000'], tr/val_loss:  0.000752/  2.404037, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-98  lr=['0.0100000'], tr/val_loss:  0.000672/  2.427400, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n",
      "epoch-99  lr=['0.0100000'], tr/val_loss:  0.000592/  2.413138, val:  85.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cea670413684e5b8143e305986d32fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▃▃▃▇███████████████████████████████████</td></tr><tr><td>summary_val_acc</td><td>▁▁▃▆▇▆█▇████████████████████████████████</td></tr><tr><td>tr_acc</td><td>▁▄▅▆▇███████████████████████████████████</td></tr><tr><td>tr_epoch_loss</td><td>█▇▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▃▅▇▇▇▇████████████████████████████████</td></tr><tr><td>val_acc_now</td><td>▁▁▃▆▇▆█▇████████████████████████████████</td></tr><tr><td>val_loss</td><td>▇▄▁▁▁▃▃▄▄▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>0.00059</td></tr><tr><td>val_acc_best</td><td>0.86667</td></tr><tr><td>val_acc_now</td><td>0.85417</td></tr><tr><td>val_loss</td><td>2.41314</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peach-sweep-22</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ep1lhqzm' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ep1lhqzm</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_084100-ep1lhqzm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3veb5mjx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_091557-3veb5mjx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3veb5mjx' target=\"_blank\">silver-sweep-24</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3veb5mjx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3veb5mjx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 1, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.25, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': False, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = da7e4b41c275f954fb82e708e43d2b96\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (4): LIF_layer(v_init=0, v_decay=0.25, v_threshold=1, v_reset=10000, sg_width=5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.25, TIME=10, sstep=True, trace_on=True)\n",
      "      (5): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss:  2.519556/  2.623683, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  10.11%\n",
      "epoch-1   lr=['0.1000000'], tr/val_loss:  2.493291/  2.401671, val:  10.00%, val_best:  10.00%, tr:   7.76%, tr_best:  10.11%\n",
      "epoch-2   lr=['0.1000000'], tr/val_loss:  2.471028/  2.441473, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  10.11%\n",
      "epoch-3   lr=['0.1000000'], tr/val_loss:  2.438038/  2.464993, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  10.11%\n",
      "epoch-4   lr=['0.1000000'], tr/val_loss:  2.435606/  2.453039, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.11%\n",
      "epoch-5   lr=['0.1000000'], tr/val_loss:  2.476113/  2.438021, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  10.11%\n",
      "epoch-6   lr=['0.1000000'], tr/val_loss:  2.473328/  2.466004, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  10.11%\n",
      "epoch-7   lr=['0.1000000'], tr/val_loss:  2.467133/  2.461899, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  10.11%\n",
      "epoch-8   lr=['0.1000000'], tr/val_loss:  2.441164/  2.368249, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.11%\n",
      "epoch-9   lr=['0.1000000'], tr/val_loss:  2.425792/  2.485260, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  10.11%\n",
      "epoch-10  lr=['0.1000000'], tr/val_loss:  2.458742/  2.376372, val:  10.00%, val_best:  10.00%, tr:  10.62%, tr_best:  10.62%\n",
      "epoch-11  lr=['0.1000000'], tr/val_loss:  2.451229/  2.414626, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  10.62%\n",
      "epoch-12  lr=['0.1000000'], tr/val_loss:  2.431249/  2.397279, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.62%\n",
      "epoch-13  lr=['0.1000000'], tr/val_loss:  2.449168/  2.426764, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  10.62%\n",
      "epoch-14  lr=['0.1000000'], tr/val_loss:  2.429092/  2.594516, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  10.62%\n",
      "epoch-15  lr=['0.1000000'], tr/val_loss:  2.468597/  2.423184, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  10.62%\n",
      "epoch-16  lr=['0.1000000'], tr/val_loss:  2.495309/  2.347171, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  10.62%\n",
      "epoch-17  lr=['0.1000000'], tr/val_loss:  2.464427/  2.479951, val:  10.00%, val_best:  10.00%, tr:   8.48%, tr_best:  10.62%\n",
      "epoch-18  lr=['0.1000000'], tr/val_loss:  2.455464/  2.348071, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  10.62%\n",
      "epoch-19  lr=['0.1000000'], tr/val_loss:  2.458870/  2.634810, val:  10.00%, val_best:  10.00%, tr:  12.16%, tr_best:  12.16%\n",
      "epoch-20  lr=['0.1000000'], tr/val_loss:  2.532136/  2.630572, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  12.16%\n",
      "epoch-21  lr=['0.1000000'], tr/val_loss:  2.469809/  2.483881, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  12.16%\n",
      "epoch-22  lr=['0.1000000'], tr/val_loss:  2.461555/  2.482759, val:  10.00%, val_best:  10.00%, tr:  10.93%, tr_best:  12.16%\n",
      "epoch-23  lr=['0.1000000'], tr/val_loss:  2.477140/  2.466634, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  12.16%\n",
      "epoch-24  lr=['0.1000000'], tr/val_loss:  2.456871/  2.388859, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-25  lr=['0.1000000'], tr/val_loss:  2.456265/  2.433586, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  12.16%\n",
      "epoch-26  lr=['0.1000000'], tr/val_loss:  2.501490/  2.344920, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  12.16%\n",
      "epoch-27  lr=['0.1000000'], tr/val_loss:  2.454522/  2.404290, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  12.16%\n",
      "epoch-28  lr=['0.1000000'], tr/val_loss:  2.434424/  2.400815, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  12.16%\n",
      "epoch-29  lr=['0.1000000'], tr/val_loss:  2.412901/  2.418111, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  12.16%\n",
      "epoch-30  lr=['0.1000000'], tr/val_loss:  2.445441/  2.436164, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  12.16%\n",
      "epoch-31  lr=['0.1000000'], tr/val_loss:  2.433619/  2.442688, val:  10.00%, val_best:  10.00%, tr:  11.03%, tr_best:  12.16%\n",
      "epoch-32  lr=['0.1000000'], tr/val_loss:  2.522616/  2.481343, val:  10.00%, val_best:  10.00%, tr:   8.07%, tr_best:  12.16%\n",
      "epoch-33  lr=['0.1000000'], tr/val_loss:  2.509247/  2.416504, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-34  lr=['0.1000000'], tr/val_loss:  2.487263/  2.553780, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  12.16%\n",
      "epoch-35  lr=['0.1000000'], tr/val_loss:  2.492729/  2.449025, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  12.16%\n",
      "epoch-36  lr=['0.1000000'], tr/val_loss:  2.446717/  2.395209, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-37  lr=['0.1000000'], tr/val_loss:  2.473824/  2.632330, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  12.16%\n",
      "epoch-38  lr=['0.1000000'], tr/val_loss:  2.493902/  2.432965, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  12.16%\n",
      "epoch-39  lr=['0.1000000'], tr/val_loss:  2.467361/  2.406721, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  12.16%\n",
      "epoch-40  lr=['0.1000000'], tr/val_loss:  2.513397/  2.540425, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  12.16%\n",
      "epoch-41  lr=['0.1000000'], tr/val_loss:  2.518450/  2.505286, val:  10.00%, val_best:  10.00%, tr:  11.03%, tr_best:  12.16%\n",
      "epoch-42  lr=['0.1000000'], tr/val_loss:  2.514067/  2.395864, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  12.16%\n",
      "epoch-43  lr=['0.1000000'], tr/val_loss:  2.499207/  2.696059, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  12.16%\n",
      "epoch-44  lr=['0.1000000'], tr/val_loss:  2.522714/  2.515920, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  12.16%\n",
      "epoch-45  lr=['0.1000000'], tr/val_loss:  2.478578/  2.442224, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  12.16%\n",
      "epoch-46  lr=['0.1000000'], tr/val_loss:  2.504034/  2.360473, val:  10.00%, val_best:  10.00%, tr:   8.89%, tr_best:  12.16%\n",
      "epoch-47  lr=['0.1000000'], tr/val_loss:  2.447176/  2.486052, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  12.16%\n",
      "epoch-48  lr=['0.1000000'], tr/val_loss:  2.491940/  2.418896, val:  10.00%, val_best:  10.00%, tr:   7.56%, tr_best:  12.16%\n",
      "epoch-49  lr=['0.1000000'], tr/val_loss:  2.443237/  2.339746, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  12.16%\n",
      "epoch-50  lr=['0.1000000'], tr/val_loss:  2.434119/  2.539275, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  12.16%\n",
      "epoch-51  lr=['0.1000000'], tr/val_loss:  2.478645/  2.349290, val:  10.00%, val_best:  10.00%, tr:   8.17%, tr_best:  12.16%\n",
      "epoch-52  lr=['0.1000000'], tr/val_loss:  2.445746/  2.427385, val:  10.00%, val_best:  10.00%, tr:  10.73%, tr_best:  12.16%\n",
      "epoch-53  lr=['0.1000000'], tr/val_loss:  2.430613/  2.369613, val:  10.00%, val_best:  10.00%, tr:   9.19%, tr_best:  12.16%\n",
      "epoch-54  lr=['0.1000000'], tr/val_loss:  2.454437/  2.403330, val:  10.00%, val_best:  10.00%, tr:  10.93%, tr_best:  12.16%\n",
      "epoch-55  lr=['0.1000000'], tr/val_loss:  2.488044/  2.367314, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  12.16%\n",
      "epoch-56  lr=['0.1000000'], tr/val_loss:  2.432323/  2.550211, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  12.16%\n",
      "epoch-57  lr=['0.1000000'], tr/val_loss:  2.514803/  2.424870, val:  10.00%, val_best:  10.00%, tr:   7.46%, tr_best:  12.16%\n",
      "epoch-58  lr=['0.1000000'], tr/val_loss:  2.499757/  2.403315, val:  10.00%, val_best:  10.00%, tr:   8.99%, tr_best:  12.16%\n",
      "epoch-59  lr=['0.1000000'], tr/val_loss:  2.465906/  2.430620, val:  10.00%, val_best:  10.00%, tr:  10.62%, tr_best:  12.16%\n",
      "epoch-60  lr=['0.1000000'], tr/val_loss:  2.475043/  2.453509, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  12.16%\n",
      "epoch-61  lr=['0.1000000'], tr/val_loss:  2.549381/  2.397437, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  12.16%\n",
      "epoch-62  lr=['0.1000000'], tr/val_loss:  2.499790/  2.493794, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-63  lr=['0.1000000'], tr/val_loss:  2.446735/  2.381926, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  12.16%\n",
      "epoch-64  lr=['0.1000000'], tr/val_loss:  2.424499/  2.391928, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  12.16%\n",
      "epoch-65  lr=['0.1000000'], tr/val_loss:  2.439331/  2.378643, val:  10.00%, val_best:  10.00%, tr:   8.78%, tr_best:  12.16%\n",
      "epoch-66  lr=['0.1000000'], tr/val_loss:  2.469206/  2.384904, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:  12.16%\n",
      "epoch-67  lr=['0.1000000'], tr/val_loss:  2.469288/  2.343184, val:  10.00%, val_best:  10.00%, tr:  10.83%, tr_best:  12.16%\n",
      "epoch-68  lr=['0.1000000'], tr/val_loss:  2.409303/  2.465325, val:  10.00%, val_best:  10.00%, tr:  11.13%, tr_best:  12.16%\n",
      "epoch-69  lr=['0.1000000'], tr/val_loss:  2.487381/  2.429583, val:  10.00%, val_best:  10.00%, tr:   7.66%, tr_best:  12.16%\n",
      "epoch-70  lr=['0.1000000'], tr/val_loss:  2.452406/  2.391922, val:  10.00%, val_best:  10.00%, tr:  10.62%, tr_best:  12.16%\n",
      "epoch-71  lr=['0.1000000'], tr/val_loss:  2.482408/  2.467104, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  12.16%\n",
      "epoch-72  lr=['0.1000000'], tr/val_loss:  2.527320/  2.480931, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-73  lr=['0.1000000'], tr/val_loss:  2.479205/  2.480726, val:  10.00%, val_best:  10.00%, tr:  10.83%, tr_best:  12.16%\n",
      "epoch-74  lr=['0.1000000'], tr/val_loss:  2.477507/  2.345716, val:  10.00%, val_best:  10.00%, tr:  11.03%, tr_best:  12.16%\n",
      "epoch-75  lr=['0.1000000'], tr/val_loss:  2.493409/  2.556630, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  12.16%\n",
      "epoch-76  lr=['0.1000000'], tr/val_loss:  2.454170/  2.432463, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  12.16%\n",
      "epoch-77  lr=['0.1000000'], tr/val_loss:  2.490857/  2.434645, val:  10.00%, val_best:  10.00%, tr:  10.21%, tr_best:  12.16%\n",
      "epoch-78  lr=['0.1000000'], tr/val_loss:  2.463014/  2.595895, val:  10.00%, val_best:  10.00%, tr:   7.87%, tr_best:  12.16%\n",
      "epoch-79  lr=['0.1000000'], tr/val_loss:  2.508440/  2.478057, val:  10.00%, val_best:  10.00%, tr:   9.70%, tr_best:  12.16%\n",
      "epoch-80  lr=['0.1000000'], tr/val_loss:  2.421750/  2.417755, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  12.16%\n",
      "epoch-81  lr=['0.1000000'], tr/val_loss:  2.482896/  2.511716, val:  10.00%, val_best:  10.00%, tr:   8.58%, tr_best:  12.16%\n",
      "epoch-82  lr=['0.1000000'], tr/val_loss:  2.528684/  2.642655, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  12.16%\n",
      "epoch-83  lr=['0.1000000'], tr/val_loss:  2.477893/  2.486252, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-84  lr=['0.1000000'], tr/val_loss:  2.489694/  2.640080, val:  10.00%, val_best:  10.00%, tr:  10.52%, tr_best:  12.16%\n",
      "epoch-85  lr=['0.1000000'], tr/val_loss:  2.458414/  2.434928, val:  10.00%, val_best:  10.00%, tr:   9.91%, tr_best:  12.16%\n",
      "epoch-86  lr=['0.1000000'], tr/val_loss:  2.435616/  2.556397, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  12.16%\n",
      "epoch-87  lr=['0.1000000'], tr/val_loss:  2.441567/  2.417959, val:  10.00%, val_best:  10.00%, tr:  10.11%, tr_best:  12.16%\n",
      "epoch-88  lr=['0.1000000'], tr/val_loss:  2.466650/  2.553967, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  12.16%\n",
      "epoch-89  lr=['0.1000000'], tr/val_loss:  2.461328/  2.407372, val:  10.00%, val_best:  10.00%, tr:  10.42%, tr_best:  12.16%\n",
      "epoch-90  lr=['0.1000000'], tr/val_loss:  2.459861/  2.420143, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  12.16%\n",
      "epoch-91  lr=['0.1000000'], tr/val_loss:  2.478359/  2.570884, val:  10.00%, val_best:  10.00%, tr:   9.30%, tr_best:  12.16%\n",
      "epoch-92  lr=['0.1000000'], tr/val_loss:  2.444057/  2.349605, val:  10.00%, val_best:  10.00%, tr:   8.68%, tr_best:  12.16%\n",
      "epoch-93  lr=['0.1000000'], tr/val_loss:  2.427895/  2.393443, val:  10.00%, val_best:  10.00%, tr:   9.60%, tr_best:  12.16%\n",
      "epoch-94  lr=['0.1000000'], tr/val_loss:  2.470145/  2.520800, val:  10.00%, val_best:  10.00%, tr:   9.40%, tr_best:  12.16%\n",
      "epoch-95  lr=['0.1000000'], tr/val_loss:  2.432628/  2.381572, val:  10.00%, val_best:  10.00%, tr:   9.09%, tr_best:  12.16%\n",
      "epoch-96  lr=['0.1000000'], tr/val_loss:  2.468352/  2.435434, val:  10.00%, val_best:  10.00%, tr:   9.50%, tr_best:  12.16%\n",
      "epoch-97  lr=['0.1000000'], tr/val_loss:  2.457888/  2.466409, val:  10.00%, val_best:  10.00%, tr:  11.44%, tr_best:  12.16%\n",
      "epoch-98  lr=['0.1000000'], tr/val_loss:  2.474241/  2.495883, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  12.16%\n",
      "epoch-99  lr=['0.1000000'], tr/val_loss:  2.501154/  2.424966, val:  10.00%, val_best:  10.00%, tr:   9.81%, tr_best:  12.16%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241812d87a0c4c0e8fc6b0fc457a8a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▅▅▂▂█▁▄▄▂▆▄▁▅▂▂▅▆▆▂▄▅▂▄▂▅▂▁▂▄▅▄▄▂▆▂▁▁▁█▂</td></tr><tr><td>summary_val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tr_acc</td><td>▅▃▄▄▄▄▅▃█▃▅▄▄▂▅▄▃▄▂▄▃▆▆▁▆▅▃▆▆▅▃▅▄▃▆▅▅▃▃▇</td></tr><tr><td>tr_epoch_loss</td><td>▆▄▂▄▂▂▂▄▃▄▃▆▁▇▅▄▄▆▇▃▃▃▃▆▄█▂▄▃▇▅▅▆▇▅▂▃▃▂▃</td></tr><tr><td>val_acc_best</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_now</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▄▄▄▂▇▄█▄▂▁▃▄▄█▃▂▅▄▁▃▂▃▃▂▂▁▂▄▆▃▄██▃▃▁▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>0.0</td></tr><tr><td>tr_acc</td><td>0.09806</td></tr><tr><td>tr_epoch_loss</td><td>2.50115</td></tr><tr><td>val_acc_best</td><td>0.1</td></tr><tr><td>val_acc_now</td><td>0.1</td></tr><tr><td>val_loss</td><td>2.42497</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silver-sweep-24</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3veb5mjx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/3veb5mjx</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_091557-3veb5mjx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m8wuuioh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b966bb0fe2f45cebb086790da2bd551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113062599259945, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_095002-m8wuuioh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m8wuuioh' target=\"_blank\">misunderstood-sweep-25</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m8wuuioh' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m8wuuioh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 2, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.0001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = da7e4b41c275f954fb82e708e43d2b96\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=2, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.0001000'], tr/val_loss:  2.303292/  2.303262, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "epoch-1   lr=['0.0001000'], tr/val_loss:  2.303565/  2.302781, val:  10.00%, val_best:  10.00%, tr:  10.01%, tr_best:  10.01%\n",
      "epoch-2   lr=['0.0001000'], tr/val_loss:  2.304521/  2.302024, val:   7.92%, val_best:  10.00%, tr:   8.38%, tr_best:  10.01%\n",
      "epoch-3   lr=['0.0001000'], tr/val_loss:  2.299979/  2.299409, val:   5.42%, val_best:  10.00%, tr:   7.76%, tr_best:  10.01%\n",
      "epoch-4   lr=['0.0001000'], tr/val_loss:  2.282019/  2.269578, val:  18.33%, val_best:  18.33%, tr:  12.26%, tr_best:  12.26%\n",
      "epoch-5   lr=['0.0001000'], tr/val_loss:  2.226830/  2.192488, val:  26.25%, val_best:  26.25%, tr:  21.25%, tr_best:  21.25%\n",
      "epoch-6   lr=['0.0001000'], tr/val_loss:  2.120531/  2.067294, val:  32.92%, val_best:  32.92%, tr:  30.85%, tr_best:  30.85%\n",
      "epoch-7   lr=['0.0001000'], tr/val_loss:  1.966450/  1.919385, val:  47.08%, val_best:  47.08%, tr:  39.43%, tr_best:  39.43%\n",
      "epoch-8   lr=['0.0001000'], tr/val_loss:  1.816178/  1.785744, val:  44.17%, val_best:  47.08%, tr:  46.07%, tr_best:  46.07%\n",
      "epoch-9   lr=['0.0001000'], tr/val_loss:  1.685283/  1.688898, val:  45.00%, val_best:  47.08%, tr:  47.09%, tr_best:  47.09%\n",
      "epoch-10  lr=['0.0001000'], tr/val_loss:  1.595544/  1.613688, val:  42.50%, val_best:  47.08%, tr:  47.40%, tr_best:  47.40%\n",
      "epoch-11  lr=['0.0001000'], tr/val_loss:  1.522360/  1.555790, val:  44.58%, val_best:  47.08%, tr:  50.05%, tr_best:  50.05%\n",
      "epoch-12  lr=['0.0001000'], tr/val_loss:  1.459719/  1.508050, val:  44.17%, val_best:  47.08%, tr:  54.14%, tr_best:  54.14%\n",
      "epoch-13  lr=['0.0001000'], tr/val_loss:  1.412628/  1.473799, val:  45.83%, val_best:  47.08%, tr:  54.75%, tr_best:  54.75%\n",
      "epoch-14  lr=['0.0001000'], tr/val_loss:  1.364708/  1.440151, val:  50.00%, val_best:  50.00%, tr:  58.12%, tr_best:  58.12%\n",
      "epoch-15  lr=['0.0001000'], tr/val_loss:  1.328768/  1.407826, val:  51.67%, val_best:  51.67%, tr:  59.86%, tr_best:  59.86%\n",
      "epoch-16  lr=['0.0001000'], tr/val_loss:  1.287401/  1.380735, val:  48.75%, val_best:  51.67%, tr:  60.27%, tr_best:  60.27%\n",
      "epoch-17  lr=['0.0001000'], tr/val_loss:  1.264116/  1.359183, val:  52.08%, val_best:  52.08%, tr:  61.59%, tr_best:  61.59%\n",
      "epoch-18  lr=['0.0001000'], tr/val_loss:  1.233581/  1.343337, val:  52.92%, val_best:  52.92%, tr:  64.25%, tr_best:  64.25%\n",
      "epoch-19  lr=['0.0001000'], tr/val_loss:  1.208873/  1.327057, val:  54.17%, val_best:  54.17%, tr:  60.98%, tr_best:  64.25%\n",
      "epoch-20  lr=['0.0001000'], tr/val_loss:  1.179466/  1.309702, val:  55.00%, val_best:  55.00%, tr:  63.33%, tr_best:  64.25%\n",
      "epoch-21  lr=['0.0001000'], tr/val_loss:  1.157392/  1.294686, val:  54.17%, val_best:  55.00%, tr:  64.04%, tr_best:  64.25%\n",
      "epoch-22  lr=['0.0001000'], tr/val_loss:  1.139800/  1.282123, val:  55.42%, val_best:  55.42%, tr:  63.43%, tr_best:  64.25%\n",
      "epoch-23  lr=['0.0001000'], tr/val_loss:  1.115704/  1.269093, val:  56.25%, val_best:  56.25%, tr:  66.09%, tr_best:  66.09%\n",
      "epoch-24  lr=['0.0001000'], tr/val_loss:  1.103693/  1.261163, val:  55.00%, val_best:  56.25%, tr:  66.70%, tr_best:  66.70%\n",
      "epoch-25  lr=['0.0001000'], tr/val_loss:  1.081323/  1.253765, val:  57.50%, val_best:  57.50%, tr:  65.88%, tr_best:  66.70%\n",
      "epoch-26  lr=['0.0001000'], tr/val_loss:  1.071678/  1.239429, val:  56.25%, val_best:  57.50%, tr:  66.39%, tr_best:  66.70%\n",
      "epoch-27  lr=['0.0001000'], tr/val_loss:  1.056379/  1.228842, val:  57.50%, val_best:  57.50%, tr:  68.13%, tr_best:  68.13%\n",
      "epoch-28  lr=['0.0001000'], tr/val_loss:  1.055180/  1.219286, val:  57.92%, val_best:  57.92%, tr:  66.39%, tr_best:  68.13%\n",
      "epoch-29  lr=['0.0001000'], tr/val_loss:  1.034562/  1.214045, val:  56.25%, val_best:  57.92%, tr:  68.64%, tr_best:  68.64%\n",
      "epoch-30  lr=['0.0001000'], tr/val_loss:  1.019655/  1.208694, val:  56.67%, val_best:  57.92%, tr:  69.25%, tr_best:  69.25%\n",
      "epoch-31  lr=['0.0001000'], tr/val_loss:  1.012964/  1.201674, val:  57.92%, val_best:  57.92%, tr:  67.72%, tr_best:  69.25%\n",
      "epoch-32  lr=['0.0001000'], tr/val_loss:  1.000541/  1.199170, val:  59.58%, val_best:  59.58%, tr:  69.25%, tr_best:  69.25%\n",
      "epoch-33  lr=['0.0001000'], tr/val_loss:  1.008011/  1.191299, val:  62.08%, val_best:  62.08%, tr:  68.95%, tr_best:  69.25%\n",
      "epoch-34  lr=['0.0001000'], tr/val_loss:  0.985649/  1.183954, val:  59.17%, val_best:  62.08%, tr:  69.56%, tr_best:  69.56%\n",
      "epoch-35  lr=['0.0001000'], tr/val_loss:  0.975487/  1.183689, val:  60.83%, val_best:  62.08%, tr:  69.46%, tr_best:  69.56%\n",
      "epoch-36  lr=['0.0001000'], tr/val_loss:  0.970502/  1.177477, val:  60.42%, val_best:  62.08%, tr:  72.52%, tr_best:  72.52%\n",
      "epoch-37  lr=['0.0001000'], tr/val_loss:  0.955092/  1.174662, val:  56.25%, val_best:  62.08%, tr:  71.71%, tr_best:  72.52%\n",
      "epoch-38  lr=['0.0001000'], tr/val_loss:  0.950636/  1.171923, val:  61.67%, val_best:  62.08%, tr:  71.20%, tr_best:  72.52%\n",
      "epoch-39  lr=['0.0001000'], tr/val_loss:  0.945159/  1.170694, val:  60.83%, val_best:  62.08%, tr:  73.24%, tr_best:  73.24%\n",
      "epoch-40  lr=['0.0001000'], tr/val_loss:  0.927910/  1.170555, val:  59.17%, val_best:  62.08%, tr:  72.01%, tr_best:  73.24%\n",
      "epoch-41  lr=['0.0001000'], tr/val_loss:  0.933369/  1.162266, val:  60.83%, val_best:  62.08%, tr:  74.16%, tr_best:  74.16%\n",
      "epoch-42  lr=['0.0001000'], tr/val_loss:  0.910890/  1.163146, val:  62.08%, val_best:  62.08%, tr:  74.46%, tr_best:  74.46%\n",
      "epoch-43  lr=['0.0001000'], tr/val_loss:  0.910774/  1.149311, val:  60.83%, val_best:  62.08%, tr:  75.28%, tr_best:  75.28%\n",
      "epoch-44  lr=['0.0001000'], tr/val_loss:  0.901676/  1.155656, val:  59.58%, val_best:  62.08%, tr:  74.26%, tr_best:  75.28%\n",
      "epoch-45  lr=['0.0001000'], tr/val_loss:  0.892067/  1.147409, val:  61.25%, val_best:  62.08%, tr:  75.08%, tr_best:  75.28%\n",
      "epoch-46  lr=['0.0001000'], tr/val_loss:  0.884408/  1.140897, val:  63.75%, val_best:  63.75%, tr:  75.59%, tr_best:  75.59%\n",
      "epoch-47  lr=['0.0001000'], tr/val_loss:  0.892230/  1.141765, val:  63.75%, val_best:  63.75%, tr:  72.93%, tr_best:  75.59%\n",
      "epoch-48  lr=['0.0001000'], tr/val_loss:  0.875245/  1.137713, val:  62.08%, val_best:  63.75%, tr:  77.63%, tr_best:  77.63%\n",
      "epoch-49  lr=['0.0001000'], tr/val_loss:  0.867650/  1.135297, val:  60.42%, val_best:  63.75%, tr:  77.43%, tr_best:  77.63%\n",
      "epoch-50  lr=['0.0001000'], tr/val_loss:  0.859358/  1.126545, val:  62.50%, val_best:  63.75%, tr:  76.51%, tr_best:  77.63%\n",
      "epoch-51  lr=['0.0001000'], tr/val_loss:  0.857265/  1.131813, val:  67.08%, val_best:  67.08%, tr:  77.02%, tr_best:  77.63%\n",
      "epoch-52  lr=['0.0001000'], tr/val_loss:  0.848305/  1.123941, val:  65.42%, val_best:  67.08%, tr:  78.55%, tr_best:  78.55%\n",
      "epoch-53  lr=['0.0001000'], tr/val_loss:  0.842287/  1.124425, val:  66.25%, val_best:  67.08%, tr:  78.35%, tr_best:  78.55%\n",
      "epoch-54  lr=['0.0001000'], tr/val_loss:  0.836176/  1.123872, val:  67.50%, val_best:  67.50%, tr:  79.67%, tr_best:  79.67%\n",
      "epoch-55  lr=['0.0001000'], tr/val_loss:  0.829539/  1.126080, val:  65.00%, val_best:  67.50%, tr:  78.86%, tr_best:  79.67%\n",
      "epoch-56  lr=['0.0001000'], tr/val_loss:  0.818958/  1.122020, val:  67.08%, val_best:  67.50%, tr:  80.18%, tr_best:  80.18%\n",
      "epoch-57  lr=['0.0001000'], tr/val_loss:  0.813040/  1.124902, val:  65.83%, val_best:  67.50%, tr:  80.59%, tr_best:  80.59%\n",
      "epoch-58  lr=['0.0001000'], tr/val_loss:  0.807014/  1.118893, val:  66.25%, val_best:  67.50%, tr:  80.18%, tr_best:  80.59%\n",
      "epoch-59  lr=['0.0001000'], tr/val_loss:  0.805567/  1.111039, val:  66.25%, val_best:  67.50%, tr:  80.69%, tr_best:  80.69%\n",
      "epoch-60  lr=['0.0001000'], tr/val_loss:  0.800470/  1.110017, val:  67.08%, val_best:  67.50%, tr:  79.37%, tr_best:  80.69%\n",
      "epoch-61  lr=['0.0001000'], tr/val_loss:  0.799123/  1.116649, val:  67.92%, val_best:  67.92%, tr:  81.72%, tr_best:  81.72%\n",
      "epoch-62  lr=['0.0001000'], tr/val_loss:  0.795587/  1.107449, val:  69.58%, val_best:  69.58%, tr:  81.82%, tr_best:  81.82%\n",
      "epoch-63  lr=['0.0001000'], tr/val_loss:  0.784549/  1.100415, val:  70.42%, val_best:  70.42%, tr:  83.15%, tr_best:  83.15%\n",
      "epoch-64  lr=['0.0001000'], tr/val_loss:  0.775739/  1.100131, val:  69.58%, val_best:  70.42%, tr:  83.45%, tr_best:  83.45%\n",
      "epoch-65  lr=['0.0001000'], tr/val_loss:  0.775598/  1.103866, val:  72.08%, val_best:  72.08%, tr:  83.35%, tr_best:  83.45%\n",
      "epoch-66  lr=['0.0001000'], tr/val_loss:  0.768696/  1.104411, val:  70.83%, val_best:  72.08%, tr:  83.25%, tr_best:  83.45%\n",
      "epoch-67  lr=['0.0001000'], tr/val_loss:  0.766542/  1.098404, val:  69.17%, val_best:  72.08%, tr:  83.66%, tr_best:  83.66%\n",
      "epoch-68  lr=['0.0001000'], tr/val_loss:  0.760003/  1.104742, val:  70.00%, val_best:  72.08%, tr:  82.84%, tr_best:  83.66%\n",
      "epoch-69  lr=['0.0001000'], tr/val_loss:  0.752030/  1.104292, val:  70.00%, val_best:  72.08%, tr:  84.98%, tr_best:  84.98%\n",
      "epoch-70  lr=['0.0001000'], tr/val_loss:  0.744000/  1.092972, val:  72.50%, val_best:  72.50%, tr:  84.58%, tr_best:  84.98%\n",
      "epoch-71  lr=['0.0001000'], tr/val_loss:  0.738786/  1.101279, val:  68.75%, val_best:  72.50%, tr:  85.70%, tr_best:  85.70%\n",
      "epoch-72  lr=['0.0001000'], tr/val_loss:  0.729748/  1.098697, val:  70.83%, val_best:  72.50%, tr:  85.19%, tr_best:  85.70%\n",
      "epoch-73  lr=['0.0001000'], tr/val_loss:  0.730684/  1.089390, val:  75.00%, val_best:  75.00%, tr:  86.82%, tr_best:  86.82%\n",
      "epoch-74  lr=['0.0001000'], tr/val_loss:  0.722201/  1.100035, val:  73.75%, val_best:  75.00%, tr:  85.90%, tr_best:  86.82%\n",
      "epoch-75  lr=['0.0001000'], tr/val_loss:  0.719693/  1.095944, val:  72.92%, val_best:  75.00%, tr:  86.31%, tr_best:  86.82%\n",
      "epoch-76  lr=['0.0001000'], tr/val_loss:  0.704194/  1.097628, val:  67.08%, val_best:  75.00%, tr:  86.41%, tr_best:  86.82%\n",
      "epoch-77  lr=['0.0001000'], tr/val_loss:  0.716285/  1.093883, val:  74.17%, val_best:  75.00%, tr:  88.15%, tr_best:  88.15%\n",
      "epoch-78  lr=['0.0001000'], tr/val_loss:  0.700688/  1.088707, val:  74.58%, val_best:  75.00%, tr:  87.74%, tr_best:  88.15%\n",
      "epoch-79  lr=['0.0001000'], tr/val_loss:  0.701650/  1.088673, val:  75.00%, val_best:  75.00%, tr:  87.74%, tr_best:  88.15%\n",
      "epoch-80  lr=['0.0001000'], tr/val_loss:  0.691733/  1.082962, val:  72.92%, val_best:  75.00%, tr:  87.44%, tr_best:  88.15%\n",
      "epoch-81  lr=['0.0001000'], tr/val_loss:  0.686824/  1.090324, val:  75.00%, val_best:  75.00%, tr:  89.17%, tr_best:  89.17%\n",
      "epoch-82  lr=['0.0001000'], tr/val_loss:  0.680676/  1.089668, val:  73.33%, val_best:  75.00%, tr:  89.68%, tr_best:  89.68%\n",
      "epoch-83  lr=['0.0001000'], tr/val_loss:  0.675039/  1.080112, val:  76.25%, val_best:  76.25%, tr:  89.79%, tr_best:  89.79%\n",
      "epoch-84  lr=['0.0001000'], tr/val_loss:  0.667669/  1.081816, val:  75.83%, val_best:  76.25%, tr:  90.30%, tr_best:  90.30%\n",
      "epoch-85  lr=['0.0001000'], tr/val_loss:  0.673846/  1.095159, val:  72.92%, val_best:  76.25%, tr:  89.68%, tr_best:  90.30%\n",
      "epoch-86  lr=['0.0001000'], tr/val_loss:  0.657642/  1.093883, val:  72.92%, val_best:  76.25%, tr:  88.97%, tr_best:  90.30%\n",
      "epoch-87  lr=['0.0001000'], tr/val_loss:  0.658376/  1.085453, val:  74.17%, val_best:  76.25%, tr:  90.09%, tr_best:  90.30%\n",
      "epoch-88  lr=['0.0001000'], tr/val_loss:  0.655375/  1.092839, val:  71.25%, val_best:  76.25%, tr:  90.50%, tr_best:  90.50%\n",
      "epoch-89  lr=['0.0001000'], tr/val_loss:  0.642680/  1.096187, val:  73.33%, val_best:  76.25%, tr:  90.40%, tr_best:  90.50%\n",
      "epoch-90  lr=['0.0001000'], tr/val_loss:  0.644818/  1.082837, val:  75.42%, val_best:  76.25%, tr:  91.11%, tr_best:  91.11%\n",
      "epoch-91  lr=['0.0001000'], tr/val_loss:  0.636925/  1.085885, val:  75.00%, val_best:  76.25%, tr:  91.22%, tr_best:  91.22%\n",
      "epoch-92  lr=['0.0001000'], tr/val_loss:  0.634970/  1.081253, val:  75.00%, val_best:  76.25%, tr:  90.50%, tr_best:  91.22%\n",
      "epoch-93  lr=['0.0001000'], tr/val_loss:  0.634685/  1.084780, val:  77.92%, val_best:  77.92%, tr:  92.24%, tr_best:  92.24%\n",
      "epoch-94  lr=['0.0001000'], tr/val_loss:  0.631095/  1.083001, val:  75.00%, val_best:  77.92%, tr:  91.11%, tr_best:  92.24%\n",
      "epoch-95  lr=['0.0001000'], tr/val_loss:  0.615419/  1.086449, val:  75.00%, val_best:  77.92%, tr:  91.73%, tr_best:  92.24%\n",
      "epoch-96  lr=['0.0001000'], tr/val_loss:  0.611237/  1.084805, val:  75.83%, val_best:  77.92%, tr:  91.83%, tr_best:  92.24%\n",
      "epoch-97  lr=['0.0001000'], tr/val_loss:  0.605772/  1.096185, val:  74.17%, val_best:  77.92%, tr:  91.73%, tr_best:  92.24%\n",
      "epoch-98  lr=['0.0001000'], tr/val_loss:  0.605468/  1.076368, val:  78.33%, val_best:  78.33%, tr:  92.85%, tr_best:  92.85%\n",
      "epoch-99  lr=['0.0001000'], tr/val_loss:  0.600746/  1.093953, val:  75.83%, val_best:  78.33%, tr:  91.62%, tr_best:  92.85%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5cf5c1986f47f88c48297d83e7120a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>iter_acc</td><td>▁▁▁▃▄▄▆▅▆▅▆▇▅▅▆▆▇▆▇▅▇▆▆██▇▇██▇▇▇▆▇█▆▆▇█▇</td></tr><tr><td>summary_val_acc</td><td>▁▁▂▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▆▇▆▇▇▇▇▇█▇█▇██████████</td></tr><tr><td>tr_acc</td><td>▁▁▁▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>tr_epoch_loss</td><td>███▇▅▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc_best</td><td>▁▁▂▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>val_acc_now</td><td>▁▁▂▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▆▇▆▇▇▇▇▇█▇█▇██████████</td></tr><tr><td>val_loss</td><td>███▆▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>0.91624</td></tr><tr><td>tr_epoch_loss</td><td>0.60075</td></tr><tr><td>val_acc_best</td><td>0.78333</td></tr><tr><td>val_acc_now</td><td>0.75833</td></tr><tr><td>val_loss</td><td>1.09395</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misunderstood-sweep-25</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m8wuuioh' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/m8wuuioh</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20250430_095002-m8wuuioh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j49bu3vy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: net_save/save_now_net_weights_{unique_name}.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243cdb1c1a0d49bfbee2f89fd402bf13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113316286355257, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20250430_102528-j49bu3vy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j49bu3vy' target=\"_blank\">devoted-sweep-28</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/hfzt99hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j49bu3vy' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/j49bu3vy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 3, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.1, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8, 'current epoch': 0} \n",
      "\n",
      "dataset_hash = ddd2d600e882f330b6acfa4755a9bed1\n",
      "cache path exists\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=2560, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=True, sstep=True)\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.25, v_threshold=0.75, v_reset=0, sg_width=3, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0, TIME=10, sstep=True, trace_on=True)\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=True, sstep=True)\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 554,410\n",
      "========================================================\n",
      "\n",
      "epoch-0   lr=['0.1000000'], tr/val_loss: 17.793169/ 25.405861, val:  26.67%, val_best:  26.67%, tr:  30.13%, tr_best:  30.13%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n"
     ]
    }
   ],
   "source": [
    "# sweep 하는 코드, 위 셀 주석처리 해야 됨.\n",
    "\n",
    "# 이런 워닝 뜨는 거는 걍 너가 main 안에서  wandb.config.update(hyperparameters)할 때 물려서임. 어차피 근데 sweep에서 지정한 걸로 덮어짐 \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes', # 'random', 'bayes'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [16]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [128]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.25]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.25, 0.5, 0.75, 1.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0, 0.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [1.0,2.0,3.0,4.0,5.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0, 0.25]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"net_save/save_now_net_weights_{unique_name}.pth\"]},\n",
    "        \"learning_rate\": {\"values\": [0.001,0.01,0.1,0.0001]}, \n",
    "        \"epoch_num\": {\"values\": [100]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [2]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True, False]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [True]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [False]},\n",
    "        \"denoise_on\": {\"values\": [True, False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [0]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [True]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [8]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  unique_name_hyper,\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "                        ) \n",
    "    # sigmoid와 BN이 있어야 잘된다.\n",
    "    # average pooling\n",
    "    # 이 낫다. \n",
    "    \n",
    "    # nda에서는 decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT 에서는 decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'hfzt99hb'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
