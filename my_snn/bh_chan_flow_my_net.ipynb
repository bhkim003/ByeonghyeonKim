{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ground-truth spike로 AE train data 꾸리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# os.chdir(\"./../data/\")\n",
    "my_path_ground_BH = '/data2/spike_sorting/quiroga/BH/'\n",
    "\n",
    "\n",
    "filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "            \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "            \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "            \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "\n",
    "spike_tot = [\"BH_Spike_e1n005.npy\", \"BH_Spike_e1n010.npy\", \"BH_Spike_e1n015.npy\", \"BH_Spike_e1n020.npy\",\n",
    "            \"BH_Spike_e2n005.npy\", \"BH_Spike_e2n010.npy\", \"BH_Spike_e2n015.npy\", \"BH_Spike_e2n020.npy\",\n",
    "            \"BH_Spike_d1n005.npy\", \"BH_Spike_d1n010.npy\", \"BH_Spike_d1n015.npy\", \"BH_Spike_d1n020.npy\",\n",
    "            \"BH_Spike_d2n005.npy\", \"BH_Spike_d2n010.npy\", \"BH_Spike_d2n015.npy\", \"BH_Spike_d2n020.npy\"]\n",
    "\n",
    "label_tot = [\"BH_Label_e1n005.npy\", \"BH_Label_e1n010.npy\", \"BH_Label_e1n015.npy\", \"BH_Label_e1n020.npy\",\n",
    "            \"BH_Label_e2n005.npy\", \"BH_Label_e2n010.npy\", \"BH_Label_e2n015.npy\", \"BH_Label_e2n020.npy\",\n",
    "            \"BH_Label_d1n005.npy\", \"BH_Label_d1n010.npy\", \"BH_Label_d1n015.npy\", \"BH_Label_d1n020.npy\",\n",
    "            \"BH_Label_d2n005.npy\", \"BH_Label_d2n010.npy\", \"BH_Label_d2n015.npy\", \"BH_Label_d2n020.npy\"]\n",
    "\n",
    "\n",
    "dataset_num = 16\n",
    "training_num = 2400\n",
    "spike_length = 50\n",
    "\n",
    "spike_train = [] # 스파이크 데이터를 저장할 배열\n",
    "spike_test = [] # 스파이크 데이터를 저장할 배열\n",
    "\n",
    "for ds in range(dataset_num):\n",
    "    print(\"\\ndata:\", filename[ds])\n",
    "    mat1 = io.loadmat(my_path_ground_BH + filename[ds])\n",
    "    raw = mat1['data'][0]\n",
    "    ans_times = mat1['spike_times'][0][0][0]\n",
    "    ans_cluster = mat1['spike_class'][0][0][0]\n",
    "\n",
    "    spike_this_dataset = []\n",
    "    labal_this_dataset = []\n",
    "\n",
    "    # raw 데이터의 기울기 계산\n",
    "    slope = np.diff(raw) # raw보다 한 사이즈 작음.\n",
    "\n",
    "    spike_group = np.zeros((len(ans_times), spike_length))\n",
    "\n",
    "    train_spike_count = 0\n",
    "    for i in range(len(ans_times)):\n",
    "        max_slope_index = ans_times[i] + np.argmax(slope[ans_times[i] : ans_times[i] + 25])\n",
    "        now_spike = raw[max_slope_index - 10 : max_slope_index - 10 + spike_length]\n",
    "        spike_this_dataset.append(now_spike)\n",
    "        labal_this_dataset.append(ans_cluster[i])\n",
    "        if train_spike_count < training_num:\n",
    "            train_spike_count += 1\n",
    "            spike_train.append(now_spike)\n",
    "        else:\n",
    "            spike_test.append(now_spike)\n",
    "\n",
    "    spike_this_dataset = np.array(spike_this_dataset)\n",
    "    labal_this_dataset = np.array(labal_this_dataset)\n",
    "\n",
    "    np.save(my_path_ground_BH + spike_tot[ds], spike_this_dataset)\n",
    "    np.save(my_path_ground_BH + label_tot[ds], labal_this_dataset)\n",
    "\n",
    "spike_train = np.array(spike_train)\n",
    "spike_test = np.array(spike_test)\n",
    "np.random.shuffle(spike_train)\n",
    "np.random.shuffle(spike_test)\n",
    "\n",
    "torch.save(torch.tensor(spike_train, dtype=torch.float32), my_path_ground_BH + 'BH_training_dataset_gt_detect.pt')\n",
    "torch.save(torch.tensor(spike_test, dtype=torch.float32), my_path_ground_BH + 'BH_test_dataset_gt_detect.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detected spike로 AE train data와 acc 측정을 위한 스파이크 꾸리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io\n",
    "import os\n",
    "\n",
    "# os.chdir(\"./../data/\")\n",
    "my_path_ground_BH = '/data2/spike_sorting/quiroga/BH/'\n",
    "\n",
    "filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "            \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "            \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "            \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "thr_tot = np.array([0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7])\n",
    "\n",
    "dataset_num = 16\n",
    "training_num = 2400\n",
    "spike_length = 50\n",
    "wait_term = 20\n",
    "\n",
    "spike_train = [] # 스파이크 데이터를 저장할 배열\n",
    "spike_test = [] # 스파이크 데이터를 저장할 배열\n",
    "\n",
    "for ds in range(dataset_num):\n",
    "    print(\"\\ndata:\", filename[ds])\n",
    "    past_tr_num = len(spike_train)\n",
    "    past_te_num = len(spike_test)\n",
    "    train_spike_count = 0\n",
    "\n",
    "    # 데이터 파일 불러오기\n",
    "    mat1 = io.loadmat(my_path_ground_BH + filename[ds])\n",
    "    raw = mat1['data'][0]\n",
    "    thr = thr_tot[ds]  # 스파이크 탐지 임계값 설정 \n",
    "    \n",
    "    # raw 데이터의 기울기 계산\n",
    "    slope = np.diff(raw) # raw보다 한 사이즈 작음.\n",
    "    \n",
    "    # 스파이크 탐지\n",
    "    wait = -20  # 스파이크 탐지 대기 시간 초기화: 처음에는 20샘플은 버림.\n",
    "    for i in range(len(raw)-2):\n",
    "        wait += 1\n",
    "        if(wait_term < wait):\n",
    "            if(raw[i+1] < raw[i+2] and raw[i+1] <= raw[i] and raw[i+1] < -thr) or (raw[i+1] > raw[i+2] and raw[i] <= raw[i+1] and raw[i+1] > thr):\n",
    "                max_slope_index = i + np.argmax(slope[i - 8 : i + 5]) - 8 # 기울기가 최대인 지점에서 스파이크 추출\n",
    "                if train_spike_count < training_num:\n",
    "                    train_spike_count += 1\n",
    "                    spike_train.append(raw[max_slope_index - 10 : max_slope_index - 10 + spike_length]) \n",
    "                else:\n",
    "                    spike_test.append(raw[max_slope_index - 10 : max_slope_index - 10 + spike_length])\n",
    "                wait = 0  # 대기 시간 초기화 # 다시 wait_term만큼 기다려라\n",
    "\n",
    "                # if train_spike_count == 1: # 그림으로 보기\n",
    "                #     plt.plot(raw[max_slope_index - 10 : max_slope_index + 40])\n",
    "                    \n",
    "                #     plt.title(f\"align, max_slope_index={max_slope_index}\")\n",
    "                #     plt.xticks(range(50), labels=range(50))  # x축 눈금 설정\n",
    "                #     plt.yticks(np.arange(min(raw[max_slope_index - 10 : max_slope_index + 40]), \n",
    "                #                         max(raw[max_slope_index - 10 : max_slope_index + 40]) + 1, \n",
    "                #                         step=1))  # y축 눈금 설정, step=1로 매 x마다 보이게 함\n",
    "                    \n",
    "                #     plt.grid(True, which='both', linestyle='--', linewidth=0.5)  # 그래프에 그리드 추가\n",
    "                #     plt.show()\n",
    "    print(\"spike_train_size\", len(spike_train)-past_tr_num)\n",
    "    print(\"spike_test_size\", len(spike_test)-past_te_num)\n",
    "    print(\"spike_total\", len(spike_train)-past_tr_num+len(spike_test)-past_te_num)\n",
    "spike_train = np.array(spike_train)\n",
    "spike_test = np.array(spike_test)\n",
    "np.random.shuffle(spike_train)\n",
    "np.random.shuffle(spike_test)\n",
    "\n",
    "torch.save(torch.tensor(spike_train, dtype=torch.float32), my_path_ground_BH + 'BH_training_dataset_real_detect.pt')\n",
    "torch.save(torch.tensor(spike_test, dtype=torch.float32), my_path_ground_BH + 'BH_test_dataset_real_detect.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 논문대로 템플릿 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "from scipy import io\n",
    "\n",
    "my_path_ground_BH = '/data2/spike_sorting/quiroga/BH/'\n",
    "\n",
    "# 데이터 파일 목록과 템플릿 파일 목록 설정\n",
    "filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "            \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "            \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "            \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "template =  [\"BH_Spike_TEMPLATE_e1n005.npy\", \"BH_Spike_TEMPLATE_e1n010.npy\", \"BH_Spike_TEMPLATE_e1n015.npy\", \"BH_Spike_TEMPLATE_e1n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_e2n005.npy\", \"BH_Spike_TEMPLATE_e2n010.npy\", \"BH_Spike_TEMPLATE_e2n015.npy\", \"BH_Spike_TEMPLATE_e2n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_d1n005.npy\", \"BH_Spike_TEMPLATE_d1n010.npy\", \"BH_Spike_TEMPLATE_d1n015.npy\", \"BH_Spike_TEMPLATE_d1n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_d2n005.npy\", \"BH_Spike_TEMPLATE_d2n010.npy\", \"BH_Spike_TEMPLATE_d2n015.npy\", \"BH_Spike_TEMPLATE_d2n020.npy\"]\n",
    "\n",
    "thr_tem = np.array([0.9, 0.9, 0.9, 0.9,   1.0, 1.0, 1.0, 1.0,   0.7, 0.7, 0.7, 0.9,   0.9, 0.9, 0.9, 0.9])\n",
    "thr_new_cluster = np.array([50, 50, 50, 50,   50, 50, 50, 50,   50, 30, 30, 50,   50, 50, 50, 50])\n",
    "thr_merge_cluster = np.array([110, 110, 110, 110,   110, 110, 110, 110,   110, 110, 110, 110,   110, 110, 110, 110])\n",
    "wait_term = np.array([20, 20, 20, 20,   20, 20, 20, 20,   20, 20, 20, 20,   20, 20, 20, 20])\n",
    "\n",
    "# thr_tot = np.array([0.5, 0.5, 0.55, 0.7,  0.5, 0.5, 0.55, 0.7,  0.5, 0.5, 0.55, 0.7,  0.5, 0.5, 0.55, 0.7])\n",
    "\n",
    "dataset_num = 16\n",
    "spike_needs = 100 # 템플릿을 만들기 위해 필요한 스파이크 수\n",
    "spike_length = 50\n",
    "num_cluster = 4  # 클러스터 수 설정 # 논문엔 4개라는데 여기서는 3개로 했네\n",
    "cluster_delete_thr = 9 # spike_needs/10 # 이 숫자 초과인 클러스터만 생존\n",
    "\n",
    "for ds in range(dataset_num):\n",
    "    print(\"\\ndata:\", filename[ds])\n",
    "\n",
    "    # 데이터 파일 불러오기\n",
    "    mat1 = io.loadmat(my_path_ground_BH + filename[ds])\n",
    "    raw = mat1['data'][0]\n",
    "    thr = thr_tem[ds]  # 스파이크 탐지 임계값 설정 \n",
    "    spike = []  # 스파이크 데이터를 저장할 배열\n",
    "\n",
    "    # raw 데이터의 기울기 계산\n",
    "    slope = np.diff(raw) # raw보다 한 사이즈 작음.\n",
    "    \n",
    "    # 스파이크 탐지\n",
    "    wait = -20  # 스파이크 탐지 대기 시간 초기화: 처음에는 20샘플은 버림.\n",
    "    spike_count = 0\n",
    "    for i in range(len(raw)-2):\n",
    "        wait += 1\n",
    "        if(wait_term[ds] < wait):\n",
    "            if(raw[i+1] < raw[i+2] and raw[i+1] <= raw[i] and raw[i+1] < -thr) or (raw[i+1] > raw[i+2] and raw[i] <= raw[i+1] and raw[i+1] > thr):\n",
    "                spike_count += 1\n",
    "                max_slope_index = i + np.argmax(slope[i - 8 : i + 5]) - 8 # 기울기가 최대인 지점에서 스파이크 추출\n",
    "                spike.append(raw[max_slope_index - 10 : max_slope_index - 10 + spike_length]) \n",
    "                wait = 0  # 대기 시간 초기화 # 다시 wait_term[ds]만큼 기다려라\n",
    "                if spike_count == spike_needs:\n",
    "                    break\n",
    "    spike = np.array(spike)\n",
    "\n",
    "    Cluster = np.zeros((num_cluster, spike_length))  # 클러스터 배열 초기화\n",
    "    cluster_num = np.zeros(num_cluster)  # 각 클러스터의 데이터 수 초기화\n",
    "    \n",
    "    sm_distance = np.zeros(num_cluster)\n",
    "    mm_distance = np.zeros((num_cluster, num_cluster))\n",
    "\n",
    "    # 훈련 사이클 시작\n",
    "    current_cluster_num = 0\n",
    "    for spike_index in range(spike_needs):\n",
    "        spike_n = spike[spike_index, :]  # 현재 스파이크\n",
    "        \n",
    "        if(spike_index == 0):\n",
    "            Cluster[0, :] = spike_n  # 첫 번째 스파이크는 첫 번째 클러스터에 배정\n",
    "            cluster_num[0] = 1  # 클러스터 데이터 수 증가\n",
    "            current_cluster_num += 1\n",
    "        else:\n",
    "            # 각 클러스터와의 거리 계산\n",
    "\n",
    "            sm_smallest = 1000000000000\n",
    "            sm_smallest_index = 0\n",
    "            for i in range(num_cluster): # 0, 1, 2 까지는 기존 클러스터와 지금 스파이크와의 거리\n",
    "                if cluster_num[i] > 0:\n",
    "                    sm_distance[i] = np.sum(abs(Cluster[i, 5:25] - spike_n[5:25])) * 17 + np.sum(abs(Cluster[i, 0:5] - spike_n[0:5])) * 2 + np.sum(abs(Cluster[i, 25:50] - spike_n[25:50])) * 2\n",
    "                    if sm_smallest > sm_distance[i]:\n",
    "                        sm_smallest = sm_distance[i]\n",
    "                        sm_smallest_index = i\n",
    "\n",
    "            mm_smallest = 1000000000000\n",
    "            mm_smallest_index_i = 0\n",
    "            mm_smallest_index_j = 0\n",
    "            for i in range(num_cluster):\n",
    "                for j in range(i+1, num_cluster):\n",
    "                    if cluster_num[i] > 0 and cluster_num[j] > 0:\n",
    "                        mer_thr = 1.5 if spike_index < 30 else 2.5\n",
    "                        mm_distance[i, j] = np.sum(abs(Cluster[i, 5:25] - Cluster[j, 5:25])) * 17 + np.sum(abs(Cluster[i, 0:5] - Cluster[j, 0:5])) * 2 + np.sum(abs(Cluster[i, 25:50] - Cluster[j, 25:50])) * 2            \n",
    "                        mm_distance[i, j] = mm_distance[i, j] * mer_thr\n",
    "                        if mm_smallest > mm_distance[i, j]:\n",
    "                            mm_smallest = mm_distance[i, j]\n",
    "                            mm_smallest_index_i = i\n",
    "                            mm_smallest_index_j = j\n",
    "\n",
    "            if current_cluster_num < num_cluster:\n",
    "                # print('sm_smallest', sm_smallest)\n",
    "                if sm_smallest > thr_new_cluster[ds]:\n",
    "                    Cluster[current_cluster_num, :] = spike_n\n",
    "                    cluster_num[current_cluster_num] = 1\n",
    "                    current_cluster_num += 1\n",
    "                else:\n",
    "                    Cluster[sm_smallest_index, :] = (Cluster[sm_smallest_index, :] * 15 + spike_n) / 16\n",
    "                    cluster_num[sm_smallest_index] += 1\n",
    "            \n",
    "            else:\n",
    "                if sm_smallest < mm_smallest:\n",
    "                    Cluster[sm_smallest_index, :] = (Cluster[sm_smallest_index, :] * 15 + spike_n) / 16\n",
    "                    cluster_num[sm_smallest_index] += 1\n",
    "                else: #merge\n",
    "                    Cluster[mm_smallest_index_i, :] = (Cluster[mm_smallest_index_i, :] + Cluster[mm_smallest_index_j, :]) / 2\n",
    "                    cluster_num[mm_smallest_index_i] += cluster_num[mm_smallest_index_j]\n",
    "                    Cluster[mm_smallest_index_j, :] = spike_n\n",
    "                    cluster_num[mm_smallest_index_j] = 1\n",
    "\n",
    "    # print('before delete under 11', cluster_num)\n",
    "    # cluster_num이 11이하면 해당 클러스터 없애기\n",
    "    Cluster_temp = np.zeros((num_cluster, spike_length))  # 클러스터 배열 초기화\n",
    "    cluster_num_temp = np.zeros(num_cluster)  # 각 클러스터의 데이터 수 초기화\n",
    "    final_index = 0\n",
    "    for i in range(num_cluster):\n",
    "        if cluster_num[i] > cluster_delete_thr:\n",
    "            Cluster_temp[final_index, :] = Cluster[i, :]\n",
    "            cluster_num_temp[final_index] = cluster_num[i]\n",
    "            final_index += 1\n",
    "    current_cluster_num = final_index\n",
    "    # print(cluster_num_temp)\n",
    "    Cluster = Cluster_temp\n",
    "    cluster_num = cluster_num_temp\n",
    "\n",
    "\n",
    "    # \n",
    "    no_more_merge = False\n",
    "    while (no_more_merge == False):\n",
    "        no_more_merge = True\n",
    "        final_mm_merge_index_i = 0\n",
    "        final_mm_merge_index_j = 0\n",
    "        for i in range(current_cluster_num):\n",
    "            for j in range(i+1, current_cluster_num):\n",
    "                if cluster_num[i] > 0 and cluster_num[j] > 0:\n",
    "                    mer_thr = 2.5\n",
    "                    final_mm_distance = np.sum(abs(Cluster[i, 5:25] - Cluster[j, 5:25])) * 17 + np.sum(abs(Cluster[i, 0:5] - Cluster[j, 0:5])) * 2 + np.sum(abs(Cluster[i, 25:50] - Cluster[j, 25:50])) * 2            \n",
    "                    # print('final_mm_distance', final_mm_distance)\n",
    "                    # print(cluster_num)\n",
    "                    final_mm_distance = final_mm_distance * mer_thr\n",
    "                    if final_mm_distance < thr_merge_cluster[ds]:\n",
    "                        final_mm_merge_index_i = i\n",
    "                        final_mm_merge_index_j = j\n",
    "                        no_more_merge = False\n",
    "                        break\n",
    "            if no_more_merge == False:\n",
    "                break\n",
    "        \n",
    "        if no_more_merge == False:\n",
    "            Cluster[final_mm_merge_index_i, :] = (Cluster[final_mm_merge_index_i, :] + Cluster[final_mm_merge_index_j, :]) / 2\n",
    "            cluster_num[final_mm_merge_index_i] += cluster_num[final_mm_merge_index_j]\n",
    "            cluster_num[final_mm_merge_index_j] = 0\n",
    "            current_cluster_num -= 1\n",
    "\n",
    "            \n",
    "            # 앞으로 다시 땡기기\n",
    "            Cluster_temp = np.zeros((num_cluster, spike_length))  # 클러스터 배열 초기화\n",
    "            cluster_num_temp = np.zeros(num_cluster)  # 각 클러스터의 데이터 수 초기화\n",
    "            final_index = 0\n",
    "            for i in range(num_cluster):\n",
    "                if cluster_num[i] > cluster_delete_thr:\n",
    "                    Cluster_temp[final_index, :] = Cluster[i, :]\n",
    "                    cluster_num_temp[final_index] = cluster_num[i]\n",
    "                    final_index += 1\n",
    "            current_cluster_num = final_index\n",
    "            Cluster = Cluster_temp\n",
    "            cluster_num = cluster_num_temp\n",
    "\n",
    "\n",
    "\n",
    "    # # Cluster plot\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # colors = ['b', 'g', 'r', 'k']  # 클러스터별 색상 지정\n",
    "    # x_axis = np.arange(spike_length)  # x축 값 (스파이크 길이)\n",
    "    # # print(cluster_num)\n",
    "    # for i in range(num_cluster):\n",
    "    #     plt.plot(x_axis, Cluster[i, :], label=f'Cluster {i+1}', color=colors[i % len(colors)])\n",
    "\n",
    "    # plt.title(f'Cluster Templates for {filename[ds]}, nums{cluster_num}')\n",
    "    # plt.xlabel('Sample Index')\n",
    "    # plt.ylabel('Amplitude')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "    \n",
    "    # 클러스터 템플릿을 파일로 저장\n",
    "    np.save(my_path_ground_BH + template[ds], Cluster)\n",
    "    print(Cluster)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ssp.train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from scipy import io\n",
    "import itertools\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path_ground_BH = '/data2/spike_sorting/quiroga/BH/'\n",
    "\n",
    "\n",
    "filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "            \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "            \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "            \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "\n",
    "spike_tot = [\"BH_Spike_e1n005.npy\", \"BH_Spike_e1n010.npy\", \"BH_Spike_e1n015.npy\", \"BH_Spike_e1n020.npy\",\n",
    "            \"BH_Spike_e2n005.npy\", \"BH_Spike_e2n010.npy\", \"BH_Spike_e2n015.npy\", \"BH_Spike_e2n020.npy\",\n",
    "            \"BH_Spike_d1n005.npy\", \"BH_Spike_d1n010.npy\", \"BH_Spike_d1n015.npy\", \"BH_Spike_d1n020.npy\",\n",
    "            \"BH_Spike_d2n005.npy\", \"BH_Spike_d2n010.npy\", \"BH_Spike_d2n015.npy\", \"BH_Spike_d2n020.npy\"]\n",
    "\n",
    "label_tot = [\"BH_Label_e1n005.npy\", \"BH_Label_e1n010.npy\", \"BH_Label_e1n015.npy\", \"BH_Label_e1n020.npy\",\n",
    "            \"BH_Label_e2n005.npy\", \"BH_Label_e2n010.npy\", \"BH_Label_e2n015.npy\", \"BH_Label_e2n020.npy\",\n",
    "            \"BH_Label_d1n005.npy\", \"BH_Label_d1n010.npy\", \"BH_Label_d1n015.npy\", \"BH_Label_d1n020.npy\",\n",
    "            \"BH_Label_d2n005.npy\", \"BH_Label_d2n010.npy\", \"BH_Label_d2n015.npy\", \"BH_Label_d2n020.npy\"]\n",
    "\n",
    "template =  [\"BH_Spike_TEMPLATE_e1n005.npy\", \"BH_Spike_TEMPLATE_e1n010.npy\", \"BH_Spike_TEMPLATE_e1n015.npy\", \"BH_Spike_TEMPLATE_e1n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_e2n005.npy\", \"BH_Spike_TEMPLATE_e2n010.npy\", \"BH_Spike_TEMPLATE_e2n015.npy\", \"BH_Spike_TEMPLATE_e2n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_d1n005.npy\", \"BH_Spike_TEMPLATE_d1n010.npy\", \"BH_Spike_TEMPLATE_d1n015.npy\", \"BH_Spike_TEMPLATE_d1n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_d2n005.npy\", \"BH_Spike_TEMPLATE_d2n010.npy\", \"BH_Spike_TEMPLATE_d2n015.npy\", \"BH_Spike_TEMPLATE_d2n020.npy\"]\n",
    "\n",
    "AE_train_path_gt_detect = 'BH_training_dataset_gt_detect.pt' \n",
    "AE_test_path_gt_detect = 'BH_test_dataset_gt_detect.pt'\n",
    "\n",
    "AE_train_path_real_detect = 'BH_training_dataset_real_detect.pt'\n",
    "AE_test_path_real_detect = 'BH_test_dataset_real_detect.pt'\n",
    "\n",
    "\n",
    "\n",
    "thr_tot = np.array([0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7])\n",
    "cos_thr = np.array([0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.85, 0.95, 0.9, 0.8, 0.95, 0.95, 0.95, 0.95, 0.8])\n",
    "\n",
    "# hyperparameter\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '1'\n",
    "dataset_num = 16\n",
    "spike_length = 50\n",
    "n_sample = spike_length\n",
    "num_cluster = 4  # 클러스터 수 설정 # 논문엔 4개라는데 여기서는 3개로 했네\n",
    "training_cycle = 2400 # 그 초기 몇개까지만 cluster update할지\n",
    "AE_train_data = AE_train_path_gt_detect #AE_train_path_gt_detect #AE_train_path_real_detect\n",
    "AE_test_data = AE_test_path_gt_detect #AE_test_path_gt_detect  #AE_test_path_real_detect\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "max_epoch = 7000\n",
    "learning_rate = 0.001\n",
    "normalize_on = False # True or False # 이거 별로 안 좋은 normalize같음 # 쓸 거면 다른 거 써라.\n",
    "need_bias = False\n",
    "# first_layer_no_train = False\n",
    "lif_add_at_first = False\n",
    "TIME = 10 # SAE일 때만 유효\n",
    "my_seed = 42\n",
    "\n",
    "seed_assign(my_seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class spikedataset(Dataset):\n",
    "    def __init__(self, path, transform = None):    \n",
    "        self.transform = transform\n",
    "        self.spike = torch.load(path)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        spike = self.spike[index]            \n",
    "        if self.transform is not None:\n",
    "            spike = self.transform(spike)\n",
    "        return spike\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.spike)\n",
    "\n",
    "train_dataset = spikedataset(my_path_ground_BH + AE_train_data)\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "test_dataset = spikedataset(my_path_ground_BH + AE_test_data)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "\n",
    "class AE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.conv1 = nn.Conv1d(1, 32, 3, stride = 2, bias = False) # 24\n",
    "        self.conv2 = nn.Conv1d(32, 64, 3, stride = 2, bias = False) # 11\n",
    "        self.conv3 = nn.Conv1d(64, 96, 3, stride = 2, bias = False) # 4 # 병현: 여기 5인데?\n",
    "        self.fc1 = nn.Linear(96 * 5, 4, bias = False)\n",
    "        \n",
    "        # decoder\n",
    "        self.fc4 = nn.Linear(4, 5 * 96, bias = False)\n",
    "        self.deconv3 = nn.ConvTranspose1d(96, 64, 3, stride = 2, bias = False) #6 + 2 + 1= 9\n",
    "        self.deconv1 = nn.ConvTranspose1d(64, 32, 3, stride = 2, output_padding=1, bias = False) #16(9-1)*stride + 4(kernel-1) + 1 = 21\n",
    "        self.deconv2 = nn.ConvTranspose1d(32, 1, 3, stride = 2, output_padding=1, bias = False) #40 + 4 + 1 = 45\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # encoder\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 96 * 5)\n",
    "        mid = self.fc1(x)\n",
    "        norm = torch.sqrt(torch.sum(torch.pow(mid, 2), dim = 1))\n",
    "        h = (mid.t()/(norm + 1e-12)).t()\n",
    "\n",
    "        # decoder\n",
    "        z = F.relu(self.fc4(h))\n",
    "        z = z.view(-1, 96, 5)\n",
    "        z = F.relu(self.deconv3(z))\n",
    "        z = F.relu(self.deconv1(z))\n",
    "        z = self.deconv2(z)\n",
    "\n",
    "        return h, z\n",
    "    \n",
    "\n",
    "\n",
    "# net = AE()\n",
    "# net = torch.nn.DataParallel(net)\n",
    "    \n",
    "# net = torch.load('/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/net_save/save_now_net_20241231_120818.pth')\n",
    "\n",
    "# # 모델 초기화\n",
    "# net = Autoencoder_only_FC(encoder_ch=[96, 64, 32, 4], decoder_ch=[32,64,96,n_sample], n_sample=n_sample, need_bias=need_bias)\n",
    "# net = torch.nn.DataParallel(net)\n",
    "\n",
    "net = Autoencoder_conv1(input_channels=1, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = 4, padding = 0, stride = 2, kernel_size = 3, need_bias=need_bias)\n",
    "net = torch.nn.DataParallel(net)\n",
    "\n",
    "\n",
    "# net = SAE_fc_only(encoder_ch=[96, 64, 32, 4], \n",
    "#                     decoder_ch=[32,64,96,n_sample], \n",
    "#                     in_channels=n_sample, # in_channel 이 여기선 걍 lenght.\n",
    "#                     synapse_fc_trace_const1=1,\n",
    "#                     synapse_fc_trace_const2=0.5,  #안씀 \n",
    "#                     TIME=TIME, v_init=0.0, v_decay=0.5, v_threshold=0.75, v_reset=10000.0, \n",
    "#                     sg_width=4.0, surrogate='sigmoid', BPTT_on=True, need_bias=need_bias, lif_add_at_first=lif_add_at_first)\n",
    "# net = torch.nn.DataParallel(net)\n",
    "    \n",
    "# net = SAE_conv1(input_channels=1, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = 4, padding = 0, stride = 2, kernel_size = 3, \n",
    "#                     synapse_fc_trace_const1=1, \n",
    "#                     synapse_fc_trace_const2=0.5, #안씀 \n",
    "#                     TIME=TIME, v_init=0.0, v_decay=0.5, v_threshold=0.75, v_reset=10000.0, \n",
    "#                     sg_width=4.0, surrogate='sigmoid', BPTT_on=True, need_bias=need_bias, lif_add_at_first=lif_add_at_first)\n",
    "# net = torch.nn.DataParallel(net)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): Autoencoder_conv1(\n",
      "    (encoder): Sequential(\n",
      "      (0): SSBH_DimChanger_for_unsuqeeze()\n",
      "      (1): Conv1d(1, 32, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      (2): ReLU()\n",
      "      (3): Conv1d(32, 64, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      (4): ReLU()\n",
      "      (5): Conv1d(64, 96, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      (6): ReLU()\n",
      "      (7): SSBH_DimChanger_for_fc()\n",
      "      (8): Linear(in_features=480, out_features=4, bias=False)\n",
      "      (9): SSBH_L2NormLayer()\n",
      "    )\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=480, bias=False)\n",
      "      (1): ReLU()\n",
      "      (2): SSBH_DimChanger_for_conv1()\n",
      "      (3): ConvTranspose1d(96, 64, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      (4): ReLU()\n",
      "      (5): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(2,), output_padding=(1,), bias=False)\n",
      "      (6): ReLU()\n",
      "      (7): ConvTranspose1d(32, 1, kernel_size=(3,), stride=(2,), output_padding=(1,), bias=False)\n",
      "      (8): SSBH_DimChanger_for_suqeeze()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Device: cuda\n",
      "\n",
      "Start Training, current_time = 20241231_175600\n",
      "\n",
      "epoch-0 accuracy check\n",
      "mean_cluster_accuracy_during_training_cycle : 71.93%\n",
      "post_traincycle_acc : 71.26%\n",
      "total_acc : 71.72%\n",
      "save model\n",
      "\n",
      "epoch-0 loss : 0.12785\n",
      "\n",
      "epoch-1 accuracy check\n",
      "mean_cluster_accuracy_during_training_cycle : 88.83%\n",
      "post_traincycle_acc : 89.27%\n",
      "total_acc : 88.96%\n",
      "save model\n",
      "\n",
      "epoch-1 loss : 0.04481\n",
      "\n",
      "epoch-2 loss : 0.04064\n",
      "\n",
      "epoch-3 loss : 0.03881\n",
      "\n",
      "epoch-4 loss : 0.03766\n",
      "\n",
      "epoch-5 loss : 0.03687\n",
      "\n",
      "epoch-6 loss : 0.03625\n",
      "\n",
      "epoch-7 loss : 0.03575\n",
      "\n",
      "epoch-8 loss : 0.03533\n",
      "\n",
      "epoch-9 loss : 0.03502\n",
      "\n",
      "epoch-10 loss : 0.03469\n",
      "\n",
      "epoch-11 loss : 0.03438\n",
      "\n",
      "epoch-12 loss : 0.03424\n",
      "\n",
      "epoch-13 loss : 0.03396\n",
      "\n",
      "epoch-14 loss : 0.03378\n",
      "\n",
      "epoch-15 loss : 0.03363\n",
      "\n",
      "epoch-16 loss : 0.03344\n",
      "\n",
      "epoch-17 loss : 0.03331\n",
      "\n",
      "epoch-18 loss : 0.03314\n",
      "\n",
      "epoch-19 loss : 0.03300\n",
      "\n",
      "epoch-20 loss : 0.03292\n",
      "\n",
      "epoch-21 loss : 0.03279\n",
      "\n",
      "epoch-22 loss : 0.03268\n",
      "\n",
      "epoch-23 loss : 0.03256\n",
      "\n",
      "epoch-24 loss : 0.03247\n",
      "\n",
      "epoch-25 loss : 0.03241\n",
      "\n",
      "epoch-26 loss : 0.03231\n",
      "\n",
      "epoch-27 loss : 0.03221\n",
      "\n",
      "epoch-28 loss : 0.03211\n",
      "\n",
      "epoch-29 loss : 0.03202\n",
      "\n",
      "epoch-30 loss : 0.03194\n",
      "\n",
      "epoch-31 loss : 0.03186\n",
      "\n",
      "epoch-32 loss : 0.03181\n",
      "\n",
      "epoch-33 loss : 0.03171\n",
      "\n",
      "epoch-34 loss : 0.03163\n",
      "\n",
      "epoch-35 loss : 0.03157\n",
      "\n",
      "epoch-36 loss : 0.03150\n",
      "\n",
      "epoch-37 loss : 0.03145\n",
      "\n",
      "epoch-38 loss : 0.03137\n",
      "\n",
      "epoch-39 loss : 0.03130\n",
      "\n",
      "epoch-40 loss : 0.03125\n",
      "\n",
      "epoch-41 loss : 0.03119\n",
      "\n",
      "epoch-42 loss : 0.03114\n",
      "\n",
      "epoch-43 loss : 0.03101\n",
      "\n",
      "epoch-44 loss : 0.03100\n",
      "\n",
      "epoch-45 loss : 0.03094\n",
      "\n",
      "epoch-46 loss : 0.03088\n",
      "\n",
      "epoch-47 loss : 0.03081\n",
      "\n",
      "epoch-48 loss : 0.03074\n",
      "\n",
      "epoch-49 loss : 0.03071\n",
      "\n",
      "epoch-50 accuracy check\n",
      "mean_cluster_accuracy_during_training_cycle : 90.54%\n",
      "post_traincycle_acc : 91.47%\n",
      "total_acc : 90.82%\n",
      "save model\n",
      "\n",
      "epoch-50 loss : 0.03066\n",
      "\n",
      "epoch-51 loss : 0.03058\n",
      "\n",
      "epoch-52 loss : 0.03056\n",
      "\n",
      "epoch-53 loss : 0.03050\n",
      "\n",
      "epoch-54 loss : 0.03046\n",
      "\n",
      "epoch-55 loss : 0.03041\n",
      "\n",
      "epoch-56 loss : 0.03034\n",
      "\n",
      "epoch-57 loss : 0.03025\n",
      "\n",
      "epoch-58 loss : 0.03024\n",
      "\n",
      "epoch-59 loss : 0.03019\n",
      "\n",
      "epoch-60 loss : 0.03015\n",
      "\n",
      "epoch-61 loss : 0.03012\n",
      "\n",
      "epoch-62 loss : 0.03010\n",
      "\n",
      "epoch-63 loss : 0.03003\n",
      "\n",
      "epoch-64 loss : 0.03000\n",
      "\n",
      "epoch-65 loss : 0.02995\n",
      "\n",
      "epoch-66 loss : 0.02989\n",
      "\n",
      "epoch-67 loss : 0.02984\n",
      "\n",
      "epoch-68 loss : 0.02980\n",
      "\n",
      "epoch-69 loss : 0.02978\n",
      "\n",
      "epoch-70 loss : 0.02975\n",
      "\n",
      "epoch-71 loss : 0.02970\n",
      "\n",
      "epoch-72 loss : 0.02966\n",
      "\n",
      "epoch-73 loss : 0.02960\n",
      "\n",
      "epoch-74 loss : 0.02962\n",
      "\n",
      "epoch-75 loss : 0.02952\n",
      "\n",
      "epoch-76 loss : 0.02950\n",
      "\n",
      "epoch-77 loss : 0.02947\n",
      "\n",
      "epoch-78 loss : 0.02944\n",
      "\n",
      "epoch-79 loss : 0.02943\n",
      "\n",
      "epoch-80 loss : 0.02939\n",
      "\n",
      "epoch-81 loss : 0.02931\n",
      "\n",
      "epoch-82 loss : 0.02934\n",
      "\n",
      "epoch-83 loss : 0.02928\n",
      "\n",
      "epoch-84 loss : 0.02927\n",
      "\n",
      "epoch-85 loss : 0.02923\n",
      "\n",
      "epoch-86 loss : 0.02919\n",
      "\n",
      "epoch-87 loss : 0.02915\n",
      "\n",
      "epoch-88 loss : 0.02912\n",
      "\n",
      "epoch-89 loss : 0.02908\n",
      "\n",
      "epoch-90 loss : 0.02906\n",
      "\n",
      "epoch-91 loss : 0.02904\n",
      "\n",
      "epoch-92 loss : 0.02902\n",
      "\n",
      "epoch-93 loss : 0.02897\n",
      "\n",
      "epoch-94 loss : 0.02892\n",
      "\n",
      "epoch-95 loss : 0.02891\n",
      "\n",
      "epoch-96 loss : 0.02888\n",
      "\n",
      "epoch-97 loss : 0.02887\n",
      "\n",
      "epoch-98 loss : 0.02883\n",
      "\n",
      "epoch-99 loss : 0.02882\n",
      "\n",
      "epoch-100 accuracy check\n",
      "mean_cluster_accuracy_during_training_cycle : 91.46%\n",
      "post_traincycle_acc : 92.01%\n",
      "total_acc : 91.63%\n",
      "save model\n",
      "\n",
      "epoch-100 loss : 0.02878\n",
      "\n",
      "epoch-101 loss : 0.02873\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "net = net.to(device)\n",
    "print(net)\n",
    "print('Device:',device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_history = []\n",
    "mean_cluster_accuracy_during_training_cycle_all_dataset_history = []\n",
    "mean_cluster_accuracy_post_training_cycle_all_dataset_history = []\n",
    "mean_cluster_accuracy_total_all_dataset_history = []\n",
    "\n",
    "tau = np.zeros(num_cluster)\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"\\nStart Training, current_time = {current_time}\")\n",
    "best_mean_cluster_accuracy_post_training_cycle_all_dataset = 0\n",
    "for epoch in range(max_epoch):\n",
    "    cluster_accuracy_during_training_cycle_all_dataset = np.zeros(dataset_num)\n",
    "    cluster_accuracy_post_training_cycle_all_dataset = np.zeros(dataset_num)\n",
    "    cluster_accuracy_total_all_dataset = np.zeros(dataset_num)    \n",
    "    \n",
    "    if(epoch % 50 == 0 or epoch == 1): \n",
    "        print(f'\\nepoch-{epoch} accuracy check')\n",
    "        for ds in range(dataset_num):\n",
    "            # print(spike_tot[ds])\n",
    "\n",
    "            thr = thr_tot[ds]\n",
    "            spike_template = np.load(my_path_ground_BH + template[ds])\n",
    "            spike = np.load(my_path_ground_BH + spike_tot[ds])\n",
    "            label = np.load(my_path_ground_BH + label_tot[ds])\n",
    "            \n",
    "            Cluster = np.zeros((num_cluster, 4))\n",
    "            assert Cluster.shape[1] == 4, '이거 hidden dim 4 아니게 할 거면 잘 바꿔라'\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for i in range(num_cluster):\n",
    "                    spike_torch = torch.from_numpy(spike_template[i, :])\n",
    "                    spike_torch = spike_torch.float().to(device)\n",
    "                    if 'SAE' in net.module.__class__.__name__:\n",
    "                        spike_torch = spike_torch.unsqueeze(-1).repeat(1, 1, TIME).permute(0,2,1) # (batch, time, feature)로 변환\n",
    "                    else:\n",
    "                        spike_torch = spike_torch.unsqueeze(0)\n",
    "                    inner_inf = net.module.encoder(spike_torch)\n",
    "                    Cluster[i, :] = inner_inf.cpu().detach().numpy()\n",
    "\n",
    "            spike_hidden = np.zeros((len(spike), 4))\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(spike)):\n",
    "                    spike_torch = torch.from_numpy(spike[i, :])\n",
    "                    spike_torch = spike_torch.float().to(device)\n",
    "                    if 'SAE' in net.module.__class__.__name__:\n",
    "                        spike_torch = spike_torch.unsqueeze(-1).repeat(1, 1, TIME).permute(0,2,1) # (batch, time, feature)로 변환\n",
    "                    else:\n",
    "                        spike_torch = spike_torch.unsqueeze(0)\n",
    "                    inner_inf = net.module.encoder(spike_torch)\n",
    "                    spike_hidden[i, :] = inner_inf.cpu().detach().numpy()\n",
    "                \n",
    "            spike_id = np.zeros(len(spike))\n",
    "\n",
    "\n",
    "            distance_sm = np.zeros(num_cluster)\n",
    "            tau = np.zeros(num_cluster)\n",
    "            \n",
    "            for spike_index in range(len(spike)): \n",
    "                for q in range(num_cluster):\n",
    "                    tau[q] = np.dot(spike_hidden[spike_index, :], Cluster[q, :]) # 이거 l2norm 거쳐서 나온 거니까 분모 1임.\n",
    "                # tau = np.dot(Cluster, spike_hidden[spike_index, :]) # 이거 l2norm 거쳐서 나온 거니까 분모 1임.\n",
    "\n",
    "                for i in range(num_cluster): # l2 distance\n",
    "                    distance_sm[i] = np.sum(np.power(np.abs(Cluster[i] - spike_hidden[spike_index, :]), 2))\n",
    "\n",
    "                m = np.argmin(distance_sm)\n",
    "                spike_id[spike_index] = m + 1\n",
    "                if(np.max(tau) >= cos_thr[ds] and spike_index < training_cycle): # 원래 1400 아니냐?\n",
    "                    Cluster[m] = (Cluster[m] * 15 + spike_hidden[spike_index, :])/16\n",
    "                            \n",
    "            # spike id 분포 확인하기\n",
    "            # unique_elements, counts = np.unique(spike_id, return_counts=True)\n",
    "            # print(\"Unique elements:\", unique_elements)\n",
    "            # print(\"Counts:\", counts)\n",
    "\n",
    "            cluster_accuracy_during_training_cycle = np.zeros(math.factorial(num_cluster))\n",
    "            cluster_accuracy_post_training_cycle = np.zeros(math.factorial(num_cluster))\n",
    "            cluster_accuracy_total = np.zeros(math.factorial(num_cluster))\n",
    "            \n",
    "            label_converter_ground = list(range(1, num_cluster + 1)) # [1, 2, 3, 4] 생성\n",
    "            label_converter_permutations = list(itertools.permutations(label_converter_ground)) # 모든 순열 구하기\n",
    "            perm_i = 0\n",
    "            for perm in label_converter_permutations:\n",
    "                label_converter = list(perm)\n",
    "                # print(label_converter)\n",
    "                correct_during_training_cycle = 0\n",
    "                correct_post_training_cycle = 0\n",
    "\n",
    "                assert len(spike_id) == len(label), 'spike_id랑 label 길이 같아야 됨.'\n",
    "                for i in range(len(spike_id)):\n",
    "                    if(label_converter[int(spike_id[i]-1)] == label[i]):\n",
    "                        if i < training_cycle:\n",
    "                            correct_during_training_cycle += 1\n",
    "                        else:\n",
    "                            correct_post_training_cycle += 1\n",
    "\n",
    "                cluster_accuracy_during_training_cycle[perm_i] = correct_during_training_cycle/training_cycle\n",
    "                cluster_accuracy_post_training_cycle[perm_i] = correct_post_training_cycle/(len(spike_id)-training_cycle)\n",
    "                cluster_accuracy_total[perm_i] = (correct_during_training_cycle+correct_post_training_cycle)/(len(spike_id))\n",
    "                perm_i += 1\n",
    "\n",
    "            cluster_accuracy_during_training_cycle_all_dataset[ds] = np.max(cluster_accuracy_during_training_cycle)\n",
    "            cluster_accuracy_post_training_cycle_all_dataset[ds] = cluster_accuracy_post_training_cycle[np.argmax(cluster_accuracy_during_training_cycle)]\n",
    "            cluster_accuracy_total_all_dataset[ds] = cluster_accuracy_total[np.argmax(cluster_accuracy_during_training_cycle)]\n",
    "\n",
    "        mean_cluster_accuracy_during_training_cycle_all_dataset = np.mean(cluster_accuracy_during_training_cycle_all_dataset)\n",
    "        mean_cluster_accuracy_post_training_cycle_all_dataset = np.mean(cluster_accuracy_post_training_cycle_all_dataset)\n",
    "        mean_cluster_accuracy_total_all_dataset = np.mean(cluster_accuracy_total_all_dataset)\n",
    "        \n",
    "        mean_cluster_accuracy_during_training_cycle_all_dataset_history.append((epoch, mean_cluster_accuracy_during_training_cycle_all_dataset))\n",
    "        mean_cluster_accuracy_post_training_cycle_all_dataset_history.append((epoch, mean_cluster_accuracy_post_training_cycle_all_dataset))\n",
    "        mean_cluster_accuracy_total_all_dataset_history.append((epoch, mean_cluster_accuracy_total_all_dataset))\n",
    "        print(f\"mean_cluster_accuracy_during_training_cycle : {mean_cluster_accuracy_during_training_cycle_all_dataset*100:.2f}%\")\n",
    "        print(f\"post_traincycle_acc : {mean_cluster_accuracy_post_training_cycle_all_dataset*100:.2f}%\")\n",
    "        print(f\"total_acc : {mean_cluster_accuracy_total_all_dataset*100:.2f}%\")\n",
    "        \n",
    "        if mean_cluster_accuracy_post_training_cycle_all_dataset > best_mean_cluster_accuracy_post_training_cycle_all_dataset:\n",
    "            torch.save(net, f\"net_save/save_now_net_{current_time}.pth\")\n",
    "            print('save model')\n",
    "            best_mean_cluster_accuracy_post_training_cycle_all_dataset = mean_cluster_accuracy_post_training_cycle_all_dataset\n",
    "\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        spike = data\n",
    "        spike = spike.to(device)\n",
    "        if 'SAE' in net.module.__class__.__name__:\n",
    "            spike = spike.unsqueeze(-1).repeat(1, 1, TIME).permute(0,2,1) # (batch, time, feature)로 변환\n",
    "        spike_class = net(spike)\n",
    "        loss1 = criterion(spike_class[:, 5:25], spike[:, 5:25])\n",
    "        loss2 = criterion(spike_class[:, 0:5], spike[:, 0:5])\n",
    "        loss3 = criterion(spike_class[:, 25:spike_length], spike[:, 25:spike_length])\n",
    "        loss = loss1 * 2.125 + (loss2 + loss3)/4\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    loss_history.append((epoch, avg_loss))\n",
    "    # print(f'\\nepoch-{epoch} loss : {avg_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# acc_metric.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def acc_det(spike_index, spike_times, ans_times):\n",
    "        k = 0\n",
    "        FN = 0\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        spike_true_index = np.zeros(10000)\n",
    "        spike_false_index = np.zeros(10000)\n",
    "        ans_index = np.zeros(10000)\n",
    "        spike_times[spike_times == 0] = 1500000\n",
    "        det_win = 20\n",
    "\n",
    "        '''\n",
    "        f = open('./../result/TP_index.txt', 'w')\n",
    "        g = open('./../result/FP_index.txt', 'w')\n",
    "        h = open('./../result/FN_index.txt', 'w')\n",
    "        '''\n",
    "        for j in range(len(ans_times)):\n",
    "                if(ans_times[j] + det_win >= spike_times[k] and spike_times[k] >= ans_times[j] - det_win):\n",
    "                        spike_true_index[TP] = k\n",
    "                        ans_index[TP] = j\n",
    "                        #f.write('%7d %7d'%(ans_times[j], spike_times[k]) +\"\\n\")\n",
    "                        TP = TP + 1\n",
    "                        k = k + 1\n",
    "                elif(ans_times[j] + det_win < spike_times[k]):\n",
    "                        FN = FN + 1\n",
    "                        #h.write('%7d'%(ans_times[j]) +\"\\n\")\n",
    "                else:\n",
    "                        while(1):\n",
    "                                spike_false_index[FP] = k\n",
    "                                FP = FP + 1\n",
    "                                #g.write('%7d'%(spike_times[k]) + \"\\n\")\n",
    "                                k = k + 1\n",
    "                                if(ans_times[j] - det_win <= spike_times[k]):\n",
    "                                        break\n",
    "                        if(ans_times[j] + det_win >= spike_times[k]):\n",
    "                                spike_true_index[TP] = k\n",
    "                                ans_index[TP] = j\n",
    "                                #f.write('%7d %7d'%(ans_times[j], spike_times[k]) +\"\\n\")\n",
    "                                TP = TP + 1\n",
    "                                k = k + 1\n",
    "                        else:\n",
    "                                FN = FN + 1\n",
    "\t\t\t\t#h.write('%7d'%(ans_times[j]) +\"\\n\")\n",
    "        print(\"# of ans : \", len(ans_times))\n",
    "        print(\"# of TP ; \", TP)\n",
    "        print(\"# of FP ; \", FP)\n",
    "        print(\"# of FN : \", FN)\n",
    "        print(\"Det acc : \", TP/len(ans_times))\n",
    "        return spike_true_index, spike_false_index, ans_index, TP, TP/len(ans_times)\n",
    "''''\n",
    "def acc_clu(numspike, spike_id, TP, spike_true_index, ans_index, ans_cluster):\n",
    "\tcluster_accuracy = np.zeros(6)\n",
    "\tfor ep in range(6):\n",
    "\t\tif(ep == 1 or ep == 4):\n",
    "\t\t\tfor i in range(numspike):\n",
    "\t\t\t\tif(spike_id[i] == 3):\n",
    "\t\t\t\t\tspike_id[i] = 2\n",
    "\t\t\t\telif(spike_id[i] == 2):\n",
    "\t\t\t\t\tspike_id[i] = 3\n",
    "\t\telif(ep == 2 or ep == 5):\n",
    "\t\t\tfor i in range(numspike):\n",
    "\t\t\t\tif(spike_id[i] == 1):\n",
    "\t\t\t\t\tspike_id[i] = 2\n",
    "\t\t\t\telif(spike_id[i] == 2):\n",
    "\t\t\t\t\tspike_id[i] = 1\n",
    "\t\telif(ep == 3):\n",
    "\t\t\tfor i in range(numspike):\n",
    "\t\t\t\tif(spike_id[i] == 1):\n",
    "\t\t\t\t\tspike_id[i] = 3\n",
    "\t\t\t\telif(spike_id[i] == 3):\n",
    "\t\t\t\t\tspike_id[i] = 1\n",
    "\t\ttrue_cluster = 0\n",
    "\t\tfor i in range(TP):\n",
    "\t\t\tif(spike_true_index[i] == 0) and (spike_true_index[i+1] == 0):\n",
    "\t\t\t\tprint(\"break\")\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tif(spike_id[int(spike_true_index[i])] == ans_cluster[int(ans_index[i])]):\n",
    "\t\t\t\t\ttrue_cluster += 1\n",
    "\t\tcluster_accuracy[ep] = true_cluster*100/TP\n",
    "\tprint('Clu acc : ', max(cluster_accuracy))\n",
    "\treturn max(cluster_accuracy)\n",
    "'''\n",
    "\n",
    "def acc(spike_index, spike_times, ans_times, spike_id, ans_cluster, training = 0):\n",
    "        k = 0\n",
    "        FN = 0\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        spike_times[spike_times == 0] = 1500000\n",
    "        det_win = 20\n",
    "        id_ssp = np.zeros(10000)\n",
    "        id_ans = np.zeros(10000)\n",
    "        id_false = np.zeros(10000)\n",
    "        training_TP = 0\n",
    "        training_ans = 0\n",
    "        training_cycle = 100\n",
    "        for j in range(len(ans_times)):\n",
    "                if(ans_times[j] + det_win >= spike_times[k] and spike_times[k] >= ans_times[j] - det_win):\n",
    "                        id_ssp[TP] = spike_id[k]\n",
    "                        id_ans[TP] = ans_cluster[j]\t\t\n",
    "                        TP = TP + 1\n",
    "                        k = k + 1\n",
    "                        if(k == training_cycle and training == 1):\n",
    "                                training_TP = TP\n",
    "                                training_ans = j\n",
    "                elif(ans_times[j] + det_win < spike_times[k]):\n",
    "                        FN = FN + 1\n",
    "                else:\n",
    "                        while(1):\n",
    "                                id_false[FP] = spike_id[k]\n",
    "                                FP = FP + 1\n",
    "\n",
    "                                k = k + 1\n",
    "                                if(k == training_cycle and training == 1):\n",
    "                                        training_TP = TP\n",
    "                                        training_ans = j\n",
    "\n",
    "                                if(ans_times[j] - det_win <= spike_times[k]):\n",
    "                                        break\n",
    "                        if(ans_times[j] + det_win >= spike_times[k]):\n",
    "                                id_ssp[TP] = spike_id[k]\n",
    "                                id_ans[TP] = ans_cluster[j]\n",
    "                                TP = TP + 1\n",
    "                                k = k + 1\n",
    "                                if(k == training_cycle and training == 1):\n",
    "                                        training_TP = TP\n",
    "                                        training_ans = j\n",
    "                        else:\n",
    "                                FN = FN + 1\n",
    "        #print(training_TP, training_ans)\n",
    "        print(\"# of ans : \", len(ans_times))\n",
    "        print(\"# of TP ; \", TP)\n",
    "        print('training miss : ', training_TP)\n",
    "        print('# of Error : ', len(ans_times)-(TP-training_TP))\n",
    "        \n",
    "        #print(\"# of FP ; \", FP)\n",
    "        print(\"# of FN : \", FN)\n",
    "        print(\"Det acc : \", (TP-training_TP)/(len(ans_times)-training_ans))\n",
    "\n",
    "        filtered_spike = 0\n",
    "        filtered_noise = 0\n",
    "        cluster_accuracy = np.zeros(6)\n",
    "        true_clusters = np.zeros(6)\n",
    "        noise = 0\n",
    "        for i in range(TP):\n",
    "                if(id_ssp[i] == 4):\n",
    "                        filtered_spike += 1\n",
    "        for i in range(FP):\n",
    "                if(id_false[i] == 4):\n",
    "                        filtered_noise += 1\n",
    "        for ep in range(6):\n",
    "                if(ep == 1 or ep == 4):\n",
    "                        for i in range(spike_index):\n",
    "                                if(id_ssp[i] == 3):\n",
    "                                        id_ssp[i] = 2\n",
    "                                elif(id_ssp[i] == 2):\n",
    "                                        id_ssp[i] = 3\n",
    "                elif(ep == 2 or ep == 5):\n",
    "                        for i in range(spike_index):\n",
    "                                if(id_ssp[i] == 1):\n",
    "                                        id_ssp[i] = 2\n",
    "                                elif(id_ssp[i] == 2):\n",
    "                                        id_ssp[i] = 1\n",
    "                elif(ep == 3):\n",
    "                        for i in range(spike_index):\n",
    "                                if(id_ssp[i] == 1):\n",
    "                                        id_ssp[i] = 3\n",
    "                                elif(id_ssp[i] == 3):\n",
    "                                        id_ssp[i] = 1\n",
    "                true_cluster = 0\n",
    "                for i in range(training_TP, TP):\n",
    "                        if(id_ssp[i] == id_ans[i]):\n",
    "                                true_cluster += 1\n",
    "                \n",
    "                cluster_accuracy[ep] = true_cluster*100/(TP-filtered_spike-training_TP)\n",
    "                true_clusters[ep] = true_cluster\n",
    "        #print('filtered noise : ', filtered_noise)\n",
    "        print('filtered spike : ', filtered_spike)\n",
    "        print(\"true cluster : \", max(true_clusters))\n",
    "        print('filtered FP : ', FP-filtered_noise)\n",
    "        print('Final det acc : ', (TP-filtered_spike-training_TP)/(len(ans_times)-training_ans))\n",
    "\n",
    "        print('Clu acc : ', max(cluster_accuracy))\n",
    "\t\n",
    "        return (TP-training_TP-filtered_spike)/(len(ans_times)-training_ans), max(cluster_accuracy), max(true_clusters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (DEPRECATED) naive 템플릿 만들기. 이것도 라벨 필요 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import math\n",
    "# import os\n",
    "# from scipy import io\n",
    "\n",
    "# my_path_ground_BH = '/data2/spike_sorting/quiroga/BH/'\n",
    "\n",
    "# # 데이터 파일 목록과 템플릿 파일 목록 설정\n",
    "# filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "#             \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "#             \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "#             \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "# template =  [\"BH_Spike_TEMPLATE_e1n005.npy\", \"BH_Spike_TEMPLATE_e1n010.npy\", \"BH_Spike_TEMPLATE_e1n015.npy\", \"BH_Spike_TEMPLATE_e1n020.npy\",\n",
    "#              \"BH_Spike_TEMPLATE_e2n005.npy\", \"BH_Spike_TEMPLATE_e2n010.npy\", \"BH_Spike_TEMPLATE_e2n015.npy\", \"BH_Spike_TEMPLATE_e2n020.npy\",\n",
    "#              \"BH_Spike_TEMPLATE_d1n005.npy\", \"BH_Spike_TEMPLATE_d1n010.npy\", \"BH_Spike_TEMPLATE_d1n015.npy\", \"BH_Spike_TEMPLATE_d1n020.npy\",\n",
    "#              \"BH_Spike_TEMPLATE_d2n005.npy\", \"BH_Spike_TEMPLATE_d2n010.npy\", \"BH_Spike_TEMPLATE_d2n015.npy\", \"BH_Spike_TEMPLATE_d2n020.npy\"]\n",
    "\n",
    "# thr_tem = np.array([0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9])\n",
    "\n",
    "# dataset_num = 16\n",
    "# spike_needs = 100 # 템플릿을 만들기 위해 필요한 스파이크 수\n",
    "# spike_length = 50\n",
    "# wait_term = 20\n",
    "# num_cluster = 3  # 클러스터 수 설정 # 논문엔 4개라는데 여기서는 3개로 했네\n",
    "\n",
    "# for ds in range(dataset_num):\n",
    "#     print(\"\\ndata:\", filename[ds])\n",
    "#     spike_count = 0\n",
    "\n",
    "#     # 데이터 파일 불러오기\n",
    "#     mat1 = io.loadmat(my_path_ground_BH + filename[ds])\n",
    "#     raw = mat1['data'][0]\n",
    "#     thr = thr_tem[ds]  # 스파이크 탐지 임계값 설정 \n",
    "#     spike = []  # 스파이크 데이터를 저장할 배열\n",
    "\n",
    "#     # raw 데이터의 기울기 계산\n",
    "#     slope = np.diff(raw) # raw보다 한 사이즈 작음.\n",
    "    \n",
    "#     # 스파이크 탐지\n",
    "#     wait = -20  # 스파이크 탐지 대기 시간 초기화: 처음에는 20샘플은 버림.\n",
    "#     for i in range(len(raw)-2):\n",
    "#         wait += 1\n",
    "#         if(wait_term < wait):\n",
    "#             if(raw[i+1] < raw[i+2] and raw[i+1] <= raw[i] and raw[i+1] < -thr) or (raw[i+1] > raw[i+2] and raw[i] <= raw[i+1] and raw[i+1] > thr):\n",
    "#                 spike_count += 1\n",
    "#                 max_slope_index = i + np.argmax(slope[i - 8 : i + 5]) - 8 # 기울기가 최대인 지점에서 스파이크 추출\n",
    "#                 spike.append(raw[max_slope_index - 10 : max_slope_index - 10 + spike_length]) \n",
    "#                 wait = 0  # 대기 시간 초기화 # 다시 wait_term만큼 기다려라\n",
    "#                 if spike_count == spike_needs:\n",
    "#                     break\n",
    "#     spike = np.array(spike)\n",
    "\n",
    "#     Cluster = np.zeros((num_cluster, spike_length))  # 클러스터 배열 초기화\n",
    "#     distance_size = 0  # 거리 계산을 위한 배열 크기\n",
    "#     cluster_num = np.zeros(num_cluster)  # 각 클러스터의 데이터 수 초기화\n",
    "    \n",
    "#     for i in range(num_cluster):\n",
    "#         distance_size += i + 1  # 거리 계산 배열 크기 계산\n",
    "#     distance = np.zeros(distance_size)  # 거리 배열 초기화 # num_cluster=3이면 1+2+3=6개의 거리를 계산해야 함.\n",
    "    \n",
    "#     # 훈련 사이클 시작\n",
    "#     for spike_index in range(spike_needs):\n",
    "#         spike_n = spike[spike_index, :]  # 현재 스파이크\n",
    "        \n",
    "#         if(spike_index == 0):\n",
    "#             Cluster[0, :] = spike_n  # 첫 번째 스파이크는 첫 번째 클러스터에 배정\n",
    "#             cluster_num[0] += 1  # 클러스터 데이터 수 증가\n",
    "#         else:\n",
    "#             # 각 클러스터와의 거리 계산\n",
    "#             for i in range(num_cluster): # 0, 1, 2 까지는 기존 클러스터와 지금 스파이크와의 거리\n",
    "#                 distance[i] = np.sum(abs(Cluster[i, 5:25] - spike_n[5:25])) * 17 + np.sum(abs(Cluster[i, 0:5] - spike_n[0:5])) * 2 + np.sum(abs(Cluster[i, 25:50] - spike_n[25:50])) * 2\n",
    "            \n",
    "#             k = 0\n",
    "#             for j in range(1, num_cluster):\n",
    "#                 k = k + j\n",
    "#                 for i in range(j, num_cluster):\n",
    "#                     # 훈련 초기 단계에서는 임계값을 1.5로 설정\n",
    "#                     if(spike_index < 30):\n",
    "#                         mer_thr = 1.5\n",
    "#                     else:\n",
    "#                         mer_thr = 2.5\n",
    "                    \n",
    "#                     # 클러스터의 데이터 수가 10 이상인 경우 거리를 크게 설정\n",
    "#                     if(cluster_num[j-1] > 10) or (cluster_num[i] > 10):\n",
    "#                         distance[i + j * num_cluster - k] = 1500000000000\n",
    "#                     else:\n",
    "#                         # 두 클러스터 간의 거리 계산\n",
    "#                         distance[i + j * num_cluster - k] = np.sum(abs(Cluster[j - 1, 5:25] - Cluster[i, 5:25])) * 17 + np.sum(abs(Cluster[j - 1, 0:5] - Cluster[i, 0:5])) * 2 + np.sum(abs(Cluster[j - 1, 25:50] - Cluster[i, 25:50])) * 2\n",
    "#                         distance[i + j * num_cluster - k] = distance[i + j * num_cluster - k] * mer_thr\n",
    "            \n",
    "#             # 가장 작은 거리를 가진 클러스터를 찾고 업데이트\n",
    "#             m = np.argmin(distance)\n",
    "#             if(m < num_cluster): #클러스터와 현재 스파이크간 거리에서 젤 작은 게 있을 때\n",
    "#                 Cluster[m, :] = (Cluster[m, :] * 15 + spike_n) / 16  # 클러스터 업데이트\n",
    "#                 cluster_num[m] += 1  # 클러스터 데이터 수 증가\n",
    "#             else:  #클러스터와 클러스터 간 거리에서 젤 작은 게 있을 때\n",
    "#                 x = num_cluster\n",
    "#                 for i in range(1, num_cluster):\n",
    "#                     y = x + num_cluster - i\n",
    "#                     if(x <= m and m < y):\n",
    "#                         # 새로운 클러스터와 기존 클러스터를 결합\n",
    "#                         Cluster[i - 1, :] = (Cluster[i - 1, :] + Cluster[m - x + i, :]) / 2\n",
    "#                         cluster_num[i - 1] = cluster_num[i - 1] + cluster_num[m - x + i]\n",
    "#                         Cluster[m - x + i, :] = spike_n  # 새로운 스파이크 할당\n",
    "#                         cluster_num[m - x + i] = 1  # 새로운 클러스터 데이터 수 1로 초기화\n",
    "#                     x = y\n",
    "#     # # Cluster plot\n",
    "#     # plt.figure(figsize=(12, 6))\n",
    "#     # colors = ['b', 'g', 'r']  # 클러스터별 색상 지정\n",
    "#     # x_axis = np.arange(spike_length)  # x축 값 (스파이크 길이)\n",
    "#     # print(cluster_num)\n",
    "#     # for i in range(num_cluster):\n",
    "#     #     plt.plot(x_axis, Cluster[i, :], label=f'Cluster {i+1}', color=colors[i % len(colors)])\n",
    "\n",
    "#     # plt.title(f'Cluster Templates for {filename[ds]}')\n",
    "#     # plt.xlabel('Sample Index')\n",
    "#     # plt.ylabel('Amplitude')\n",
    "#     # plt.legend()\n",
    "#     # plt.grid(True)\n",
    "#     # plt.show()\n",
    "    \n",
    "#     # 클러스터 템플릿을 파일로 저장\n",
    "#     np.save(my_path_ground_BH + template[ds], Cluster)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
