{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ground-truth spike로 AE train data 꾸리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data: C_Easy1_noise005.mat\n",
      "\n",
      "data: C_Easy1_noise01.mat\n",
      "\n",
      "data: C_Easy1_noise015.mat\n",
      "\n",
      "data: C_Easy1_noise02.mat\n",
      "\n",
      "data: C_Easy2_noise005.mat\n",
      "\n",
      "data: C_Easy2_noise01.mat\n",
      "\n",
      "data: C_Easy2_noise015.mat\n",
      "\n",
      "data: C_Easy2_noise02.mat\n",
      "\n",
      "data: C_Difficult1_noise005.mat\n",
      "\n",
      "data: C_Difficult1_noise01.mat\n",
      "\n",
      "data: C_Difficult1_noise015.mat\n",
      "\n",
      "data: C_Difficult1_noise02.mat\n",
      "\n",
      "data: C_Difficult2_noise005.mat\n",
      "\n",
      "data: C_Difficult2_noise01.mat\n",
      "\n",
      "data: C_Difficult2_noise015.mat\n",
      "\n",
      "data: C_Difficult2_noise02.mat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# os.chdir(\"./../data/\")\n",
    "my_path_ground_BH = '/data2/spike_sorting/quiroga/BH/'\n",
    "\n",
    "\n",
    "filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "            \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "            \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "            \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "\n",
    "spike_tot = [\"BH_Spike_e1n005.npy\", \"BH_Spike_e1n010.npy\", \"BH_Spike_e1n015.npy\", \"BH_Spike_e1n020.npy\",\n",
    "            \"BH_Spike_e2n005.npy\", \"BH_Spike_e2n010.npy\", \"BH_Spike_e2n015.npy\", \"BH_Spike_e2n020.npy\",\n",
    "            \"BH_Spike_d1n005.npy\", \"BH_Spike_d1n010.npy\", \"BH_Spike_d1n015.npy\", \"BH_Spike_d1n020.npy\",\n",
    "            \"BH_Spike_d2n005.npy\", \"BH_Spike_d2n010.npy\", \"BH_Spike_d2n015.npy\", \"BH_Spike_d2n020.npy\"]\n",
    "\n",
    "label_tot = [\"BH_Label_e1n005.npy\", \"BH_Label_e1n010.npy\", \"BH_Label_e1n015.npy\", \"BH_Label_e1n020.npy\",\n",
    "            \"BH_Label_e2n005.npy\", \"BH_Label_e2n010.npy\", \"BH_Label_e2n015.npy\", \"BH_Label_e2n020.npy\",\n",
    "            \"BH_Label_d1n005.npy\", \"BH_Label_d1n010.npy\", \"BH_Label_d1n015.npy\", \"BH_Label_d1n020.npy\",\n",
    "            \"BH_Label_d2n005.npy\", \"BH_Label_d2n010.npy\", \"BH_Label_d2n015.npy\", \"BH_Label_d2n020.npy\"]\n",
    "\n",
    "\n",
    "dataset_num = 16\n",
    "training_num = 2400\n",
    "spike_length = 50\n",
    "\n",
    "spike_train = [] # 스파이크 데이터를 저장할 배열\n",
    "spike_test = [] # 스파이크 데이터를 저장할 배열\n",
    "\n",
    "for ds in range(dataset_num):\n",
    "    print(\"\\ndata:\", filename[ds])\n",
    "    mat1 = io.loadmat(my_path_ground_BH + filename[ds])\n",
    "    raw = mat1['data'][0]\n",
    "    ans_times = mat1['spike_times'][0][0][0]\n",
    "    ans_cluster = mat1['spike_class'][0][0][0]\n",
    "\n",
    "    spike_this_dataset = []\n",
    "    labal_this_dataset = []\n",
    "\n",
    "    # raw 데이터의 기울기 계산\n",
    "    slope = np.diff(raw) # raw보다 한 사이즈 작음.\n",
    "\n",
    "    spike_group = np.zeros((len(ans_times), spike_length))\n",
    "\n",
    "    train_spike_count = 0\n",
    "    for i in range(len(ans_times)):\n",
    "        max_slope_index = ans_times[i] + np.argmax(slope[ans_times[i] : ans_times[i] + 25])\n",
    "        now_spike = raw[max_slope_index - 10 : max_slope_index - 10 + spike_length]\n",
    "        spike_this_dataset.append(now_spike)\n",
    "        labal_this_dataset.append(ans_cluster[i])\n",
    "        if train_spike_count < training_num:\n",
    "            train_spike_count += 1\n",
    "            spike_train.append(now_spike)\n",
    "        else:\n",
    "            spike_test.append(now_spike)\n",
    "\n",
    "    spike_this_dataset = np.array(spike_this_dataset)\n",
    "    labal_this_dataset = np.array(labal_this_dataset)\n",
    "\n",
    "    np.save(my_path_ground_BH + spike_tot[ds], spike_this_dataset)\n",
    "    np.save(my_path_ground_BH + label_tot[ds], labal_this_dataset)\n",
    "\n",
    "spike_train = np.array(spike_train)\n",
    "spike_test = np.array(spike_test)\n",
    "np.random.shuffle(spike_train)\n",
    "np.random.shuffle(spike_test)\n",
    "\n",
    "torch.save(torch.tensor(spike_train, dtype=torch.float32), my_path_ground_BH + 'BH_quiroga_training_dataset_gt_detect.pt')\n",
    "torch.save(torch.tensor(spike_test, dtype=torch.float32), my_path_ground_BH + 'BH_quiroga_test_dataset_gt_detect.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detected spike로 AE train data와 acc 측정을 위한 스파이크 꾸리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data: C_Easy1_noise005.mat\n",
      "spike_train_size 2400\n",
      "spike_test_size 1101\n",
      "spike_total 3501\n",
      "\n",
      "data: C_Easy1_noise01.mat\n",
      "spike_train_size 2400\n",
      "spike_test_size 1532\n",
      "spike_total 3932\n",
      "\n",
      "data: C_Easy1_noise015.mat\n",
      "spike_train_size 2400\n",
      "spike_test_size 1709\n",
      "spike_total 4109\n",
      "\n",
      "data: C_Easy1_noise02.mat\n",
      "spike_train_size 2400\n",
      "spike_test_size 1602\n",
      "spike_total 4002\n",
      "\n",
      "data: C_Easy2_noise005.mat\n",
      "spike_train_size 2400\n",
      "spike_test_size 893\n",
      "spike_total 3293\n",
      "\n",
      "data: C_Easy2_noise01.mat\n",
      "spike_train_size 2400\n",
      "spike_test_size 1014\n",
      "spike_total 3414\n",
      "\n",
      "data: C_Easy2_noise015.mat\n",
      "spike_train_size 2400\n",
      "spike_test_size 1026\n",
      "spike_total 3426\n",
      "\n",
      "data: C_Easy2_noise02.mat\n",
      "spike_train_size 2400\n",
      "spike_test_size 1065\n",
      "spike_total 3465\n",
      "\n",
      "data: C_Difficult1_noise005.mat\n",
      "spike_train_size 2400\n",
      "spike_test_size 897\n",
      "spike_total 3297\n",
      "\n",
      "data: C_Difficult1_noise01.mat\n",
      "spike_train_size 2400\n",
      "spike_test_size 1012\n",
      "spike_total 3412\n",
      "\n",
      "data: C_Difficult1_noise015.mat\n",
      "spike_train_size 2400\n",
      "spike_test_size 1190\n",
      "spike_total 3590\n",
      "\n",
      "data: C_Difficult1_noise02.mat\n",
      "spike_train_size 2400\n",
      "spike_test_size 1083\n",
      "spike_total 3483\n",
      "\n",
      "data: C_Difficult2_noise005.mat\n",
      "spike_train_size 2400\n",
      "spike_test_size 859\n",
      "spike_total 3259\n",
      "\n",
      "data: C_Difficult2_noise01.mat\n",
      "spike_train_size 2400\n",
      "spike_test_size 981\n",
      "spike_total 3381\n",
      "\n",
      "data: C_Difficult2_noise015.mat\n",
      "spike_train_size 2400\n",
      "spike_test_size 1070\n",
      "spike_total 3470\n",
      "\n",
      "data: C_Difficult2_noise02.mat\n",
      "spike_train_size 2400\n",
      "spike_test_size 978\n",
      "spike_total 3378\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io\n",
    "import os\n",
    "\n",
    "# os.chdir(\"./../data/\")\n",
    "my_path_ground_BH = '/data2/spike_sorting/quiroga/BH/'\n",
    "\n",
    "filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "            \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "            \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "            \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "thr_tot = np.array([0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7])\n",
    "\n",
    "dataset_num = 16\n",
    "training_num = 2400\n",
    "spike_length = 50\n",
    "wait_term = 20\n",
    "\n",
    "spike_train = [] # 스파이크 데이터를 저장할 배열\n",
    "spike_test = [] # 스파이크 데이터를 저장할 배열\n",
    "\n",
    "for ds in range(dataset_num):\n",
    "    print(\"\\ndata:\", filename[ds])\n",
    "    past_tr_num = len(spike_train)\n",
    "    past_te_num = len(spike_test)\n",
    "    train_spike_count = 0\n",
    "\n",
    "    # 데이터 파일 불러오기\n",
    "    mat1 = io.loadmat(my_path_ground_BH + filename[ds])\n",
    "    raw = mat1['data'][0]\n",
    "    \n",
    "    # raw 데이터의 기울기 계산\n",
    "    slope = np.diff(raw) # raw보다 한 사이즈 작음.\n",
    "    \n",
    "    # 스파이크 탐지\n",
    "    wait = -20  # 스파이크 탐지 대기 시간 초기화: 처음에는 20샘플은 버림.\n",
    "    for i in range(len(raw)-2):\n",
    "        wait += 1\n",
    "        if(wait_term < wait):\n",
    "            if(raw[i+1] < raw[i+2] and raw[i+1] <= raw[i] and raw[i+1] < -thr) or (raw[i+1] > raw[i+2] and raw[i] <= raw[i+1] and raw[i+1] > thr):\n",
    "                max_slope_index = i + np.argmax(slope[i - 8 : i + 5]) - 8 # 기울기가 최대인 지점에서 스파이크 추출\n",
    "                if train_spike_count < training_num:\n",
    "                    train_spike_count += 1\n",
    "                    spike_train.append(raw[max_slope_index - 10 : max_slope_index - 10 + spike_length]) \n",
    "                else:\n",
    "                    spike_test.append(raw[max_slope_index - 10 : max_slope_index - 10 + spike_length])\n",
    "                wait = 0  # 대기 시간 초기화 # 다시 wait_term만큼 기다려라\n",
    "\n",
    "                # if train_spike_count == 1: # 그림으로 보기\n",
    "                #     plt.plot(raw[max_slope_index - 10 : max_slope_index + 40])\n",
    "                    \n",
    "                #     plt.title(f\"align, max_slope_index={max_slope_index}\")\n",
    "                #     plt.xticks(range(50), labels=range(50))  # x축 눈금 설정\n",
    "                #     plt.yticks(np.arange(min(raw[max_slope_index - 10 : max_slope_index + 40]), \n",
    "                #                         max(raw[max_slope_index - 10 : max_slope_index + 40]) + 1, \n",
    "                #                         step=1))  # y축 눈금 설정, step=1로 매 x마다 보이게 함\n",
    "                    \n",
    "                #     plt.grid(True, which='both', linestyle='--', linewidth=0.5)  # 그래프에 그리드 추가\n",
    "                #     plt.show()\n",
    "    print(\"spike_train_size\", len(spike_train)-past_tr_num)\n",
    "    print(\"spike_test_size\", len(spike_test)-past_te_num)\n",
    "    print(\"spike_total\", len(spike_train)-past_tr_num+len(spike_test)-past_te_num)\n",
    "spike_train = np.array(spike_train)\n",
    "spike_test = np.array(spike_test)\n",
    "np.random.shuffle(spike_train)\n",
    "np.random.shuffle(spike_test)\n",
    "\n",
    "torch.save(torch.tensor(spike_train, dtype=torch.float32), my_path_ground_BH + 'BH_quiroga_training_dataset_real_detect.pt')\n",
    "torch.save(torch.tensor(spike_test, dtype=torch.float32), my_path_ground_BH + 'BH_quiroga_test_dataset_real_detect.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 논문대로 템플릿 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data: C_Easy1_noise005.mat\n",
      "\n",
      "data: C_Easy1_noise01.mat\n",
      "\n",
      "data: C_Easy1_noise015.mat\n",
      "\n",
      "data: C_Easy1_noise02.mat\n",
      "\n",
      "data: C_Easy2_noise005.mat\n",
      "\n",
      "data: C_Easy2_noise01.mat\n",
      "\n",
      "data: C_Easy2_noise015.mat\n",
      "\n",
      "data: C_Easy2_noise02.mat\n",
      "\n",
      "data: C_Difficult1_noise005.mat\n",
      "\n",
      "data: C_Difficult1_noise01.mat\n",
      "\n",
      "data: C_Difficult1_noise015.mat\n",
      "\n",
      "data: C_Difficult1_noise02.mat\n",
      "\n",
      "data: C_Difficult2_noise005.mat\n",
      "\n",
      "data: C_Difficult2_noise01.mat\n",
      "\n",
      "data: C_Difficult2_noise015.mat\n",
      "\n",
      "data: C_Difficult2_noise02.mat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "from scipy import io\n",
    "\n",
    "my_path_ground_BH = '/data2/spike_sorting/quiroga/BH/'\n",
    "\n",
    "# 데이터 파일 목록과 템플릿 파일 목록 설정\n",
    "filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "            \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "            \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "            \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "template =  [\"BH_Spike_TEMPLATE_e1n005.npy\", \"BH_Spike_TEMPLATE_e1n010.npy\", \"BH_Spike_TEMPLATE_e1n015.npy\", \"BH_Spike_TEMPLATE_e1n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_e2n005.npy\", \"BH_Spike_TEMPLATE_e2n010.npy\", \"BH_Spike_TEMPLATE_e2n015.npy\", \"BH_Spike_TEMPLATE_e2n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_d1n005.npy\", \"BH_Spike_TEMPLATE_d1n010.npy\", \"BH_Spike_TEMPLATE_d1n015.npy\", \"BH_Spike_TEMPLATE_d1n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_d2n005.npy\", \"BH_Spike_TEMPLATE_d2n010.npy\", \"BH_Spike_TEMPLATE_d2n015.npy\", \"BH_Spike_TEMPLATE_d2n020.npy\"]\n",
    "\n",
    "thr_tem = np.array([0.9, 0.9, 0.9, 0.9,   1.0, 1.0, 1.0, 1.0,   0.7, 0.7, 0.7, 0.9,   0.9, 0.9, 0.9, 0.9])\n",
    "thr_new_cluster = np.array([50, 50, 50, 50,   50, 50, 50, 50,   50, 30, 30, 50,   50, 50, 50, 50])\n",
    "thr_merge_cluster = np.array([110, 110, 110, 110,   110, 110, 110, 110,   110, 110, 110, 110,   110, 110, 110, 110])\n",
    "wait_term = np.array([20, 20, 20, 20,   20, 20, 20, 20,   20, 20, 20, 20,   20, 20, 20, 20])\n",
    "\n",
    "# thr_tot = np.array([0.5, 0.5, 0.55, 0.7,  0.5, 0.5, 0.55, 0.7,  0.5, 0.5, 0.55, 0.7,  0.5, 0.5, 0.55, 0.7])\n",
    "\n",
    "dataset_num = 16\n",
    "spike_needs = 100 # 템플릿을 만들기 위해 필요한 스파이크 수\n",
    "spike_length = 50\n",
    "num_cluster = 4  # 클러스터 수 설정 # 논문엔 4개라는데 여기서는 3개로 했네\n",
    "cluster_delete_thr = 9 # spike_needs/10 # 이 숫자 초과인 클러스터만 생존\n",
    "\n",
    "for ds in range(dataset_num):\n",
    "    print(\"\\ndata:\", filename[ds])\n",
    "\n",
    "    # 데이터 파일 불러오기\n",
    "    mat1 = io.loadmat(my_path_ground_BH + filename[ds])\n",
    "    raw = mat1['data'][0]\n",
    "    thr = thr_tem[ds]  # 스파이크 탐지 임계값 설정 \n",
    "    spike = []  # 스파이크 데이터를 저장할 배열\n",
    "\n",
    "    # raw 데이터의 기울기 계산\n",
    "    slope = np.diff(raw) # raw보다 한 사이즈 작음.\n",
    "    \n",
    "    # 스파이크 탐지\n",
    "    wait = -20  # 스파이크 탐지 대기 시간 초기화: 처음에는 20샘플은 버림.\n",
    "    spike_count = 0\n",
    "    for i in range(len(raw)-2):\n",
    "        wait += 1\n",
    "        if(wait_term[ds] < wait):\n",
    "            if(raw[i+1] < raw[i+2] and raw[i+1] <= raw[i] and raw[i+1] < -thr) or (raw[i+1] > raw[i+2] and raw[i] <= raw[i+1] and raw[i+1] > thr):\n",
    "                spike_count += 1\n",
    "                max_slope_index = i + np.argmax(slope[i - 8 : i + 5]) - 8 # 기울기가 최대인 지점에서 스파이크 추출\n",
    "                spike.append(raw[max_slope_index - 10 : max_slope_index - 10 + spike_length]) \n",
    "                wait = 0  # 대기 시간 초기화 # 다시 wait_term[ds]만큼 기다려라\n",
    "                if spike_count == spike_needs:\n",
    "                    break\n",
    "    spike = np.array(spike)\n",
    "\n",
    "    Cluster = np.zeros((num_cluster, spike_length))  # 클러스터 배열 초기화\n",
    "    cluster_num = np.zeros(num_cluster)  # 각 클러스터의 데이터 수 초기화\n",
    "    \n",
    "    sm_distance = np.zeros(num_cluster)\n",
    "    mm_distance = np.zeros((num_cluster, num_cluster))\n",
    "\n",
    "    # 훈련 사이클 시작\n",
    "    current_cluster_num = 0\n",
    "    for spike_index in range(spike_needs):\n",
    "        spike_n = spike[spike_index, :]  # 현재 스파이크\n",
    "        \n",
    "        if(spike_index == 0):\n",
    "            Cluster[0, :] = spike_n  # 첫 번째 스파이크는 첫 번째 클러스터에 배정\n",
    "            cluster_num[0] = 1  # 클러스터 데이터 수 증가\n",
    "            current_cluster_num += 1\n",
    "        else:\n",
    "            # 각 클러스터와의 거리 계산\n",
    "\n",
    "            sm_smallest = 1000000000000\n",
    "            sm_smallest_index = 0\n",
    "            for i in range(num_cluster): # 0, 1, 2 까지는 기존 클러스터와 지금 스파이크와의 거리\n",
    "                if cluster_num[i] > 0:\n",
    "                    sm_distance[i] = np.sum(abs(Cluster[i, 5:25] - spike_n[5:25])) * 17 + np.sum(abs(Cluster[i, 0:5] - spike_n[0:5])) * 2 + np.sum(abs(Cluster[i, 25:50] - spike_n[25:50])) * 2\n",
    "                    if sm_smallest > sm_distance[i]:\n",
    "                        sm_smallest = sm_distance[i]\n",
    "                        sm_smallest_index = i\n",
    "\n",
    "            mm_smallest = 1000000000000\n",
    "            mm_smallest_index_i = 0\n",
    "            mm_smallest_index_j = 0\n",
    "            for i in range(num_cluster):\n",
    "                for j in range(i+1, num_cluster):\n",
    "                    if cluster_num[i] > 0 and cluster_num[j] > 0:\n",
    "                        mer_thr = 1.5 if spike_index < 30 else 2.5\n",
    "                        mm_distance[i, j] = np.sum(abs(Cluster[i, 5:25] - Cluster[j, 5:25])) * 17 + np.sum(abs(Cluster[i, 0:5] - Cluster[j, 0:5])) * 2 + np.sum(abs(Cluster[i, 25:50] - Cluster[j, 25:50])) * 2            \n",
    "                        mm_distance[i, j] = mm_distance[i, j] * mer_thr\n",
    "                        if mm_smallest > mm_distance[i, j]:\n",
    "                            mm_smallest = mm_distance[i, j]\n",
    "                            mm_smallest_index_i = i\n",
    "                            mm_smallest_index_j = j\n",
    "\n",
    "            if current_cluster_num < num_cluster:\n",
    "                # print('sm_smallest', sm_smallest)\n",
    "                if sm_smallest > thr_new_cluster[ds]:\n",
    "                    Cluster[current_cluster_num, :] = spike_n\n",
    "                    cluster_num[current_cluster_num] = 1\n",
    "                    current_cluster_num += 1\n",
    "                else:\n",
    "                    Cluster[sm_smallest_index, :] = (Cluster[sm_smallest_index, :] * 15 + spike_n) / 16\n",
    "                    cluster_num[sm_smallest_index] += 1\n",
    "            \n",
    "            else:\n",
    "                if sm_smallest < mm_smallest:\n",
    "                    Cluster[sm_smallest_index, :] = (Cluster[sm_smallest_index, :] * 15 + spike_n) / 16\n",
    "                    cluster_num[sm_smallest_index] += 1\n",
    "                else: #merge\n",
    "                    Cluster[mm_smallest_index_i, :] = (Cluster[mm_smallest_index_i, :] + Cluster[mm_smallest_index_j, :]) / 2\n",
    "                    cluster_num[mm_smallest_index_i] += cluster_num[mm_smallest_index_j]\n",
    "                    Cluster[mm_smallest_index_j, :] = spike_n\n",
    "                    cluster_num[mm_smallest_index_j] = 1\n",
    "\n",
    "    # print('before delete under 11', cluster_num)\n",
    "    # cluster_num이 11이하면 해당 클러스터 없애기\n",
    "    Cluster_temp = np.zeros((num_cluster, spike_length))  # 클러스터 배열 초기화\n",
    "    cluster_num_temp = np.zeros(num_cluster)  # 각 클러스터의 데이터 수 초기화\n",
    "    final_index = 0\n",
    "    for i in range(num_cluster):\n",
    "        if cluster_num[i] > cluster_delete_thr:\n",
    "            Cluster_temp[final_index, :] = Cluster[i, :]\n",
    "            cluster_num_temp[final_index] = cluster_num[i]\n",
    "            final_index += 1\n",
    "    current_cluster_num = final_index\n",
    "    # print(cluster_num_temp)\n",
    "    Cluster = Cluster_temp\n",
    "    cluster_num = cluster_num_temp\n",
    "\n",
    "\n",
    "    # \n",
    "    no_more_merge = False\n",
    "    while (no_more_merge == False):\n",
    "        no_more_merge = True\n",
    "        final_mm_merge_index_i = 0\n",
    "        final_mm_merge_index_j = 0\n",
    "        for i in range(current_cluster_num):\n",
    "            for j in range(i+1, current_cluster_num):\n",
    "                if cluster_num[i] > 0 and cluster_num[j] > 0:\n",
    "                    mer_thr = 2.5\n",
    "                    final_mm_distance = np.sum(abs(Cluster[i, 5:25] - Cluster[j, 5:25])) * 17 + np.sum(abs(Cluster[i, 0:5] - Cluster[j, 0:5])) * 2 + np.sum(abs(Cluster[i, 25:50] - Cluster[j, 25:50])) * 2            \n",
    "                    # print('final_mm_distance', final_mm_distance)\n",
    "                    # print(cluster_num)\n",
    "                    final_mm_distance = final_mm_distance * mer_thr\n",
    "                    if final_mm_distance < thr_merge_cluster[ds]:\n",
    "                        final_mm_merge_index_i = i\n",
    "                        final_mm_merge_index_j = j\n",
    "                        no_more_merge = False\n",
    "                        break\n",
    "            if no_more_merge == False:\n",
    "                break\n",
    "        \n",
    "        if no_more_merge == False:\n",
    "            Cluster[final_mm_merge_index_i, :] = (Cluster[final_mm_merge_index_i, :] + Cluster[final_mm_merge_index_j, :]) / 2\n",
    "            cluster_num[final_mm_merge_index_i] += cluster_num[final_mm_merge_index_j]\n",
    "            cluster_num[final_mm_merge_index_j] = 0\n",
    "            current_cluster_num -= 1\n",
    "\n",
    "            \n",
    "            # 앞으로 다시 땡기기\n",
    "            Cluster_temp = np.zeros((num_cluster, spike_length))  # 클러스터 배열 초기화\n",
    "            cluster_num_temp = np.zeros(num_cluster)  # 각 클러스터의 데이터 수 초기화\n",
    "            final_index = 0\n",
    "            for i in range(num_cluster):\n",
    "                if cluster_num[i] > cluster_delete_thr:\n",
    "                    Cluster_temp[final_index, :] = Cluster[i, :]\n",
    "                    cluster_num_temp[final_index] = cluster_num[i]\n",
    "                    final_index += 1\n",
    "            current_cluster_num = final_index\n",
    "            Cluster = Cluster_temp\n",
    "            cluster_num = cluster_num_temp\n",
    "\n",
    "\n",
    "\n",
    "    # # Cluster plot\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # colors = ['b', 'g', 'r', 'k']  # 클러스터별 색상 지정\n",
    "    # x_axis = np.arange(spike_length)  # x축 값 (스파이크 길이)\n",
    "    # # print(cluster_num)\n",
    "    # for i in range(num_cluster):\n",
    "    #     plt.plot(x_axis, Cluster[i, :], label=f'Cluster {i+1}', color=colors[i % len(colors)])\n",
    "\n",
    "    # plt.title(f'Cluster Templates for {filename[ds]}, nums{cluster_num}')\n",
    "    # plt.xlabel('Sample Index')\n",
    "    # plt.ylabel('Amplitude')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "    \n",
    "    # 클러스터 템플릿을 파일로 저장\n",
    "    np.save(my_path_ground_BH + template[ds], Cluster)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ssp.train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from scipy import io\n",
    "import itertools\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules 폴더에 새모듈.py 만들면\n",
    "# modules/__init__py 파일에 form .새모듈 import * 하셈\n",
    "# 그리고 새모듈.py에서 from modules.새모듈 import * 하셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path_ground_BH = '/data2/spike_sorting/quiroga/BH/'\n",
    "\n",
    "\n",
    "filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "            \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "            \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "            \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "\n",
    "spike_tot = [\"BH_Spike_e1n005.npy\", \"BH_Spike_e1n010.npy\", \"BH_Spike_e1n015.npy\", \"BH_Spike_e1n020.npy\",\n",
    "            \"BH_Spike_e2n005.npy\", \"BH_Spike_e2n010.npy\", \"BH_Spike_e2n015.npy\", \"BH_Spike_e2n020.npy\",\n",
    "            \"BH_Spike_d1n005.npy\", \"BH_Spike_d1n010.npy\", \"BH_Spike_d1n015.npy\", \"BH_Spike_d1n020.npy\",\n",
    "            \"BH_Spike_d2n005.npy\", \"BH_Spike_d2n010.npy\", \"BH_Spike_d2n015.npy\", \"BH_Spike_d2n020.npy\"]\n",
    "\n",
    "label_tot = [\"BH_Label_e1n005.npy\", \"BH_Label_e1n010.npy\", \"BH_Label_e1n015.npy\", \"BH_Label_e1n020.npy\",\n",
    "            \"BH_Label_e2n005.npy\", \"BH_Label_e2n010.npy\", \"BH_Label_e2n015.npy\", \"BH_Label_e2n020.npy\",\n",
    "            \"BH_Label_d1n005.npy\", \"BH_Label_d1n010.npy\", \"BH_Label_d1n015.npy\", \"BH_Label_d1n020.npy\",\n",
    "            \"BH_Label_d2n005.npy\", \"BH_Label_d2n010.npy\", \"BH_Label_d2n015.npy\", \"BH_Label_d2n020.npy\"]\n",
    "\n",
    "template =  [\"BH_Spike_TEMPLATE_e1n005.npy\", \"BH_Spike_TEMPLATE_e1n010.npy\", \"BH_Spike_TEMPLATE_e1n015.npy\", \"BH_Spike_TEMPLATE_e1n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_e2n005.npy\", \"BH_Spike_TEMPLATE_e2n010.npy\", \"BH_Spike_TEMPLATE_e2n015.npy\", \"BH_Spike_TEMPLATE_e2n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_d1n005.npy\", \"BH_Spike_TEMPLATE_d1n010.npy\", \"BH_Spike_TEMPLATE_d1n015.npy\", \"BH_Spike_TEMPLATE_d1n020.npy\",\n",
    "             \"BH_Spike_TEMPLATE_d2n005.npy\", \"BH_Spike_TEMPLATE_d2n010.npy\", \"BH_Spike_TEMPLATE_d2n015.npy\", \"BH_Spike_TEMPLATE_d2n020.npy\"]\n",
    "\n",
    "AE_train_path_gt_detect = 'BH_quiroga_training_dataset_gt_detect.pt' \n",
    "AE_test_path_gt_detect = 'BH_quiroga_test_dataset_gt_detect.pt'\n",
    "\n",
    "AE_train_path_real_detect = 'BH_quiroga_training_dataset_real_detect.pt'\n",
    "AE_test_path_real_detect = 'BH_quiroga_test_dataset_real_detect.pt'\n",
    "\n",
    "\n",
    "\n",
    "# thr_tot = np.array([0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7, 0.5, 0.5, 0.55, 0.7])\n",
    "cos_thr = np.array([0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.85, 0.95, 0.9, 0.8, 0.95, 0.95, 0.95, 0.95, 0.8])\n",
    "\n",
    "Conv_net = True\n",
    "SAE_net = True\n",
    "\n",
    "# hyperparameter\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '3'\n",
    "dataset_num = 16\n",
    "spike_length = 50\n",
    "n_sample = spike_length\n",
    "num_cluster = 4  # 클러스터 수 설정 # 논문엔 4개라는데 여기서는 3개로 했네\n",
    "training_cycle = 2400 # 그 초기 몇개까지만 cluster update할지\n",
    "AE_train_data = AE_train_path_real_detect #AE_train_path_gt_detect #AE_train_path_real_detect\n",
    "AE_test_data = AE_test_path_real_detect #AE_test_path_gt_detect  #AE_test_path_real_detect\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "max_epoch = 7000\n",
    "learning_rate = 0.001\n",
    "normalize_on = False # True or False # 이거 별로 안 좋은 normalize같음 # 쓸 거면 다른 거 써라.\n",
    "need_bias = False\n",
    "# first_layer_no_train = False\n",
    "lif_add_at_first = False\n",
    "TIME = 10 # SAE일 때만 유효\n",
    "my_seed = 42\n",
    "\n",
    "\n",
    "seed_assign(my_seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class spikedataset(Dataset):\n",
    "    def __init__(self, path, transform = None):    \n",
    "        self.transform = transform\n",
    "        self.spike = torch.load(path)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        spike = self.spike[index]            \n",
    "        if self.transform is not None:\n",
    "            spike = self.transform(spike)\n",
    "        return spike\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.spike)\n",
    "\n",
    "train_dataset = spikedataset(my_path_ground_BH + AE_train_data)\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "test_dataset = spikedataset(my_path_ground_BH + AE_test_data)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "\n",
    "class AE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.conv1 = nn.Conv1d(1, 32, 3, stride = 2, bias = False) # 24\n",
    "        self.conv2 = nn.Conv1d(32, 64, 3, stride = 2, bias = False) # 11\n",
    "        self.conv3 = nn.Conv1d(64, 96, 3, stride = 2, bias = False) # 4 # 병현: 여기 5인데?\n",
    "        self.fc1 = nn.Linear(96 * 5, 4, bias = False)\n",
    "        \n",
    "        # decoder\n",
    "        self.fc4 = nn.Linear(4, 5 * 96, bias = False)\n",
    "        self.deconv3 = nn.ConvTranspose1d(96, 64, 3, stride = 2, bias = False) #6 + 2 + 1= 9\n",
    "        self.deconv1 = nn.ConvTranspose1d(64, 32, 3, stride = 2, output_padding=1, bias = False) #16(9-1)*stride + 4(kernel-1) + 1 = 21\n",
    "        self.deconv2 = nn.ConvTranspose1d(32, 1, 3, stride = 2, output_padding=1, bias = False) #40 + 4 + 1 = 45\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # encoder\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 96 * 5)\n",
    "        mid = self.fc1(x)\n",
    "        norm = torch.sqrt(torch.sum(torch.pow(mid, 2), dim = 1))\n",
    "        h = (mid.t()/(norm + 1e-12)).t()\n",
    "\n",
    "        # decoder\n",
    "        z = F.relu(self.fc4(h))\n",
    "        z = z.view(-1, 96, 5)\n",
    "        z = F.relu(self.deconv3(z))\n",
    "        z = F.relu(self.deconv1(z))\n",
    "        z = self.deconv2(z)\n",
    "\n",
    "        return h, z\n",
    "    \n",
    "\n",
    "\n",
    "# net = AE()\n",
    "# net = torch.nn.DataParallel(net)\n",
    "    \n",
    "# net = torch.load('/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/net_save/save_now_net_20241231_120818.pth')\n",
    "\n",
    "# 모델 초기화\n",
    "if SAE_net == False:\n",
    "    if Conv_net == True:\n",
    "        net = Autoencoder_conv1(input_channels=1, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = 4, padding = 0, stride = 2, kernel_size = 3, need_bias=need_bias)\n",
    "        net = torch.nn.DataParallel(net)\n",
    "    else:\n",
    "        net = Autoencoder_only_FC(encoder_ch=[96, 64, 32, 4], decoder_ch=[32,64,96,n_sample], n_sample=n_sample, need_bias=need_bias)\n",
    "        net = torch.nn.DataParallel(net)\n",
    "else:\n",
    "    if Conv_net == True: \n",
    "        net = SAE_conv1(input_channels=1, input_length=n_sample, encoder_ch = [32, 64, 96], fc_dim = 4, padding = 0, stride = 2, kernel_size = 3, \n",
    "                            synapse_fc_trace_const1=1, \n",
    "                            synapse_fc_trace_const2=0.5, #안씀 \n",
    "                            TIME=TIME, v_init=0.0, v_decay=0.5, v_threshold=0.75, v_reset=10000.0, \n",
    "                            sg_width=4.0, surrogate='sigmoid', BPTT_on=True, need_bias=need_bias, lif_add_at_first=lif_add_at_first)\n",
    "        net = torch.nn.DataParallel(net)\n",
    "    else:\n",
    "        net = SAE_fc_only(encoder_ch=[96, 64, 32, 4], \n",
    "                            decoder_ch=[32,64,96,n_sample], \n",
    "                            in_channels=n_sample, # in_channel 이 여기선 걍 lenght.\n",
    "                            synapse_fc_trace_const1=1,\n",
    "                            synapse_fc_trace_const2=0.5,  #안씀 \n",
    "                            TIME=TIME, v_init=0.0, v_decay=0.5, v_threshold=0.75, v_reset=10000.0, \n",
    "                            sg_width=4.0, surrogate='sigmoid', BPTT_on=True, need_bias=need_bias, lif_add_at_first=lif_add_at_first)\n",
    "        net = torch.nn.DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): SAE_conv1(\n",
      "    (encoder): Sequential(\n",
      "      (0): SSBH_DimChanger_one_two()\n",
      "      (1): SSBH_DimChanger_for_unsuqeeze()\n",
      "      (2): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (3): Conv1d(1, 32, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      (4): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (5): LIF_layer()\n",
      "      (6): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (7): Conv1d(32, 64, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      (8): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (9): LIF_layer()\n",
      "      (10): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (11): Conv1d(64, 96, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      (12): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (13): LIF_layer()\n",
      "      (14): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (15): SSBH_DimChanger_for_fc()\n",
      "      (16): Linear(in_features=480, out_features=4, bias=False)\n",
      "      (17): SSBH_L2NormLayer()\n",
      "      (18): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (19): SSBH_DimChanger_one_two()\n",
      "    )\n",
      "    (decoder): Sequential(\n",
      "      (0): SSBH_DimChanger_one_two()\n",
      "      (1): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (2): Linear(in_features=4, out_features=480, bias=False)\n",
      "      (3): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (4): LIF_layer()\n",
      "      (5): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (6): SSBH_DimChanger_for_conv1()\n",
      "      (7): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (8): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (9): ConvTranspose1d(96, 64, kernel_size=(3,), stride=(2,), bias=False)\n",
      "      (10): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (11): LIF_layer()\n",
      "      (12): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (13): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(2,), output_padding=(1,), bias=False)\n",
      "      (14): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (15): LIF_layer()\n",
      "      (16): SSBH_DimChanger_for_one_two_coupling()\n",
      "      (17): ConvTranspose1d(32, 1, kernel_size=(3,), stride=(2,), output_padding=(1,), bias=False)\n",
      "      (18): SSBH_DimChanger_for_one_two_decoupling()\n",
      "      (19): SSBH_DimChanger_for_suqeeze()\n",
      "      (20): SSBH_DimChanger_one_two()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Device: cuda\n",
      "\n",
      "Start Training, current_time = 20241231_210505_671\n",
      "\n",
      "epoch-0 accuracy check\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_accuracy_post_training_cycle_all_dataset [0.33393178 0.35561497 0.30269266 0.34729981 0.35544554 0.35\n",
      " 0.34124629 0.3365897  0.33265514 0.33587786 0.34235075 0.31360947\n",
      " 0.33609959 0.33709981 0.33557692 0.34217749]\n",
      "mean_cluster_accuracy_during_training_cycle : 34.17%, post_traincycle_acc : 33.74%, total_acc : 34.04%\n",
      "save model\n",
      "\n",
      "epoch-0 loss : 0.54151\n",
      "\n",
      "epoch-1 loss : 0.36991\n",
      "\n",
      "epoch-2 loss : 0.28610\n",
      "\n",
      "epoch-3 loss : 0.26038\n",
      "\n",
      "epoch-4 loss : 0.21060\n",
      "\n",
      "epoch-5 loss : 0.18183\n",
      "\n",
      "epoch-6 loss : 0.15890\n",
      "\n",
      "epoch-7 loss : 0.14289\n",
      "\n",
      "epoch-8 loss : 0.13590\n",
      "\n",
      "epoch-9 loss : 0.13014\n",
      "\n",
      "epoch-10 loss : 0.12535\n",
      "\n",
      "epoch-11 loss : 0.12216\n",
      "\n",
      "epoch-12 loss : 0.12058\n",
      "\n",
      "epoch-13 loss : 0.11865\n",
      "\n",
      "epoch-14 loss : 0.11789\n",
      "\n",
      "epoch-15 loss : 0.11661\n",
      "\n",
      "epoch-16 loss : 0.11536\n",
      "\n",
      "epoch-17 loss : 0.11591\n",
      "\n",
      "epoch-18 loss : 0.11458\n",
      "\n",
      "epoch-19 loss : 0.11461\n",
      "\n",
      "epoch-20 loss : 0.11354\n",
      "\n",
      "epoch-21 loss : 0.11304\n",
      "\n",
      "epoch-22 loss : 0.11306\n",
      "\n",
      "epoch-23 loss : 0.11214\n",
      "\n",
      "epoch-24 loss : 0.11217\n",
      "\n",
      "epoch-25 loss : 0.11235\n",
      "\n",
      "epoch-26 loss : 0.11169\n",
      "\n",
      "epoch-27 loss : 0.11098\n",
      "\n",
      "epoch-28 loss : 0.11063\n",
      "\n",
      "epoch-29 loss : 0.11043\n",
      "\n",
      "epoch-30 loss : 0.10964\n",
      "\n",
      "epoch-31 loss : 0.10957\n",
      "\n",
      "epoch-32 loss : 0.10953\n",
      "\n",
      "epoch-33 loss : 0.10911\n",
      "\n",
      "epoch-34 loss : 0.10938\n",
      "\n",
      "epoch-35 loss : 0.10890\n",
      "\n",
      "epoch-36 loss : 0.10874\n",
      "\n",
      "epoch-37 loss : 0.10826\n",
      "\n",
      "epoch-38 loss : 0.10834\n",
      "\n",
      "epoch-39 loss : 0.10771\n",
      "\n",
      "epoch-40 loss : 0.10833\n",
      "\n",
      "epoch-41 loss : 0.10799\n",
      "\n",
      "epoch-42 loss : 0.10805\n",
      "\n",
      "epoch-43 loss : 0.10750\n",
      "\n",
      "epoch-44 loss : 0.10775\n",
      "\n",
      "epoch-45 loss : 0.10709\n",
      "\n",
      "epoch-46 loss : 0.10725\n",
      "\n",
      "epoch-47 loss : 0.10654\n",
      "\n",
      "epoch-48 loss : 0.10637\n",
      "\n",
      "epoch-49 loss : 0.10634\n",
      "\n",
      "epoch-50 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.96499102 0.91087344 0.90250696 0.8538175  0.72772277 0.52946429\n",
      " 0.49357072 0.48579041 0.69379451 0.54580153 0.3983209  0.51183432\n",
      " 0.90352697 0.78154426 0.53942308 0.61939616]\n",
      "mean_cluster_accuracy_during_training_cycle : 68.39%, post_traincycle_acc : 67.89%, total_acc : 68.24%\n",
      "save model\n",
      "\n",
      "epoch-50 loss : 0.10578\n",
      "\n",
      "epoch-51 loss : 0.10588\n",
      "\n",
      "epoch-52 loss : 0.10551\n",
      "\n",
      "epoch-53 loss : 0.10544\n",
      "\n",
      "epoch-54 loss : 0.10553\n",
      "\n",
      "epoch-55 loss : 0.10498\n",
      "\n",
      "epoch-56 loss : 0.10474\n",
      "\n",
      "epoch-57 loss : 0.10486\n",
      "\n",
      "epoch-58 loss : 0.10490\n",
      "\n",
      "epoch-59 loss : 0.10501\n",
      "\n",
      "epoch-60 loss : 0.10454\n",
      "\n",
      "epoch-61 loss : 0.10436\n",
      "\n",
      "epoch-62 loss : 0.10488\n",
      "\n",
      "epoch-63 loss : 0.10489\n",
      "\n",
      "epoch-64 loss : 0.10443\n",
      "\n",
      "epoch-65 loss : 0.10402\n",
      "\n",
      "epoch-66 loss : 0.10379\n",
      "\n",
      "epoch-67 loss : 0.10391\n",
      "\n",
      "epoch-68 loss : 0.10396\n",
      "\n",
      "epoch-69 loss : 0.10409\n",
      "\n",
      "epoch-70 loss : 0.10346\n",
      "\n",
      "epoch-71 loss : 0.10369\n",
      "\n",
      "epoch-72 loss : 0.10374\n",
      "\n",
      "epoch-73 loss : 0.10355\n",
      "\n",
      "epoch-74 loss : 0.10329\n",
      "\n",
      "epoch-75 loss : 0.10309\n",
      "\n",
      "epoch-76 loss : 0.10353\n",
      "\n",
      "epoch-77 loss : 0.10292\n",
      "\n",
      "epoch-78 loss : 0.10320\n",
      "\n",
      "epoch-79 loss : 0.10316\n",
      "\n",
      "epoch-80 loss : 0.10285\n",
      "\n",
      "epoch-81 loss : 0.10270\n",
      "\n",
      "epoch-82 loss : 0.10279\n",
      "\n",
      "epoch-83 loss : 0.10294\n",
      "\n",
      "epoch-84 loss : 0.10287\n",
      "\n",
      "epoch-85 loss : 0.10247\n",
      "\n",
      "epoch-86 loss : 0.10222\n",
      "\n",
      "epoch-87 loss : 0.10234\n",
      "\n",
      "epoch-88 loss : 0.10204\n",
      "\n",
      "epoch-89 loss : 0.10205\n",
      "\n",
      "epoch-90 loss : 0.10230\n",
      "\n",
      "epoch-91 loss : 0.10176\n",
      "\n",
      "epoch-92 loss : 0.10187\n",
      "\n",
      "epoch-93 loss : 0.10188\n",
      "\n",
      "epoch-94 loss : 0.10148\n",
      "\n",
      "epoch-95 loss : 0.10144\n",
      "\n",
      "epoch-96 loss : 0.10138\n",
      "\n",
      "epoch-97 loss : 0.10144\n",
      "\n",
      "epoch-98 loss : 0.10152\n",
      "\n",
      "epoch-99 loss : 0.10123\n",
      "\n",
      "epoch-100 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.79802513 0.89750446 0.88393686 0.84729981 0.64752475 0.51875\n",
      " 0.46092977 0.48046181 0.69684639 0.54198473 0.39272388 0.52859961\n",
      " 0.88174274 0.77589454 0.53557692 0.6010979 ]\n",
      "mean_cluster_accuracy_during_training_cycle : 66.79%, post_traincycle_acc : 65.56%, total_acc : 66.42%\n",
      "\n",
      "epoch-100 loss : 0.10137\n",
      "\n",
      "epoch-101 loss : 0.10127\n",
      "\n",
      "epoch-102 loss : 0.10106\n",
      "\n",
      "epoch-103 loss : 0.10117\n",
      "\n",
      "epoch-104 loss : 0.10102\n",
      "\n",
      "epoch-105 loss : 0.10113\n",
      "\n",
      "epoch-106 loss : 0.10076\n",
      "\n",
      "epoch-107 loss : 0.10117\n",
      "\n",
      "epoch-108 loss : 0.10072\n",
      "\n",
      "epoch-109 loss : 0.10049\n",
      "\n",
      "epoch-110 loss : 0.10068\n",
      "\n",
      "epoch-111 loss : 0.10057\n",
      "\n",
      "epoch-112 loss : 0.10044\n",
      "\n",
      "epoch-113 loss : 0.10044\n",
      "\n",
      "epoch-114 loss : 0.10023\n",
      "\n",
      "epoch-115 loss : 0.10053\n",
      "\n",
      "epoch-116 loss : 0.10037\n",
      "\n",
      "epoch-117 loss : 0.10015\n",
      "\n",
      "epoch-118 loss : 0.10052\n",
      "\n",
      "epoch-119 loss : 0.10016\n",
      "\n",
      "epoch-120 loss : 0.10036\n",
      "\n",
      "epoch-121 loss : 0.10013\n",
      "\n",
      "epoch-122 loss : 0.09968\n",
      "\n",
      "epoch-123 loss : 0.09982\n",
      "\n",
      "epoch-124 loss : 0.10008\n",
      "\n",
      "epoch-125 loss : 0.09959\n",
      "\n",
      "epoch-126 loss : 0.09940\n",
      "\n",
      "epoch-127 loss : 0.09949\n",
      "\n",
      "epoch-128 loss : 0.09967\n",
      "\n",
      "epoch-129 loss : 0.09929\n",
      "\n",
      "epoch-130 loss : 0.09945\n",
      "\n",
      "epoch-131 loss : 0.09947\n",
      "\n",
      "epoch-132 loss : 0.09953\n",
      "\n",
      "epoch-133 loss : 0.09915\n",
      "\n",
      "epoch-134 loss : 0.09929\n",
      "\n",
      "epoch-135 loss : 0.09905\n",
      "\n",
      "epoch-136 loss : 0.09900\n",
      "\n",
      "epoch-137 loss : 0.09933\n",
      "\n",
      "epoch-138 loss : 0.09911\n",
      "\n",
      "epoch-139 loss : 0.09888\n",
      "\n",
      "epoch-140 loss : 0.09892\n",
      "\n",
      "epoch-141 loss : 0.09907\n",
      "\n",
      "epoch-142 loss : 0.09892\n",
      "\n",
      "epoch-143 loss : 0.09877\n",
      "\n",
      "epoch-144 loss : 0.09863\n",
      "\n",
      "epoch-145 loss : 0.09862\n",
      "\n",
      "epoch-146 loss : 0.09899\n",
      "\n",
      "epoch-147 loss : 0.09879\n",
      "\n",
      "epoch-148 loss : 0.09858\n",
      "\n",
      "epoch-149 loss : 0.09858\n",
      "\n",
      "epoch-150 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.80251346 0.89839572 0.90250696 0.81750466 0.71881188 0.52053571\n",
      " 0.52225519 0.48046181 0.69888098 0.54293893 0.39272388 0.50690335\n",
      " 0.89315353 0.78719397 0.52884615 0.44281793]\n",
      "mean_cluster_accuracy_during_training_cycle : 66.59%, post_traincycle_acc : 65.35%, total_acc : 66.21%\n",
      "\n",
      "epoch-150 loss : 0.09866\n",
      "\n",
      "epoch-151 loss : 0.09891\n",
      "\n",
      "epoch-152 loss : 0.09861\n",
      "\n",
      "epoch-153 loss : 0.09873\n",
      "\n",
      "epoch-154 loss : 0.09853\n",
      "\n",
      "epoch-155 loss : 0.09868\n",
      "\n",
      "epoch-156 loss : 0.09867\n",
      "\n",
      "epoch-157 loss : 0.09862\n",
      "\n",
      "epoch-158 loss : 0.09872\n",
      "\n",
      "epoch-159 loss : 0.09865\n",
      "\n",
      "epoch-160 loss : 0.09861\n",
      "\n",
      "epoch-161 loss : 0.09852\n",
      "\n",
      "epoch-162 loss : 0.09865\n",
      "\n",
      "epoch-163 loss : 0.09838\n",
      "\n",
      "epoch-164 loss : 0.09847\n",
      "\n",
      "epoch-165 loss : 0.09844\n",
      "\n",
      "epoch-166 loss : 0.09864\n",
      "\n",
      "epoch-167 loss : 0.09843\n",
      "\n",
      "epoch-168 loss : 0.09869\n",
      "\n",
      "epoch-169 loss : 0.09838\n",
      "\n",
      "epoch-170 loss : 0.09869\n",
      "\n",
      "epoch-171 loss : 0.09855\n",
      "\n",
      "epoch-172 loss : 0.09813\n",
      "\n",
      "epoch-173 loss : 0.09815\n",
      "\n",
      "epoch-174 loss : 0.09800\n",
      "\n",
      "epoch-175 loss : 0.09793\n",
      "\n",
      "epoch-176 loss : 0.09817\n",
      "\n",
      "epoch-177 loss : 0.09809\n",
      "\n",
      "epoch-178 loss : 0.09818\n",
      "\n",
      "epoch-179 loss : 0.09799\n",
      "\n",
      "epoch-180 loss : 0.09799\n",
      "\n",
      "epoch-181 loss : 0.09795\n",
      "\n",
      "epoch-182 loss : 0.09799\n",
      "\n",
      "epoch-183 loss : 0.09782\n",
      "\n",
      "epoch-184 loss : 0.09784\n",
      "\n",
      "epoch-185 loss : 0.09760\n",
      "\n",
      "epoch-186 loss : 0.09780\n",
      "\n",
      "epoch-187 loss : 0.09771\n",
      "\n",
      "epoch-188 loss : 0.09764\n",
      "\n",
      "epoch-189 loss : 0.09779\n",
      "\n",
      "epoch-190 loss : 0.09774\n",
      "\n",
      "epoch-191 loss : 0.09772\n",
      "\n",
      "epoch-192 loss : 0.09763\n",
      "\n",
      "epoch-193 loss : 0.09781\n",
      "\n",
      "epoch-194 loss : 0.09767\n",
      "\n",
      "epoch-195 loss : 0.09756\n",
      "\n",
      "epoch-196 loss : 0.09765\n",
      "\n",
      "epoch-197 loss : 0.09751\n",
      "\n",
      "epoch-198 loss : 0.09741\n",
      "\n",
      "epoch-199 loss : 0.09741\n",
      "\n",
      "epoch-200 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.85816876 0.94741533 0.90157846 0.7877095  0.71980198 0.52053571\n",
      " 0.55093966 0.48401421 0.66531027 0.38740458 0.39085821 0.50986193\n",
      " 0.91286307 0.80414313 0.53076923 0.64958829]\n",
      "mean_cluster_accuracy_during_training_cycle : 66.61%, post_traincycle_acc : 66.38%, total_acc : 66.55%\n",
      "\n",
      "epoch-200 loss : 0.09753\n",
      "\n",
      "epoch-201 loss : 0.09753\n",
      "\n",
      "epoch-202 loss : 0.09724\n",
      "\n",
      "epoch-203 loss : 0.09738\n",
      "\n",
      "epoch-204 loss : 0.09727\n",
      "\n",
      "epoch-205 loss : 0.09726\n",
      "\n",
      "epoch-206 loss : 0.09722\n",
      "\n",
      "epoch-207 loss : 0.09727\n",
      "\n",
      "epoch-208 loss : 0.09747\n",
      "\n",
      "epoch-209 loss : 0.09728\n",
      "\n",
      "epoch-210 loss : 0.09718\n",
      "\n",
      "epoch-211 loss : 0.09737\n",
      "\n",
      "epoch-212 loss : 0.09693\n",
      "\n",
      "epoch-213 loss : 0.09713\n",
      "\n",
      "epoch-214 loss : 0.09717\n",
      "\n",
      "epoch-215 loss : 0.09706\n",
      "\n",
      "epoch-216 loss : 0.09712\n",
      "\n",
      "epoch-217 loss : 0.09707\n",
      "\n",
      "epoch-218 loss : 0.09710\n",
      "\n",
      "epoch-219 loss : 0.09703\n",
      "\n",
      "epoch-220 loss : 0.09708\n",
      "\n",
      "epoch-221 loss : 0.09714\n",
      "\n",
      "epoch-222 loss : 0.09681\n",
      "\n",
      "epoch-223 loss : 0.09693\n",
      "\n",
      "epoch-224 loss : 0.09699\n",
      "\n",
      "epoch-225 loss : 0.09699\n",
      "\n",
      "epoch-226 loss : 0.09686\n",
      "\n",
      "epoch-227 loss : 0.09720\n",
      "\n",
      "epoch-228 loss : 0.09687\n",
      "\n",
      "epoch-229 loss : 0.09678\n",
      "\n",
      "epoch-230 loss : 0.09689\n",
      "\n",
      "epoch-231 loss : 0.09690\n",
      "\n",
      "epoch-232 loss : 0.09679\n",
      "\n",
      "epoch-233 loss : 0.09700\n",
      "\n",
      "epoch-234 loss : 0.09672\n",
      "\n",
      "epoch-235 loss : 0.09699\n",
      "\n",
      "epoch-236 loss : 0.09661\n",
      "\n",
      "epoch-237 loss : 0.09669\n",
      "\n",
      "epoch-238 loss : 0.09661\n",
      "\n",
      "epoch-239 loss : 0.09679\n",
      "\n",
      "epoch-240 loss : 0.09673\n",
      "\n",
      "epoch-241 loss : 0.09654\n",
      "\n",
      "epoch-242 loss : 0.09679\n",
      "\n",
      "epoch-243 loss : 0.09646\n",
      "\n",
      "epoch-244 loss : 0.09659\n",
      "\n",
      "epoch-245 loss : 0.09651\n",
      "\n",
      "epoch-246 loss : 0.09663\n",
      "\n",
      "epoch-247 loss : 0.09672\n",
      "\n",
      "epoch-248 loss : 0.09661\n",
      "\n",
      "epoch-249 loss : 0.09676\n",
      "\n",
      "epoch-250 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.77468582 0.93939394 0.89322191 0.66387337 0.64158416 0.51964286\n",
      " 0.44312562 0.48845471 0.65818922 0.53625954 0.39179104 0.50493097\n",
      " 0.91286307 0.80508475 0.64326923 0.64867338]\n",
      "mean_cluster_accuracy_during_training_cycle : 66.69%, post_traincycle_acc : 65.41%, total_acc : 66.31%\n",
      "\n",
      "epoch-250 loss : 0.09650\n",
      "\n",
      "epoch-251 loss : 0.09655\n",
      "\n",
      "epoch-252 loss : 0.09647\n",
      "\n",
      "epoch-253 loss : 0.09666\n",
      "\n",
      "epoch-254 loss : 0.09641\n",
      "\n",
      "epoch-255 loss : 0.09648\n",
      "\n",
      "epoch-256 loss : 0.09646\n",
      "\n",
      "epoch-257 loss : 0.09616\n",
      "\n",
      "epoch-258 loss : 0.09632\n",
      "\n",
      "epoch-259 loss : 0.09637\n",
      "\n",
      "epoch-260 loss : 0.09660\n",
      "\n",
      "epoch-261 loss : 0.09647\n",
      "\n",
      "epoch-262 loss : 0.09630\n",
      "\n",
      "epoch-263 loss : 0.09607\n",
      "\n",
      "epoch-264 loss : 0.09632\n",
      "\n",
      "epoch-265 loss : 0.09617\n",
      "\n",
      "epoch-266 loss : 0.09610\n",
      "\n",
      "epoch-267 loss : 0.09614\n",
      "\n",
      "epoch-268 loss : 0.09625\n",
      "\n",
      "epoch-269 loss : 0.09597\n",
      "\n",
      "epoch-270 loss : 0.09622\n",
      "\n",
      "epoch-271 loss : 0.09611\n",
      "\n",
      "epoch-272 loss : 0.09623\n",
      "\n",
      "epoch-273 loss : 0.09610\n",
      "\n",
      "epoch-274 loss : 0.09604\n",
      "\n",
      "epoch-275 loss : 0.09601\n",
      "\n",
      "epoch-276 loss : 0.09602\n",
      "\n",
      "epoch-277 loss : 0.09598\n",
      "\n",
      "epoch-278 loss : 0.09610\n",
      "\n",
      "epoch-279 loss : 0.09595\n",
      "\n",
      "epoch-280 loss : 0.09591\n",
      "\n",
      "epoch-281 loss : 0.09599\n",
      "\n",
      "epoch-282 loss : 0.09609\n",
      "\n",
      "epoch-283 loss : 0.09611\n",
      "\n",
      "epoch-284 loss : 0.09576\n",
      "\n",
      "epoch-285 loss : 0.09600\n",
      "\n",
      "epoch-286 loss : 0.09611\n",
      "\n",
      "epoch-287 loss : 0.09612\n",
      "\n",
      "epoch-288 loss : 0.09580\n",
      "\n",
      "epoch-289 loss : 0.09608\n",
      "\n",
      "epoch-290 loss : 0.09602\n",
      "\n",
      "epoch-291 loss : 0.09581\n",
      "\n",
      "epoch-292 loss : 0.09577\n",
      "\n",
      "epoch-293 loss : 0.09585\n",
      "\n",
      "epoch-294 loss : 0.09594\n",
      "\n",
      "epoch-295 loss : 0.09584\n",
      "\n",
      "epoch-296 loss : 0.09578\n",
      "\n",
      "epoch-297 loss : 0.09583\n",
      "\n",
      "epoch-298 loss : 0.09594\n",
      "\n",
      "epoch-299 loss : 0.09582\n",
      "\n",
      "epoch-300 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.76929982 0.92602496 0.8904364  0.66945996 0.71782178 0.52142857\n",
      " 0.44411474 0.48401421 0.65005086 0.38835878 0.39272388 0.50394477\n",
      " 0.9159751  0.8079096  0.65192308 0.6431839 ]\n",
      "mean_cluster_accuracy_during_training_cycle : 66.54%, post_traincycle_acc : 64.85%, total_acc : 66.02%\n",
      "\n",
      "epoch-300 loss : 0.09566\n",
      "\n",
      "epoch-301 loss : 0.09574\n",
      "\n",
      "epoch-302 loss : 0.09563\n",
      "\n",
      "epoch-303 loss : 0.09563\n",
      "\n",
      "epoch-304 loss : 0.09587\n",
      "\n",
      "epoch-305 loss : 0.09577\n",
      "\n",
      "epoch-306 loss : 0.09587\n",
      "\n",
      "epoch-307 loss : 0.09585\n",
      "\n",
      "epoch-308 loss : 0.09591\n",
      "\n",
      "epoch-309 loss : 0.09571\n",
      "\n",
      "epoch-310 loss : 0.09572\n",
      "\n",
      "epoch-311 loss : 0.09577\n",
      "\n",
      "epoch-312 loss : 0.09571\n",
      "\n",
      "epoch-313 loss : 0.09574\n",
      "\n",
      "epoch-314 loss : 0.09561\n",
      "\n",
      "epoch-315 loss : 0.09585\n",
      "\n",
      "epoch-316 loss : 0.09568\n",
      "\n",
      "epoch-317 loss : 0.09567\n",
      "\n",
      "epoch-318 loss : 0.09581\n",
      "\n",
      "epoch-319 loss : 0.09568\n",
      "\n",
      "epoch-320 loss : 0.09568\n",
      "\n",
      "epoch-321 loss : 0.09575\n",
      "\n",
      "epoch-322 loss : 0.09563\n",
      "\n",
      "epoch-323 loss : 0.09566\n",
      "\n",
      "epoch-324 loss : 0.09573\n",
      "\n",
      "epoch-325 loss : 0.09558\n",
      "\n",
      "epoch-326 loss : 0.09571\n",
      "\n",
      "epoch-327 loss : 0.09572\n",
      "\n",
      "epoch-328 loss : 0.09575\n",
      "\n",
      "epoch-329 loss : 0.09577\n",
      "\n",
      "epoch-330 loss : 0.09575\n",
      "\n",
      "epoch-331 loss : 0.09562\n",
      "\n",
      "epoch-332 loss : 0.09553\n",
      "\n",
      "epoch-333 loss : 0.09563\n",
      "\n",
      "epoch-334 loss : 0.09569\n",
      "\n",
      "epoch-335 loss : 0.09579\n",
      "\n",
      "epoch-336 loss : 0.09580\n",
      "\n",
      "epoch-337 loss : 0.09569\n",
      "\n",
      "epoch-338 loss : 0.09584\n",
      "\n",
      "epoch-339 loss : 0.09569\n",
      "\n",
      "epoch-340 loss : 0.09553\n",
      "\n",
      "epoch-341 loss : 0.09556\n",
      "\n",
      "epoch-342 loss : 0.09567\n",
      "\n",
      "epoch-343 loss : 0.09562\n",
      "\n",
      "epoch-344 loss : 0.09549\n",
      "\n",
      "epoch-345 loss : 0.09551\n",
      "\n",
      "epoch-346 loss : 0.09549\n",
      "\n",
      "epoch-347 loss : 0.09548\n",
      "\n",
      "epoch-348 loss : 0.09571\n",
      "\n",
      "epoch-349 loss : 0.09552\n",
      "\n",
      "epoch-350 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.7567325  0.89572193 0.86536676 0.77932961 0.68217822 0.54196429\n",
      " 0.67655786 0.5035524  0.65005086 0.53625954 0.39738806 0.50788955\n",
      " 0.91493776 0.83898305 0.62692308 0.6431839 ]\n",
      "mean_cluster_accuracy_during_training_cycle : 68.31%, post_traincycle_acc : 67.61%, total_acc : 68.09%\n",
      "\n",
      "epoch-350 loss : 0.09560\n",
      "\n",
      "epoch-351 loss : 0.09550\n",
      "\n",
      "epoch-352 loss : 0.09531\n",
      "\n",
      "epoch-353 loss : 0.09533\n",
      "\n",
      "epoch-354 loss : 0.09541\n",
      "\n",
      "epoch-355 loss : 0.09538\n",
      "\n",
      "epoch-356 loss : 0.09541\n",
      "\n",
      "epoch-357 loss : 0.09532\n",
      "\n",
      "epoch-358 loss : 0.09530\n",
      "\n",
      "epoch-359 loss : 0.09538\n",
      "\n",
      "epoch-360 loss : 0.09527\n",
      "\n",
      "epoch-361 loss : 0.09546\n",
      "\n",
      "epoch-362 loss : 0.09547\n",
      "\n",
      "epoch-363 loss : 0.09538\n",
      "\n",
      "epoch-364 loss : 0.09531\n",
      "\n",
      "epoch-365 loss : 0.09525\n",
      "\n",
      "epoch-366 loss : 0.09527\n",
      "\n",
      "epoch-367 loss : 0.09553\n",
      "\n",
      "epoch-368 loss : 0.09546\n",
      "\n",
      "epoch-369 loss : 0.09547\n",
      "\n",
      "epoch-370 loss : 0.09522\n",
      "\n",
      "epoch-371 loss : 0.09534\n",
      "\n",
      "epoch-372 loss : 0.09537\n",
      "\n",
      "epoch-373 loss : 0.09536\n",
      "\n",
      "epoch-374 loss : 0.09523\n",
      "\n",
      "epoch-375 loss : 0.09536\n",
      "\n",
      "epoch-376 loss : 0.09546\n",
      "\n",
      "epoch-377 loss : 0.09554\n",
      "\n",
      "epoch-378 loss : 0.09524\n",
      "\n",
      "epoch-379 loss : 0.09510\n",
      "\n",
      "epoch-380 loss : 0.09539\n",
      "\n",
      "epoch-381 loss : 0.09558\n",
      "\n",
      "epoch-382 loss : 0.09523\n",
      "\n",
      "epoch-383 loss : 0.09524\n",
      "\n",
      "epoch-384 loss : 0.09531\n",
      "\n",
      "epoch-385 loss : 0.09528\n",
      "\n",
      "epoch-386 loss : 0.09529\n",
      "\n",
      "epoch-387 loss : 0.09514\n",
      "\n",
      "epoch-388 loss : 0.09524\n",
      "\n",
      "epoch-389 loss : 0.09520\n",
      "\n",
      "epoch-390 loss : 0.09509\n",
      "\n",
      "epoch-391 loss : 0.09521\n",
      "\n",
      "epoch-392 loss : 0.09506\n",
      "\n",
      "epoch-393 loss : 0.09524\n",
      "\n",
      "epoch-394 loss : 0.09543\n",
      "\n",
      "epoch-395 loss : 0.09519\n",
      "\n",
      "epoch-396 loss : 0.09539\n",
      "\n",
      "epoch-397 loss : 0.09522\n",
      "\n",
      "epoch-398 loss : 0.09506\n",
      "\n",
      "epoch-399 loss : 0.09518\n",
      "\n",
      "epoch-400 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.76122083 0.89661319 0.87465181 0.78212291 0.4990099  0.59821429\n",
      " 0.55489614 0.5053286  0.69277721 0.53816794 0.40018657 0.50098619\n",
      " 0.91493776 0.80696798 0.62692308 0.63860933]\n",
      "mean_cluster_accuracy_during_training_cycle : 67.53%, post_traincycle_acc : 66.20%, total_acc : 67.12%\n",
      "\n",
      "epoch-400 loss : 0.09510\n",
      "\n",
      "epoch-401 loss : 0.09511\n",
      "\n",
      "epoch-402 loss : 0.09511\n",
      "\n",
      "epoch-403 loss : 0.09513\n",
      "\n",
      "epoch-404 loss : 0.09507\n",
      "\n",
      "epoch-405 loss : 0.09528\n",
      "\n",
      "epoch-406 loss : 0.09503\n",
      "\n",
      "epoch-407 loss : 0.09517\n",
      "\n",
      "epoch-408 loss : 0.09508\n",
      "\n",
      "epoch-409 loss : 0.09508\n",
      "\n",
      "epoch-410 loss : 0.09523\n",
      "\n",
      "epoch-411 loss : 0.09516\n",
      "\n",
      "epoch-412 loss : 0.09526\n",
      "\n",
      "epoch-413 loss : 0.09497\n",
      "\n",
      "epoch-414 loss : 0.09513\n",
      "\n",
      "epoch-415 loss : 0.09492\n",
      "\n",
      "epoch-416 loss : 0.09507\n",
      "\n",
      "epoch-417 loss : 0.09516\n",
      "\n",
      "epoch-418 loss : 0.09499\n",
      "\n",
      "epoch-419 loss : 0.09495\n",
      "\n",
      "epoch-420 loss : 0.09492\n",
      "\n",
      "epoch-421 loss : 0.09514\n",
      "\n",
      "epoch-422 loss : 0.09499\n",
      "\n",
      "epoch-423 loss : 0.09500\n",
      "\n",
      "epoch-424 loss : 0.09503\n",
      "\n",
      "epoch-425 loss : 0.09500\n",
      "\n",
      "epoch-426 loss : 0.09488\n",
      "\n",
      "epoch-427 loss : 0.09506\n",
      "\n",
      "epoch-428 loss : 0.09506\n",
      "\n",
      "epoch-429 loss : 0.09517\n",
      "\n",
      "epoch-430 loss : 0.09516\n",
      "\n",
      "epoch-431 loss : 0.09503\n",
      "\n",
      "epoch-432 loss : 0.09511\n",
      "\n",
      "epoch-433 loss : 0.09501\n",
      "\n",
      "epoch-434 loss : 0.09503\n",
      "\n",
      "epoch-435 loss : 0.09494\n",
      "\n",
      "epoch-436 loss : 0.09512\n",
      "\n",
      "epoch-437 loss : 0.09494\n",
      "\n",
      "epoch-438 loss : 0.09492\n",
      "\n",
      "epoch-439 loss : 0.09500\n",
      "\n",
      "epoch-440 loss : 0.09500\n",
      "\n",
      "epoch-441 loss : 0.09494\n",
      "\n",
      "epoch-442 loss : 0.09497\n",
      "\n",
      "epoch-443 loss : 0.09509\n",
      "\n",
      "epoch-444 loss : 0.09489\n",
      "\n",
      "epoch-445 loss : 0.09496\n",
      "\n",
      "epoch-446 loss : 0.09483\n",
      "\n",
      "epoch-447 loss : 0.09478\n",
      "\n",
      "epoch-448 loss : 0.09501\n",
      "\n",
      "epoch-449 loss : 0.09480\n",
      "\n",
      "epoch-450 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.75314183 0.89750446 0.89786444 0.83333333 0.63564356 0.56964286\n",
      " 0.57566766 0.5        0.63987792 0.38358779 0.39925373 0.50887574\n",
      " 0.91493776 0.80696798 0.62596154 0.63586459]\n",
      "mean_cluster_accuracy_during_training_cycle : 66.92%, post_traincycle_acc : 66.11%, total_acc : 66.67%\n",
      "\n",
      "epoch-450 loss : 0.09500\n",
      "\n",
      "epoch-451 loss : 0.09478\n",
      "\n",
      "epoch-452 loss : 0.09481\n",
      "\n",
      "epoch-453 loss : 0.09497\n",
      "\n",
      "epoch-454 loss : 0.09477\n",
      "\n",
      "epoch-455 loss : 0.09481\n",
      "\n",
      "epoch-456 loss : 0.09478\n",
      "\n",
      "epoch-457 loss : 0.09485\n",
      "\n",
      "epoch-458 loss : 0.09479\n",
      "\n",
      "epoch-459 loss : 0.09488\n",
      "\n",
      "epoch-460 loss : 0.09497\n",
      "\n",
      "epoch-461 loss : 0.09484\n",
      "\n",
      "epoch-462 loss : 0.09475\n",
      "\n",
      "epoch-463 loss : 0.09483\n",
      "\n",
      "epoch-464 loss : 0.09483\n",
      "\n",
      "epoch-465 loss : 0.09458\n",
      "\n",
      "epoch-466 loss : 0.09459\n",
      "\n",
      "epoch-467 loss : 0.09466\n",
      "\n",
      "epoch-468 loss : 0.09461\n",
      "\n",
      "epoch-469 loss : 0.09478\n",
      "\n",
      "epoch-470 loss : 0.09471\n",
      "\n",
      "epoch-471 loss : 0.09480\n",
      "\n",
      "epoch-472 loss : 0.09496\n",
      "\n",
      "epoch-473 loss : 0.09467\n",
      "\n",
      "epoch-474 loss : 0.09460\n",
      "\n",
      "epoch-475 loss : 0.09475\n",
      "\n",
      "epoch-476 loss : 0.09470\n",
      "\n",
      "epoch-477 loss : 0.09464\n",
      "\n",
      "epoch-478 loss : 0.09474\n",
      "\n",
      "epoch-479 loss : 0.09488\n",
      "\n",
      "epoch-480 loss : 0.09466\n",
      "\n",
      "epoch-481 loss : 0.09461\n",
      "\n",
      "epoch-482 loss : 0.09478\n",
      "\n",
      "epoch-483 loss : 0.09459\n",
      "\n",
      "epoch-484 loss : 0.09453\n",
      "\n",
      "epoch-485 loss : 0.09458\n",
      "\n",
      "epoch-486 loss : 0.09463\n",
      "\n",
      "epoch-487 loss : 0.09465\n",
      "\n",
      "epoch-488 loss : 0.09473\n",
      "\n",
      "epoch-489 loss : 0.09440\n",
      "\n",
      "epoch-490 loss : 0.09457\n",
      "\n",
      "epoch-491 loss : 0.09464\n",
      "\n",
      "epoch-492 loss : 0.09479\n",
      "\n",
      "epoch-493 loss : 0.09449\n",
      "\n",
      "epoch-494 loss : 0.09453\n",
      "\n",
      "epoch-495 loss : 0.09467\n",
      "\n",
      "epoch-496 loss : 0.09469\n",
      "\n",
      "epoch-497 loss : 0.09452\n",
      "\n",
      "epoch-498 loss : 0.09476\n",
      "\n",
      "epoch-499 loss : 0.09476\n",
      "\n",
      "epoch-500 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.75852783 0.89304813 0.8727948  0.83147114 0.63366337 0.60714286\n",
      " 0.57665678 0.5008881  0.64598169 0.53816794 0.39925373 0.5\n",
      " 0.91493776 0.80414313 0.62211538 0.63586459]\n",
      "mean_cluster_accuracy_during_training_cycle : 67.85%, post_traincycle_acc : 67.09%, total_acc : 67.61%\n",
      "\n",
      "epoch-500 loss : 0.09456\n",
      "\n",
      "epoch-501 loss : 0.09450\n",
      "\n",
      "epoch-502 loss : 0.09474\n",
      "\n",
      "epoch-503 loss : 0.09480\n",
      "\n",
      "epoch-504 loss : 0.09472\n",
      "\n",
      "epoch-505 loss : 0.09471\n",
      "\n",
      "epoch-506 loss : 0.09470\n",
      "\n",
      "epoch-507 loss : 0.09468\n",
      "\n",
      "epoch-508 loss : 0.09459\n",
      "\n",
      "epoch-509 loss : 0.09495\n",
      "\n",
      "epoch-510 loss : 0.09447\n",
      "\n",
      "epoch-511 loss : 0.09452\n",
      "\n",
      "epoch-512 loss : 0.09453\n",
      "\n",
      "epoch-513 loss : 0.09455\n",
      "\n",
      "epoch-514 loss : 0.09465\n",
      "\n",
      "epoch-515 loss : 0.09461\n",
      "\n",
      "epoch-516 loss : 0.09460\n",
      "\n",
      "epoch-517 loss : 0.09446\n",
      "\n",
      "epoch-518 loss : 0.09459\n",
      "\n",
      "epoch-519 loss : 0.09471\n",
      "\n",
      "epoch-520 loss : 0.09440\n",
      "\n",
      "epoch-521 loss : 0.09471\n",
      "\n",
      "epoch-522 loss : 0.09459\n",
      "\n",
      "epoch-523 loss : 0.09462\n",
      "\n",
      "epoch-524 loss : 0.09442\n",
      "\n",
      "epoch-525 loss : 0.09447\n",
      "\n",
      "epoch-526 loss : 0.09462\n",
      "\n",
      "epoch-527 loss : 0.09464\n",
      "\n",
      "epoch-528 loss : 0.09458\n",
      "\n",
      "epoch-529 loss : 0.09471\n",
      "\n",
      "epoch-530 loss : 0.09482\n",
      "\n",
      "epoch-531 loss : 0.09436\n",
      "\n",
      "epoch-532 loss : 0.09457\n",
      "\n",
      "epoch-533 loss : 0.09447\n",
      "\n",
      "epoch-534 loss : 0.09461\n",
      "\n",
      "epoch-535 loss : 0.09458\n",
      "\n",
      "epoch-536 loss : 0.09453\n",
      "\n",
      "epoch-537 loss : 0.09446\n",
      "\n",
      "epoch-538 loss : 0.09457\n",
      "\n",
      "epoch-539 loss : 0.09457\n",
      "\n",
      "epoch-540 loss : 0.09471\n",
      "\n",
      "epoch-541 loss : 0.09456\n",
      "\n",
      "epoch-542 loss : 0.09452\n",
      "\n",
      "epoch-543 loss : 0.09447\n",
      "\n",
      "epoch-544 loss : 0.09446\n",
      "\n",
      "epoch-545 loss : 0.09451\n",
      "\n",
      "epoch-546 loss : 0.09432\n",
      "\n",
      "epoch-547 loss : 0.09453\n",
      "\n",
      "epoch-548 loss : 0.09439\n",
      "\n",
      "epoch-549 loss : 0.09449\n",
      "\n",
      "epoch-550 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.7540395  0.90285205 0.87093779 0.83147114 0.6        0.60803571\n",
      " 0.60534125 0.5053286  0.6948118  0.60591603 0.39925373 0.49704142\n",
      " 0.91493776 0.79943503 0.61923077 0.63129003]\n",
      "mean_cluster_accuracy_during_training_cycle : 67.65%, post_traincycle_acc : 67.75%, total_acc : 67.68%\n",
      "\n",
      "epoch-550 loss : 0.09446\n",
      "\n",
      "epoch-551 loss : 0.09439\n",
      "\n",
      "epoch-552 loss : 0.09458\n",
      "\n",
      "epoch-553 loss : 0.09421\n",
      "\n",
      "epoch-554 loss : 0.09434\n",
      "\n",
      "epoch-555 loss : 0.09449\n",
      "\n",
      "epoch-556 loss : 0.09449\n",
      "\n",
      "epoch-557 loss : 0.09440\n",
      "\n",
      "epoch-558 loss : 0.09446\n",
      "\n",
      "epoch-559 loss : 0.09441\n",
      "\n",
      "epoch-560 loss : 0.09426\n",
      "\n",
      "epoch-561 loss : 0.09441\n",
      "\n",
      "epoch-562 loss : 0.09432\n",
      "\n",
      "epoch-563 loss : 0.09448\n",
      "\n",
      "epoch-564 loss : 0.09451\n",
      "\n",
      "epoch-565 loss : 0.09427\n",
      "\n",
      "epoch-566 loss : 0.09425\n",
      "\n",
      "epoch-567 loss : 0.09432\n",
      "\n",
      "epoch-568 loss : 0.09434\n",
      "\n",
      "epoch-569 loss : 0.09453\n",
      "\n",
      "epoch-570 loss : 0.09425\n",
      "\n",
      "epoch-571 loss : 0.09433\n",
      "\n",
      "epoch-572 loss : 0.09435\n",
      "\n",
      "epoch-573 loss : 0.09428\n",
      "\n",
      "epoch-574 loss : 0.09428\n",
      "\n",
      "epoch-575 loss : 0.09445\n",
      "\n",
      "epoch-576 loss : 0.09449\n",
      "\n",
      "epoch-577 loss : 0.09433\n",
      "\n",
      "epoch-578 loss : 0.09427\n",
      "\n",
      "epoch-579 loss : 0.09430\n",
      "\n",
      "epoch-580 loss : 0.09438\n",
      "\n",
      "epoch-581 loss : 0.09420\n",
      "\n",
      "epoch-582 loss : 0.09427\n",
      "\n",
      "epoch-583 loss : 0.09432\n",
      "\n",
      "epoch-584 loss : 0.09429\n",
      "\n",
      "epoch-585 loss : 0.09425\n",
      "\n",
      "epoch-586 loss : 0.09432\n",
      "\n",
      "epoch-587 loss : 0.09422\n",
      "\n",
      "epoch-588 loss : 0.09427\n",
      "\n",
      "epoch-589 loss : 0.09425\n",
      "\n",
      "epoch-590 loss : 0.09438\n",
      "\n",
      "epoch-591 loss : 0.09421\n",
      "\n",
      "epoch-592 loss : 0.09435\n",
      "\n",
      "epoch-593 loss : 0.09409\n",
      "\n",
      "epoch-594 loss : 0.09415\n",
      "\n",
      "epoch-595 loss : 0.09402\n",
      "\n",
      "epoch-596 loss : 0.09409\n",
      "\n",
      "epoch-597 loss : 0.09423\n",
      "\n",
      "epoch-598 loss : 0.09432\n",
      "\n",
      "epoch-599 loss : 0.09421\n",
      "\n",
      "epoch-600 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.78096948 0.90285205 0.85979573 0.83519553 0.62772277 0.57767857\n",
      " 0.57467854 0.5044405  0.69277721 0.38263359 0.52145522 0.49605523\n",
      " 0.9159751  0.80225989 0.62211538 0.63677951]\n",
      "mean_cluster_accuracy_during_training_cycle : 67.40%, post_traincycle_acc : 67.08%, total_acc : 67.30%\n",
      "\n",
      "epoch-600 loss : 0.09415\n",
      "\n",
      "epoch-601 loss : 0.09408\n",
      "\n",
      "epoch-602 loss : 0.09425\n",
      "\n",
      "epoch-603 loss : 0.09408\n",
      "\n",
      "epoch-604 loss : 0.09422\n",
      "\n",
      "epoch-605 loss : 0.09435\n",
      "\n",
      "epoch-606 loss : 0.09393\n",
      "\n",
      "epoch-607 loss : 0.09420\n",
      "\n",
      "epoch-608 loss : 0.09423\n",
      "\n",
      "epoch-609 loss : 0.09416\n",
      "\n",
      "epoch-610 loss : 0.09424\n",
      "\n",
      "epoch-611 loss : 0.09414\n",
      "\n",
      "epoch-612 loss : 0.09412\n",
      "\n",
      "epoch-613 loss : 0.09399\n",
      "\n",
      "epoch-614 loss : 0.09411\n",
      "\n",
      "epoch-615 loss : 0.09419\n",
      "\n",
      "epoch-616 loss : 0.09420\n",
      "\n",
      "epoch-617 loss : 0.09406\n",
      "\n",
      "epoch-618 loss : 0.09400\n",
      "\n",
      "epoch-619 loss : 0.09429\n",
      "\n",
      "epoch-620 loss : 0.09410\n",
      "\n",
      "epoch-621 loss : 0.09405\n",
      "\n",
      "epoch-622 loss : 0.09413\n",
      "\n",
      "epoch-623 loss : 0.09414\n",
      "\n",
      "epoch-624 loss : 0.09441\n",
      "\n",
      "epoch-625 loss : 0.09424\n",
      "\n",
      "epoch-626 loss : 0.09434\n",
      "\n",
      "epoch-627 loss : 0.09421\n",
      "\n",
      "epoch-628 loss : 0.09422\n",
      "\n",
      "epoch-629 loss : 0.09415\n",
      "\n",
      "epoch-630 loss : 0.09408\n",
      "\n",
      "epoch-631 loss : 0.09403\n",
      "\n",
      "epoch-632 loss : 0.09410\n",
      "\n",
      "epoch-633 loss : 0.09433\n",
      "\n",
      "epoch-634 loss : 0.09416\n",
      "\n",
      "epoch-635 loss : 0.09426\n",
      "\n",
      "epoch-636 loss : 0.09413\n",
      "\n",
      "epoch-637 loss : 0.09417\n",
      "\n",
      "epoch-638 loss : 0.09424\n",
      "\n",
      "epoch-639 loss : 0.09406\n",
      "\n",
      "epoch-640 loss : 0.09435\n",
      "\n",
      "epoch-641 loss : 0.09423\n",
      "\n",
      "epoch-642 loss : 0.09422\n",
      "\n",
      "epoch-643 loss : 0.09418\n",
      "\n",
      "epoch-644 loss : 0.09413\n",
      "\n",
      "epoch-645 loss : 0.09411\n",
      "\n",
      "epoch-646 loss : 0.09424\n",
      "\n",
      "epoch-647 loss : 0.09418\n",
      "\n",
      "epoch-648 loss : 0.09431\n",
      "\n",
      "epoch-649 loss : 0.09437\n",
      "\n",
      "epoch-650 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.81418312 0.90196078 0.87000929 0.80726257 0.62574257 0.6\n",
      " 0.60731949 0.5044405  0.68972533 0.58396947 0.3983209  0.49309665\n",
      " 0.89522822 0.80131827 0.62692308 0.63129003]\n",
      "mean_cluster_accuracy_during_training_cycle : 67.31%, post_traincycle_acc : 67.82%, total_acc : 67.47%\n",
      "\n",
      "epoch-650 loss : 0.09420\n",
      "\n",
      "epoch-651 loss : 0.09442\n",
      "\n",
      "epoch-652 loss : 0.09440\n",
      "\n",
      "epoch-653 loss : 0.09433\n",
      "\n",
      "epoch-654 loss : 0.09434\n",
      "\n",
      "epoch-655 loss : 0.09422\n",
      "\n",
      "epoch-656 loss : 0.09434\n",
      "\n",
      "epoch-657 loss : 0.09431\n",
      "\n",
      "epoch-658 loss : 0.09417\n",
      "\n",
      "epoch-659 loss : 0.09429\n",
      "\n",
      "epoch-660 loss : 0.09425\n",
      "\n",
      "epoch-661 loss : 0.09429\n",
      "\n",
      "epoch-662 loss : 0.09423\n",
      "\n",
      "epoch-663 loss : 0.09420\n",
      "\n",
      "epoch-664 loss : 0.09427\n",
      "\n",
      "epoch-665 loss : 0.09420\n",
      "\n",
      "epoch-666 loss : 0.09411\n",
      "\n",
      "epoch-667 loss : 0.09413\n",
      "\n",
      "epoch-668 loss : 0.09410\n",
      "\n",
      "epoch-669 loss : 0.09433\n",
      "\n",
      "epoch-670 loss : 0.09426\n",
      "\n",
      "epoch-671 loss : 0.09410\n",
      "\n",
      "epoch-672 loss : 0.09418\n",
      "\n",
      "epoch-673 loss : 0.09430\n",
      "\n",
      "epoch-674 loss : 0.09409\n",
      "\n",
      "epoch-675 loss : 0.09415\n",
      "\n",
      "epoch-676 loss : 0.09429\n",
      "\n",
      "epoch-677 loss : 0.09413\n",
      "\n",
      "epoch-678 loss : 0.09433\n",
      "\n",
      "epoch-679 loss : 0.09411\n",
      "\n",
      "epoch-680 loss : 0.09419\n",
      "\n",
      "epoch-681 loss : 0.09424\n",
      "\n",
      "epoch-682 loss : 0.09422\n",
      "\n",
      "epoch-683 loss : 0.09415\n",
      "\n",
      "epoch-684 loss : 0.09416\n",
      "\n",
      "epoch-685 loss : 0.09414\n",
      "\n",
      "epoch-686 loss : 0.09419\n",
      "\n",
      "epoch-687 loss : 0.09402\n",
      "\n",
      "epoch-688 loss : 0.09411\n",
      "\n",
      "epoch-689 loss : 0.09415\n",
      "\n",
      "epoch-690 loss : 0.09409\n",
      "\n",
      "epoch-691 loss : 0.09401\n",
      "\n",
      "epoch-692 loss : 0.09409\n",
      "\n",
      "epoch-693 loss : 0.09398\n",
      "\n",
      "epoch-694 loss : 0.09403\n",
      "\n",
      "epoch-695 loss : 0.09398\n",
      "\n",
      "epoch-696 loss : 0.09397\n",
      "\n",
      "epoch-697 loss : 0.09395\n",
      "\n",
      "epoch-698 loss : 0.09389\n",
      "\n",
      "epoch-699 loss : 0.09410\n",
      "\n",
      "epoch-700 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.81597846 0.90285205 0.87093779 0.83519553 0.62871287 0.53482143\n",
      " 0.58259149 0.5035524  0.69175992 0.58301527 0.52145522 0.49901381\n",
      " 0.89211618 0.79943503 0.60576923 0.63677951]\n",
      "mean_cluster_accuracy_during_training_cycle : 67.71%, post_traincycle_acc : 68.15%, total_acc : 67.85%\n",
      "save model\n",
      "\n",
      "epoch-700 loss : 0.09416\n",
      "\n",
      "epoch-701 loss : 0.09411\n",
      "\n",
      "epoch-702 loss : 0.09411\n",
      "\n",
      "epoch-703 loss : 0.09399\n",
      "\n",
      "epoch-704 loss : 0.09405\n",
      "\n",
      "epoch-705 loss : 0.09404\n",
      "\n",
      "epoch-706 loss : 0.09394\n",
      "\n",
      "epoch-707 loss : 0.09412\n",
      "\n",
      "epoch-708 loss : 0.09393\n",
      "\n",
      "epoch-709 loss : 0.09391\n",
      "\n",
      "epoch-710 loss : 0.09386\n",
      "\n",
      "epoch-711 loss : 0.09412\n",
      "\n",
      "epoch-712 loss : 0.09391\n",
      "\n",
      "epoch-713 loss : 0.09394\n",
      "\n",
      "epoch-714 loss : 0.09409\n",
      "\n",
      "epoch-715 loss : 0.09392\n",
      "\n",
      "epoch-716 loss : 0.09397\n",
      "\n",
      "epoch-717 loss : 0.09393\n",
      "\n",
      "epoch-718 loss : 0.09401\n",
      "\n",
      "epoch-719 loss : 0.09389\n",
      "\n",
      "epoch-720 loss : 0.09408\n",
      "\n",
      "epoch-721 loss : 0.09386\n",
      "\n",
      "epoch-722 loss : 0.09389\n",
      "\n",
      "epoch-723 loss : 0.09387\n",
      "\n",
      "epoch-724 loss : 0.09389\n",
      "\n",
      "epoch-725 loss : 0.09372\n",
      "\n",
      "epoch-726 loss : 0.09387\n",
      "\n",
      "epoch-727 loss : 0.09399\n",
      "\n",
      "epoch-728 loss : 0.09401\n",
      "\n",
      "epoch-729 loss : 0.09392\n",
      "\n",
      "epoch-730 loss : 0.09392\n",
      "\n",
      "epoch-731 loss : 0.09388\n",
      "\n",
      "epoch-732 loss : 0.09397\n",
      "\n",
      "epoch-733 loss : 0.09387\n",
      "\n",
      "epoch-734 loss : 0.09383\n",
      "\n",
      "epoch-735 loss : 0.09399\n",
      "\n",
      "epoch-736 loss : 0.09384\n",
      "\n",
      "epoch-737 loss : 0.09381\n",
      "\n",
      "epoch-738 loss : 0.09396\n",
      "\n",
      "epoch-739 loss : 0.09380\n",
      "\n",
      "epoch-740 loss : 0.09385\n",
      "\n",
      "epoch-741 loss : 0.09385\n",
      "\n",
      "epoch-742 loss : 0.09404\n",
      "\n",
      "epoch-743 loss : 0.09386\n",
      "\n",
      "epoch-744 loss : 0.09387\n",
      "\n",
      "epoch-745 loss : 0.09377\n",
      "\n",
      "epoch-746 loss : 0.09371\n",
      "\n",
      "epoch-747 loss : 0.09374\n",
      "\n",
      "epoch-748 loss : 0.09383\n",
      "\n",
      "epoch-749 loss : 0.09371\n",
      "\n",
      "epoch-750 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.78007181 0.90196078 0.87093779 0.80726257 0.62871287 0.59375\n",
      " 0.58061325 0.5062167  0.64089522 0.57347328 0.52238806 0.50295858\n",
      " 0.89419087 0.80037665 0.62307692 0.63494968]\n",
      "mean_cluster_accuracy_during_training_cycle : 67.13%, post_traincycle_acc : 67.89%, total_acc : 67.36%\n",
      "\n",
      "epoch-750 loss : 0.09381\n",
      "\n",
      "epoch-751 loss : 0.09384\n",
      "\n",
      "epoch-752 loss : 0.09396\n",
      "\n",
      "epoch-753 loss : 0.09372\n",
      "\n",
      "epoch-754 loss : 0.09364\n",
      "\n",
      "epoch-755 loss : 0.09373\n",
      "\n",
      "epoch-756 loss : 0.09361\n",
      "\n",
      "epoch-757 loss : 0.09376\n",
      "\n",
      "epoch-758 loss : 0.09366\n",
      "\n",
      "epoch-759 loss : 0.09358\n",
      "\n",
      "epoch-760 loss : 0.09361\n",
      "\n",
      "epoch-761 loss : 0.09371\n",
      "\n",
      "epoch-762 loss : 0.09373\n",
      "\n",
      "epoch-763 loss : 0.09360\n",
      "\n",
      "epoch-764 loss : 0.09385\n",
      "\n",
      "epoch-765 loss : 0.09358\n",
      "\n",
      "epoch-766 loss : 0.09374\n",
      "\n",
      "epoch-767 loss : 0.09366\n",
      "\n",
      "epoch-768 loss : 0.09365\n",
      "\n",
      "epoch-769 loss : 0.09343\n",
      "\n",
      "epoch-770 loss : 0.09350\n",
      "\n",
      "epoch-771 loss : 0.09352\n",
      "\n",
      "epoch-772 loss : 0.09365\n",
      "\n",
      "epoch-773 loss : 0.09371\n",
      "\n",
      "epoch-774 loss : 0.09368\n",
      "\n",
      "epoch-775 loss : 0.09369\n",
      "\n",
      "epoch-776 loss : 0.09353\n",
      "\n",
      "epoch-777 loss : 0.09365\n",
      "\n",
      "epoch-778 loss : 0.09356\n",
      "\n",
      "epoch-779 loss : 0.09359\n",
      "\n",
      "epoch-780 loss : 0.09361\n",
      "\n",
      "epoch-781 loss : 0.09355\n",
      "\n",
      "epoch-782 loss : 0.09349\n",
      "\n",
      "epoch-783 loss : 0.09358\n",
      "\n",
      "epoch-784 loss : 0.09358\n",
      "\n",
      "epoch-785 loss : 0.09369\n",
      "\n",
      "epoch-786 loss : 0.09360\n",
      "\n",
      "epoch-787 loss : 0.09357\n",
      "\n",
      "epoch-788 loss : 0.09361\n",
      "\n",
      "epoch-789 loss : 0.09350\n",
      "\n",
      "epoch-790 loss : 0.09361\n",
      "\n",
      "epoch-791 loss : 0.09367\n",
      "\n",
      "epoch-792 loss : 0.09358\n",
      "\n",
      "epoch-793 loss : 0.09349\n",
      "\n",
      "epoch-794 loss : 0.09341\n",
      "\n",
      "epoch-795 loss : 0.09336\n",
      "\n",
      "epoch-796 loss : 0.09348\n",
      "\n",
      "epoch-797 loss : 0.09353\n",
      "\n",
      "epoch-798 loss : 0.09344\n",
      "\n",
      "epoch-799 loss : 0.09347\n",
      "\n",
      "epoch-800 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.81597846 0.90196078 0.87093779 0.80633147 0.63366337 0.58928571\n",
      " 0.60633037 0.5079929  0.68362157 0.57633588 0.3983209  0.5\n",
      " 0.89419087 0.79001883 0.60576923 0.63311985]\n",
      "mean_cluster_accuracy_during_training_cycle : 66.99%, post_traincycle_acc : 67.59%, total_acc : 67.18%\n",
      "\n",
      "epoch-800 loss : 0.09351\n",
      "\n",
      "epoch-801 loss : 0.09360\n",
      "\n",
      "epoch-802 loss : 0.09340\n",
      "\n",
      "epoch-803 loss : 0.09340\n",
      "\n",
      "epoch-804 loss : 0.09351\n",
      "\n",
      "epoch-805 loss : 0.09354\n",
      "\n",
      "epoch-806 loss : 0.09338\n",
      "\n",
      "epoch-807 loss : 0.09340\n",
      "\n",
      "epoch-808 loss : 0.09337\n",
      "\n",
      "epoch-809 loss : 0.09350\n",
      "\n",
      "epoch-810 loss : 0.09324\n",
      "\n",
      "epoch-811 loss : 0.09316\n",
      "\n",
      "epoch-812 loss : 0.09345\n",
      "\n",
      "epoch-813 loss : 0.09333\n",
      "\n",
      "epoch-814 loss : 0.09333\n",
      "\n",
      "epoch-815 loss : 0.09316\n",
      "\n",
      "epoch-816 loss : 0.09337\n",
      "\n",
      "epoch-817 loss : 0.09343\n",
      "\n",
      "epoch-818 loss : 0.09327\n",
      "\n",
      "epoch-819 loss : 0.09317\n",
      "\n",
      "epoch-820 loss : 0.09342\n",
      "\n",
      "epoch-821 loss : 0.09321\n",
      "\n",
      "epoch-822 loss : 0.09334\n",
      "\n",
      "epoch-823 loss : 0.09340\n",
      "\n",
      "epoch-824 loss : 0.09327\n",
      "\n",
      "epoch-825 loss : 0.09338\n",
      "\n",
      "epoch-826 loss : 0.09338\n",
      "\n",
      "epoch-827 loss : 0.09337\n",
      "\n",
      "epoch-828 loss : 0.09329\n",
      "\n",
      "epoch-829 loss : 0.09336\n",
      "\n",
      "epoch-830 loss : 0.09326\n",
      "\n",
      "epoch-831 loss : 0.09325\n",
      "\n",
      "epoch-832 loss : 0.09328\n",
      "\n",
      "epoch-833 loss : 0.09328\n",
      "\n",
      "epoch-834 loss : 0.09346\n",
      "\n",
      "epoch-835 loss : 0.09336\n",
      "\n",
      "epoch-836 loss : 0.09338\n",
      "\n",
      "epoch-837 loss : 0.09334\n",
      "\n",
      "epoch-838 loss : 0.09325\n",
      "\n",
      "epoch-839 loss : 0.09321\n",
      "\n",
      "epoch-840 loss : 0.09335\n",
      "\n",
      "epoch-841 loss : 0.09319\n",
      "\n",
      "epoch-842 loss : 0.09314\n",
      "\n",
      "epoch-843 loss : 0.09320\n",
      "\n",
      "epoch-844 loss : 0.09342\n",
      "\n",
      "epoch-845 loss : 0.09306\n",
      "\n",
      "epoch-846 loss : 0.09324\n",
      "\n",
      "epoch-847 loss : 0.09328\n",
      "\n",
      "epoch-848 loss : 0.09323\n",
      "\n",
      "epoch-849 loss : 0.09329\n",
      "\n",
      "epoch-850 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.78635548 0.8885918  0.87093779 0.80353818 0.64059406 0.59285714\n",
      " 0.60929773 0.50976909 0.63275687 0.5639313  0.5261194  0.49605523\n",
      " 0.89522822 0.78813559 0.60096154 0.61756633]\n",
      "mean_cluster_accuracy_during_training_cycle : 66.96%, post_traincycle_acc : 67.64%, total_acc : 67.18%\n",
      "\n",
      "epoch-850 loss : 0.09328\n",
      "\n",
      "epoch-851 loss : 0.09328\n",
      "\n",
      "epoch-852 loss : 0.09327\n",
      "\n",
      "epoch-853 loss : 0.09315\n",
      "\n",
      "epoch-854 loss : 0.09329\n",
      "\n",
      "epoch-855 loss : 0.09332\n",
      "\n",
      "epoch-856 loss : 0.09332\n",
      "\n",
      "epoch-857 loss : 0.09331\n",
      "\n",
      "epoch-858 loss : 0.09321\n",
      "\n",
      "epoch-859 loss : 0.09331\n",
      "\n",
      "epoch-860 loss : 0.09320\n",
      "\n",
      "epoch-861 loss : 0.09315\n",
      "\n",
      "epoch-862 loss : 0.09326\n",
      "\n",
      "epoch-863 loss : 0.09327\n",
      "\n",
      "epoch-864 loss : 0.09338\n",
      "\n",
      "epoch-865 loss : 0.09327\n",
      "\n",
      "epoch-866 loss : 0.09334\n",
      "\n",
      "epoch-867 loss : 0.09313\n",
      "\n",
      "epoch-868 loss : 0.09308\n",
      "\n",
      "epoch-869 loss : 0.09321\n",
      "\n",
      "epoch-870 loss : 0.09313\n",
      "\n",
      "epoch-871 loss : 0.09326\n",
      "\n",
      "epoch-872 loss : 0.09313\n",
      "\n",
      "epoch-873 loss : 0.09304\n",
      "\n",
      "epoch-874 loss : 0.09312\n",
      "\n",
      "epoch-875 loss : 0.09317\n",
      "\n",
      "epoch-876 loss : 0.09311\n",
      "\n",
      "epoch-877 loss : 0.09327\n",
      "\n",
      "epoch-878 loss : 0.09307\n",
      "\n",
      "epoch-879 loss : 0.09315\n",
      "\n",
      "epoch-880 loss : 0.09308\n",
      "\n",
      "epoch-881 loss : 0.09307\n",
      "\n",
      "epoch-882 loss : 0.09316\n",
      "\n",
      "epoch-883 loss : 0.09312\n",
      "\n",
      "epoch-884 loss : 0.09333\n",
      "\n",
      "epoch-885 loss : 0.09302\n",
      "\n",
      "epoch-886 loss : 0.09319\n",
      "\n",
      "epoch-887 loss : 0.09305\n",
      "\n",
      "epoch-888 loss : 0.09314\n",
      "\n",
      "epoch-889 loss : 0.09311\n",
      "\n",
      "epoch-890 loss : 0.09308\n",
      "\n",
      "epoch-891 loss : 0.09320\n",
      "\n",
      "epoch-892 loss : 0.09312\n",
      "\n",
      "epoch-893 loss : 0.09311\n",
      "\n",
      "epoch-894 loss : 0.09318\n",
      "\n",
      "epoch-895 loss : 0.09312\n",
      "\n",
      "epoch-896 loss : 0.09325\n",
      "\n",
      "epoch-897 loss : 0.09325\n",
      "\n",
      "epoch-898 loss : 0.09327\n",
      "\n",
      "epoch-899 loss : 0.09307\n",
      "\n",
      "epoch-900 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.78725314 0.90196078 0.8718663  0.80353818 0.64554455 0.58928571\n",
      " 0.60830861 0.52220249 0.62563581 0.5629771  0.39738806 0.49408284\n",
      " 0.90248963 0.79001883 0.60865385 0.60658737]\n",
      "mean_cluster_accuracy_during_training_cycle : 66.76%, post_traincycle_acc : 66.99%, total_acc : 66.83%\n",
      "\n",
      "epoch-900 loss : 0.09311\n",
      "\n",
      "epoch-901 loss : 0.09314\n",
      "\n",
      "epoch-902 loss : 0.09321\n",
      "\n",
      "epoch-903 loss : 0.09324\n",
      "\n",
      "epoch-904 loss : 0.09318\n",
      "\n",
      "epoch-905 loss : 0.09316\n",
      "\n",
      "epoch-906 loss : 0.09316\n",
      "\n",
      "epoch-907 loss : 0.09314\n",
      "\n",
      "epoch-908 loss : 0.09321\n",
      "\n",
      "epoch-909 loss : 0.09305\n",
      "\n",
      "epoch-910 loss : 0.09309\n",
      "\n",
      "epoch-911 loss : 0.09305\n",
      "\n",
      "epoch-912 loss : 0.09310\n",
      "\n",
      "epoch-913 loss : 0.09307\n",
      "\n",
      "epoch-914 loss : 0.09294\n",
      "\n",
      "epoch-915 loss : 0.09297\n",
      "\n",
      "epoch-916 loss : 0.09296\n",
      "\n",
      "epoch-917 loss : 0.09308\n",
      "\n",
      "epoch-918 loss : 0.09305\n",
      "\n",
      "epoch-919 loss : 0.09297\n",
      "\n",
      "epoch-920 loss : 0.09295\n",
      "\n",
      "epoch-921 loss : 0.09301\n",
      "\n",
      "epoch-922 loss : 0.09292\n",
      "\n",
      "epoch-923 loss : 0.09298\n",
      "\n",
      "epoch-924 loss : 0.09297\n",
      "\n",
      "epoch-925 loss : 0.09293\n",
      "\n",
      "epoch-926 loss : 0.09288\n",
      "\n",
      "epoch-927 loss : 0.09282\n",
      "\n",
      "epoch-928 loss : 0.09304\n",
      "\n",
      "epoch-929 loss : 0.09289\n",
      "\n",
      "epoch-930 loss : 0.09308\n",
      "\n",
      "epoch-931 loss : 0.09287\n",
      "\n",
      "epoch-932 loss : 0.09305\n",
      "\n",
      "epoch-933 loss : 0.09291\n",
      "\n",
      "epoch-934 loss : 0.09294\n",
      "\n",
      "epoch-935 loss : 0.09282\n",
      "\n",
      "epoch-936 loss : 0.09293\n",
      "\n",
      "epoch-937 loss : 0.09297\n",
      "\n",
      "epoch-938 loss : 0.09304\n",
      "\n",
      "epoch-939 loss : 0.09307\n",
      "\n",
      "epoch-940 loss : 0.09285\n",
      "\n",
      "epoch-941 loss : 0.09290\n",
      "\n",
      "epoch-942 loss : 0.09294\n",
      "\n",
      "epoch-943 loss : 0.09289\n",
      "\n",
      "epoch-944 loss : 0.09279\n",
      "\n",
      "epoch-945 loss : 0.09293\n",
      "\n",
      "epoch-946 loss : 0.09307\n",
      "\n",
      "epoch-947 loss : 0.09300\n",
      "\n",
      "epoch-948 loss : 0.09301\n",
      "\n",
      "epoch-949 loss : 0.09292\n",
      "\n",
      "epoch-950 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.78725314 0.89483066 0.87093779 0.80540037 0.54752475 0.58928571\n",
      " 0.60731949 0.52131439 0.62156663 0.56774809 0.53358209 0.49506903\n",
      " 0.90248963 0.79001883 0.6125     0.6084172 ]\n",
      "mean_cluster_accuracy_during_training_cycle : 66.62%, post_traincycle_acc : 67.22%, total_acc : 66.81%\n",
      "\n",
      "epoch-950 loss : 0.09295\n",
      "\n",
      "epoch-951 loss : 0.09280\n",
      "\n",
      "epoch-952 loss : 0.09274\n",
      "\n",
      "epoch-953 loss : 0.09283\n",
      "\n",
      "epoch-954 loss : 0.09282\n",
      "\n",
      "epoch-955 loss : 0.09298\n",
      "\n",
      "epoch-956 loss : 0.09291\n",
      "\n",
      "epoch-957 loss : 0.09304\n",
      "\n",
      "epoch-958 loss : 0.09293\n",
      "\n",
      "epoch-959 loss : 0.09278\n",
      "\n",
      "epoch-960 loss : 0.09283\n",
      "\n",
      "epoch-961 loss : 0.09288\n",
      "\n",
      "epoch-962 loss : 0.09281\n",
      "\n",
      "epoch-963 loss : 0.09283\n",
      "\n",
      "epoch-964 loss : 0.09290\n",
      "\n",
      "epoch-965 loss : 0.09284\n",
      "\n",
      "epoch-966 loss : 0.09272\n",
      "\n",
      "epoch-967 loss : 0.09295\n",
      "\n",
      "epoch-968 loss : 0.09281\n",
      "\n",
      "epoch-969 loss : 0.09286\n",
      "\n",
      "epoch-970 loss : 0.09278\n",
      "\n",
      "epoch-971 loss : 0.09277\n",
      "\n",
      "epoch-972 loss : 0.09277\n",
      "\n",
      "epoch-973 loss : 0.09281\n",
      "\n",
      "epoch-974 loss : 0.09276\n",
      "\n",
      "epoch-975 loss : 0.09259\n",
      "\n",
      "epoch-976 loss : 0.09277\n",
      "\n",
      "epoch-977 loss : 0.09267\n",
      "\n",
      "epoch-978 loss : 0.09284\n",
      "\n",
      "epoch-979 loss : 0.09287\n",
      "\n",
      "epoch-980 loss : 0.09289\n",
      "\n",
      "epoch-981 loss : 0.09273\n",
      "\n",
      "epoch-982 loss : 0.09271\n",
      "\n",
      "epoch-983 loss : 0.09275\n",
      "\n",
      "epoch-984 loss : 0.09278\n",
      "\n",
      "epoch-985 loss : 0.09279\n",
      "\n",
      "epoch-986 loss : 0.09272\n",
      "\n",
      "epoch-987 loss : 0.09282\n",
      "\n",
      "epoch-988 loss : 0.09272\n",
      "\n",
      "epoch-989 loss : 0.09272\n",
      "\n",
      "epoch-990 loss : 0.09262\n",
      "\n",
      "epoch-991 loss : 0.09272\n",
      "\n",
      "epoch-992 loss : 0.09270\n",
      "\n",
      "epoch-993 loss : 0.09259\n",
      "\n",
      "epoch-994 loss : 0.09277\n",
      "\n",
      "epoch-995 loss : 0.09276\n",
      "\n",
      "epoch-996 loss : 0.09270\n",
      "\n",
      "epoch-997 loss : 0.09274\n",
      "\n",
      "epoch-998 loss : 0.09260\n",
      "\n",
      "epoch-999 loss : 0.09276\n",
      "\n",
      "epoch-1000 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.79084381 0.88948307 0.87093779 0.80633147 0.65049505 0.59196429\n",
      " 0.51533136 0.52309059 0.62360122 0.56774809 0.39738806 0.49605523\n",
      " 0.90248963 0.79190207 0.65961538 0.61207685]\n",
      "mean_cluster_accuracy_during_training_cycle : 67.12%, post_traincycle_acc : 66.81%, total_acc : 67.03%\n",
      "\n",
      "epoch-1000 loss : 0.09266\n",
      "\n",
      "epoch-1001 loss : 0.09280\n",
      "\n",
      "epoch-1002 loss : 0.09277\n",
      "\n",
      "epoch-1003 loss : 0.09261\n",
      "\n",
      "epoch-1004 loss : 0.09275\n",
      "\n",
      "epoch-1005 loss : 0.09271\n",
      "\n",
      "epoch-1006 loss : 0.09264\n",
      "\n",
      "epoch-1007 loss : 0.09274\n",
      "\n",
      "epoch-1008 loss : 0.09274\n",
      "\n",
      "epoch-1009 loss : 0.09261\n",
      "\n",
      "epoch-1010 loss : 0.09269\n",
      "\n",
      "epoch-1011 loss : 0.09266\n",
      "\n",
      "epoch-1012 loss : 0.09281\n",
      "\n",
      "epoch-1013 loss : 0.09268\n",
      "\n",
      "epoch-1014 loss : 0.09256\n",
      "\n",
      "epoch-1015 loss : 0.09253\n",
      "\n",
      "epoch-1016 loss : 0.09268\n",
      "\n",
      "epoch-1017 loss : 0.09268\n",
      "\n",
      "epoch-1018 loss : 0.09256\n",
      "\n",
      "epoch-1019 loss : 0.09263\n",
      "\n",
      "epoch-1020 loss : 0.09260\n",
      "\n",
      "epoch-1021 loss : 0.09265\n",
      "\n",
      "epoch-1022 loss : 0.09245\n",
      "\n",
      "epoch-1023 loss : 0.09256\n",
      "\n",
      "epoch-1024 loss : 0.09250\n",
      "\n",
      "epoch-1025 loss : 0.09269\n",
      "\n",
      "epoch-1026 loss : 0.09267\n",
      "\n",
      "epoch-1027 loss : 0.09264\n",
      "\n",
      "epoch-1028 loss : 0.09265\n",
      "\n",
      "epoch-1029 loss : 0.09271\n",
      "\n",
      "epoch-1030 loss : 0.09276\n",
      "\n",
      "epoch-1031 loss : 0.09257\n",
      "\n",
      "epoch-1032 loss : 0.09250\n",
      "\n",
      "epoch-1033 loss : 0.09259\n",
      "\n",
      "epoch-1034 loss : 0.09252\n",
      "\n",
      "epoch-1035 loss : 0.09269\n",
      "\n",
      "epoch-1036 loss : 0.09251\n",
      "\n",
      "epoch-1037 loss : 0.09256\n",
      "\n",
      "epoch-1038 loss : 0.09241\n",
      "\n",
      "epoch-1039 loss : 0.09266\n",
      "\n",
      "epoch-1040 loss : 0.09255\n",
      "\n",
      "epoch-1041 loss : 0.09251\n",
      "\n",
      "epoch-1042 loss : 0.09264\n",
      "\n",
      "epoch-1043 loss : 0.09267\n",
      "\n",
      "epoch-1044 loss : 0.09268\n",
      "\n",
      "epoch-1045 loss : 0.09266\n",
      "\n",
      "epoch-1046 loss : 0.09258\n",
      "\n",
      "epoch-1047 loss : 0.09267\n",
      "\n",
      "epoch-1048 loss : 0.09269\n",
      "\n",
      "epoch-1049 loss : 0.09255\n",
      "\n",
      "epoch-1050 accuracy check\n",
      "cluster_accuracy_post_training_cycle_all_dataset [0.78815081 0.8885918  0.87093779 0.80540037 0.65049505 0.59375\n",
      " 0.60731949 0.52220249 0.62360122 0.57538168 0.3983209  0.50098619\n",
      " 0.90145228 0.7834275  0.59519231 0.62122598]\n",
      "mean_cluster_accuracy_during_training_cycle : 66.88%, post_traincycle_acc : 67.04%, total_acc : 66.93%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 147\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAE\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m:\n\u001b[1;32m    146\u001b[0m     spike \u001b[38;5;241m=\u001b[39m spike\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, TIME)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (batch, time, feature)로 변환\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m spike_class \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAE\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m:\n\u001b[1;32m    150\u001b[0m     spike \u001b[38;5;241m=\u001b[39m spike\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m# Time 방향으로 평균\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:166\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    168\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/ae_network.py:359\u001b[0m, in \u001b[0;36mSAE_conv1.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 359\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x)\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/aedat2/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/neuron.py:65\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input_current)\u001b[0m\n\u001b[1;32m     32\u001b[0m from modules.old_fashioned import *\n\u001b[1;32m     33\u001b[0m from modules.ae_network import *\n\u001b[1;32m     35\u001b[0m ######## LIF Neuron #####################################################\n\u001b[1;32m     36\u001b[0m ######## LIF Neuron #####################################################\n\u001b[1;32m     37\u001b[0m ######## LIF Neuron #####################################################\n\u001b[1;32m     38\u001b[0m # class LIF_layer(nn.Module):\n\u001b[1;32m     39\u001b[0m #     def __init__ (self, v_init , v_decay , v_threshold , v_reset , sg_width, surrogate, BPTT_on):\n\u001b[1;32m     40\u001b[0m #         super(LIF_layer, self).__init__()\n\u001b[1;32m     41\u001b[0m #         self.v_init = v_init\n\u001b[1;32m     42\u001b[0m #         self.v_decay = v_decay\n\u001b[1;32m     43\u001b[0m #         self.v_threshold = v_threshold\n\u001b[1;32m     44\u001b[0m #         self.v_reset = v_reset\n\u001b[1;32m     45\u001b[0m #         self.sg_width = sg_width\n\u001b[1;32m     46\u001b[0m #         self.surrogate = surrogate\n\u001b[1;32m     47\u001b[0m #         self.BPTT_on = BPTT_on\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m #     def forward(self, input_current):\n\u001b[1;32m     50\u001b[0m #         v = torch.full_like(input_current, fill_value = self.v_init, dtype = torch.float, requires_grad=False) # v (membrane potential) init\n\u001b[1;32m     51\u001b[0m #         v_start = torch.full_like(input_current[0], fill_value = self.v_init, dtype = torch.float, requires_grad=False)\n\u001b[1;32m     52\u001b[0m #         post_spike = torch.full_like(input_current, fill_value = self.v_init, device=input_current.device, dtype = torch.float, requires_grad=False) \n\u001b[1;32m     53\u001b[0m #         # i와 v와 post_spike size는 여기서 다 같음: [Time, Batch, Channel, Height, Width] \n\u001b[1;32m     54\u001b[0m #         Time = v.shape[0]\n\u001b[1;32m     55\u001b[0m #         for t in range(Time):\n\u001b[1;32m     56\u001b[0m #             # leaky하고 input_current 더하고 fire하고 reset까지 (backward직접처리)\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m #             # post_spike[t], v[t] = LIF_METHOD.apply(input_current[t], v[t], \n\u001b[1;32m     59\u001b[0m #             #                             self.v_decay, self.v_threshold, self.v_reset, self.sg_width, self.surrogate, self.BPTT_on) \n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m #             if t == 0:\n\u001b[1;32m     62\u001b[0m #                 post_spike[t], v[t] = LIF_METHOD.apply(input_current[t], v_start, \n\u001b[1;32m     63\u001b[0m #                                             self.v_decay, self.v_threshold, self.v_reset, self.sg_width, self.surrogate, self.BPTT_on) \n\u001b[1;32m     64\u001b[0m #             else:\n\u001b[0;32m---> 65\u001b[0m #                 post_spike[t], v[t] = LIF_METHOD.apply(input_current[t], v[t-1], \n\u001b[1;32m     66\u001b[0m #                                             self.v_decay, self.v_threshold, self.v_reset, self.sg_width, self.surrogate, self.BPTT_on) \n\u001b[1;32m     67\u001b[0m             \n\u001b[1;32m     68\u001b[0m             \n\u001b[1;32m     69\u001b[0m             \n\u001b[1;32m     70\u001b[0m #             # if t != 0:\n\u001b[1;32m     71\u001b[0m #             #     print(torch.equal(v[t], v[t-1]))\n\u001b[1;32m     72\u001b[0m #             #     if torch.equal(v[t], v[t-1]):\n\u001b[1;32m     73\u001b[0m #             #         print('v',v[t])\n\u001b[1;32m     74\u001b[0m #             #     print(torch.equal(post_spike[t], post_spike[t-1]))\n\u001b[1;32m     75\u001b[0m #             #     if torch.equal(post_spike[t], post_spike[t-1]):\n\u001b[1;32m     76\u001b[0m #             #         print('post_spike',post_spike[t])\n\u001b[1;32m     77\u001b[0m         \n\u001b[1;32m     78\u001b[0m #         return post_spike \n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m # class LIF_METHOD(torch.autograd.Function):\n\u001b[1;32m     83\u001b[0m #     @staticmethod\n\u001b[1;32m     84\u001b[0m #     def forward(ctx, input_current_one_time, v_one_time, v_decay, v_threshold, v_reset, sg_width, surrogate, BPTT_on):\n\u001b[1;32m     85\u001b[0m #         v_one_time = v_one_time * v_decay + input_current_one_time # leak + pre-synaptic current integrate\n\u001b[1;32m     86\u001b[0m #         spike = (v_one_time >= v_threshold).float() #fire\n\u001b[1;32m     87\u001b[0m #         if surrogate == 'sigmoid':\n\u001b[1;32m     88\u001b[0m #             surrogate = 1\n\u001b[1;32m     89\u001b[0m #         elif surrogate == 'rectangle':\n\u001b[1;32m     90\u001b[0m #             surrogate = 2\n\u001b[1;32m     91\u001b[0m #         elif surrogate == 'rough_rectangle':\n\u001b[1;32m     92\u001b[0m #             surrogate = 3\n\u001b[1;32m     93\u001b[0m #         elif surrogate == 'hard_sigmoid':\n\u001b[1;32m     94\u001b[0m #             surrogate = 4\n\u001b[1;32m     95\u001b[0m #         else:\n\u001b[1;32m     96\u001b[0m #             pass\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m #         if (BPTT_on == True):\n\u001b[1;32m     99\u001b[0m #             BPTT_on = 1\n\u001b[1;32m    100\u001b[0m #         else:\n\u001b[1;32m    101\u001b[0m #             BPTT_on = 0\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m #         ctx.save_for_backward(v_one_time, torch.tensor([v_decay], requires_grad=False), \n\u001b[1;32m    104\u001b[0m #                             torch.tensor([v_threshold], requires_grad=False), \n\u001b[1;32m    105\u001b[0m #                             torch.tensor([v_reset], requires_grad=False), \n\u001b[1;32m    106\u001b[0m #                             torch.tensor([sg_width], requires_grad=False),\n\u001b[1;32m    107\u001b[0m #                             torch.tensor([surrogate], requires_grad=False),\n\u001b[1;32m    108\u001b[0m #                             torch.tensor([BPTT_on], requires_grad=False)) # save before reset\n\u001b[1;32m    109\u001b[0m         \n\u001b[1;32m    110\u001b[0m #         if (v_reset >= 0 and v_reset < 10000): # soft reset\n\u001b[1;32m    111\u001b[0m #             v_one_time = (v_one_time - spike * v_threshold) # reset\n\u001b[1;32m    112\u001b[0m #             # v_one_time = (v_one_time - spike * v_threshold).clamp_min(0) # reset # 0미만으로는 안 가게 하려면 이 줄 on\n\u001b[1;32m    113\u001b[0m #         elif (v_reset >= 10000 and v_reset < 20000): # hard reset \n\u001b[1;32m    114\u001b[0m #             v_reset -= 10000 \n\u001b[1;32m    115\u001b[0m #             v_one_time = v_one_time * (1 - spike) + v_reset * spike # reset\n\u001b[1;32m    116\u001b[0m #             v_reset += 10000\n\u001b[1;32m    117\u001b[0m #         return spike, v_one_time\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m #     @staticmethod\n\u001b[1;32m    120\u001b[0m #     def backward(ctx, grad_output_spike, grad_output_v):\n\u001b[1;32m    121\u001b[0m #         v_one_time, v_decay, v_threshold, v_reset, sg_width, surrogate, BPTT_on = ctx.saved_tensors\n\u001b[1;32m    122\u001b[0m #         v_decay=v_decay.item()\n\u001b[1;32m    123\u001b[0m #         v_threshold=v_threshold.item()\n\u001b[1;32m    124\u001b[0m #         v_reset=v_reset.item()\n\u001b[1;32m    125\u001b[0m #         sg_width=sg_width.item()\n\u001b[1;32m    126\u001b[0m #         surrogate=surrogate.item()\n\u001b[1;32m    127\u001b[0m #         BPTT_on=BPTT_on.item()\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m #         grad_input_current = grad_output_spike.clone()\n\u001b[1;32m    130\u001b[0m #         if BPTT_on == 1:\n\u001b[1;32m    131\u001b[0m #             grad_input_v = grad_output_v.clone() \n\u001b[1;32m    132\u001b[0m #             # print('grad_output_v',grad_output_v)\n\u001b[1;32m    133\u001b[0m #         ################ select one of the following surrogate gradient functions ################\n\u001b[1;32m    134\u001b[0m #         if (surrogate == 1):\n\u001b[1;32m    135\u001b[0m #             #===========surrogate gradient function (sigmoid)\n\u001b[1;32m    136\u001b[0m #             alpha = sg_width \n\u001b[1;32m    137\u001b[0m #             sig = torch.sigmoid(alpha*(v_one_time - v_threshold))\n\u001b[1;32m    138\u001b[0m #             grad_input_current *= alpha*sig*(1-sig)\n\u001b[1;32m    139\u001b[0m #             # grad_x = grad_output * (1. - sgax) * sgax * ctx.alpha\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m #         elif (surrogate == 2):\n\u001b[1;32m    142\u001b[0m #             # ===========surrogate gradient function (rectangle)\n\u001b[1;32m    143\u001b[0m #             grad_input_current *= ((v_one_time - v_threshold).abs() < sg_width/2).float() / sg_width\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m #         elif (surrogate == 3):\n\u001b[1;32m    146\u001b[0m #             #===========surrogate gradient function (rough rectangle)\n\u001b[1;32m    147\u001b[0m #             grad_input_current[(v_one_time - v_threshold).abs() > sg_width/2] = 0\n\u001b[1;32m    148\u001b[0m #             grad_input_current = grad_input_current / sg_width\n\u001b[1;32m    149\u001b[0m #         elif (surrogate == 4):\n\u001b[1;32m    150\u001b[0m #             #===========surrogate gradient function (hard sigmoid)\n\u001b[1;32m    151\u001b[0m #             alpha = sg_width \n\u001b[1;32m    152\u001b[0m #             sig = torch.clamp(alpha*(v_one_time - v_threshold) * 0.2 + 0.5, min=0, max=1)\n\u001b[1;32m    153\u001b[0m #             grad_input_current = alpha*sig*(1-sig)*grad_input_current\n\u001b[1;32m    154\u001b[0m #         else: \n\u001b[1;32m    155\u001b[0m #             assert False, 'surrogate doesn\\'t exist'\n\u001b[1;32m    156\u001b[0m #         ###########################################################################################\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m #         ## if BPTT_on == 1, then second return value is not None\n\u001b[1;32m    159\u001b[0m #         if (BPTT_on == 1):\n\u001b[1;32m    160\u001b[0m #             grad_input_v = v_decay * grad_input_v \n\u001b[1;32m    161\u001b[0m #         else:\n\u001b[1;32m    162\u001b[0m #             grad_input_v = None \n\u001b[1;32m    163\u001b[0m #         return grad_input_current, grad_input_v, None, None, None, None, None, None\n\u001b[1;32m    164\u001b[0m ######## LIF Neuron #####################################################\n\u001b[1;32m    165\u001b[0m ######## LIF Neuron #####################################################\n\u001b[1;32m    166\u001b[0m ######## LIF Neuron #####################################################\n\u001b[1;32m    167\u001b[0m     \n\u001b[1;32m    168\u001b[0m     \n\u001b[1;32m    169\u001b[0m # deprecated!!!!!!!!\n\u001b[1;32m    170\u001b[0m ######## LIF Neuron trace #####################################################\n\u001b[1;32m    171\u001b[0m ######## LIF Neuron trace #####################################################\n\u001b[1;32m    172\u001b[0m ######## LIF Neuron trace #####################################################\n\u001b[1;32m    173\u001b[0m # deprecated!!!!!!!!\n\u001b[1;32m    174\u001b[0m # class LIF_layer_trace(nn.Module):\n\u001b[1;32m    175\u001b[0m #     def __init__ (self, v_init , v_decay , v_threshold , v_reset , sg_width, surrogate, BPTT_on, trace_const1=1, trace_const2=0.7):\n\u001b[1;32m    176\u001b[0m #         super(LIF_layer_trace, self).__init__()\n\u001b[1;32m    177\u001b[0m #         self.v_init = v_init\n\u001b[1;32m    178\u001b[0m #         self.v_decay = v_decay\n\u001b[1;32m    179\u001b[0m #         self.v_threshold = v_threshold\n\u001b[1;32m    180\u001b[0m #         self.v_reset = v_reset\n\u001b[1;32m    181\u001b[0m #         self.sg_width = sg_width\n\u001b[1;32m    182\u001b[0m #         self.surrogate = surrogate\n\u001b[1;32m    183\u001b[0m #         self.BPTT_on = BPTT_on\n\u001b[1;32m    184\u001b[0m #         self.trace_const1 = trace_const1\n\u001b[1;32m    185\u001b[0m #         self.trace_const2 = trace_const2\n\u001b[1;32m    186\u001b[0m \n\u001b[1;32m    187\u001b[0m #     def forward(self, input_current):\n\u001b[1;32m    188\u001b[0m #         v = torch.full_like(input_current, fill_value = self.v_init, dtype = torch.float, requires_grad=False) # v (membrane potential) init\n\u001b[1;32m    189\u001b[0m #         post_spike = torch.full_like(input_current, fill_value = self.v_init, device=input_current.device, dtype = torch.float, requires_grad=False) \n\u001b[1;32m    190\u001b[0m #         # i와 v와 post_spike size는 여기서 다 같음: [Time, Batch, Channel, Height, Width] \n\u001b[1;32m    191\u001b[0m #         Time = v.shape[0]\n\u001b[1;32m    192\u001b[0m #         trace = torch.full_like(input_current, fill_value = self.v_init, device=input_current.device, dtype = torch.float, requires_grad=False) \n\u001b[1;32m    193\u001b[0m #         trace_past = torch.full_like(input_current[0], fill_value = self.v_init, device=input_current.device, dtype = torch.float, requires_grad=False) \n\u001b[1;32m    194\u001b[0m #         for t in range(Time):\n\u001b[1;32m    195\u001b[0m #             # leaky하고 input_current 더하고 fire하고 reset까지 (backward직접처리)\n\u001b[1;32m    196\u001b[0m #             post_spike[t], v[t] = LIF_METHOD.apply(input_current[t], v[t-1], \n\u001b[1;32m    197\u001b[0m #                                             self.v_decay, self.v_threshold, self.v_reset, self.sg_width, self.surrogate, self.BPTT_on) \n\u001b[1;32m    198\u001b[0m #             trace[t] = self.trace_const1*((post_spike[t]).detach()) + self.trace_const2*trace_past\n\u001b[1;32m    199\u001b[0m #             trace_past = trace[t]\n\u001b[1;32m    200\u001b[0m #         return [post_spike, trace] \n\u001b[1;32m    201\u001b[0m ######## LIF Neuron trace #####################################################\n\u001b[1;32m    202\u001b[0m ######## LIF Neuron trace #####################################################\n\u001b[1;32m    203\u001b[0m ######## LIF Neuron trace #####################################################\n\u001b[1;32m    208\u001b[0m class LIF_layer(nn.Module):\n\u001b[1;32m    209\u001b[0m     def __init__ (self, v_init , v_decay , v_threshold , v_reset , sg_width, surrogate, BPTT_on):\n",
      "File \u001b[0;32m/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/modules/neuron.py:110\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(ctx, input_current_one_time, v_one_time, v_decay, v_threshold, v_reset, sg_width, surrogate, BPTT_on)\u001b[0m\n\u001b[1;32m     32\u001b[0m from modules.old_fashioned import *\n\u001b[1;32m     33\u001b[0m from modules.ae_network import *\n\u001b[1;32m     35\u001b[0m ######## LIF Neuron #####################################################\n\u001b[1;32m     36\u001b[0m ######## LIF Neuron #####################################################\n\u001b[1;32m     37\u001b[0m ######## LIF Neuron #####################################################\n\u001b[1;32m     38\u001b[0m # class LIF_layer(nn.Module):\n\u001b[1;32m     39\u001b[0m #     def __init__ (self, v_init , v_decay , v_threshold , v_reset , sg_width, surrogate, BPTT_on):\n\u001b[1;32m     40\u001b[0m #         super(LIF_layer, self).__init__()\n\u001b[1;32m     41\u001b[0m #         self.v_init = v_init\n\u001b[1;32m     42\u001b[0m #         self.v_decay = v_decay\n\u001b[1;32m     43\u001b[0m #         self.v_threshold = v_threshold\n\u001b[1;32m     44\u001b[0m #         self.v_reset = v_reset\n\u001b[1;32m     45\u001b[0m #         self.sg_width = sg_width\n\u001b[1;32m     46\u001b[0m #         self.surrogate = surrogate\n\u001b[1;32m     47\u001b[0m #         self.BPTT_on = BPTT_on\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m #     def forward(self, input_current):\n\u001b[1;32m     50\u001b[0m #         v = torch.full_like(input_current, fill_value = self.v_init, dtype = torch.float, requires_grad=False) # v (membrane potential) init\n\u001b[1;32m     51\u001b[0m #         v_start = torch.full_like(input_current[0], fill_value = self.v_init, dtype = torch.float, requires_grad=False)\n\u001b[1;32m     52\u001b[0m #         post_spike = torch.full_like(input_current, fill_value = self.v_init, device=input_current.device, dtype = torch.float, requires_grad=False) \n\u001b[1;32m     53\u001b[0m #         # i와 v와 post_spike size는 여기서 다 같음: [Time, Batch, Channel, Height, Width] \n\u001b[1;32m     54\u001b[0m #         Time = v.shape[0]\n\u001b[1;32m     55\u001b[0m #         for t in range(Time):\n\u001b[1;32m     56\u001b[0m #             # leaky하고 input_current 더하고 fire하고 reset까지 (backward직접처리)\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m #             # post_spike[t], v[t] = LIF_METHOD.apply(input_current[t], v[t], \n\u001b[1;32m     59\u001b[0m #             #                             self.v_decay, self.v_threshold, self.v_reset, self.sg_width, self.surrogate, self.BPTT_on) \n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m #             if t == 0:\n\u001b[1;32m     62\u001b[0m #                 post_spike[t], v[t] = LIF_METHOD.apply(input_current[t], v_start, \n\u001b[1;32m     63\u001b[0m #                                             self.v_decay, self.v_threshold, self.v_reset, self.sg_width, self.surrogate, self.BPTT_on) \n\u001b[1;32m     64\u001b[0m #             else:\n\u001b[1;32m     65\u001b[0m #                 post_spike[t], v[t] = LIF_METHOD.apply(input_current[t], v[t-1], \n\u001b[1;32m     66\u001b[0m #                                             self.v_decay, self.v_threshold, self.v_reset, self.sg_width, self.surrogate, self.BPTT_on) \n\u001b[1;32m     67\u001b[0m             \n\u001b[1;32m     68\u001b[0m             \n\u001b[1;32m     69\u001b[0m             \n\u001b[1;32m     70\u001b[0m #             # if t != 0:\n\u001b[1;32m     71\u001b[0m #             #     print(torch.equal(v[t], v[t-1]))\n\u001b[1;32m     72\u001b[0m #             #     if torch.equal(v[t], v[t-1]):\n\u001b[1;32m     73\u001b[0m #             #         print('v',v[t])\n\u001b[1;32m     74\u001b[0m #             #     print(torch.equal(post_spike[t], post_spike[t-1]))\n\u001b[1;32m     75\u001b[0m #             #     if torch.equal(post_spike[t], post_spike[t-1]):\n\u001b[1;32m     76\u001b[0m #             #         print('post_spike',post_spike[t])\n\u001b[1;32m     77\u001b[0m         \n\u001b[1;32m     78\u001b[0m #         return post_spike \n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m # class LIF_METHOD(torch.autograd.Function):\n\u001b[1;32m     83\u001b[0m #     @staticmethod\n\u001b[1;32m     84\u001b[0m #     def forward(ctx, input_current_one_time, v_one_time, v_decay, v_threshold, v_reset, sg_width, surrogate, BPTT_on):\n\u001b[1;32m     85\u001b[0m #         v_one_time = v_one_time * v_decay + input_current_one_time # leak + pre-synaptic current integrate\n\u001b[1;32m     86\u001b[0m #         spike = (v_one_time >= v_threshold).float() #fire\n\u001b[1;32m     87\u001b[0m #         if surrogate == 'sigmoid':\n\u001b[1;32m     88\u001b[0m #             surrogate = 1\n\u001b[1;32m     89\u001b[0m #         elif surrogate == 'rectangle':\n\u001b[1;32m     90\u001b[0m #             surrogate = 2\n\u001b[1;32m     91\u001b[0m #         elif surrogate == 'rough_rectangle':\n\u001b[1;32m     92\u001b[0m #             surrogate = 3\n\u001b[1;32m     93\u001b[0m #         elif surrogate == 'hard_sigmoid':\n\u001b[1;32m     94\u001b[0m #             surrogate = 4\n\u001b[1;32m     95\u001b[0m #         else:\n\u001b[1;32m     96\u001b[0m #             pass\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m #         if (BPTT_on == True):\n\u001b[1;32m     99\u001b[0m #             BPTT_on = 1\n\u001b[1;32m    100\u001b[0m #         else:\n\u001b[1;32m    101\u001b[0m #             BPTT_on = 0\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m #         ctx.save_for_backward(v_one_time, torch.tensor([v_decay], requires_grad=False), \n\u001b[1;32m    104\u001b[0m #                             torch.tensor([v_threshold], requires_grad=False), \n\u001b[1;32m    105\u001b[0m #                             torch.tensor([v_reset], requires_grad=False), \n\u001b[1;32m    106\u001b[0m #                             torch.tensor([sg_width], requires_grad=False),\n\u001b[1;32m    107\u001b[0m #                             torch.tensor([surrogate], requires_grad=False),\n\u001b[1;32m    108\u001b[0m #                             torch.tensor([BPTT_on], requires_grad=False)) # save before reset\n\u001b[1;32m    109\u001b[0m         \n\u001b[0;32m--> 110\u001b[0m #         if (v_reset >= 0 and v_reset < 10000): # soft reset\n\u001b[1;32m    111\u001b[0m #             v_one_time = (v_one_time - spike * v_threshold) # reset\n\u001b[1;32m    112\u001b[0m #             # v_one_time = (v_one_time - spike * v_threshold).clamp_min(0) # reset # 0미만으로는 안 가게 하려면 이 줄 on\n\u001b[1;32m    113\u001b[0m #         elif (v_reset >= 10000 and v_reset < 20000): # hard reset \n\u001b[1;32m    114\u001b[0m #             v_reset -= 10000 \n\u001b[1;32m    115\u001b[0m #             v_one_time = v_one_time * (1 - spike) + v_reset * spike # reset\n\u001b[1;32m    116\u001b[0m #             v_reset += 10000\n\u001b[1;32m    117\u001b[0m #         return spike, v_one_time\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m #     @staticmethod\n\u001b[1;32m    120\u001b[0m #     def backward(ctx, grad_output_spike, grad_output_v):\n\u001b[1;32m    121\u001b[0m #         v_one_time, v_decay, v_threshold, v_reset, sg_width, surrogate, BPTT_on = ctx.saved_tensors\n\u001b[1;32m    122\u001b[0m #         v_decay=v_decay.item()\n\u001b[1;32m    123\u001b[0m #         v_threshold=v_threshold.item()\n\u001b[1;32m    124\u001b[0m #         v_reset=v_reset.item()\n\u001b[1;32m    125\u001b[0m #         sg_width=sg_width.item()\n\u001b[1;32m    126\u001b[0m #         surrogate=surrogate.item()\n\u001b[1;32m    127\u001b[0m #         BPTT_on=BPTT_on.item()\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m #         grad_input_current = grad_output_spike.clone()\n\u001b[1;32m    130\u001b[0m #         if BPTT_on == 1:\n\u001b[1;32m    131\u001b[0m #             grad_input_v = grad_output_v.clone() \n\u001b[1;32m    132\u001b[0m #             # print('grad_output_v',grad_output_v)\n\u001b[1;32m    133\u001b[0m #         ################ select one of the following surrogate gradient functions ################\n\u001b[1;32m    134\u001b[0m #         if (surrogate == 1):\n\u001b[1;32m    135\u001b[0m #             #===========surrogate gradient function (sigmoid)\n\u001b[1;32m    136\u001b[0m #             alpha = sg_width \n\u001b[1;32m    137\u001b[0m #             sig = torch.sigmoid(alpha*(v_one_time - v_threshold))\n\u001b[1;32m    138\u001b[0m #             grad_input_current *= alpha*sig*(1-sig)\n\u001b[1;32m    139\u001b[0m #             # grad_x = grad_output * (1. - sgax) * sgax * ctx.alpha\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m #         elif (surrogate == 2):\n\u001b[1;32m    142\u001b[0m #             # ===========surrogate gradient function (rectangle)\n\u001b[1;32m    143\u001b[0m #             grad_input_current *= ((v_one_time - v_threshold).abs() < sg_width/2).float() / sg_width\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m #         elif (surrogate == 3):\n\u001b[1;32m    146\u001b[0m #             #===========surrogate gradient function (rough rectangle)\n\u001b[1;32m    147\u001b[0m #             grad_input_current[(v_one_time - v_threshold).abs() > sg_width/2] = 0\n\u001b[1;32m    148\u001b[0m #             grad_input_current = grad_input_current / sg_width\n\u001b[1;32m    149\u001b[0m #         elif (surrogate == 4):\n\u001b[1;32m    150\u001b[0m #             #===========surrogate gradient function (hard sigmoid)\n\u001b[1;32m    151\u001b[0m #             alpha = sg_width \n\u001b[1;32m    152\u001b[0m #             sig = torch.clamp(alpha*(v_one_time - v_threshold) * 0.2 + 0.5, min=0, max=1)\n\u001b[1;32m    153\u001b[0m #             grad_input_current = alpha*sig*(1-sig)*grad_input_current\n\u001b[1;32m    154\u001b[0m #         else: \n\u001b[1;32m    155\u001b[0m #             assert False, 'surrogate doesn\\'t exist'\n\u001b[1;32m    156\u001b[0m #         ###########################################################################################\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m #         ## if BPTT_on == 1, then second return value is not None\n\u001b[1;32m    159\u001b[0m #         if (BPTT_on == 1):\n\u001b[1;32m    160\u001b[0m #             grad_input_v = v_decay * grad_input_v \n\u001b[1;32m    161\u001b[0m #         else:\n\u001b[1;32m    162\u001b[0m #             grad_input_v = None \n\u001b[1;32m    163\u001b[0m #         return grad_input_current, grad_input_v, None, None, None, None, None, None\n\u001b[1;32m    164\u001b[0m ######## LIF Neuron #####################################################\n\u001b[1;32m    165\u001b[0m ######## LIF Neuron #####################################################\n\u001b[1;32m    166\u001b[0m ######## LIF Neuron #####################################################\n\u001b[1;32m    167\u001b[0m     \n\u001b[1;32m    168\u001b[0m     \n\u001b[1;32m    169\u001b[0m # deprecated!!!!!!!!\n\u001b[1;32m    170\u001b[0m ######## LIF Neuron trace #####################################################\n\u001b[1;32m    171\u001b[0m ######## LIF Neuron trace #####################################################\n\u001b[1;32m    172\u001b[0m ######## LIF Neuron trace #####################################################\n\u001b[1;32m    173\u001b[0m # deprecated!!!!!!!!\n\u001b[1;32m    174\u001b[0m # class LIF_layer_trace(nn.Module):\n\u001b[1;32m    175\u001b[0m #     def __init__ (self, v_init , v_decay , v_threshold , v_reset , sg_width, surrogate, BPTT_on, trace_const1=1, trace_const2=0.7):\n\u001b[1;32m    176\u001b[0m #         super(LIF_layer_trace, self).__init__()\n\u001b[1;32m    177\u001b[0m #         self.v_init = v_init\n\u001b[1;32m    178\u001b[0m #         self.v_decay = v_decay\n\u001b[1;32m    179\u001b[0m #         self.v_threshold = v_threshold\n\u001b[1;32m    180\u001b[0m #         self.v_reset = v_reset\n\u001b[1;32m    181\u001b[0m #         self.sg_width = sg_width\n\u001b[1;32m    182\u001b[0m #         self.surrogate = surrogate\n\u001b[1;32m    183\u001b[0m #         self.BPTT_on = BPTT_on\n\u001b[1;32m    184\u001b[0m #         self.trace_const1 = trace_const1\n\u001b[1;32m    185\u001b[0m #         self.trace_const2 = trace_const2\n\u001b[1;32m    186\u001b[0m \n\u001b[1;32m    187\u001b[0m #     def forward(self, input_current):\n\u001b[1;32m    188\u001b[0m #         v = torch.full_like(input_current, fill_value = self.v_init, dtype = torch.float, requires_grad=False) # v (membrane potential) init\n\u001b[1;32m    189\u001b[0m #         post_spike = torch.full_like(input_current, fill_value = self.v_init, device=input_current.device, dtype = torch.float, requires_grad=False) \n\u001b[1;32m    190\u001b[0m #         # i와 v와 post_spike size는 여기서 다 같음: [Time, Batch, Channel, Height, Width] \n\u001b[1;32m    191\u001b[0m #         Time = v.shape[0]\n\u001b[1;32m    192\u001b[0m #         trace = torch.full_like(input_current, fill_value = self.v_init, device=input_current.device, dtype = torch.float, requires_grad=False) \n\u001b[1;32m    193\u001b[0m #         trace_past = torch.full_like(input_current[0], fill_value = self.v_init, device=input_current.device, dtype = torch.float, requires_grad=False) \n\u001b[1;32m    194\u001b[0m #         for t in range(Time):\n\u001b[1;32m    195\u001b[0m #             # leaky하고 input_current 더하고 fire하고 reset까지 (backward직접처리)\n\u001b[1;32m    196\u001b[0m #             post_spike[t], v[t] = LIF_METHOD.apply(input_current[t], v[t-1], \n\u001b[1;32m    197\u001b[0m #                                             self.v_decay, self.v_threshold, self.v_reset, self.sg_width, self.surrogate, self.BPTT_on) \n\u001b[1;32m    198\u001b[0m #             trace[t] = self.trace_const1*((post_spike[t]).detach()) + self.trace_const2*trace_past\n\u001b[1;32m    199\u001b[0m #             trace_past = trace[t]\n\u001b[1;32m    200\u001b[0m #         return [post_spike, trace] \n\u001b[1;32m    201\u001b[0m ######## LIF Neuron trace #####################################################\n\u001b[1;32m    202\u001b[0m ######## LIF Neuron trace #####################################################\n\u001b[1;32m    203\u001b[0m ######## LIF Neuron trace #####################################################\n\u001b[1;32m    208\u001b[0m class LIF_layer(nn.Module):\n\u001b[1;32m    209\u001b[0m     def __init__ (self, v_init , v_decay , v_threshold , v_reset , sg_width, surrogate, BPTT_on):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "net = net.to(device)\n",
    "print(net)\n",
    "print('Device:',device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_history = []\n",
    "mean_cluster_accuracy_during_training_cycle_all_dataset_history = []\n",
    "mean_cluster_accuracy_post_training_cycle_all_dataset_history = []\n",
    "mean_cluster_accuracy_total_all_dataset_history = []\n",
    "\n",
    "tau = np.zeros(num_cluster)\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + f\"_{str(int(datetime.datetime.now().microsecond / 1000)).zfill(3)}\"\n",
    "print(f\"\\nStart Training, current_time = {current_time}\")\n",
    "best_mean_cluster_accuracy_post_training_cycle_all_dataset = 0\n",
    "for epoch in range(max_epoch):\n",
    "    cluster_accuracy_during_training_cycle_all_dataset = np.zeros(dataset_num)\n",
    "    cluster_accuracy_post_training_cycle_all_dataset = np.zeros(dataset_num)\n",
    "    cluster_accuracy_total_all_dataset = np.zeros(dataset_num)    \n",
    "    \n",
    "    if(epoch % 50 == 0): \n",
    "        print(f'\\nepoch-{epoch} accuracy check')\n",
    "        for ds in range(dataset_num):\n",
    "            # print('\\n', spike_tot[ds])\n",
    "\n",
    "            spike_template = np.load(my_path_ground_BH + template[ds])\n",
    "            spike = np.load(my_path_ground_BH + spike_tot[ds])\n",
    "            label = np.load(my_path_ground_BH + label_tot[ds])\n",
    "            \n",
    "            Cluster = np.zeros((num_cluster, 4))\n",
    "            assert Cluster.shape[-1] == 4, '이거 hidden dim 4 아니게 할 거면 잘 바꿔라'\n",
    "            \n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for i in range(num_cluster):\n",
    "                    spike_torch = torch.from_numpy(spike_template[i, :])\n",
    "                    spike_torch = spike_torch.float().to(device)\n",
    "                    if 'SAE' in net.module.__class__.__name__:\n",
    "                        spike_torch = spike_torch.unsqueeze(-1).repeat(1, 1, TIME).permute(0,2,1) # (batch, time, feature)로 변환\n",
    "                    else:\n",
    "                        spike_torch = spike_torch.unsqueeze(0)\n",
    "                    inner_inf = net.module.encoder(spike_torch)\n",
    "                    # print(inner_inf)\n",
    "                    if 'SAE' in net.module.__class__.__name__:\n",
    "                        inner_inf = inner_inf.mean(dim=1)# Time 방향으로 평균\n",
    "                    Cluster[i, :] = inner_inf.cpu().detach().numpy()\n",
    "\n",
    "            spike_hidden = np.zeros((len(spike), 4))\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(spike)):\n",
    "                    spike_torch = torch.from_numpy(spike[i, :])\n",
    "                    spike_torch = spike_torch.float().to(device)\n",
    "                    if 'SAE' in net.module.__class__.__name__:\n",
    "                        spike_torch = spike_torch.unsqueeze(-1).repeat(1, 1, TIME).permute(0,2,1) # (batch, time, feature)로 변환\n",
    "                    else:\n",
    "                        spike_torch = spike_torch.unsqueeze(0)\n",
    "                    inner_inf = net.module.encoder(spike_torch)\n",
    "                    if 'SAE' in net.module.__class__.__name__:\n",
    "                        inner_inf = inner_inf.mean(dim=1)# Time 방향으로 평균\n",
    "                    spike_hidden[i, :] = inner_inf.cpu().detach().numpy()\n",
    "                \n",
    "            spike_id = np.zeros(len(spike))\n",
    "\n",
    "\n",
    "            distance_sm = np.zeros(num_cluster)\n",
    "            tau = np.zeros(num_cluster)\n",
    "            \n",
    "            for spike_index in range(len(spike)): \n",
    "                for q in range(num_cluster):\n",
    "                    tau[q] = np.dot(spike_hidden[spike_index, :], Cluster[q, :]) # 이거 l2norm 거쳐서 나온 거니까 분모 1임.\n",
    "                # tau = np.dot(Cluster, spike_hidden[spike_index, :]) # 이거 l2norm 거쳐서 나온 거니까 분모 1임.\n",
    "\n",
    "                for i in range(num_cluster): # l2 distance\n",
    "                    distance_sm[i] = np.sum(np.power(np.abs(Cluster[i] - spike_hidden[spike_index, :]), 2))\n",
    "\n",
    "                m = np.argmin(distance_sm)\n",
    "                spike_id[spike_index] = m + 1\n",
    "                if(np.max(tau) >= cos_thr[ds] and spike_index < training_cycle): # 원래 1400 아니냐?\n",
    "                    Cluster[m] = (Cluster[m] * 15 + spike_hidden[spike_index, :])/16\n",
    "                            \n",
    "            # spike id 분포 확인하기\n",
    "            # unique_elements, counts = np.unique(spike_id, return_counts=True)\n",
    "            # print(\"Unique elements:\", unique_elements)\n",
    "            # print(\"Counts:\", counts)\n",
    "\n",
    "            cluster_accuracy_during_training_cycle = np.zeros(math.factorial(num_cluster))\n",
    "            cluster_accuracy_post_training_cycle = np.zeros(math.factorial(num_cluster))\n",
    "            cluster_accuracy_total = np.zeros(math.factorial(num_cluster))\n",
    "            \n",
    "            label_converter_ground = list(range(1, num_cluster + 1)) # [1, 2, 3, 4] 생성\n",
    "            label_converter_permutations = list(itertools.permutations(label_converter_ground)) # 모든 순열 구하기\n",
    "            perm_i = 0\n",
    "            for perm in label_converter_permutations:\n",
    "                label_converter = list(perm)\n",
    "                # print(label_converter)\n",
    "                correct_during_training_cycle = 0\n",
    "                correct_post_training_cycle = 0\n",
    "\n",
    "                assert len(spike_id) == len(label), 'spike_id랑 label 길이 같아야 됨.'\n",
    "                for i in range(len(spike_id)):\n",
    "                    if(label_converter[int(spike_id[i]-1)] == label[i]):\n",
    "                        if i < training_cycle:\n",
    "                            correct_during_training_cycle += 1\n",
    "                        else:\n",
    "                            correct_post_training_cycle += 1\n",
    "\n",
    "                cluster_accuracy_during_training_cycle[perm_i] = correct_during_training_cycle/training_cycle\n",
    "                cluster_accuracy_post_training_cycle[perm_i] = correct_post_training_cycle/(len(spike_id)-training_cycle)\n",
    "                cluster_accuracy_total[perm_i] = (correct_during_training_cycle+correct_post_training_cycle)/(len(spike_id))\n",
    "                perm_i += 1\n",
    "\n",
    "            cluster_accuracy_during_training_cycle_all_dataset[ds] = np.max(cluster_accuracy_during_training_cycle)\n",
    "            cluster_accuracy_post_training_cycle_all_dataset[ds] = cluster_accuracy_post_training_cycle[np.argmax(cluster_accuracy_during_training_cycle)]\n",
    "            cluster_accuracy_total_all_dataset[ds] = cluster_accuracy_total[np.argmax(cluster_accuracy_during_training_cycle)]\n",
    "\n",
    "        print('cluster_accuracy_post_training_cycle_all_dataset', cluster_accuracy_post_training_cycle_all_dataset)\n",
    "\n",
    "        mean_cluster_accuracy_during_training_cycle_all_dataset = np.mean(cluster_accuracy_during_training_cycle_all_dataset)\n",
    "        mean_cluster_accuracy_post_training_cycle_all_dataset = np.mean(cluster_accuracy_post_training_cycle_all_dataset)\n",
    "        mean_cluster_accuracy_total_all_dataset = np.mean(cluster_accuracy_total_all_dataset)\n",
    "        \n",
    "        mean_cluster_accuracy_during_training_cycle_all_dataset_history.append((epoch, mean_cluster_accuracy_during_training_cycle_all_dataset*100))\n",
    "        mean_cluster_accuracy_post_training_cycle_all_dataset_history.append((epoch, mean_cluster_accuracy_post_training_cycle_all_dataset*100))\n",
    "        mean_cluster_accuracy_total_all_dataset_history.append((epoch, mean_cluster_accuracy_total_all_dataset*100))\n",
    "        print(f\"mean_cluster_accuracy_during_training_cycle : {mean_cluster_accuracy_during_training_cycle_all_dataset*100:.2f}%, post_traincycle_acc : {mean_cluster_accuracy_post_training_cycle_all_dataset*100:.2f}%, total_acc : {mean_cluster_accuracy_total_all_dataset*100:.2f}%\")\n",
    "\n",
    "        if mean_cluster_accuracy_post_training_cycle_all_dataset > best_mean_cluster_accuracy_post_training_cycle_all_dataset:\n",
    "            torch.save(net, f\"net_save/save_now_net_{current_time}.pth\")\n",
    "            print('save model')\n",
    "            best_mean_cluster_accuracy_post_training_cycle_all_dataset = mean_cluster_accuracy_post_training_cycle_all_dataset\n",
    "\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        spike = data\n",
    "        spike = spike.to(device)\n",
    "        if 'SAE' in net.module.__class__.__name__:\n",
    "            spike = spike.unsqueeze(-1).repeat(1, 1, TIME).permute(0,2,1) # (batch, time, feature)로 변환\n",
    "        spike_class = net(spike)\n",
    "\n",
    "        if 'SAE' in net.module.__class__.__name__:\n",
    "            spike = spike.mean(dim=1)# Time 방향으로 평균\n",
    "            spike_class = spike_class.mean(dim=1)# Time 방향으로 평균\n",
    "\n",
    "        # if 'SAE' in net.module.__class__.__name__:\n",
    "        #     loss1 = criterion(spike_class[:, :, 5:25], spike[:, :, 5:25])\n",
    "        #     loss2 = criterion(spike_class[:, :, 0:5], spike[:, :, 0:5])\n",
    "        #     loss3 = criterion(spike_class[:, :, 25:spike_length], spike[:, :, 25:spike_length])\n",
    "        # else:\n",
    "        loss1 = criterion(spike_class[:, 5:25], spike[:, 5:25])\n",
    "        loss2 = criterion(spike_class[:, 0:5], spike[:, 0:5])\n",
    "        loss3 = criterion(spike_class[:, 25:spike_length], spike[:, 25:spike_length])\n",
    "\n",
    "        loss = loss1 * 2.125 + (loss2 + loss3)/4\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    loss_history.append((epoch, avg_loss))\n",
    "    print(f'\\nepoch-{epoch} loss : {avg_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# 설정 정보 제목 작성\n",
    "title = (\n",
    "    f\"Dataset Num: {dataset_num}, Conv {Conv_net}, SAE {SAE_net}, Current time {current_time}, Spike Length: {spike_length}, Num Cluster: {num_cluster}, \"\n",
    "    f\"Training Cycle: {training_cycle}, Batch Size: {batch_size}, Max Epoch: {max_epoch}, \\n\"\n",
    "    f\"Learning Rate: {learning_rate}, Input Normalize: {normalize_on}, Need Bias: {need_bias}, \"\n",
    "    f\"LIF Add at First: {lif_add_at_first}, TIME: {TIME}, Seed: {my_seed}, Data: {AE_train_data}\"\n",
    ")\n",
    "\n",
    "# 데이터 리스트와 라벨 설정 (Loss 제외)\n",
    "data_list = [\n",
    "    (\"Mean Cluster Accuracy (During Training Cycle)\", mean_cluster_accuracy_during_training_cycle_all_dataset_history),\n",
    "    (\"Mean Cluster Accuracy (Post Training Cycle)\", mean_cluster_accuracy_post_training_cycle_all_dataset_history),\n",
    "    (\"Mean Cluster Accuracy (Total)\", mean_cluster_accuracy_total_all_dataset_history),\n",
    "]\n",
    "\n",
    "# 플롯 생성\n",
    "fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# 첫 번째 y축: Accuracy 관련 데이터\n",
    "for label, data in data_list:\n",
    "    epochs, values = zip(*data)  # epoch, value 분리\n",
    "    ax1.plot(epochs, values, label=label)\n",
    "\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Clurstering Accuracy [%]\", color=\"blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "ax1.legend(loc=\"center right\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# x축을 정수만 표시하도록 설정\n",
    "ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "# 두 번째 y축: Loss History\n",
    "ax2 = ax1.twinx()\n",
    "epochs, values = zip(*loss_history)\n",
    "ax2.plot(epochs, values, label=\"AE Loss History\", color=\"red\", linestyle=\"--\")\n",
    "ax2.set_ylabel(\"Loss\", color=\"red\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "ax2.legend(loc=\"center left\")\n",
    "\n",
    "# 제목 추가\n",
    "plt.title(title, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'net_save/{current_time}', dpi=300, bbox_inches=\"tight\")  # dpi=300은 고해상도로 저장, bbox_inches=\"tight\"는 여백 최소화\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# acc_metric.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def acc_det(spike_index, spike_times, ans_times):\n",
    "        k = 0\n",
    "        FN = 0\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        spike_true_index = np.zeros(10000)\n",
    "        spike_false_index = np.zeros(10000)\n",
    "        ans_index = np.zeros(10000)\n",
    "        spike_times[spike_times == 0] = 1500000\n",
    "        det_win = 20\n",
    "\n",
    "        '''\n",
    "        f = open('./../result/TP_index.txt', 'w')\n",
    "        g = open('./../result/FP_index.txt', 'w')\n",
    "        h = open('./../result/FN_index.txt', 'w')\n",
    "        '''\n",
    "        for j in range(len(ans_times)):\n",
    "                if(ans_times[j] + det_win >= spike_times[k] and spike_times[k] >= ans_times[j] - det_win):\n",
    "                        spike_true_index[TP] = k\n",
    "                        ans_index[TP] = j\n",
    "                        #f.write('%7d %7d'%(ans_times[j], spike_times[k]) +\"\\n\")\n",
    "                        TP = TP + 1\n",
    "                        k = k + 1\n",
    "                elif(ans_times[j] + det_win < spike_times[k]):\n",
    "                        FN = FN + 1\n",
    "                        #h.write('%7d'%(ans_times[j]) +\"\\n\")\n",
    "                else:\n",
    "                        while(1):\n",
    "                                spike_false_index[FP] = k\n",
    "                                FP = FP + 1\n",
    "                                #g.write('%7d'%(spike_times[k]) + \"\\n\")\n",
    "                                k = k + 1\n",
    "                                if(ans_times[j] - det_win <= spike_times[k]):\n",
    "                                        break\n",
    "                        if(ans_times[j] + det_win >= spike_times[k]):\n",
    "                                spike_true_index[TP] = k\n",
    "                                ans_index[TP] = j\n",
    "                                #f.write('%7d %7d'%(ans_times[j], spike_times[k]) +\"\\n\")\n",
    "                                TP = TP + 1\n",
    "                                k = k + 1\n",
    "                        else:\n",
    "                                FN = FN + 1\n",
    "\t\t\t\t#h.write('%7d'%(ans_times[j]) +\"\\n\")\n",
    "        print(\"# of ans : \", len(ans_times))\n",
    "        print(\"# of TP ; \", TP)\n",
    "        print(\"# of FP ; \", FP)\n",
    "        print(\"# of FN : \", FN)\n",
    "        print(\"Det acc : \", TP/len(ans_times))\n",
    "        return spike_true_index, spike_false_index, ans_index, TP, TP/len(ans_times)\n",
    "''''\n",
    "def acc_clu(numspike, spike_id, TP, spike_true_index, ans_index, ans_cluster):\n",
    "\tcluster_accuracy = np.zeros(6)\n",
    "\tfor ep in range(6):\n",
    "\t\tif(ep == 1 or ep == 4):\n",
    "\t\t\tfor i in range(numspike):\n",
    "\t\t\t\tif(spike_id[i] == 3):\n",
    "\t\t\t\t\tspike_id[i] = 2\n",
    "\t\t\t\telif(spike_id[i] == 2):\n",
    "\t\t\t\t\tspike_id[i] = 3\n",
    "\t\telif(ep == 2 or ep == 5):\n",
    "\t\t\tfor i in range(numspike):\n",
    "\t\t\t\tif(spike_id[i] == 1):\n",
    "\t\t\t\t\tspike_id[i] = 2\n",
    "\t\t\t\telif(spike_id[i] == 2):\n",
    "\t\t\t\t\tspike_id[i] = 1\n",
    "\t\telif(ep == 3):\n",
    "\t\t\tfor i in range(numspike):\n",
    "\t\t\t\tif(spike_id[i] == 1):\n",
    "\t\t\t\t\tspike_id[i] = 3\n",
    "\t\t\t\telif(spike_id[i] == 3):\n",
    "\t\t\t\t\tspike_id[i] = 1\n",
    "\t\ttrue_cluster = 0\n",
    "\t\tfor i in range(TP):\n",
    "\t\t\tif(spike_true_index[i] == 0) and (spike_true_index[i+1] == 0):\n",
    "\t\t\t\tprint(\"break\")\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tif(spike_id[int(spike_true_index[i])] == ans_cluster[int(ans_index[i])]):\n",
    "\t\t\t\t\ttrue_cluster += 1\n",
    "\t\tcluster_accuracy[ep] = true_cluster*100/TP\n",
    "\tprint('Clu acc : ', max(cluster_accuracy))\n",
    "\treturn max(cluster_accuracy)\n",
    "'''\n",
    "\n",
    "def acc(spike_index, spike_times, ans_times, spike_id, ans_cluster, training = 0):\n",
    "        k = 0\n",
    "        FN = 0\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        spike_times[spike_times == 0] = 1500000\n",
    "        det_win = 20\n",
    "        id_ssp = np.zeros(10000)\n",
    "        id_ans = np.zeros(10000)\n",
    "        id_false = np.zeros(10000)\n",
    "        training_TP = 0\n",
    "        training_ans = 0\n",
    "        training_cycle = 100\n",
    "        for j in range(len(ans_times)):\n",
    "                if(ans_times[j] + det_win >= spike_times[k] and spike_times[k] >= ans_times[j] - det_win):\n",
    "                        id_ssp[TP] = spike_id[k]\n",
    "                        id_ans[TP] = ans_cluster[j]\t\t\n",
    "                        TP = TP + 1\n",
    "                        k = k + 1\n",
    "                        if(k == training_cycle and training == 1):\n",
    "                                training_TP = TP\n",
    "                                training_ans = j\n",
    "                elif(ans_times[j] + det_win < spike_times[k]):\n",
    "                        FN = FN + 1\n",
    "                else:\n",
    "                        while(1):\n",
    "                                id_false[FP] = spike_id[k]\n",
    "                                FP = FP + 1\n",
    "\n",
    "                                k = k + 1\n",
    "                                if(k == training_cycle and training == 1):\n",
    "                                        training_TP = TP\n",
    "                                        training_ans = j\n",
    "\n",
    "                                if(ans_times[j] - det_win <= spike_times[k]):\n",
    "                                        break\n",
    "                        if(ans_times[j] + det_win >= spike_times[k]):\n",
    "                                id_ssp[TP] = spike_id[k]\n",
    "                                id_ans[TP] = ans_cluster[j]\n",
    "                                TP = TP + 1\n",
    "                                k = k + 1\n",
    "                                if(k == training_cycle and training == 1):\n",
    "                                        training_TP = TP\n",
    "                                        training_ans = j\n",
    "                        else:\n",
    "                                FN = FN + 1\n",
    "        #print(training_TP, training_ans)\n",
    "        print(\"# of ans : \", len(ans_times))\n",
    "        print(\"# of TP ; \", TP)\n",
    "        print('training miss : ', training_TP)\n",
    "        print('# of Error : ', len(ans_times)-(TP-training_TP))\n",
    "        \n",
    "        #print(\"# of FP ; \", FP)\n",
    "        print(\"# of FN : \", FN)\n",
    "        print(\"Det acc : \", (TP-training_TP)/(len(ans_times)-training_ans))\n",
    "\n",
    "        filtered_spike = 0\n",
    "        filtered_noise = 0\n",
    "        cluster_accuracy = np.zeros(6)\n",
    "        true_clusters = np.zeros(6)\n",
    "        noise = 0\n",
    "        for i in range(TP):\n",
    "                if(id_ssp[i] == 4):\n",
    "                        filtered_spike += 1\n",
    "        for i in range(FP):\n",
    "                if(id_false[i] == 4):\n",
    "                        filtered_noise += 1\n",
    "        for ep in range(6):\n",
    "                if(ep == 1 or ep == 4):\n",
    "                        for i in range(spike_index):\n",
    "                                if(id_ssp[i] == 3):\n",
    "                                        id_ssp[i] = 2\n",
    "                                elif(id_ssp[i] == 2):\n",
    "                                        id_ssp[i] = 3\n",
    "                elif(ep == 2 or ep == 5):\n",
    "                        for i in range(spike_index):\n",
    "                                if(id_ssp[i] == 1):\n",
    "                                        id_ssp[i] = 2\n",
    "                                elif(id_ssp[i] == 2):\n",
    "                                        id_ssp[i] = 1\n",
    "                elif(ep == 3):\n",
    "                        for i in range(spike_index):\n",
    "                                if(id_ssp[i] == 1):\n",
    "                                        id_ssp[i] = 3\n",
    "                                elif(id_ssp[i] == 3):\n",
    "                                        id_ssp[i] = 1\n",
    "                true_cluster = 0\n",
    "                for i in range(training_TP, TP):\n",
    "                        if(id_ssp[i] == id_ans[i]):\n",
    "                                true_cluster += 1\n",
    "                \n",
    "                cluster_accuracy[ep] = true_cluster*100/(TP-filtered_spike-training_TP)\n",
    "                true_clusters[ep] = true_cluster\n",
    "        #print('filtered noise : ', filtered_noise)\n",
    "        print('filtered spike : ', filtered_spike)\n",
    "        print(\"true cluster : \", max(true_clusters))\n",
    "        print('filtered FP : ', FP-filtered_noise)\n",
    "        print('Final det acc : ', (TP-filtered_spike-training_TP)/(len(ans_times)-training_ans))\n",
    "\n",
    "        print('Clu acc : ', max(cluster_accuracy))\n",
    "\t\n",
    "        return (TP-training_TP-filtered_spike)/(len(ans_times)-training_ans), max(cluster_accuracy), max(true_clusters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (DEPRECATED) naive 템플릿 만들기. 이것도 라벨 필요 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import math\n",
    "# import os\n",
    "# from scipy import io\n",
    "\n",
    "# my_path_ground_BH = '/data2/spike_sorting/quiroga/BH/'\n",
    "\n",
    "# # 데이터 파일 목록과 템플릿 파일 목록 설정\n",
    "# filename = [\"C_Easy1_noise005.mat\", \"C_Easy1_noise01.mat\", \"C_Easy1_noise015.mat\", \"C_Easy1_noise02.mat\",\n",
    "#             \"C_Easy2_noise005.mat\", \"C_Easy2_noise01.mat\", \"C_Easy2_noise015.mat\", \"C_Easy2_noise02.mat\",\n",
    "#             \"C_Difficult1_noise005.mat\", \"C_Difficult1_noise01.mat\", \"C_Difficult1_noise015.mat\", \"C_Difficult1_noise02.mat\",\n",
    "#             \"C_Difficult2_noise005.mat\", \"C_Difficult2_noise01.mat\", \"C_Difficult2_noise015.mat\", \"C_Difficult2_noise02.mat\"]\n",
    "\n",
    "# template =  [\"BH_Spike_TEMPLATE_e1n005.npy\", \"BH_Spike_TEMPLATE_e1n010.npy\", \"BH_Spike_TEMPLATE_e1n015.npy\", \"BH_Spike_TEMPLATE_e1n020.npy\",\n",
    "#              \"BH_Spike_TEMPLATE_e2n005.npy\", \"BH_Spike_TEMPLATE_e2n010.npy\", \"BH_Spike_TEMPLATE_e2n015.npy\", \"BH_Spike_TEMPLATE_e2n020.npy\",\n",
    "#              \"BH_Spike_TEMPLATE_d1n005.npy\", \"BH_Spike_TEMPLATE_d1n010.npy\", \"BH_Spike_TEMPLATE_d1n015.npy\", \"BH_Spike_TEMPLATE_d1n020.npy\",\n",
    "#              \"BH_Spike_TEMPLATE_d2n005.npy\", \"BH_Spike_TEMPLATE_d2n010.npy\", \"BH_Spike_TEMPLATE_d2n015.npy\", \"BH_Spike_TEMPLATE_d2n020.npy\"]\n",
    "\n",
    "# thr_tem = np.array([0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9])\n",
    "\n",
    "# dataset_num = 16\n",
    "# spike_needs = 100 # 템플릿을 만들기 위해 필요한 스파이크 수\n",
    "# spike_length = 50\n",
    "# wait_term = 20\n",
    "# num_cluster = 3  # 클러스터 수 설정 # 논문엔 4개라는데 여기서는 3개로 했네\n",
    "\n",
    "# for ds in range(dataset_num):\n",
    "#     print(\"\\ndata:\", filename[ds])\n",
    "#     spike_count = 0\n",
    "\n",
    "#     # 데이터 파일 불러오기\n",
    "#     mat1 = io.loadmat(my_path_ground_BH + filename[ds])\n",
    "#     raw = mat1['data'][0]\n",
    "#     thr = thr_tem[ds]  # 스파이크 탐지 임계값 설정 \n",
    "#     spike = []  # 스파이크 데이터를 저장할 배열\n",
    "\n",
    "#     # raw 데이터의 기울기 계산\n",
    "#     slope = np.diff(raw) # raw보다 한 사이즈 작음.\n",
    "    \n",
    "#     # 스파이크 탐지\n",
    "#     wait = -20  # 스파이크 탐지 대기 시간 초기화: 처음에는 20샘플은 버림.\n",
    "#     for i in range(len(raw)-2):\n",
    "#         wait += 1\n",
    "#         if(wait_term < wait):\n",
    "#             if(raw[i+1] < raw[i+2] and raw[i+1] <= raw[i] and raw[i+1] < -thr) or (raw[i+1] > raw[i+2] and raw[i] <= raw[i+1] and raw[i+1] > thr):\n",
    "#                 spike_count += 1\n",
    "#                 max_slope_index = i + np.argmax(slope[i - 8 : i + 5]) - 8 # 기울기가 최대인 지점에서 스파이크 추출\n",
    "#                 spike.append(raw[max_slope_index - 10 : max_slope_index - 10 + spike_length]) \n",
    "#                 wait = 0  # 대기 시간 초기화 # 다시 wait_term만큼 기다려라\n",
    "#                 if spike_count == spike_needs:\n",
    "#                     break\n",
    "#     spike = np.array(spike)\n",
    "\n",
    "#     Cluster = np.zeros((num_cluster, spike_length))  # 클러스터 배열 초기화\n",
    "#     distance_size = 0  # 거리 계산을 위한 배열 크기\n",
    "#     cluster_num = np.zeros(num_cluster)  # 각 클러스터의 데이터 수 초기화\n",
    "    \n",
    "#     for i in range(num_cluster):\n",
    "#         distance_size += i + 1  # 거리 계산 배열 크기 계산\n",
    "#     distance = np.zeros(distance_size)  # 거리 배열 초기화 # num_cluster=3이면 1+2+3=6개의 거리를 계산해야 함.\n",
    "    \n",
    "#     # 훈련 사이클 시작\n",
    "#     for spike_index in range(spike_needs):\n",
    "#         spike_n = spike[spike_index, :]  # 현재 스파이크\n",
    "        \n",
    "#         if(spike_index == 0):\n",
    "#             Cluster[0, :] = spike_n  # 첫 번째 스파이크는 첫 번째 클러스터에 배정\n",
    "#             cluster_num[0] += 1  # 클러스터 데이터 수 증가\n",
    "#         else:\n",
    "#             # 각 클러스터와의 거리 계산\n",
    "#             for i in range(num_cluster): # 0, 1, 2 까지는 기존 클러스터와 지금 스파이크와의 거리\n",
    "#                 distance[i] = np.sum(abs(Cluster[i, 5:25] - spike_n[5:25])) * 17 + np.sum(abs(Cluster[i, 0:5] - spike_n[0:5])) * 2 + np.sum(abs(Cluster[i, 25:50] - spike_n[25:50])) * 2\n",
    "            \n",
    "#             k = 0\n",
    "#             for j in range(1, num_cluster):\n",
    "#                 k = k + j\n",
    "#                 for i in range(j, num_cluster):\n",
    "#                     # 훈련 초기 단계에서는 임계값을 1.5로 설정\n",
    "#                     if(spike_index < 30):\n",
    "#                         mer_thr = 1.5\n",
    "#                     else:\n",
    "#                         mer_thr = 2.5\n",
    "                    \n",
    "#                     # 클러스터의 데이터 수가 10 이상인 경우 거리를 크게 설정\n",
    "#                     if(cluster_num[j-1] > 10) or (cluster_num[i] > 10):\n",
    "#                         distance[i + j * num_cluster - k] = 1500000000000\n",
    "#                     else:\n",
    "#                         # 두 클러스터 간의 거리 계산\n",
    "#                         distance[i + j * num_cluster - k] = np.sum(abs(Cluster[j - 1, 5:25] - Cluster[i, 5:25])) * 17 + np.sum(abs(Cluster[j - 1, 0:5] - Cluster[i, 0:5])) * 2 + np.sum(abs(Cluster[j - 1, 25:50] - Cluster[i, 25:50])) * 2\n",
    "#                         distance[i + j * num_cluster - k] = distance[i + j * num_cluster - k] * mer_thr\n",
    "            \n",
    "#             # 가장 작은 거리를 가진 클러스터를 찾고 업데이트\n",
    "#             m = np.argmin(distance)\n",
    "#             if(m < num_cluster): #클러스터와 현재 스파이크간 거리에서 젤 작은 게 있을 때\n",
    "#                 Cluster[m, :] = (Cluster[m, :] * 15 + spike_n) / 16  # 클러스터 업데이트\n",
    "#                 cluster_num[m] += 1  # 클러스터 데이터 수 증가\n",
    "#             else:  #클러스터와 클러스터 간 거리에서 젤 작은 게 있을 때\n",
    "#                 x = num_cluster\n",
    "#                 for i in range(1, num_cluster):\n",
    "#                     y = x + num_cluster - i\n",
    "#                     if(x <= m and m < y):\n",
    "#                         # 새로운 클러스터와 기존 클러스터를 결합\n",
    "#                         Cluster[i - 1, :] = (Cluster[i - 1, :] + Cluster[m - x + i, :]) / 2\n",
    "#                         cluster_num[i - 1] = cluster_num[i - 1] + cluster_num[m - x + i]\n",
    "#                         Cluster[m - x + i, :] = spike_n  # 새로운 스파이크 할당\n",
    "#                         cluster_num[m - x + i] = 1  # 새로운 클러스터 데이터 수 1로 초기화\n",
    "#                     x = y\n",
    "#     # # Cluster plot\n",
    "#     # plt.figure(figsize=(12, 6))\n",
    "#     # colors = ['b', 'g', 'r']  # 클러스터별 색상 지정\n",
    "#     # x_axis = np.arange(spike_length)  # x축 값 (스파이크 길이)\n",
    "#     # print(cluster_num)\n",
    "#     # for i in range(num_cluster):\n",
    "#     #     plt.plot(x_axis, Cluster[i, :], label=f'Cluster {i+1}', color=colors[i % len(colors)])\n",
    "\n",
    "#     # plt.title(f'Cluster Templates for {filename[ds]}')\n",
    "#     # plt.xlabel('Sample Index')\n",
    "#     # plt.ylabel('Amplitude')\n",
    "#     # plt.legend()\n",
    "#     # plt.grid(True)\n",
    "#     # plt.show()\n",
    "    \n",
    "#     # 클러스터 템플릿을 파일로 저장\n",
    "#     np.save(my_path_ground_BH + template[ds], Cluster)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
