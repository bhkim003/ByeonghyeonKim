{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33020/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    lif_layer_sg_width2 = None,\n",
    "                    lif_layer_v_threshold2 = None,\n",
    "                    learning_rate2 = None,\n",
    "                    init_scaling = None,\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp,\n",
    "                    ANPI_MODE=False,\n",
    "                    lif_layer_sg_width2=lif_layer_sg_width2,\n",
    "                    lif_layer_v_threshold2=lif_layer_v_threshold2,\n",
    "                    init_scaling=init_scaling).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                # lr = group['lr']\n",
    "\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        lr = learning_rate\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        lr = learning_rate2\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        lr = 1.0\n",
    "\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "            if extra_train_dataset == -1:\n",
    "                # print(inputs.shape)\n",
    "                assert BATCH == 1\n",
    "                now_T = inputs.shape[1]\n",
    "                now_time_steps = temporal_filter*TIME\n",
    "                # start_idx = random.randint(0, now_T - now_time_steps)\n",
    "                start_idx = random.choice(range(0, now_T - now_time_steps + 1, now_time_steps))\n",
    "                # start_idx = random.choice([i for i in range(0, now_T - now_time_steps + 1, now_time_steps)])\n",
    "                inputs = inputs[:, start_idx : start_idx + now_time_steps]\n",
    "                if dvs_clipping != 0:\n",
    "                    inputs[inputs<dvs_clipping] = 0.0\n",
    "                    inputs[inputs>=dvs_clipping] = 1.0\n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if extra_train_dataset == -1:\n",
    "                            assert BATCH == 1\n",
    "                            now_T = inputs_val.shape[1]\n",
    "                            now_time_steps = temporal_filter*TIME\n",
    "                            start_idx = 0\n",
    "                            inputs_val = inputs_val[:, start_idx : start_idx + now_time_steps]\n",
    "\n",
    "                            if dvs_clipping != 0:\n",
    "                                inputs_val[inputs_val<dvs_clipping] = 0.0\n",
    "                                inputs_val[inputs_val>=dvs_clipping] = 1.0\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "                for name, module in net.module.named_modules():\n",
    "                    if isinstance(module, SYNAPSE_FC):\n",
    "                        module.sparsity_print_and_reset()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    assert val_acc_best > 0.2\n",
    "                elif epoch > 10:\n",
    "                    assert val_acc_best > 0.4\n",
    "                elif epoch > 30:\n",
    "                    assert val_acc_best > 0.5\n",
    "                elif epoch > 100:\n",
    "                    assert val_acc_best > 0.6\n",
    "                    \n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 2871,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 128.0,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 4.0*2, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 200,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = -1, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[0,0],[0,0],[0,0]], \n",
    "#                 lif_layer_sg_width2 = 4.0*2,\n",
    "#                 lif_layer_v_threshold2 = 128.0,\n",
    "#                 learning_rate2 = 1,\n",
    "#                 init_scaling = [10000+ 9,10000+ 9,10000+ 8],\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAABQgAAAK2CAYAAAACB3HlAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAP+lSURBVHhe7N0HfBRV1wbwB0JTadKkB+kYEEVUioJgEBTkFUWj8AGioBRFUJqAQao0BVEEBaW9gLGhvKKUSFMwINICUoJIpPdeEgh8c2bu3Z3d7Ca7aYTs8/c3MjM7O31mZ07OvTfbdQOIiIiIiIiIiIgoIGVX/xIREREREREREVEAyha7L5YZhERERERERERERAEqW8nSwQwQEhERERERERERBSgWMSYiIiIiIiIiIgpgDBASEREREREREREFMAYIiYiIiIiIiIiIAhgDhERERERERERERAGMAUIiIiIiIiIiIqIAxgAhERERERERERFRAGOAkIiIiIiIiIiIKIAxQEhERERERERERBTAGCAkIiIiIiIiIiIKYAwQEhERERERERERBTAGCImIiIiIiIiIiAIYA4REREREREREREQBjAFCIiIiIiIiIiKiAMYAIRERERERERERUQBjgJCIiIiIiIiIiCiAMUBIREREREREREQUwBggJCIiIiIiIiIiCmAMEBIREREREREREQUwBgiJiIiIiIiIiIgCGAOEREREREREREREAYwBQiIiIiIiIiIiogDGACEREREREREREVEAY4CQiIiIiIiIiIgogDFASEREREREREREFMAYICQiIiIiIiIiIgpgQfnyF3xX9WdK99W6F2/27IEH7r8PW6K3Ii4uTn3iytfpmoQ2Ro/XuqFqlSqIWrtOjXVVunQp9O/bG0+2aI4iRYqY86O017PHa3g+7FkkJCRgz55/1Ngbo0CBAuj95ht45umnkjw30puv5zH5T1/7jzZuhJMnT+LQocPqk8CWJ3dulCxVEhcvXsK1a9fUWMqM/q/tC3ipYwc0ePgh7NwVg7Nnz6lPiIiIiIiIUocZhB7kve02XLlyxew/cuSI+S9lHfKSPXb0SDMY58nxEydUH2UEOR4TPhibqBv4dl8zcJtWTp48ZV7X0h05elSNTRvJnVOZ1SMNG+C9kcPMoPSwIYNxV7Wq6hPKzC5cuIBz586roczN/fpO6+uaiIiIiIjSBgOEHuzYuQuDhwxHzzf7YGnkMjU2dSQrceTwIUm+JMmwjNfTyPTyPTsJQEggQk9zMwYlbrQihQurPqczZ85gxHtjzGP+3znz1Fi6kYoWLYpBA/ql2fn954aN6NNvAAYMGoz9+w+osWnD0zmV2VWqVBEtmj+O2f+di4HvvGvc637Bix3aoWDBgmoKymzk3iT3KLlXyT0rs5Ms8dr31VJDlrS+romymuXLluLAvr2ObtzY0eoTJxlnnyZi3hz1iZP7NDJfu/r16+HPP6JcptGdp2XKMuQzT8sS+nNv83Bf3t49MejT+031qZMv209ERETpgwHCDCBBvm5dXsGtt96qxlgvSa91e9URJJR/ZVjGazK9fE8HCSUTo93/tUHOnDnNYSH9UkyXL1uU0SpWKG+cd/e4nI+psf7PDWbwQzoJ0B87dsycdzVmtaX5vha17r0HmzZvwYaNm8yMtF+WLcfJU6dwd43q5uf3318b7wzsj/BBA/DgA/eb4yjru+WWW1DnwftRovgdakzKyG9SqVIlzYxdCULra1uu82PHjqd5Fu/NQK7f2vfdi/J3lkO2bNnUWCKLDqBVrlRJjbG88HyYS5BM+mWc3UMP1XcJ3Em/+zQyX/cgoa/ke7IMb2R57p/b17v1M0/ji2lTUbx4cXNYyPXQvVtXR5DQ1+0nIiKi9JOtZOng66o/U5KXDAmAnT592qyTTeoPE/LS8WXE12ZWkLBPt3LVb/hPyxaOl+mFP/3syASUesiaP/E49u6NxYSJH7uMEzLtuj/+NIN1kkmjl6GnkZcbobMiLl68iE+mfIY7ihUzly/LdF83CezJ9HqZ9mCgXjc9fwmKfPzJp2Z2iGRflCsXbC5TMkeebPEE6tWtg2++nW/O2z4fPU1S9Hpo7t9xD1K6b4em10uz719P2yY87Xc9H/t0nuhje+DAQWz76y/HsRLu3/UUaNXT6Pm4B1n0fvC2Pu7fsx8j4c/6uQeLkzqP7cvIjOQFV4qk5smT27g2t5nbkhL6vHQ/H93PGT0s+/SOO+4wv2Pff74eJ/mOXLM6i9DX8979+pH1kGLL9mVqeluSWydfpdW+tpP7ScUKFcx9e/36deTOnRuDBvbHggU/msGbXm+8bgZ2rhmftf+/Npjw4cf4d98+9W3PUnqf1PvW12OWFPdrTC9bvqvnl9y16ut07lK6/Zr+vua+P9yvCSH3LQnE/WAct4YNHnKcxyk914RcEyF3VcU//+zFocMpq2ZDr2tS6+Hp+OrjJ/vIvn883Z+T+i3SkprG23H2dGyS+/30lQRga95dHUePHsM/xnGUa89fEkiZOOED5M2bD1OnTUPnTp2M/tvMz377bTXCXmhrBnR0oGVXTAwaNW5iBmlGDB9mTjvvywj07tPP/Fzo6fX3feEeFLJ/VwexZJzQ08m+nfTJZIwd94E57Ot0vrBvnzh//gIGDnrHeGb6zuP26fXX+0dIsEqCVidOnECPnm9i9eo1jnH6Pn748GHHZ1py08jy7yxXDjNnzUaL5k84AmX2Zet9YR+n11vPT8ixl+/rY6iXHRcXb26vkP0g9Pbredv3q95f58+fS7Q9dvZzSbifI3o+uXPncsxbL0+vd4/Xupv7Wn9Xn8OyHXp7fdl+b+tIREREaeOmySCUlx4dHBTyECYP9vKAbyfTtX6mleMhTTR6pKEjC8+dfP+xJqHmC9y4DyZ4ffHT5AXB/pIgL6G93+zpktkn/0pjF7JMedEKLlvGHC8vIEJelNb9sd7sD7nrLpd/Y//d53iR+tV4kJKHOfm+zOd/P/6EtweGO15a8uXLi9tusx6Ek6srUV7W7OstZFheeoSsa78+b7oE1tz3sUwjxZ7tL1tCXqr0fNKTLNf+8izsx1a2ccjgQS7bIOT4up8nvpLtcs/alPnLvnI/p5JbP5mXnCv2TFL7uXKzkRfbv7bvwOXLcbi7RojLPkpLt912q3n+aw8ZLxb6XJZA6u6/9/h1nOzks+TOeyFBBvfrR4519RDruvVE5i3H1n2dJBjpr/TY179HrTMDSx1fbI+ad9cwXjBfxdUrV7F5SzTKlC5tvpBJduGmTZtx4OBBlC9fTn0zebKv7Psrufuk3NskkCR/lJFsSa1QodvN6WTbfQkOynnSoV1bl2tM+u2Z2CK5a1XzdTp3/m6/rLdUL+G+LDlfOnZop4a8k3nJ7579PJZ+CQKnhPwGbftrB+68s1yKMwm379hp/q7Keni7DuXalWtY1l+Cp0L+lWHpdPaw7B+5D8j8ZL56f3n6LZLfAeHLNJr7cZZl2+8Byf1++uPSpUvGNbYVxYoVxZ3GclOTSSiBsF4933AExIQEYaRYpj2gI/0SDJNA0f9+/NEcd59teyRYkz9fPjOg9vU336qxSZNgjg7maTIsy7GTcfbprH37nLlMO1+n80ame7t/X5d9If0SuJIA1p8qaF/sDus8E7q/ZImS5jRC/vgky969+28zGCXb2fONHuY4TYJaEtzS6+bLNEI+7/TyS47goJBjI0EwIcG+UmXKOYJj4qeffjafAzX5I3HhwoXNYxUVtdYcJwG5f/buNbe3Tp0HzeNcpVqI2Um/mP/9D+Y93RNZn6++nJviIr2yTFm2BFXX/B5ljtPLk3WVdZagoGybDizKvl2+YqXZr/my/URERJS+bqoixvIXe/fihw/bHig1ya6Q6STgJy8U8nJWrWoV9amTvLDIC5r49rvvfXoBFXo9JBNBk/WR9dLLlHXTLzxCv9housEEe/BDhrcbL8KaZPFY09xmBgM1CVZI/YM62JRURoumA5Ayrd6Hq9f87siAkMYKZF56O3RRMNkO/ZLmaRq9DyS7KbmX5rSQ1LGVfbBrV4xLcTaZXm+DBFaHjxxtrr/sVz2dtywQ2R7daIN9v8n3ZbmyP9wltX4SBJHP5TP3eXk6P28G6RUklH0vgRhhD5oLuV70NSh1scm14e9x0nw57yU4IAEE+zkjx3DW7DmYaXTezik5pjJvfU5IJ8V45Tspkdb7+vjx4xhjvFhKwEJejmNidpvbFR8fj3379psvjVIM+Z57aqJ0qVLmcfCHP/dJOb4yf/t+F3Lfcr8vJkWCixJktB9Pybj2dH9P6lq183U6d/5u/6LFS837l3xHOjmXZNslkOTLvdV+/smyhf7jUkqkNkgo+1sHAGSfye+V/G7Zg2qejrv+V85LXbenPq6SdSfzlVbeJfAo+1D2pX0/6986X6ax08dL3ztknfTzRXK/n/5KyyChBIre6Pkmnnu+jUsASLLLJNiiM/N0MEyCSvIde1BMB50OHjroCCglRwcY9XJk+UuXRrpklmmyDjLNhA8nmuepDhq583U6T/Q2yD6QdZH5TPv8C6P73Nwm9+2WTvpPnTplZr7JeS5ku2TZ6//80wzu6d8hvW56P+t182UaO328ZDrZd0IyC3UxW290wFJzP1ZHj1hF9uWPO0mxB/Fkm2Xb3UkWnz1IKAE7WV/J7EuKfR3l37Pnzln3uDuSvn/o4G1S3LefiIiI0sdNEyCUB3YJsAh5qdAZeO7ZRTLd9JmzzX55kZAXCk9y5crlyDRZYjzU2osSJUVeFvVLgc6QkE5e+mW9pGVJqc9LHook+8We5eeN/OVetiOl7C878vKlGzCRTjeGorMXdfaErOvX31gPl/K5vEjKQ7G8pMpnYsXKVea2yWfy0KkzIWXf62mkOLbsc19fmpPibd01X46tFFOTwKpuEEZnj6SkEQkd4JHlynYK+7nn/vKd3PrJd6VYoAQEpHEZe7ajtwdomb/sB/t+SUnWSnpKHLjKoT7xj2Tk6G3UwW85/+Q8tJP9rO8Fwt/jpPly3ss0+vqSzDp9n5DjK9l1SdH3BzkHJagvZL3luymVVvtak3N03pdf4Z3BQ8x1k3uXkKLEUsSyefPH0bJFc0R89Y1Z3NRX/t4nzemM7ZJjofe7BMUkgKKzRH2hM9LkupJMTZnPb8ZLpfv93Zd7ifB1Oncp2X5ZR7l/6fugzjY0X7Btf2zyxv47pvdlasm6OoKEJZxZT76SP9rYA5ZCrnP7vV1nv+t7tPwrx/DUqdOO4KjOJNUtzOtrUgKQ+nrSv0X6O75Mo8m4pJ4vkvr9TCl7kNA9y9FXcox1AEyCJxJEERKo0sWH9+3fb/4rzxkSzJJpJbiks81E7fvuM/evL4EaTU+rg0my/Bdf6mSOs5Ogkg4aSmBKAlSe+DqdN/o79sy9we8OdRRRPnT4MKQorQ4G6uDY3+reIvtAviP7SYrqyv3OHnSc+PEkczrZTjmnZH/Jd3yZxk4yOHVgT2fZmde4h2cAWR/JopT10ZmdOsPRX1LEV/aNrJcOtMm++XjSJ46ApT3IbM8wTU5yQUlPJED7ZIsW5vJkP3jiafuJiIgofQVsIyUlS5ZwBGceuL+242UlrdlfBL295MlfWS9csLJJ9MtiUiTIZM90kJeL5IJG8qKmsyfkJUdeQPVLmg5iyvLlpdQ9SCPkRUmmkRcSyX7U5EVJ1l0k91fi9CYvfBIYlPWWIIr7i6m/9PbI9sl2as7sT9fMzuRIcTUJDMo+lpd5WT8JJGQF2bPLreS6mQmTLVva3FZk33hqcdj9eKT0OPly3tuD98kV43cn6y0BHx0klHmnRavjqd3X8r3GjR4xW5Id//4Yx3ZLJ8N93uplFjleu+4PfPrZNMz9MgKxsf+qb6cfCW5JXXC6mLEO/LpnkCZFppP67uS+KPd3CcLLdklQJ7PTgUHJhpWMN51leKNlz25lt1nnXcpIsFTudzorUo6NLv6sA6hynVWtUtkMmMkxP3jokON3U4J98j0JfMpvlr4m9W+ZdPY/uOTLK9d20tP4I6nfz9TQ+zZHUJD5b0bRxTYlCCRBmIoVK5iZbbrIqi8kAKkz4CRIKEVTpYEJmd+NIEEvqaNOAk72IrM6E04+lwCqeU4ZvxkSuJPA06+//Wb+KxmWOtins/N0MM69CK5sr+bLNCk1fNgQc772oKL8Dsmx0wFfX8g+kOLbEoTVgWNt3PvjHfPWgU3hz/x1EFr2oS9kvlIcXAK0X0Z85QhYuvO0/URERJS+0uZN/iYlAQjp5IVBiiOlJ3mAtAf/dEaEt2CHkBcja5oLZqDRnXzPPUtKv4jpTopg6vnrbA4dWNQvaTqIKQ+duoia+zykonr3jBdhf1nzN4DiLql194UuLiovcjp7JzX09sj22V8EncfO83HxRhdXk30sx8IXsv2yH+z7JS22LS0FGS+3VatUQp48eRC99S+ziGpK6CJ+utONMCQnpcfJl/NeXnzkGhX2a9NXEiSUIKfMT7ZP1qdZU2f9Sv5Ki30t10nzJ5ph46YtZgDz40+mODoJCEr2oNRLKEGDVzq9hNe6dUG/vm+Z3e23F1RzSR+SqSX7SIqZyv7WQSF/2K8ZHdTxpd7AG0nWTQKDcl8ePfaDREHxG0WOtwRq//13vxm8TS0JAksmrtAZg/o6lGB9mTKlzeMv17Q+7lLEXa5tORckO1yOr74mdZFfeyfXmwQdk5vG333s7fczpWSbalQPwYkTJ33OkE0rOttOitjWNeuPy+dX8WJN1xmnM88kmCOZajeKBJruu7+OS/FdyVLTRal1IEsy3iSYJRmFv0etNbdd9oUEouX808V1dTBOtk0XW7Z3kvHoyzQpIXU5Sv2Esh3uQT0hx6yEsb81HZzT26hJ0WXZfnvDH+nFHlSUf2XYvG7Vb7QmgT8JxCbVCE1y209ERETp46YJENofxuVlylv9ZL6SB3wpOqYbAkmvF0hZN1lHoTMVpZN+oYsu6X/lJVGvhwSU5GFVb6MUU3TPWtDFqNwDje4kO0UXc5TpdGBRXtL0Osqy3BvMkAYhZHmetsPqd9bzJC9lMp17QEUypqShkPSmXzjty5VMKE9kW+2BTk90Zos9gGw/dv6ce/I9eSG0L1fXbZdSMk85H9yz0vwdnxr2gNXWbdsRFxenPsk4KT1Ovpz3Ql+bci7p/SafPdvaeunU3M8pmVYylvR8ddFlCYSkRFrt69BHG2H+9wvMwInUOyhZNbrbsXOXWZxYsgdDH21s1q8oQZFRo8fh6tWrZvZUetLHslLFCmbnT/FiIfta9rk+TrpIqRwbX4rp3ij6j0H2bNfWT7dyZLLeCPbg4P4D/gcs5f4mmXb27Ha5buSPWUIXF7Zfh2XLljV/j+U80HXwFr+jmLlfpHi3Durpa9K9ASqpM1Nfb75Mo9nvHfKZ+/NFUr+fKeEeHJSqAzKSzqaTYsbNmz9h/utP8WIhARzduIY980wHqjKaBMGiN29w1OWni+/a6xfU9RCWMc5BCV7JPpB1l4CgTFe5SmXznJP6B4W92LIEtTQJfg15N9zs92UaOzm3dBBNF/t1z96U/SoZf56CY3p5csyebf2MOU62WeoxdJ+PZA5K4ynSgImn4KAsx571Kf/qc1/vG1/o/Srb0uqp/5jj5F8ZlnWVdRYyf1merGtSwcGktt9Xsk/27olJlNXq73giIqJAc1NlEOo6ypKqn8xXOqCmMxpkfvJClh50YEBeQuTlVRdzkhdXXWeafpGV9ZDtk+2U4JHeRnlpkaJXeh7yuZ5G6JchT+S7EnjUxRyl0y/6+iVNr6N9+dJJy5j6xcl9O+zzkSK9+uVNz1MfL3vLnXZ6utS0Mmzny3LlmOsApi4q5q14tmyPbJfQ0+pj5++552m5et/Z6ZdiWUZyrZfqivtlG+0NO/g7PqXsAast0dtuSHBQpOY4+XLe24NMuiiyzL9+vbqO+sg8nVMS4JeAiJ6vvm9JtpS/0mpfy0uxBFv+2r5djfEs2lhGGVsQ5fCRI/hl2QpUMV6g05McSwkEyX6Tzp8gvJDsSPme/TjJeSDXlFxbmZW+7t3v/zeKDg5KPWwpCQ4KHaC31y3q7bqUDCO5viRopjN+9bkg08t+0fd34emalK59u7aORol8mcZOX7v6OtXr6Mvvpz90cFC27UYEBzUJgsk5J1la/hYvlmDcPffcY37XvUitzr7LaBIsK1SokBkQk/WR4r4SoNL1CQpdD2Fw2bLmZzrbTgcEZbx9envg076tMu/nw8LM/eDLNHayXF0UWYJgYtOmTY7sTQmOyXyELrqtOwnK2pcn35fxugVl+3wkOKiPiX29pJMg2Nv9+5lZk/b10ftMzgdd558OnMnner30cnWAWJYpyxZ6nfWyZV1lnSXoJnVDyvxlXfVx0p0uCp7c9vtK16vp3lCMv+OJiIgCzU0TIJTiefb65OTBP62KYukggbwA6CyBtCTrqOsi02T9pa4s/fIr/+q6szSZXr4n35dOikXpInOaPOAnV2RVf9e9vjvZn7q4qrdpZHnymZB/ZZ/b11HINPZir9Jvn4/0e6pPS1ekLw9laRGw8rRcXeeVnWSO2rchqSwQmaf7Pk/puee+XNn/7vOWeeqGENwr0nenG2SQ7bMXw/R3fEqVCy7rCFi57+OMltLjJJ8ld97LtSlFVj1No687T+eUFJH2tE5ynfsrrfa1XGvi0qXL5r/eXLh4wVyeXXxcHIJSUQ+dr3RWd0rOUzkP3K95+300s5J1k5aW9XrLv7Id7udcRrjllltQpXIlM0giVUuklFwzujiunWyTexFfnTkq9B/vhA7AuZ8L+pq0PxMIGdaZcL5Mo8k6Rm/dpoZc7x3SJff76Su5/qSeRbnH/2PM70YFB4Vkb0lmmfC3eLFMW6VaSKJWbaVxlJQWqU0tWa5u/ViTQNfAQe84tk0CVZIZJ2Q6XfRVznUJDAr3fSEZbNKIh8xLk/5xHziz33yZRpOWnu3TSZac3mcSJNPBsaTI8uR7dvZ9L0FJKVqdlFW//moeQ/f5SNblS506+3U+CFm2rIOdPQNQZ0smxdft94UOgNszGIW/44mIiAJNtpKlg2/cEyoR3bSk0QJ5wb2RL7n+kCzV58OeNV8CMnvAyF1a7mspYplcC8w5cuRASMhd2Lx5ixpjtfwuRWHd67hKa/o4STDb/kcUyjiSsZqQkKCGsq4bca5lpn2rs7VSU5STfMN9TURERDeDgG6khIhS7tq1azdNcFCktHGZzCAt93VywUEh9Q3ag4NCGkRJ7+CgcK97VUixYak/Uxfx9NS5189KKRcIwcEbJbPs26TqrbMX6/TU6eKgGUHXW+dpPXTHeuMCA88FIiKi9McAIRFlaVJMe+TwIY56w+xFGCnzkOodJNAnVT1IcdOk6o4kopTRdcp5qreOiIiIiAIbixgTUZYmDYno4KCuA5ABwsxFgrjdurziaBziZisCTjenQCzOLg096MYxbmSdgYGGRYyJiIjoZsAAIRERERERERERUQBjEWMiIiIiIiIiIqIAxgAhERERERERERFRAGOAkIiIiIiIiIiIKIAxQEhERERERERERBTAGCAkIiIiIiIiIiIKYAwQEhERERERERERBTAGCImIiIiIiIiIiAIYA4REREREREREREQBjAFCIiIiIiIiIiKiAMYAIRERERERERERUQBjgJCIiIiIiIiIiCiAMUBIREREREREREQUwBggJCIiIiIiIiIiCmAMEBIREREREREREQUwBgiJiIiIiIiIiIgCGAOEREREREREREREAYwBQiIiIiIiIiIiogDGACEREREREREREVEAY4CQiIiIiIiIiIgogDFASEREREREREREFMAYICQiIiIiIiIiIgpgDBASEREREREREREFsGzXDarfo72x+1AuuIwaIiIiIiIiIiIioqyEGYREREREREREREQBjAFCIiIiIiIiIiKiAMYAIRERERERERERUQBjgJCIiIiIiIiIiCiAMUBIREREREREREQUwBggJCIiIiIiIiIiCmAMEBIREREREREREQUwBgiJiIiIiIiIiIgCGAOEREREREREREREAYwBQiIiIiIiIiIiogDGACEREREREREREVEAY4CQiIiIiIiIiIgogGW7blD9Hu2N3YdywWXUUOpdvnwZV65cwbVr19SY9Jc9e3bkzJkTefLkUWOIiIiIiIiIiIhIZGgG4fnz5xEXF5ehwUEhy5PlyvKJiIiIiIiIiIjIKcMChJI5mJCQoIZuDFm+rAcRERERERERERFZMixAKMWKM4PMsh5ERERERERERESZQYYFCDO6WLE3mWU9iIiIiIiIiIiIMoMMa6TkzJkzqu/GK1CggOojIsq61u7ajv+tW4Pf/orGjv3/4vhZ6z5cJH8BVC1dFg/dVQNPPlAPD1auZo4nIiIiIiKiwMQAIRFRFvPT+rUY/e08rNq2WY1JWr0qIXjt8acQenctNYbIu+zZsyNHjiDkzJkTuXPlQlBQ2hRGSLgeh7iEM4i/dh5Xr13GtetWlSDZs+VEjux5kCt7XuQOKoCgbLnN8URERERElHYYICQiykJenfQ+Plv8oxryT4dHHsPY9q+qISLf5M6dC7fdemuKA4USGLxw5TAuJ5xWY5KWJ6ggbstZnIFCIiIiIqI0xAAhEVEWcOr8Ofxn+ED8+le0GpMykk0447W+KHhbXjWGyDf58t6GPHn8C9pdTjiJs/H71JB/8ucqgzxBhdQQERERERGlRoY1UhI4DmBqy+q4s5LR9VmlxiVt37Rnrekr9cNyNc67c1gzsi0aPNISXSP2qHFENzP/rxlKLC2Cg2LNzm148eMxaojId+fOX8DFS5fVUPIuXj2a4uCgkO/KPIiIiIiIKPUYILzZxH6LkdM3Y9+BPVj04XfYoEZnScv6qcBpdfRcpsYRUSKdPxqbJsFBTYKEfWZ9qoaIfHfhwkVcvhynhryTzMHzVw6pIc+i//4Dl+MvqSHPZB4yLyIiIiIiSh0GCJNky2xy66o+0BTPdJ+IRbHJvwilqdIN8J86+Yye3KjYtjnSr0mBc9gwtx+eeeA+a5ur18UzgyOxz8vmnt00Dz2frYuq5v65DzWfHeFl38Rh38/j0La5nlb2ZVv0nLYWxxLUJFqFaghRvUTk2Y/rojBt6U9qKO3MXLEEkVuy9J8gKJ1IJmFCwjU1lJjUOZhc5uDpc8cx++eJWLXpZzXGO5mXzJOIiIiIiFIuU9ZBuG37TsyJ+FYNJS9njpx4o1snFCp0uxqTNN/rIJQAYVOM3K4GPSqCsGmLMaqhrnfJ9p2nPsE/YxtYo5MgRYwbjJYvNMcXMaPRyBp9A53D8j5P4qXvj6thmxq9sfTrF1ExSA0bzi7rh9BXF+KYGnYIqoYBP32NzuXVcNx2TH3x/zByvZcXOfd5x85Ai9Bx2IZ8aD/vdwyprcZTFuP/NUNOD/d7Hb9t36qG0pbUR/h9v6FqiMh30nBJ/nye67E8Gx+bbIMk6/5agY27fkdCwlV0e+YdNdY7abgkf65gNURERERERP7KlBmEOXPmxN0hd/ncVatSSX0zHUngImar1e34E+tmv4IQM5B1HBHTPATHbmbrJ6KfGRwsgmajFmOHsc07Fvawtjd6HF76yBYxTdiM9wdZ21/08aFYtdXYP1sX4K0a8tl2jHxlIraZExpyV0OjR8sbEzbAgNkrsGOHTLsCc7pUsz435v3+ElvwMIf6F6VRuqjqJSKH33dsS7fgoJCixhv2xKihQPEHRjRpjhG/q8GM9vtYNGoyFlFq8GYVFxfvMYtQMv18aa04Zt82PFSzKU6cOYqrV+PVWO9knswiJCIiIiJKuUwZIKxcsTyef/YpvzpfswfTRFBuFK3TA689robPnIXHatklqfBwJEa+2NRR9LZB9xnYcNb6ODlnf+xpFe+t9Ah6LjmnxtqKPbs06LAKPc1pX0bEMSnGO8JWPLgp2k5YC3OxpzZjaveWqFlVppWiwOOw/JQ5A4flEfOsgGftVzH0mVLmZuSu/Aom97YCeftm2+o+XDkXs8yJa+K1AU+jjDlxebw2vrdVPDh2Hr5aLz2Wip2+wOZFn6BznSLILQHH3EVQr+er+I/1MfYdsGUtliqPimaP8a9LYohb8eeqdfHAsyMQsV3vI++OLRmHtk3rWt+T7W/eE++vPADrtdK2b3tFmmM0Z0MyPfHDRTVSXFyIrub4uhjs9kZ/bPVEdLUv69l+mLXJyzoeXov3HcdFFbueu9k6Zg56/YxlGfv07PoZzvlLEfA+32F3ku/Habt9Z9e7FStv/jJG/qz3pY2uS7L7QsQlHMeiwW1Rs7p8pyU+TrKdnXP4obtMZ3T1+mGR7TxN+jha9DbVHLbZw3nfDxG7Eu+suNhVmDroZTTR51YS05pObUfESGP6+9R6msdhHja4XVN+nQt++D7qN9WXfhZv+kP1eXHwO7zSpDkauXQ9EHFQfU4BKy4+cWAvLiH50gTXjf/+3v8XKpaqhsplq2P3wSTT+B18mTcREREREXnGOgjTQoH8yKN67cr89SkeeKQnpq7WgYs47FsyDs90/y75jMM9M9CmtxXECen3BSY8JvUO+uIvTHuxKRr0kCCFCmrEHcCaSS+j6zBj2Q+3xcgle3DWrO8vDmc3zcBLHWbAWRvUZqxQDYJUbPgg7Il7ZR58GGZh87NrsUYFdjasVkHKyg1Rr7jVawp+EI1KSc85rFlvjwLlQ/78qlc7dsyx/AL5dFFtURoVKxv/lCqFYtYI04ZhT+KZwQud25dwDsc2zUP/lk9icJSXQI7hWMTLeKD7DGPddWDG2P5dkfi4U1O8NFcCk6VQr6G50sCqtbYGYOKwYaN+QY3E7/Z4yV+bsMbseRC17jZ7DMY2D2uJB178DIvsy9q0EIOffRI9l7kGhs5GjUCTR17Gx47jYkx9ajN+GNwWoX1WuQUJxTn83OsR1HxhnHP+ceew4ftwNHlhBna71+XokLbbV/OFEfhh0znj20L25VpM7dEUDw/ytM6Gfdsxq39rdJXAp3wp6B6E6OLnHuye9hJ6LjF6pKj6f0ejmfobQPLH0dXZn9/AA3Xdz/uFxvnyf5hqOzUlIF8ztBtGRqzFbn1u2aeNVaM04xp95uFn0X+6Mb3eYPM4jED4zO1qv/h3LvhrdTpmD2prY3aovqRUQJeZC7F8qdXN6wxM6XDzZ8FR6ly5ckX1OcVfO6/6vFu18SeULnoncue6BfVrPoYlUd8Z80o+i9CXeRMRERERkWcMEKZE3Dns/rYnwn+UgSJo/8bTLoE0bV/0ZuCxoVi6WYol/47JLVSQL2oevj9g9Xp0dhV6/t84bEsAij71CeZ2SiKKkoixbruARv3mYPOOrdixZjT+owJya2bNwIb8oRgV+ae1Pk8VsT7YPg9f6QZQE45JQqQppLLbcouXtgKE2IPde80eHD2pAhx3VVXZflpRlFZVV+7+e7/V487Yj/uiv0P/DiOsYFXRp9HpSbVOptzILfHCQvnhjCmuwldzrCBQSM+vHcWUfxz1NOp1/BBD6tgDjHYH8H3EWrMvf4vRWCdFoXf8iaXTXkGjFkMxoY213JB77zH/xdlN+FMfo4S1WP6L8W/+fOZ6rFnvzGbZvV5lZtZpgHq3mqMQt2w4us7aAwSVR9jEBdY6Gsv6ccCDxveP44eR053Fri+uQnj3eWZQr2LYBOtckSLdC3qjnrGwY9+PwFQPyTPHDh83pzeLdBvH8ltbMe1+EYmDZFqabp8hpMvn1r60FRU/FtEN/X70EPjaPgMj/1cUnadZxdb/2TzQa32bUq9lG7NeziL4zydfOOux9PE4ujh2HMeCn8ZkOe+N5W6ep6oHkCLwA1W2rCH/4x0RFlwe7Ud9jXWO46CK1hvTfjzLuJ4147j1N67RDRIF1EXmZZvWzcGAjsZ6vF7NzLz161xIgV0HvFxbbrIZXVD2IOTMkcPscgQFGeNkbPJiDvm2DLviz7VDKFbglxtVTJcyhatXE/+14uo1j/n2ZmvFm3evxcSvBmP99l/xdKOO5ngJFDa6rwU+/Pod/PLH9zh6yntqqrd5ExERERFR8jJlgFAaKRnw7kifu8HDx+LkSbcyfWnt+26qeKBVjLBJ/0gckwZKPv2f98Yzag/EwvFPo6IEVoLyoVnbFirQtR3bdpo9iSXswdQXu+EHiVrU6I25oxqo7/iuTKcP8UWnmsgfBOQu2hz/aaw+QDW89d8JCAvOba1P+6dVwO8A9ukoyf792K16EylaxC0Qanzvb9WbSBEUs2cUulnex9qPDZ4OR8QeY9aNe+PbhUPRSAWhLKXQ+but+Oe7F9V6Gi6ewxn1zlmsSGlHMeWQZ4ZizoCa1gcencXZk1ZfgaKlkV+iN0G5UbFhD3xhHCPHdtVpgGZmz3Zs2KiyyP7ahA0SwGvRAtWNwX0r16qMxzhsi7YCZWXuqabmcRzfT19oBtVqhX+BUY+Xt9bRWFZIx95oLwl8sQuxSAVkj/1vJn6QiY1zZe7wUOtcMeSu9iIGtJGJD+CHnzxECFtMwFJjerNIt3Esa701FK+p5MANP6/ynqGahtuH4Fcw+q0HUVTWQYqKvzXZcS0smuA58NVs3BcY0NAqtm5Ffz2Q7NluVr2WZvZsY3v2rI/H0UVzTP5pKJrJeW/IX7sHRndWO2t9JJbpnRVUE0MiF2DIM8a2Oo7DK46qBM7+sdGR6SrHLcL8Xj60/0AVmZfB22ui8wDjmpdj7ue5kBInz5tHIlk5cuTA8w83Qv9nXjC7tg1DkTOHuZLJOnk+dVmOlsOI6GIrgjzalqYqde51+Q6Hzbr3jM+k3xgdNdo2vRqnHf6qh/Mz9+LM7vPz8P1ExaLt6+OV2gbbtNZ62DMlpf5C+/pY9Rl6W475fWOcY3s8rYfajle+srbAddtty1bbHeXtc5Hsdns/Tl6Xm4Rr1xLXQXjteuKswn8O7sLgqV2wbP0CNLinGXo+PwIF8xVWnxo/hRXuxxvPDUfOnLkx/stB+H7lDPWJK0/zJiIiIiIi37CRklQ5johX6+IBb0UqS5dCUfs7eNFSzkCXR+fwQ/+XMNIMGFTDgPGurQX7Kn9hD1lUpvKuRTrz2zPzbqxjq+dh6rfude55cGsonguztm/5oLqo+XRPvP/tdhzTJUK9qobnOqg6FKe3Rc3Ql9F/2irstte3J259EI/UsXrXbLSKVu5b/6sZGLqv8dOoJUGd7ZuwwfzeWvwumXfIh0YNVQYf/sLv6s15w+BHnEFls3sWH5tZe86A7Nb1VjYc1o/AAy7TVkeLKVaK377DHsJ9OXKpHq0aQu5VvX9tdwSyEknD7SvzeBOrnkmHIqjXUJ1gsZuxNdFqV0OtGskUlT+10JE9KwHyyYmyZ308jm7MwJxNSA2VSWlsz19uRYfjjm3Homnj0PXZpnjgvuroamYKGw4ccAReHcctfwv8R+3PxPw7F9JTULbsqFCiJO4tX8nsKpcqg+zZ0+/2f/ir2YjEI3i0rjUcNfplTLnzXUcR5PfwriPgZfr7c7yw6hHr8ylPo/jvY/F25CN4T02/vMPf+FxlI0qg6oWpZZyfDS2DKR3cgoT2+S19F6HGcLhjeYcRMfRvvKS/P/NlVIp814dGSYojrMMjwD/7VLDxMFYuAypVsGVK/r4CkRUeQcOSMiDBwXcR2/lztR6fo8s/7yYOyhnLDscAa5p+96uRigT9wveZxbc/e664GeALnwpnce6ZFfCL2378Qs/L6N4LXYG3HctLbrslOOh+nGZb+9VYD9d9DrztHnRNhTsKlcTD9zTD1YQrWLz2W2z923UfHT99BP9d9BGWrpuPkDvvRc1K6sQiIiIiIqI0kykDhJmykRJ7K8ZS9HDdAkwOs4IXZpHKn5ONUPlgFX4wWw8W2/HxhMjkA2ZprXRpt6LCNlJUU/VaSqFMBdWbyHEcTeLtsdFY2Y9/YvOaBfiiewMUjTuARaPbos00K2PNu9xoNOR/xnceNIOvZ6Mj8XH/Z/FAzUfw0tw9qt43z8p0/C+WjmqOirmBuNi1iBjdDU1q3ocmI1c56v6TIFf1e6zssrMbJWMsDhvMIrcP4t67dBAuEssl8LNnB/40v/cgat0l/xoO7PGegZnIAez2moHpv9y61eezZ+G9qv602778BRKHl3Pn0FmBZ3E2maCdRysXWtmzIvpTvO9onMfJt+OYDEcL2eccReolezfi1UdQtd6z6Dp6BhZtOoBj9gvQsV9tx61UKS9Ziwa/zoWUKZTXtxB/3NUreO+buWjz/nCzGzJvBuI81A/nSaG8vtR/+jemdHBmnlnBpD4wY6cHv8MXEuyzBb/qtHsZWLbGFmBy/dyyD7E66Fe3DwaaMSEJyv2NSp3bWfMWxmfvhRrLn20PKtnndz8eDVW9puIIm6LWTZSsh0eN+1jsPh/CXXUfQejfK7DSXK99xvE17juNKzi+G7VqBSo1rmcswSDBQvlcAnsmY7nhEpRTQTetwssY6pjGRrL9wlcgdOhEhJkBR+1v7NZB7ZJPY6DLd+3Lk4ThR4zLeYXK9ktmu3+fjSl/ux2nfrLsw4iYKeth+27ddugCvR+88xSEzp4tp+pzujVPXrSo/wL6tB2Dpxq2x9I/vse6v1aYn50+fxJTvhuO8qWq4J2OH+H/mr2OO0tWMT9z52neRERERETkm0wZILwZ5L69PJoNH+0o1rlotcooSrUi+E+/F83MrLM/DsfIlWkRePRDUCmUUdu0bZdbsO7wfjPTzMxWU4HBMqXVxH/tcAuGHMN+lcYWUsVbFDE38hctj0Y9P8GENlYQYtvkebbGM7wIymd853Os27wCP07sjf9Uzg0kHMfywS3RM8lAbW5UfGY0lm7+E0tnDzSLhUox2t3Tu6HFR85ivCENVbHu6O3YlqCy6ErVRPWiwH11HpRPzEY94v76y9pmW/18zpaXgXrDV7gEle3dBLPYdylU1LumzlCs8zCd2Y1toCZKSpwzIFe5QpKZqmm1fWcdkTVP8iO/S3FxPxRtjgGdJEvwHH54dyyWJwo0+nYckxJ3UZ8nxvaUs/o2jHwJ/ZdJgN6Y/1O9Mfm7xdi8eSu+eMr63Ml23GxZhYn4dS6kTOVSpVVf0q5fv44rV6/icny82cUb/TLOF5VK+LIM10ZKluvgoIj9GzFYgbcdxVONrsPnxrgk1O1jzKMdduugoyMLbp8ZnA0u4xpQK3encUAcmX2+cRaZfRlTfA7UlzGOvQrQSQDwzjKo89AjKth5GLH/ONft8D7jBlihAtTpZSlZBsH2AJ9Xxv7qsEJ2q2vgsuTT+Gzp56g4U+2X5LL4giugkj3QavC23R7X12Tt88hwtUw/9lkOD8XYc2T31KSXU5Wyd6Nj8174eU2EcY5ew+rNS1CnemM0vLc5cuXUf4DwLLl5ExERERGRdwwQpkbCWZxVCU75cxewelLJrHOtU28MbS9Bj+OIeHucWT9cxqmGWiqBZPfKtS7Bj22/LLQChPnvwX3B5ihnoxe7VmKN/U11+1L8YBahzIf7VMaa2L3eczHiArpp4ySz3xS9P6TuwcdfxIQFCzBA1323LIlArf6e1FlX5wUMmL0YX4RZgcl9xvdUPBO492E0Mt9rt2P3EpVF99A9ZtC26P0PmkGffZu2Y81OKxjlrJ9P3IVaal3WzPkuiRaFLdXvVfUmRs0z62JMsYu/YpFZHNiQqMEYN6ncvroq8rPv56Vu9Qwex/JfVIAu2Ao4+k21WNy59yC0l+8f+w793rc1DiJ8PY5exWH5EquFcFleiHkub3Zk75bp9AWWjn0RzWqUMoOccVfN0S4qVlFFrs/+iB9UMeLE/DsXUqJ+Nak1Mn09WKmq6kshM0hlKy6sOylKrCbx7H4MNKe1iuZaRZIlQJc422/vP38Dd5ZJZn6aVS+go1ivzF8HfJNVHA0bV0Dkqj/MbMHQBsbNUjLxJJvu9zX45W9nseriZcoAf/8N1Z6T5eA+xKICKqr7Z1Ikc/CzKe8ieOpI14xDMxPQ2ofv3fk5XnAvsmxnBmfLINhR5Nn7dntcX5O1z0OHWst0du6ZjYlJdSHucmXPq/q8K1zgDhQpWByHTuzD7n1bcXfFB9QnSfNl3kRERERE5FmmDBBu/WuHx8ZIvHXSSMmx4yfUtzNG3OHtmNW7H2aZ0a58aFQ/lS/RpuZ4S9W5VuuNPmgmQZxj89Bzgm9ZUWmlUdgLVkBo/USERxyA5FrFbf8M/VSdeCFdX0Qts8/QsI0VyMFmvD/sO+yTiS9ux8cDP7MCNdVeRSfdiMueGej5f23xQPMRWLTrnDlfJMRh98px6DfVmjfqPGg2lOGVtB778H1oMjgSu89aWWBxJzdh2y6zF2WKe4tKHcDUZ+/DA6/OwJrDKnvs4h5s26IivGWKopjVBwTdhbpmkHQPls9aaWbRNVOZdSj/oNm6MKIiMXu7rLO9fj5RBGGdm1sZetsnok2vedimK0i8eBxrpo3AVFujFEWfelW1Mr0d7/9fT8yKPu7YL8eiZmDwdC/H/n/h6PntHsRJ0MnY37N6DMciMwBVCq+1TybjMJXb91RHtX2xxjnx/lqr/sc4Y9ve74rB6+UDY349O7rVT+ijJ1+3WiwOqom3BljlQo/N6ov3HfvMj+PosBD9en1n1VNo7Ndtc3sh/GfrkzKdX1TBUqczh/fbtulZZx2ENmWee9G6PnEOs97shqlR6rgZ67JoUDe1vv6dCynxVJ2HVF/6aXqPs8hpiphFWe114YnDOJxU8dTfx9rqxiuO4DuBmH/kjmIF6GKmznZpnOPtyAro0s7H9TSDdLaiuGbRWqvXF8UfegSV/lmBX/7RwUBZv7+xe9XfiAl9xFYM9xGzJecvXOo+/NyYpl2ygTUpKmzN+3683BmYMlRlCh78DiNsdQ4mzpy0txz9B0aEr3AWx05uu9X62o/T4d//MOatgqLhrg2THD7oGqT1JHcu97pSjXFBvv0xrULpu7D9n004e/EMihVy/pEpKb7Om4iIiIiIEsuUAcKiRQq7NEKSXFf9rqrIlzedMwfsrRgbXdWHn8XgH62Mo6JhozG0cdJFn/yWvzkG9FONMUwJx8epyS7zV+3eGN1GMhjPYdGgpqgq29tyoqPhiAkdbS9rQTUx4L0XrPoAl4SjQXVj/9R81gqQSDbYBGcLxGfPnjXryYvbNQ9dm9c153tn1fvQpNMMa965jXkN8dYSrUVaj/3+WBx2z+2JJvfdZx2Lev2sloBzN0D3tvZgllNc1DzM/isOx5aNQ9uHre+Z6ynxt6Ai6NyxOZxHUBrbsOazYb1kr0njGvrTqqhlvm2vwvKV8q+tfj4ld+NBmNxe1U/58wi0qKeX9wjajp6HkWO+c2Zm3toAQye9YDVGcywSg59+xLFfHmg3DrNGj0OEx/fw4/ihf0tUrWptx+CV1rkY0m8y3qph9iYh7bZv25SX8YAc8+rGtk2xgplFwz7B6Ba+1FuXtPwt+mCAuS0HzICzZOD5dxwVY98e+zEcTWpa+7XFYNXKs3Euf9FTny818Z+nrMZvzv7Yz2Wbiha3xrswrs/RnxjnqnncVmFkO3XcarZE14hV+HjkPHMZfp0LKVC3aggeSscswnpVQlCrfGobgbIy3qRhEnsR1fDfvAeYzECUrUjr2/+8jHmqbrziz03EvM77nEWWzUY8ks9mcyj5NF6Sxjv091c9YszPnkJo1VnoHhBzkGLCf0v9gs7iuFLXX2TkClS60164XzIgJQPwZbUdqgGQRHUtJq34cwPQBZ/jhS7fIVqKJi/T81N1PbpkYj5iXLpqu5q8i8jQd63GTYQP2y3rGxrpPE4vzJyNlb9b+9xsmER/Vz4baq9DMrHcuXMhKCjxI0ZQttzIE1RQDXlXqXQIVm38CRVKe76nu5N5yryJiIiIiChlsl1PpiKqvbH7UC44qRrNfHPmTLIFRzNMgQK+ZhkcwNSWTTHSUxJX/iKoWKMBnuv4Kto3LGULSti+Iw2b2OuPi52BFqHjzGKZ//nUWffYvmnPosFo+UJzfBEzGo2s0UDCdrzf9Fl8LC+FNXpj6dfSqrG3+a9Cz0rd8IPRF9JvMX7s5AziLe9THS99L31u8/eyPpY47P42HP1GR2LDqTjjbS8faj0zDBMGhKKMh3ewuF3fod/AsVi0STIDcyP/PU9j9LjeaBbsNrE0RvJ+ON7/+S/sPmxlfUl9jvc99SIGdH0aIT60NRMXuwqzPv4UH/+8GWYSoazb4z0wcMALqJXU909tR8TkcZj27VrsloBiUD5UrPMM3hryeuL1jJ6IBk+rLMj8L+DbPwc6sib3TTeOlz4ppO7A2Z6DmmfXz0P46E+xPPq42XiGtZ09MKRnKCq61893ajNmjRyDj5dtthrGMLapYm1j3cKNdSuv18157PO3mYDpVZYi/P2F2GZMn7/8gwjrORRvPW4/F5OQBtt3bPUMjJjwqfOYV74HYa95WIdl/XDnqwuNnmoYEPk1OicqXpnENWNbz5B+C4zzurzPx9FxXRnbN/nT8lg0ciJ+iDbOufzlUe+ZHhj1ltu5nHAOy0d3Qc+51nmVv3woXhsxDJ1zT1fr4Hb9GOJiI/H+4ImIWL/HOhfNeSc+l/06F/z047ooPDn8bTWUtub2HIjQux35wpSZmS0ew9k4zA1W6PaCHgOEIuF6HE5ctlpR9yYh4SpmLpxgtm5cqWzyQfDCeaoyQEhERERElAoMEBLdNJIIpFEiXgPvWVDnj8Zi2tKf1FDa6PDIYxjb/lU1RJleJgoQ5st7G/LkSTpYdznhJM7Gm3+iSLX8ucogT1AhNURERERERCnBRkqIiG5yU1/vg4fvSrZsuc+kaDGDg5QSt912a7LBQSEBvbw5S6ihlJN5MDhIRERERJR6DBASEWUBPwwakSZBQgkOznitrxqim0bdPlh+g7MHJXPw1lvyqKHk3ZqjmJn9l1LyXZkHERERERGlHgOERERZwO1582HVqIl4pWkLNcZ/Uqz4+35DUfC2dG70ibIUaZBE6hz0JXPQnWT/Sf2BvjRcosm01neYOUhERERElFYYICQiykI+7f4WFoaPQoOQmmpM8iRrUBokYbFi8kX27NmRK1dOszixBAbz58vrtUESX0jjIvlzBZtBPykynCsoH7Jny6k+NZZn9Ms4+UymkWnZIAkRERERUdpiIyVERFnU2l3b8b91a/DbX9HYsf9fHD9r3YeL5C+AqqXL4qG7auDJB+rhwcrVzPFEREREREQUmDIsQHju3Dlcu3ZNDd04kvmQL18+NURERERERERERBTYMqyIcc6czuJCN1JmWQ8iIiIiIiIiIqLMIMMChHny5EFQUJAaujFk+bIeREREREREREREZMnQRkry5s2L3Llzm8V8M5IsT5YryyciIiIiIiIiIiKnDKuDkIiIiIiIiIiIiDKfjE3lIyIiIiIiIiIiokyFAUIiIiIiIiIiIqIAxgAhERERERERERFRAGOAkIiIiIiIiIiIKIAxQEhERERERERERBTAGCAkIiIiIiIiIiIKYAwQEhERERERERERBTAGCImIiIiIiIiIiAIYA4REREREREREREQBjAFCIiIiIiIiIiKiAMYAIRERERERERERUQBjgJCIiIiIiIiIiCiAMUBIREREREREREQUwLJdN6h+j/bG7lN9RERERERERERElNX4FCAsF1xGDRERERERZX3R0dGoUaOGGiIiIiLK2ljEmIiIiIiIiIiIKIAxQEhERERERERERBTAGCAkIiIiIiIiIiIKYAwQEhERERERERERBTAGCImIiIiIiIiIiAIYA4REREREREREREQBjAFCIiIiIiIiIiKiAMYAIRERERERERERUQBjgJCIiIiIiIiIiCiAMUBIREREREREREQUwBggJCIiIiIiIiIiCmAMEBIREREREREREQUwBgiJiIiIiIiIiIgCGAOEREREREREREREAYwBQiIiIiIiIiIiogDGACEREREREREREVEAy3bdoPo92hu7D+WCy6ghIiIiIqKsLzo6GjVq1FBDqSOP26dOn8GFCxeRkJCgxhIRERFlHgwQEhERERG5SasAoTxqHzp8FHny5Ea+vHmRM2cO9QkRERFR5sEixkRERERE6UQyByU4WOj2ggwOEhERUabFACERERERUTqRYsWSOUhERESUmTFASERERESUTqTOQWYOEhERUWbHACEREREREREREVEAY4CQiIiIiIiIiIgogDFASEREREREREREFMAYICQiIiIiIiIiIgpgDBASERHddA5hzosN8GB9oxsWpcZRaqwZpvbnixE4qMYRZQbnzl/A6qg/8fmsrzBi7CS8+94EfDZ9Hpav+h0nT51WUxERERGlDgOERERERESZTPyVK4j49keMev8TLFq6Apfj4lCtSgXcd08NXL9+Hct/jcK4iVMxY843uHjpkvoWERERUcpkv3j1qurNQEc3YM6wrniiqfprfYMn8MQrQzEn6hDi1SRElFbicXDZJHRv84R1vRldaJuemLjM+/V2btt8hL/yBB42pw9F6CvjsXy/p6n9mXcUwtU0nrr2cw+p6dwknMea8e2s6ZLK7Ek4ieUTeyJM31eatkP/udtwLkF9bndmG74x7kGhjdS0jZ5Ap2HzEX1GfW4Xf8h1vsb9KqznJC/7I2Xi98xHr5bW/MNXq5EenNsix+UphDawpn34ia4In+9pG1Owr439HD1/KDq10sdSjvtQfLPlvJrAs51TwpzzTuL4+LqNGUFnink95zKar+d4ZrQ/Au3N4z8Ua9QoIkq9qwkJ+HxmBDZv3Y6QapXxdu9u6N65HZ5u2QxPPv4oXn2pDd7p9zrqPVgLu3b/g8nT5iAuLus9RR9d+BkGDDH2gxq+mWX4tuyPxNghYzFg5kY1Iglq2pnr1TAREQWk7JevXlO9GePchvEIa90TExdtwwn93mm8HJ3YFomJb4Xh0dfns2gPUVpJiMWCt55Dq3cisD7WGeg5F7sBc94JQ6e5sWqM07nVQxH2yngs3nZeBfnizYBh/zZdMcc+eQrm7Z94nNgwH+EvPoVe3yQzL2Nd5rzyFPpHbMBevSrnY7F8UleEjYzCOTVKmPegJ7tirHEPOqffpeLPI3rReHRqNxRr7PGw2IXo1TrMdb7G/WrvHxGJ90dKXDLWcWJPPPXieKw5ocZ5YR6XrnJcTjoCgvFntmHxuMTb6DfZf689hU7jIhF9VG+oHHfjhaHrcxgR5eWlc8t49JqdTJDNj23MGMZ2ZZpEGz/O8czq/LnUnXsUkI6fOIkPJk5BfHzie8v/flqC5atu8F8RMgHJGNx34JAZDGzz3H9w6y23qE+ccubIgRbNjM+fbYmTJ09h/o+L1Se+s4JWY21d1gjGkeHwCZySf0+fwFFzhNiImUM+w9L9apCIiMgm+5VrGRkg3Ibp787HXnm5rdoek+ZHYu3qVVi7/HvMGtAK5XIVQqPnm6CkNTERpVL8ng1YHnMSKFwHPT76Hr/K9Rb5OfrUKWR+vnPSKHzjfGoEErZhyuhISByncON+mL9crs/Z6FJVPovBxL7TsNOcMAXzvqQDjsHoMteYVqa3dbPalDA/dVg9Ck+8Ph6L93gJTtmcWDgeE3cYPUGV0HHyT+b8IkeGorB8tqgvRizT8ziP6GUbzHtQuZbDEREpy47ET2OM+0+Q8fGJSIRP2WBNaqztzj9WYqexMwrX6Y5JC6z71a8zeqGezFj2x6j55r5KmUOY07WdGXw84SnL0UUM5kx0Oy5qveV+eWLReMyJMSe0+Lmvd07ti4lbjG/I/pNjucqYzrgvT+peB+WahaNHnVxqSptLURgxyNr+kqXdjp2DP9uYUU7geGb5K5Qf53imdeIg/6hHfpPisctW/ooBg0fiiltJlkuXLmHoyHFYumylGhN4Tp85i6h1G1H3gVpml5zqd1XBo40ewpatO3Dk6HE1NnmbZ47FhPUFEDa4D0aqrmftM4hgkDBrqB1mHdc3QlFMjcJ+e7CQiIjIVfYE4yEtw+zfivXqbbpp+06oXUy9dOYqhCrNexkv699jaP281riEKISrYnRhs10zKxwVibd2BitMO6ahlVnUqatrYALnsXfheHR6ItT6ni4i6OUX8sQf09DfUWRSFbPbZk8rEroIX08sOCHFLG3zb9oO3adFJSr2d271aDwh29SqLxYk+Zc7Xfn8Exi7xfjeFslWUutjrHv7YZFWkFWKPo5zFpMMbTMI39heNHdOU0X/GgzFmkQv59swVhWZ7DT/pBrnp6OrMLFnO1txx3bob2z3Qdu77sG5L1vrNn5bomKdD4cNwpxk96ut+Kps+7hVqQw0qGKUtnPhiVfGY0GMWo8sdt7lqtQK4z+ajGmzx6BtrUIwr7hbKqF1vw6oYU6xDVu2mz2WqG/xjXmNhuCl15ujpHwhVzA6DumOKjJ6/3dYYJyTwu95nzihAgm5kEuCcWnmEBZ9ZwX18rXsji53W/eQfA3DMaiZ2Yvl85eqQF5e1OttrPNHszG7XwOUMxMycqFw/V7o09L63rnNW7HX7MuFKq3H4JPJkxExJgy1C1v3K9nuQR1DzH5sicaWVJ2PvjL2nbpn1G6mjou53s+jdSXpP4SD9vPKn319aRWmm0Vt86LpsA/RRY6lfMe4L9duMwYR79RBPnNCu/NYM26UcQ4avVW7o89/1H07nSR7bcTOUsVcG6DVFHuk1NaQx4BIxJj3ozBMVJPsnOQsHu0s9qyvsUFYfMlY9krnNabvCfH7ozBndE+E6WtPrdOCGxXoM84Hl30kReaNe6X9Xmzn273GkOQ9Xu2nvpHmpEAkepnzM7pkikmfWGmvlsBYfrtBmOKpipHzsVhg+40z73ETvf0G+HO/dfsdaPqyWVTfa1jlvFuVBE0l23YhduoqCdLyd8Ovbfb1WPr3u5ERihYpjI8/eA87d+3GQLcg4XPP/Mfs3hszASt/DcyC6+s3bEGePHnQrElDNSZ5DR96EAUL5Me6P30M7e2PxBLjx65K8zDUVKNEseavYORg13FEREQUGDK2kRLbi+qa5asSP5DaX2SDaqH+Q1bv3nVb1cu9iMEW/exz6Hest5VuO7Frq/VSUuI+1NB/KjOLQf4fwkZK/WLq9UMXEWz9slsRwfNYP74dnug5C8sdRSZVMbtX/s94gfTwAoVdmNLpCbR6xzZ/4wF//fS+bsX+TmLxfxdaD/lHo/DNKl/qvjJewsca69PVeMnR62Os+85FQ9Ft5CSMCAtDf+OlRheTPBe7CmO7jsIaVXyuSrOnraBOQiQWuTdyuWUJFpuzDEGz+lbGl1+OLkT31oMw549YW3HHWCw3trvVW4mzqs5FjULYU67FOuP3Gy+fXQdhgccXuF2Y09Ot+Kps+/xBaDdxmzWcAtHj/88qRmk7F05sm48RL/4fxm4wxmW5884QHIIaBVS/VqyEI1N372HnxkSvUydK+bqo7fhzs6F0LdQzk8SMdd1sW3k/5u0UjJLeEs7s6oerjLcI9DCDYF6c2IA1KuBT737XTIvadRpYPRv+tAXy8qJGrWCJqbgoqVcq9pBLcKPc3SHI5xZkK2xsoyXWNTDnlxJoO0Nl9Y0JVeO8qYy777b61nwZgb3qkON8DHaauzgEd3vcR8nv6/jflmC57Ju8TfBsQ98CfedWf4Dhi+QPC5XQJTwMhc96OkeFP9voiY/XRnB7DGxnbejBuZMc95T41VMx3Tw3KqFHt1DcZo711SHjfjMU7QY4r7G7KwXj3NJBeDSsLyYu2IC9+tpT6zTixa6Y40+xLV/P8eQsGuS6j6TIvHGvbNXVOFdcfmf9uNf4eY/31YkFPfHEAHu1BMby96zC9LfC0Mv+xyop3t+mHUbYfuPMe1yEsa2vuG2Xn/fbNcPcfgeM60iK6o9YZA26SNiGsW3cqiQ4f9LYv6PR/sXxWC/j0up3w59tTu/fjQxQLrgsPvpgJLbv2JUoSNi184t49pn/YMjIcQEZJJQ6BatVrWgWIfZVUPbsZl2F8l1/HD2SVDjfSbINvRdDPoilH9o/H4uxC13nK983x62PUNPYi7lKsVf79z1lMLouw33+rqz5SX16Sa+3wbE+XqYx6+az1tVlXh9GesjEc9sOH+r+cy/i7W27zGW7LdP6rltxYZe6BK310fM0p/98I07hDJZ/7n15LuvkcTudkjyual30vBIvy/246851m7wdQ72eLvUmqmV6249ERJS0jA0QlmiOtuqPofKSFfrEywifG4W9HuuDyoUa1dUb0+atzr+879+ANcZDdr688iIbg+it+ika2LJBZRHVr2sFxgwHI4ZiRJRbMcjl32N8K+NlUooITltlPFJb4o2X3v5SD1RQMFoOm20Vs1sViVk9aiGfBPgmfumaAWA6jxNH86Je98mINIvlRWBgYyvgdmLRUDhKK6IQmv5fcxSWQEOxOmjdIJm3duXgnliUbDncKlIY+Tm6qF1yYlEEFpzQxSkjEaE/OB+JBb+pLSrdBC1VUGHxMtcI4c416mXg7sfQyB4I8tHB5d9hvRlUCMVQXfRy7hh0rB+Kge+0Mot2utgfi70Fjc8irGkjh4VaWUkJG/DNck9BJOPlbs951Oio9+ts57bP/zGFleFHYYF6Aa3S6XNHMcpZA5qjdtgI9KklIaOsdt55IcfD7MmLGtWc5+LxM+qlsnJllLP6lMKOQNPevZ6Ol42XeeOkLoq4DVO62htAGZpsQxhJOn9CZf5UQpUKZo9DrjsKq+y3WBxMZrX3/q2ijDWrO46jNwdjVcQhb3Xc7dulnEqF0Lp/d1Qx7h/nNkxCWOgTaD9gEsLfGGQG+qt074/W9uvYj339j97u+2qg3P5VGKsbp2n0BMIGRCBRwy3njWmGWsWdq3QPR8dg45w7lszOTSF/ro0qncPR2iz6vQEjPpLr6yQW/DfSvM8Vbv0m2pYGSrb53Lj/jEFT8xuy/hHm/Ui6ofXVSIcYzJkUiSJhY6z7r7HcPvcb13njF9CydDBaD/gcP5lF1KXYeSfz2Mi1Pf3rlP8BI8WMm0mNjhPwk7meP2FaO3UP2zEJwxc4g27+7M/k7/F1MNQYN7+7WhZCMV7ty7UzwrxUFXIIi35Q98sm4Wp9jd+v99ujXpN+GNpK/7HqEOYMHm3WWelSvH/BGLQ2jqNs15SVznuwv/fbcDO4bUzeWBXXl33Qu07i3y3xx48qq7oSuszQ6/E5BrashbZDeqG2yuZN/e+Gf9t8Q3430oF7kDAhwRkFNYOET7c0g4Srf1+nxgaG02fOoUih29WQ70qVvAMnTp4yGzhJVum7cE9B4NT6OckEU6zAXIRxjXsuhiyfz8Gmim0dn49sXtaY74+ugSvDqd0/Yuy2ymq6V9BEzm0zoBOJneXc5u8SXPvXWN6PwDP68wIe5+9u58Kx2BSi1mlwWzQqKPOxBQAlqLUQtiLWappEgT0roOaY18v34vbTGzHTvt/UdqC5bV6nI5MOEhrLn7AeaPSy/k4oinnZrpohZY0TYw82Oz47iM275Qf6DDZtdq7H0c17cAplcU9tNcLGzA41jg1QwLHMPs1d79ayz2aihbU+nrbTA4/HVfbt53twj8u22c81CQ7aj7vse2N0wXvRU88jmXNPtifMeFjdudB2Ln67EaeMebpvFxER+SZjA4RShG3AZHS8W+XuyF/tJ/U1XnZD0UqKAsU6H35FyVp1rZcM44Vvi9QvZojfHm0+9NZr0sR88V8epZ9oY7Bzq9VXWz+oGy/Hc2bKy29etH7XVgwyVyHjAbkzGkn/siVYbgYoT2LRl9bLZI1eHxoPzcFWMbugXKgS1h2tJQiwfymWq/WwK9flQ4xvo7KMcpVAywGvWfM2HsYXr3S+LOar3w8/yUP5/DHGC6YamZw6/TCtXwOrSOEtldD0Mb1tqjigWZwyF8q1fkEt0/h5PKbzFwqh5dMqg2rpElsx40NYH2W90Nd4rKHnl6JknDulggz5CqNkPut45gqugy5jwtHSY8CxDgbONj4rbU2br/EzaKqSlXbuUsEWNyXbfIhpnfR+NV6CnlbZYQm78E8yD4UeXTrvyIQpXLiEoxhlleb9MKmHKjJqyGrnnSfRX8+zXh5LP+0IIst5cdCK7HlQCEV8DCR7nrfhnG7M4BD2brM3gGI8VHf1lvHig/17VUDSg0IlvQQq3BxdiDlLrd5GrZokfU0kGMf3SyuoVvLZFqo4dQYIDsO091XwXbJpV0ZgsXFe5KofbpwHweYkDn7sa0dw7+A8dJdqCvT08eexd+Ukt4ZbjHNrpBWUlKLFo9yXm6b8vDaCQvBGP/WHh2WTMGXuTHwhxeHzGi8KnZ3Xt18ahhv3hjrW/ddYrrX8EPSJmG28fFRCYdVmQK5K7dGxsdUvRdSTfpVKB03CjXtlLZil4IPyokaX/uioAtfRy35XGW3+7U//7/G+MO7BKuCcr1AJmLM1ll+uTieMf7e587rb8qWV+Zm3FYbai/cXroM3Olm/acsjddAvZfdbc97GMvWxrdJqDPo0kQ9cxRvXkqUwCqtqUXIVroSW/Sagh+3+lurfDb+2+cb8bqQXCRJOGDvcDBKOen+iGmuRIGGbsKfxztBR2LzlxqzfjRAUlB3XUlBHeI4gObDGaXjVhwChccY2ecMKykiQ0MzO8hDMOrrwRyw/XRZhHe5VYyQw08IMpG0yM7dkPm6BptoPG5+7Bq4s5dHBNh9HQEeCQi7zfwUjXaYzlu8IGsnn96OKW2DMk9trt0UHR6DMWM9n7sXt0OttMOvosxenNqapXxbYu8sZRFSqNO/jnFfpUDxWTgJjf6nsOmdgKtHyPMxLO3rEuCEWLI+ajneCe9HBtp0ualc2t/ngYTWMIzhoHJdGEiw9fkSNM371jxvzLFc5xUXEZTsdxzLRdnrjflwlqPivsf9b2LbF2DYJHOt5rd9l3B8LoNGj+ntq39uCoMmfe0DNDqHGfvkXSyTwuP5XY3r7PImIyF8ZHCA05A1Bl8k/4afP+6Ht/SWsB3njkfeg2SroE+gUYQsWVa2LemYQ6RCid1l/8V+/YZXx/0qo8p8aMH+D9V/rD1l/qQdqoZ6ZCWbYvxXR5jvOeXzzumTQ2LrQoVhuTnQIx823p13GvM0RiB73lOu09V/GdHPeh3BQx95schsP5S5uMdavvNV7bs/eFBfFMhXUGVDuSqBKBRVhE7fkVfvSVa6HHkMjeV60FzN2FMlMYfFiQ5UWT1sZM4ci0KlpGLqPjsAatwCvq7woYltdCTglV/Qx3+2u61a4cMrW1eGWBmjZ0prHmtFPIPTlQZiyMAYn3Fc7q593O6Yh/BvZrkJo2at9stlyfklq3nVew6hWDdBaZ7tIIxsfGdOY7zMnsXj0TESbE2Y048V4wmgrW6pqd/Ro7OlKcto5daiVUVS4Ofq01UHh9Ld3QV881XM+ThQOQdOGlRzFnuNXD0W7txa6Fj9Myb6OicHxhv2cDbd8FGYdvxORmLLACiKeWDAI4dJuQFAtDHzPW5ZYWvH/2shV/030MbPUD2HOJKsYbO3ur6GRy73Hd1Wqh3i5/xr7/UQMls+dhP6vhOGJpg3QXwWYceig49rTdbC6dMPc63tIA+73ArlXVVe9u3apgKV/+9P/e7wvKqHls9Y1czCiKx4N64kRHkoRHNy61QrinZ+P7qpuP909/K7ciw3GipqZw37eb6P1m/pDdVFbXUNabrdhkeuhFmhpRi6jMOJJydydhgU7TqpAnU0qfzf82ubM+rySCtmyZTM6oHJFtzRwQ+7cuZEvX14UKiTpRYGh0O0Fcey4dR7546jxnTx5chv7LOnfMScruCeZWZKJhb2RZqDQmeWlstQSBZxKophxONyLJzuLps7B8tNwCVx5tP8vbDKmu73iXc5GNNJT6cLmchIXq7YVdV34rzF8Bkf9+UO02o4qIW6BKXN53udVrGZ5M0NvgnuxZo/uQEljn+/cpoK4EmAz3hFqyjwcQciN2CT1SrqvR0ZTwb97aro9JRQvbGzvCePulJQCKGYGFX0991TgUYLciYKSRETkr4wPEJpyoXDV5ugxIQK/Rs7GqLBaVtFb45E7emJfOOuYr4S777P61m/fZfw/Blv+MP6RYn2V1EOt1OsjP7y7dloP3iWqo4pOQ9i/1xrni0O6WGRasDUMcO484lTvDXFLA7RWjS/oYsYnVi+F+Ye3FBYvNpUOw7QZ/dC0vPEQGn8I6xdMQq82oXi43SSsURkimU8u1Ov9X4zvaJ1v53aswvSRL+OJ0KfQa36s7YUvC59356MQ3neWGTAo3LI/+ri0TlsCJV3LFducxPGk/3yczLwNQYXQqPdw9NHZLnIfqNUJ43up7K4TG7DFnwdyrXQ5t+LQNo6itt7tnfuGCnpVQo8hSQe9zq0eil6z5dG2EFoO6IV6KoMs3e2PQPjoKJyQdfxoMoaO/ByRkd9jUscQYy8auy5qNMIjbI/cKdnXId0xbVhzZ8Mttbqjo8qo2rl1m3F9OBuDMYvxtnIGJXrputtiJpkNL7Q3Gz1JpRRdG3lRr34d1W+4pTk6Nk/lHxbcSX13fZ/Cwy1fRv9JEVi+7RBOqGQ703mdvXljOYJden383Z/pdI8vGTYZEQNCUU5mu38DFqhSBGETnQ1l7N3neAhInj/3W0OcWkaVCj5mv94i2e9j0LGWcR6ZmbuzMOJl4/i36uvSKFhqfzf82uYs9ryyN/ZfvNF7IJ5/tpVZpNhuxuwv8e38/+Gj90eiTOlSamzWV6lCOezcvce3osI2W7ftRJWKKuLrp5odnMU8ExXfVYFDexdhnIQ6AKgDgxN2l0dPe3FRHxW7I33/3ORkBdmcgUsdGLQVDTaL4KaMFM913U+RxrVuz/pzUzoUfQZbGXAR6jsu9em5KImaFQsAp61WiCX70Ays2oOQZgvFBVCyuPmFG8xZz6GjU/UfmuuqskyX/6KzVg9i6ep/EwcEkzn3TGZ2pSiLx1i0mIgoVW5QgNDmlmA06jEBP01opTI1DmHNH/rlMhdq11HFSrfuwl79V/f7ahg/BMGoXUsCXzGI3h6Pndutoif2euAkcGD150XryZIV46n73KybCiWCHUGG2v2+9zCd1SWup8qT8/KcbSnnbLDhRqnd2CrepIsZ79xsveSntHixlqt8cwydHYlf505AHxXkjd8TgV6vuLXWmJkE5UW9Tsb5Fvk9Zg3rbr38JpzEmnHtEL5Mv+xl0fPu/DZMeaMvFkuKSNXu+KR3HWNLXZUsrtI6dxnbbfUpJxx1+Hl8sfZh3t44G/xIoWIlcafZE4Odf5s9DvFHTqhgTTDu9PAX5RNLB6HbJHkxL4Sm731oHRMvzm2bhu5v67r3PsRA9wBoOtq7/EfrmmrcAW317pdWhjtNxvhW1h8Adv6yKtlgqKd9Xa6CyoI8mTiwdWewWpgjcykDpeTauBSFsRNVhp4EPS4txPSF/mfhJCV64hsYsVrmmQvlmnXHqM8jEBm5CuNVi9l2Vp2Hbuv8ji2AmW7icU5n5ZUvZ90LUrA/0+ceb+y35uGIiIxExEe9zCLBsr57I/qivdWiDMqVUedk3laY5mEdzU7Xc+jP/dZm59+eq7bwqEAddJG6DRd8jlHdreCmNDY29sWhquiySN3vhl/bnIWeVyQ4+PqbA/DCc63QJuwZNdYyeeoMfP+/nzDx/ZEoWyaJm3MWVOue6oiPv4J165PPK9O279yNw0ePoe6Dro11+UcXw3ULatnqB3TppOjn/kjMXH/GLM478o3QFGUC+tpQSupJsVzg9iJ3mEObZ0oAT4ov24oPp4IUz/W0n5KetxQrtqaz6tPzHiS0Mg6lCK6VXWcFVu/FPeWs4tZm/YMuRZZvJGc9h66dLkKtAp6OAOAcLC9onGcuRZUNSZ17inkcCxYwi48nrj+SiIj8keEBwr0rVzlb4LQzXvI9PZgWrqUeoPfsxT/brb+6V6lpZc3UqGW9aK3fugF7/7WiF8564AwlKqGG+e58Hou/W5VMVoezldD13/3o1lqgn2JWYrEOplS2rc+NUquFVcl5QhRWb9uG1b/JyJQXL3ZQ+yhXcC207jEB349pbgUi96ssicxIH1epe7BxGIbOmO2oR2r5apUdZchy511CLOa80RXTpU6qqp0QMTkM5XTWiE2VGirDbI9xDO0Zg45550WN6m5BJh/nfWJPTOKWyw36Zdl4XUa+lGTk3RKCGmpz1/zhPIYSdFi9ShXNq3Wf40Vck2zAdu+uwgkJDo78r/EybQXaPIqNQPeus7DTWP8qXWZjWrrWvZdY3AV104yPt2W6WorcocL8tn3rz7521J12aCmWuyQxxWNnjAqiVAs2prG1SOzWOYJjlbpjvjE8q00qg74m/6+N6CmjrPoRG4ZjmmrVaP24D1SL7YnFJXj6MUrKNixSDVxIHakR74ShUdUS5r7UmWk3hPt2XIrCcvM+b3A0OJSCe42axtd7vE/7QE8TlAvlarVCj4++wniV5X7wtw1mkLtk1erWcs4vxdcrvRw8za/7bTCq6FvjVltjIqbzOH5a9brT+6FwJTRqE46Iud2tukcTVmG57ZaTmt8Nv7Y5izyv6OBg66ef9Bgc/HlxZEAGB0X+fHlR94F7sThyJQ4eTi59Hzh1+gy++f5nVK9WGWVLp2WYVwdxvNejZ/FQnNQXuqGUZOu4SyNm0Ve3jMVU1NfnoLbDUfw3haz69JIImKpswYOHrfoHdUMkJYpY9RCa9Q8WtIpReyXFfFVvulH1JSZZR6QZWLY30OIa9PP53DPms2RvATR65hX0kezPvX8k23gNERF5l6EBQnkh7zZgEMKatsNYqftN/eU9/nwsFnyqGjYwXkLr3W97uSxdXT38b8Mcs3VI4/Oa6vPq95n1+pzb8B0WSYkeez1wIqgW2nawHnil1eTuE511Hckyl4+bhOWO5/BCaPl/qoL7mGno9u587NSV0106ifVzx2OOhwq/hdRJNmWDVSdR/IkojB1sFbNEUAO0benclnOrR+MJqVuoVV8syNAfL+Olpomsx3msmfyx1eBAaooXGw5GvIyHW/fFHNlueTkxXlD/2b5LvZyVQJHUpCamF+OleUSrUISNW4W9561jG396G3buMXtRsqhtpbPQeWcG8F5ph4kyn8KhGP9heysDxpM6z1gtwRrbPWXCQhyUmV+KwfRRat6VOqCtejE1+Trv81H4sOfLCG032ll/l3HO7F02GuGz1Ntp46fRLEXnTQk0Uw3YnJs/CRPNVnrjcWLZKIxdJmPzounzrg2PyL0orK8zG3Bow6SDg+3bTTKDg4WbjcGkdsHmC39GqvJAHescWfkx+s3doOrNNLZxwzSET7WCePmqV7ICff7u66pPo6O5+w5h+uDxWGPOPB4HFw7Fh1L0WvbfQ6nJSEkpP6+N2FkYa9Z/WQId24eiRtibqlXjVRhrXPPOwFEJ3KlK4O398UdEy7Vp7B/zPuaHc8cOWcch3liXKS876yC8EZZ+gF5zt1lBYeN6/WbQB1hubo+xL57VGYv+7U9f7/ElS+lgeSQWL1T3o3g130QOGfeLUDzRNwLrj+plx2LndnVDLFkYReTfWs+jo3kLPY/F77yBiVGx6vjE49we43hOtB1Pv+63JdBIN/S1fxZGTFHXknEM14zvihEeqoeMjzJ+t0ONZ5ZlsThnrrJx3W3dqTKsS8gqO6Xmd8Ofbc6I3410duToMbz25tt46snH0aFtmBpr+erbH/DTokhMGDsiIIOD2mOPNkDxO4risy/m4vd1G3Dl6lX1iZO0+rx+YzQ+/nQWLl2+jKJFfP/D7+aZVnFN1xaMnY2GPK6CT7pRiIgPI12CeJsXqmEVtHIGg4x5fGjVQZg83TCFW0u56yOSKGrru1Prf7UFl6yGMyQjTWf0SWDNJQBltmosdRD6SzdIEum2PzdiqcuwKzkGLtN7q7vPQbIFgaOr/zDrH9RXrJlZuPcPLDH2ebL1DyY6XunBWS+gy3HcH4mlfhzXZM89fb6Wu9/KSqwdhrByZ7D8W9fpiYjId9kOnL94veRt3tN29sbuQ7ngMmoodc5FTUKntyM8ZxCacqFGj88xLcw1O2fNMFsdV0GhGL88HPXMDKUYTGmtK+Q2lGiPWd90cs0UMuuLesN48PdczKxw68n4SdfLZTyYrzdeErp/46XoUa1++Okj3dJiFMLr98Vi6ZV1MR/k7Qqh6Rh7VtJJfNP1KYyVVjUNVbpHJJFlY7xEvRiGiZLN02yMS5E0qfS+lVkkshJ6RNiLTTnXx+O8j85Hp1bjHQ0T1Oj9Paa1SmEGYfwGTGzXE3O8BDkLhxn7VLUK7Fxf47itNo6bOVZ428YktmP1UDzYN9Locd9235xY0BNPjd5gvhglkqsOBkaMcWmdM0ucd/YAnsibF7nO61ZtNdf9GR81Hk/1nY8T7vOW+u9mG9Ppy9OfeeeKRPirQ7HY2xNbseYYP7ufquRf2M4PL1zPj0PG9WW8xG9JfHQlqBdhnF/my7TBHhyUe06+vMYLuCOAoOhz0hYcFPny5jKmdVuGZM3pon/+cJzP3tivmfPG+fh/xvno+Xwyg7NzjWll/x31d18bjO3s9OIkRHu4ONz3nyeOa8V9X/i1jZ74em2cxILXn8IIyeZqGI7IkVbw5NzKoWg1QFp7db0unPclp6YjV2Go2cBJ0vfS6PFPoZMZiHRVuFghnDgq45PbJs3fc9yd8/vSgNOJE4nXqUr32cb37b+nPu5PP+7x8oeX8KeM/WW/hsp3QsTs9o5isFr8hkkI6xmBg4nuW4agQmj78ffOloFjF6LX66OxxrpQ3RRC68nfo4+e1p/7rft9y6Zw4bzGfjQ2xHEeG+dVz+cw4g8PF4YhV51++P59W+vLhlT9bvizzen6vJL+zhm/Fat+i0LzZqFqjNOGjVtQsGABlL8zYzO1PYmOjkaNGqlvqz6lz9LxV67g6+8WYtuOGLPhkXvvDkHZMiWRI0cO/LvvADZu3oYLFy+hwp1lUbpkCaxcvRaPNX4YjzzsfG70RuqBK3bYQ0BMinS6F/M0SDBL6n5zKFgWYc+EWcVZXQJrUrT0FRT7xZjeuB/qeZnfP30venoqhrw/EmPN+um0sqhSuzAeb26cHws/w4T1BRDm0tqw1B0YiaO127q2nuygP78XxdZvdGYLJ9o2t2CmfP7oCWNd9uAeYxvMoJNat2L2VowNnrfHWq49O/l2CUgayzSn0dsprTa/URiH1gObFrpO79JasgdS3+MEVaTbue16uda+dzbS4WU/2Y+XuS7GNvi1nU7+HdcCxnrfb6yLdQz0tiTidpw8n3sP4+i3cuykiLjt3HDZx57XmYiIvMvQAKEp/hDWfDMLc35eiS17VDAhV16Uq9kcbbt3QMtKiR9Q45cOcrbi5/LQazyMjzYexhdY/fmMl4BIR9DFLh4Hl03FiOkLHcvMVywE9dp3xhvNa6GwWzrQuS3zMfbjmViz46SZkZGrQDDubtYZfTo3UBX4C+cDd+0ek9H6yFRMnL8BB+NzIV9IA3R57U20vtt1WySDMOzthThRuA4GfjQGLb0GuNIhQGh/gUYI+sw31jk1v5pSWfuimZg4ZyHWx1pvhvmCa6Flp37o0li3Tp25AoQifn8Uvp4+E9OXbbOyQYxzr0Zj4zzo0Qo1CljTaFnivEs2QCMS78/4PQsxfNQkLN8m6y3zbo6B4d3RqLRtpf2dtz5nvnTuD72NA7s0QEmX/ZGC4Ikx/zXTB+HDrzdgr5ySeYPRqEN/DAwLcbT468t8TeqcdHnZ9yZDAoQi8fnkdf/5ta+VM9vwzcSPMUVdG7kKhKBR59fQp5X3lny19AsQWpK7NiRzLNS8Vkug4+cR6FLV+p7Lfc8eRJXgyrRBGDFHrh1jMG8h1Osi9TnK+ZTMvVTOs0l9ED7f2k/5ghugY//+aJvrS7R6WbKxfNumFJ3jLvT3g9Fl7mTU3vABxk4xXhCNc9/TvdjOp3uNj/d4U2wkwod+gMU7ZDrjflG+FUZ93h21PZ5nMVgwcxLmLFTXaZDx+1+rObr07ux6fxHG88LyKaMxZdFW7D1j7Gxj2sJV6+ClLq+hZa1Cbtvmx/1W5jtxKEYsdDuG5z6wzleX89iYb9R8TPliprof2q6NlvZ7iyXVvxt+bXM6/W6Qw40OEGo7dv1t1jEoLRtLd+HiRWTPlg3FixdDnfvvRe17rXVc8VsUlvzyKxo3qIvQRg+Z4wJTcgFEuuEkSLm6cOIgnpdAJRERZYyMDxBmGckF5DKfnVPC0F5aYE1pQIMygZvvvCMiohuJvxsplVkChP5YtXodFkWuRNuwpxBSVRWpDzgMEGZ2ZlagLcPUgQFCIqIb6sa3YkwZIyEGy5dbZZtq/KdJouCgZAA9WN+HbpiHSppuGMmg8bCOHrr2c3W5LqL0cXNeQ0RElJU0qP8Awp5ugVIlrJZ6iTIjq/5H9wZFNmKmKh6s68AkIqKMxQBhVpcg3XlETx2F6fIjLBWRN0tl68VERERElCnVrFENBQvkV0NEmU+x5q+gZ21g+edWYzlWZ2V9jmTdgURENwyLGKfYzVBkx17voMj4isgprbGoGBER+YO/Gyl1MxYxJiIiIkopZhBmaXuxc5fVl6t0LbQd+QWDg0RERERERERE5IIZhEREREREbphBSERERIGEGYREREREROkkKCgIV65cVUNEREREmRMDhERERERE6eS2227FufPn1RARERFR5pQ9KFs21UtERERERGnp9oIFcPlyHE6eOs1MQiIiIsq0sp24FHe9UJ5cajAx1ptCRERERIEmreogFNevX8ep02dw4cJFJCQkqLFEREREmUe2C1euXL81Rw41mBgDhEREREQUaNIyQEhERESU2WVPKjhIREREREREREREWRsbKSEiIiIiIiIiIgpgDBASEREREREREREFMAYIiYiIiIiIiIiIAhgDhERERERERERERAGMAUIiIiIiIiIiIqIAxgAhERERERERERFRAGOAkIiIiIiIiIiIKIAxQEhERERERERERBTAGCAkIiIiIiIiIiIKYAwQEhERERERERERBbDs2aZ8A+neXrtVjSIiIiIiIiIiIqJAkf30S/9B7P89gcj9R/Dv+YtqNBEREREREREREQWC7AW/+AHB//0JO06fQ+w5BgiJiIiIiIiIiIgCSbbTcfHXC+TKqQYT2xu7D+WCy6ghIiIiIqKsLzo6GjVq1FBDRERERFmbmUEodRCWmr2QRYyJiIiIiIiIiIgCTLbrBtXvETMIiYiIiCjQMIOQiIiIAgkDhEREREREbiRAmC9/QTVERERElLUxQEhERERE5EYChCEhIWqIiIiIKGvLrv4lIiIiIiIiIiKiAMQAIRERERERERERUQDL0CLGsqhTp8/gwoWLSEhIUGOJiIiIiFImKCgIt912K24vWADZsmVTY1OPRYyJiIgokGRYgFAWc+jwUeTJkxv58uZFzpw51CdERERERClz5cpVnDt/Hpcvx6FE8WJpFiRkgJCIiIgCSYYVMZbMQQkOFrq9IIODRERERJQm5LlSni/lOVOeN4mIiIjIfxkWIJRixZI5SERERESU1uQ5U543iYiIiMh/GRYglDoHmTlIREREROlBnjNZxzURERFRyrAVYyIiIiJKNanU+vTVazgQdxX/Xr6SJp3MS+aZZIXZRERERJRqDBASERERUapIAO9I/FWcvZqAhKTbv/OLzEvmKfNmkJCIiIgo/TBASERERESpcubqNcRfS78QnsxblkFERERE6YMBQiIiIiJKlQsJ6R+8y4hlEBEREQWqbNcNqt+jvbH7UC64jBpKudTO59z5C9iydQd27Pobh48cw5WrV1Cy+B2oVKEcataohkK3F1RTEhEREVFGkvoCM0LZPDlVn2dp9dwqoqOjERISooaIiIiIsrZMn0EYf+UKIr79EaPe/wSLlq7A5bg4VKtSAffdUwMS21z+axTGTZyKGXO+wcVLl9S3iIiIiOhmIM9zR06dUUNEN6dr166nuCMiIsoMMnUG4dWEBEydPg/7DhxC9WqV8dSTj+HWW25Rn1quXL2KxZErsWbtBhQudDtee6U9cufOpT7NGo4u/AwT1hdA2OAw1FTjiIiIiDKL1GQQrtgUjXHzvsWPo99VY7xjBiFlNhcuXsKlS5fVUMrdckse3Har63sOERFRRsrUGYSSMSjBwScffxRtnvtPouCgyJkjB1o0Mz5/tiVOnjyF+T8uVp/4TgJwA4aMtXUR2Kw+IyIiIqK0tXv/ISxYvdbMHrwqLR8nJJjjj50+i/8uXY6Ea6xvkDK/i5cup0lwUMh8ZH5EREQ3SqYNEJ4+cxZR6zai7gO1zC451e+qgkcbPWTWU3jk6HE1NnmbZ45V2Xl9MFJ1PWufQQSDhERERETpImb/AQybMQ+dx0zE/mPHzHHf//o7Wr8zAhG/rDKDhkSZ3eXLcaovbaT1/IiIiPyRaQOE6zdsQZ48edCsSUM1JnkNH3oQBQvkx7o/fQzt7Y/Ekr1AleauRXeLNX8FI1mcl4iIiChdPF6nNr4a+jby3XorPv72RzNz8MOvfsDLzR/Dt8MGIHeupIsSE2UG19I40zWt50dEROSPoHcNqt8jyeQrWLCAGko5f+ezKHIlypUrY9Y96Kvs2bIZyzmHnTF7UO/B5LMOcXYP1mw8jAsF7kT9yvnUSG82YuaQ/+KrlWvwi3Sxt+LRe0qoz8RBLP1wCj5frD43ug0XXOcr2YqzDxjjzv6EAVN/NqbZhmsV70OF/MaH+yMxdvy3WKjnv/kSatQpj9uMjy7E/Imog3lQ/ZFCiLYtw33+rmR9/4djxvzPzx+LiT/oeebBGpnHTrf1Xx9hrNMa5/qo7d2ctz5KbvwMI+YtU9u1H0UeqY7i5pcssl3m/NW67ze+c09J9SERERFleWeu+h/YOH3+Atb+tRP/HjlqDufKlRP3VCyPGuXLISgoyBznrkAOz+O1tHpuFUePHkWxYsXUEFFi6VEk+FbWQ0hERDdIJi5ifA5FCt2uhnxXquQdOHHylNnASbJK34V7CgKn1s/B2IUH1UgPJHg3JBJorosht0Wj05EYMHOjmkCCg3OwqWJbRzHlkc3LGvP9EUv3q0mUU7t/xNhtldV0r6BJaWOkBOc+N+ZV2/n9sIIbMdNlnf5FxJAfgWesz3vWLuBx/q7OYNO3n2FTiFqnN0Lh72PuzoVjMRMt1HqFooqsh2O7rfobI/aWdRbRfvleYxyLZxMREZF3/1u9Ds+Fv4fzFy+h+9MtULRgfvR69il8vnAJnh44AnHxKW/0hG5i+7/Ciw8/gsFr1DARERFlmEwbIAwKyp6iNPsc6i/OCT7VXVMSTd5oi0YqSGg2UGILflkOYum3G3GqXCg61Faj5HvP3Ivb9+5SgTCZTx/0aW5Lm6v9sDHfM9i02T3wWB4dOtyr+oUEAv8FjPnbv1+zg9v8IEE4FVA0FGt+P6pIADDR/N1UbGFbb//dXrutbT3uxeO1CwCO7QYOHT9jrHtlZ3Hs0sZ2sHg2ERERJeHOknfg3Zfa4rO+r6NMsaLmuP88XAffDh+EZx6pj1w5c5jjsqI5X36HTt3ewtyI79QYy9mz5zB2/Cd4d8T72LZ9pxqbxagAYF1b9+K8Q+rDjLIWgx/ujLlJ/pGdiIgo8GTaAGGh2wvi2PGTash3R43v5MmTG7lz51JjkmMF98ysvXLG4N5IM1DoyCjc/xc2nQaqhNiDeobShVEMZ3DU7eHC2SLyHCw3vnfq+BH1iRfrd0EeARPN/yZRM6Sstc8+jIRVQIiIiIgoadXvDEbzuvcjW7ZsyJ7d6Ix/hWQSdnyiiTk+K5Ii0D/+vBQXLlzE/35aijPGsLbi1zXYsCkaO3ftxvKVq9XYrOPgvM6o+8InCB69Ar//qrp53YBPXsjYjMH9sYhVvekuegZef3OA6mawhA0REWVqmTZAWKlCOezcvce3osI2W7ftRJWK5dWQfyRrzyw+bGYUuhbflaK2VuBPd5HYiTM4eNj6XAcGJ+wuj55mcVtrPr4pgJL2Sv1uJrXDzGLFt5/eiAnmfvksmWLPRERERE4PVquC919/RQ1lbdKYXrUqlcz+alUroYAxrN1V1VnvdshdVVRfFrH/Kwz4JAaPjV6BIfXUOFH6Ocz41W1cVnFkKYZPP44W/Ubiow9GYlCz45g2ZimSSR0gIiK6YTJtgLDWPdURH38F69b7/re27Tt34/DRY6jrSwMlXqniw7bgn6jiqH/QtTOL7+6PxMz1Z8ziuCmp50/qCrQv66ZjFiuW/SFB0TNY/jmDhEREROSb227Jg6plVR0qASB8wJvo3bMrwt9+U42xVKxwJz77eCzeHzUYjRrUV2PT3qHDRzFr7te4fv26GpPYilVrsH5D2uW7/T7jE+ys1A2v+hEINDMOdVHkl76Ce6U6vw93Lapcd/ha9YlFPjeLL68ZpqbpjClTrCzGnYjBRy9Y33MWcZaix7b5PTwMv6tPUuLI5s04UqsJmt5hDd/xWHu0wHIsiraGiYiIMptMGyDMny8v6j5wLxZHrsRB40EmOadOn8E33/9stnpctnQaNqGrGjLZuc29bkJ3BXBPzRQst3ZlyN+Ik5//zcBzcJWIiIiInO67927V5yqf8fxbskT6FivJm/c2bN6yDdNmzPUYJJTg4Mw5X6FI4UJqTGqtxaLFxj/lg40nRd8s6fcIBmCooxhylZhPMMBWV6EE/97EaJeiylUW90tUVHnn0nC8uOwxNd1UdOkyFb+PftT4pBJen2d9d8YLJYzhQ5j7Uj8saeqc57fd/sWbbkHH1CmG4sYO2HeIlfIQEVHmlGkDhOKxRxug+B1F8dkXc/H7ug24cvWq+sQpISEB6zdG4+NPZ+HS5csoWsT3h5nNM63iwq4tGKtGSQpKgxwyrBskiXSbbiOW6mFVH6GzwRCrVWOpgzB5uuEPyUJUowzSEnD6ZOGVRM2Ksrw/nPOXVpRXn4H/bUbLdo51Xe/Ne3AKZXFPKhpGISIioqzhty3b8O2K1X5XGZOVLY5cgQGD38OSX1aqMZYrV65gxuwIfDT5c/y9Z68am/by5b0N77z9Jv7avst4xv6vS5Dw19VrMWvO1xjY9w2UCy6jxqaNKhXKqr7kSVFkK3BnKP0cXm0qwb5fHVmEdQetwO+DHlRDBjXNkmXuAb0mGGmfzqt/sTPGWG5j57QlX5jqugw/3VGiCLBho7PewegZmLZB9RMREWVCmTpAKC0Sv9whzKyP8H8//4KR4yZhwU+R2BT9F7YaDzU/LVmOUR9MxncLFqFkiWJoWP9BLP81CiuMLnkHUeLRPhjZvKyzBWOzm4PlBUNdiwqbRWiNYZfpjPU4fkQ1zHEvOrjM50fgGdXoiQ+KNX/FXA97PYcTdhtruDl9sgqLNW+higKr5W2rjJHPpKDexv1HUKz+vThqX+/1QKOX2YoxERFRoLty5Sp6T5qG9/77FTbF7FFjA1tcXBz+O+9b7I3dZ/z7DeLi49UnVuaeBA/XRK03+9OT1IU4eOBb2PbXTkeQUIKDn8+chwF9e5jFnW8KjuLDj+BNyVLcE+sIIvrnQTSTAGO/NGxVucaLVr2DupGSzfeiU2pqQSIiIkpn2YwHAu8VkBjkASYt/oKY2vns2PW3WcegtGws3YWLF80W74oXL4Y699+L2vfWMKdb8VsUlvzyKxo3qIvQRg+Z44iIiIgo/fx7+Yrqc/XiyPE4aDy3zRr4JooX9r+sgruyeXKqPs/S6rlVREdHIyQkRA2lnbfDR5rreaexniOHDlBjgW3bd2H4qPFmf4e2z6HZY43M/vR07PgJvDt8nPk8HbP7H/Tv/ZpLYylpQ+r2U8V3k8vI2/8VXlQtHdsbLjGLFO/phm+/eM4qpiyBwX6/GD2P4oNf30FdD9Mk+o5mfvdfvD5vKtq4VX0p9R4+80mMNVDJw3fdHD9xSvX54igWj5mAw4+PRAfrtcWjImlwnRAREaVEps4gtKtauQJaPdkUr3R8AQP7dDcbCBke3huvvdLeERwUjzxUB81CG2LZqt+xbYf6gSciIiKiDDdjQC8s+WBYmgQHs4rBA95Ch/97DuHGv3Yh1SpjxLv9MaBPjwwJDoqiRQqbmYTZsmVDv7e6p0NwUFjZeSnP7nO3FoMlOGjWF2gFB9OKWaxY12kY8wmeeekr9UkaOLIZfxwuhOL+t2ZIRESUIW6aAKE/GtR/AGFPt0CpEqrZMCIiIiKiTCBPntxo1qSR+a+78ncGo0b1amooYxQrWgSD+vVESDVpNi991G38KBDzCT5Nw5LT9voC/VK2itlAYJJKP4eR3SoZ67xTjUiJzVi8RDdIchSLZy53adWYiIgos8mSAUJRs0Y1s34VIiIiIkpfQdmyqb70kxHLyAgbNkXjg4mfYqPxr7sff16KL7/+AYcPZ7GWbuu9g2+7VTLr+HNpaViKFEsdgn61FlwWVSq5NkhiFieWOgh9UToYwYjBkl9tdQ2a6zEMv6tBadV4xdIYoOljajgFoo/ij00TrPoH35yAH0uG4aP/Yy3dRESUeWXZACERERERZYzbgtL/kTIjlpERJn82E3/8uQmfTJ2lxlh+XbMWc778Dj/8uAjLVv6mxmYdZvHded0Q289qVMTsXvgE6DbPz9aCS6DN0G6osrifYz6LGq8wA5C+eRBDRj+KnZ+8YH3/pa+w6d9gvNrtX7yp1+vhF/ARuuHbVLRijBpNMKjvSHz0geoYHCQiokzupmmkhIiIiIgyJ3mYPBJ/FfHXknysTLFc2bPhjlw5kFwO4c3QSMkbvd/B0WPHcUexIpgwdpgaC/y5cQvGTZhs9j/3TEu0avm42U+Zl3+NlPiGjZQQEdGNwgAhEREREaWaPFCeuXoNFxKuISHpx0ufSbFiyRwskCN7ssFBcTMECPcfOIgVq35Hw4frokxp1zZyJbPw3LnzeKRBPWTPzoI+mR0DhERElJUwQEhEREREWcLNECCkrIMBQiIiykoy7E+TQUFBuHLlqhoiIiIiIko78pwpz5tEGSUojevFTOv5ERER+SPDfoVuu+1WnDt/Xg0REREREaUdec6U502ijJInd27VlzbSen5ERET+yLAA4e0FC+Dy5TicPHWamYRERERElCbkuVKeL+U5U543iTLKLbfkQd7bbkXOnDnMOiNT2sn3ZT4yPyIiohslw+ogFLKoU6fP4MKFi0hISFBjiYiIiIhSRooVS+agBAezZfOlKRPfsA5CIiIiCiQZGiAkIiIiIroZMEBIREREgYQ14RIREREREREREQUwBgiJiIiIiIiIiIgCGAOEREREREREREREAYwBQiIiIiIiIiIiogDGACEREREREREREVEAY4CQiIiIiIiIiIgogDFASEREREREREREFMCyXTeofo/2xu5DueAyaij1Ll++jCtXruDatWtqTPrLnj07cubMiTx58qgxvuP6Ji8160tERERZR1Z6DomOjkZISIgaIiIiIsraMjSD8Pz584iLi8vQh0Yhy5PlyvL9wfX1TUrXl4iIiLIOPocQERER3bwyLEAof1FOSEhQQzeGLF/WwxdcX//5s75ERESUdfA5hIiIiOjmlmEBQilukhn4uh5c35TJLOtBREREGYfPIUREREQ3twwLEGZ0cRNvfF0Prm/KZJb1ICIioozD5xAiIiKimxtbMSYiIiIiIiIiIgpgGdaK8ZkzZ1TfjVegQAHV5x3XN+V8WV8iIiLKOrLic0hatmJ84sQpnDp9lkWgiYiIKNNigNALrm/KMUBIREQUWBgg9E6CgxcuXkKJ4sWQO3cuNZaIiIgoc2ERYyIiIiKidCKZgwwOEhERUWbHACERERERUTqRYsUMDhIREVFmxyLGXvizvtu278Tq39fh330HcO160q3n5cyRExXuDEbTJo1wR7GiamzSWMSYiIiIMjMWMfZu+47dCLmrshoiIiIiypyYQZhKW//agQGDR+KvHbtQtUpF3B1yV5JdcNnSWPHrGvToPRCnT9+4h+lr165h2ow55voTEREREREREVHgynQZhLI6O3bGIOFa0pl47vLmvQ3lyvq2nmmZkffhJ1Nx6NARjBo2SI1JXnx8PNq+1B0vd2iDZk0aqbHepXUGoQQHP/joU/y6Ogq5c+XCkEF9Ua1qJfWp06XLl3Et4Rpuu+1WNcY3zCAkIiIKLL4+hyxb8Zv57OTu+WefwgvPtjL7h773Pv7cuMXs14KCgvDBqCE+PZMyg5CIiIjIf5kuQLjnn1j0HjAECQkJaoxv8t52G2ZN+8h8gExOWgbcBg8fiyqVK6DNc0+rMb55d8RYVKpQHm2ff0aN8S4t11eCg+9/OBkbNkcjKHt2M/h36tQZDB7YGyHVqqipLJ998V9cvnwZPbp1UmN8wwAhERFRYPH1OSQ+/gp27f5bDTnJs6Y8y4ljx0/gyNFjZr8mzyxVq1RCtmzZ1BjvGCAkIiIi8h/rIPTC1/V9Z9ho3FWlMl54zvqrt6/kr+PlgsuifZtn1Rjv0mp9JTg4+oOPsSvmbwwL74eRYyeiVcsncPjIUSxYuAhD3+mHasbDt/bJZ9MRFxePXq+/qsb4hgFCIiKiwHKzPef5ggFCIiIiCiSsgzCN/e+nJWYdg2L7jhh8+fX3ji5q3Z/m+BtFiutIQypjRw5G6VIlrZHXr6PdC62N7llM+vQLa5ziy1/piYiIiIiIiIjo5sYAYRqSugV/X7ceUWutQOCm6K3Ysu0vR7f2jw3m+Bvl/vvuweQPR6NI4UJqjDN5tGXzpvj4g/fUkCWZ5FIiIiIiIiIiIsoCMnUR4527duPK1atqyLP8+fOhbOlSasg3gVrE2F23nv3Q6skn0OTRhmqMKxYxJiIiIl/4+hyyJfovfPnt92rIqWloIzR8qK7ZPyfiO2zbvsPs13IE5cDrXV9G0SKF1RjvWMSYiIiIyH+ZNoPw0OEjGDhkFAa++16S3eBhY9Q3iIiIiCgzy5krJ+4OuStRd9utt6opYJZ0cP/8rqoMsBERERGlJzZS4kVKMgil5eX2nV5H0aKFMWHMMEybMcesk1CrEVINwwf3N/szRwZhf7R68nGXDMKTJ0+hUKHbzf7JU2fg8uU4Rwah/bOkMIOQiIgosNxsz3m+YAYhERERBRLWQZiGgoKCMOLdt9G3V3dzuHWrJ81h3UnRmMzsypUrZtDQHtTUFi1dji49+iIuLk6NISIiIiIiIiKirIABwjQm2ZYlSxQ3+wsWyI/qd1V1dHcUK2qOzzxck0dz5syJYYP74b9ffoPI5auskdlg9n8xcy6GvNMXuXPntsYTEREREREREVGWkOkDhPsPHMTWv3Z47KTI640mlWbHxcerId9djotDzhw51NCNkStXLuTKnUsNWSpVKI8hg/pi6vT/YuPmrdj21058+vksDB7YG9WqVFJTEREREfkvPv6Kx2e68xcuqCmAY8dPJPp8+45dSKZWHCIiIiJKhUxfB+FLXXrihJdA4MP166D3G13VkO/Ssk4/qWfwr+278MHoIWpM8qRev7YvdcUb3V5Bg4fqqLHepVcdhEmJ+XsPBg0ZhWvXrmHoO/38Cg6yDkIiIqLA4utzyLIVv+HDT6aqIafnn30KLzzbyuwfMvJ9bNi0xezXpBqXD0YN8emZlHUQEhEREfmPjZR44ev6Hj5yDG/0GYg8efKgZPE7kC17NvWJZwlXE7D3330oXKgQPhw33KcswhsRIBS7Yv5GwrVrfmcOMkBIREQUWG625zxfMEBIREREgYQBQi/8Wd99+w9i9e/r1FDysmfPhhaPP4Zbb71FjUnajQoQphQDhERERIGFAULvGCAkIiKimwEDhF5wfVOOAUIiIqLAwgChdwwQEhER0c2ArRgTEREREREREREFMAYIiYiIiIiIiIiIAhgDhERERERERERERAGMAUIiIiIiIiIiIqIAlmEBwuzZM0cs0tf14PqmTGZZDyIiIso4fA4hIiIiurll2FNUzpw5Vd+N5et6cH1TJrOsBxEREWUcPocQERER3dwyLECYJ08eBAUFqaEbQ5Yv6+ELrq///FlfIiIiyjr4HEJERER0c8vQchh58+ZF7ty5M7z4hyxPlivL9wfX1zcpXV8iIiLKOvgcQkRERHTzynbdoPo92hu7D+WCy6ghIiIiIqKsLzo6GiEhIWoo5bbv2I2QuyqrISIiIqLMKWP/xEtERERERERERESZCgOEREREREREREREAYwBQiIiIiIiIiIiogDGACEREREREREREVEAY4CQiIiIiIiIiIgogDFASEREREREREREFMAYICQiIiIiIiIiIgpgDBASEREREREREREFsGzXDarfo72x+3Du7Gk1REREREQUGEJCQlRfym3fsRshd1VWQ0RERESZk08BwhLFi6khIiIiIqKsb9euXQwQEhERUcBgEWMiIiIiIiKijLR6KB6s38Ds2s89pEamhUOY86Ix3xcjcFCNyZxulvUkChwMEBIREREREdHNQwfXPASX1gxLj6BbWotCeN9IoNkYrF29CrPalFDjKe0Y+zjTnwdEmQsDhERERERERHTziVmC5ftVvykKixap3sxsfyz2Gv9UqRBsDVPaU/uYiHzHACERERERERHdhGIwcXqU6jesXoLFqjdTi92JnaqXiCizYICQiIiIiIiIbjKV0LRZJWDREqwxhw9hzlQpthuKpuawnarvTtX55140WRdLdnTDbEFHVVQ1fLXrPJIuump9xzE/+/KkeLQULzbsnBSWuJi0Lj7tYR30tAfnvuyct/v8U8Cn+dnqTDQ72/q577/w1eoDHy23f99lu4XbvnT73OOyZV3DJplBWHMf6/FElCQGCImIiIiIiOgmEwNUeAxVEIlFEvzZvwqLYyqhR+MqbkVLJbAXhonojvmrV2Ht6gj0wCS00oGm/RGY8rf+bBXmd5egY99EAaXFfQcBw61pxjeTwNNUFZh0JwGtvlhcSc9zDJrGGMvTQbf64Vg7JtScskr3CKydEYaS5pBSvzN6GKvgDHwazMxIY9uGy7RRmDIpGOPV+przMubfP8V17SU/PzOA2DfSWl+ZJiIC8zsGG9tjBU17LTLWLcL6/nzjsy7BfqyLsaydjdV31b53Bl/VvlR1NZr70v756qHGsoGmY6zvrx0zxvxOon1sfDa0vjlIRElggJCIiIiIiIhuPsFh6NIMWLwsCgdXLcHOSo+hkXu1fqunYmIM0LSzDsSVQKPHbJmHpcMwyxakK9lAgo7A3li3IFezzmhb2uotV0EieLH4x6X+Q8vBuVPNYs7O5dVBFwl8xUzCFJ+y2EqgbWcJbjnnv2ZZJCDbZi6/DoauDkc98xND/cfMjMmdf8daw35Lbn4SQDR2YKXuGKUbUyldAiWlU/u2Svfhjn1jjjc6nxnz7aKCdyXbdHZZtrUvK6FHxzrmsKxrMwnOLlllZVLGWtPJ8TfVN7bF6IgoZRggJCIiIiIioptSvcahZrBvyt8xqPJYA9dsPJvFfZ3FUFtJwMuFrfiwLpqa4oCbqIQqtkBlyWBrIFHQ0RszSBeDxatkeqvhFfdtcxat7WvVu2isb2qKGSc7vwrBXvdtueA0boXZZdkxmBim102yFdVoQ8k2n1uZgov6qs+HesnqJCJfMEBIRERERERENyczmBaJxYsqoWkD74EqRzFUR6ey5sy69cIwsYIqxhrR3cwgTJ0Y7LTFF3Wmm++BNFumnCpe7Ni2/RFoL4EyR7HoMWbWXYr5Or8kApA+Bz6To1sedglGOosvOzp7sWwpTizjzOMWiV6prI+RKJAxQEhEREREREQ3KSuY5iyC60bV6be4rz277BAOquK7VvDOWYzVLKps9qWMLia7eKoOVDmL6OqitL4wMyNjlqC/NLxi3zbVArKjCHNqW25Odn7OItLOegmjsEaKS6t9u3PSIMzRxa1XG5+pXp8Y27hcF6WebmVvNm1sHQtrX8Zg4iBb0G+/cexUrwQ3w/U6lW6AplLyWwuuYgZ6U5cJShRYGCAkIiIiIiKim1a9d9yyylyUQNsZEehRKRK9VDFVyRhspYJOJdsMNz5zFmPtD9VISIpJnX6qYRJzWarBEq/r54UqZrzTpf5EQ/1ws5EUR5HpZY+Zw06qjkVZfqIWgT1Idn5WUV5pQES3CCzb1MsMgOp9aysG3LcvptgaOElOle6dgUHWd3WDI84GRdz3pdGFGcfO3K5DmDNoEhY71ikME2NCMV7v59JhGKUaPZHPk251mohEtusG1e/R3th9KFG8mBqi9BYXF4/LcXFISLimxlBaCgrKjjy5cyN37lxqDBEREVFiu3btQkhIiBpKue07diPkrspqiIiIiChzYoAwE5Hg4IWLl9QQpafbbr2FQUIiIiLyigFCIsoKDs592UOjLEqzMVj7Tvq0+nujlktEKccAYSZy5uw5Zg5mEMkkLJA/nxoiIiIicsUAIREREQUS1kGYiTA4mHG4r4mIiIiIiIiILDckg/DMmbNYErkM69ZvwJ49e3H4yFHkypUTxe+4AxUrlEeDh+uh8SMPIygoSH0jMJw8dUb1UUYodHsB1UdERETkihmEREREFEgyPED4y/KVGDn6A7O+vXp1HkDNu6ujbJnSZsMcBw4cNIOGfxhdlcoVMXTwAJS/s5z6ZtbHAGHGYoCQiIiIvGGAkIiIiAJJhhYxnjRlGt4eNBTPPfMUfln0Pca8NwStn26JokWLoFLF8mjXNgyfTByHhd9HoHTpUmjXsQuWLV+lvk03p2NY8v476Pl+JI6oMelrC2b3MZY3d4saJiIiIiIiIiKipGRYgHDaF7PwzXc/YPb0T/Fq547InTs3vl+wEI2aPIm2HTrjmbD2ePzJZ7ElehuKFSuKUcMHI3xgH/QfNARrotapufjh9/fQoHETNHjlGxxSo8ShiK7G+K748oAaoR34Bp1kek+feZmXiHpPvtMEnSIOqzGH8eUr1jjXzsN8/aaCbRIAM7vZSEkY7EjkB+b3Z29VIzLYjV4+kb9cr3FF3xfes92ffLiPJJqPX9ZhmDn/Jhj2uxoVaPR+d+8cx8HLPdjD/du+P60uift0ouW+hyj1Udoc25uQ3ic+/DbqYZfzVl8vHo+NJ4mPrX2fW7/vrp9LFxA8XRce9qu3fWTuR6/HQ18ncs7rY2A7/zUfrhEiIiIiyrwyJED4z95YfDptBoYPGWQWHRb/W7gIkz/9AoPf6Ydlixfgx+8j8GL7Nni9Z1/s3LXbnKZpk0fNcUNHjEF8/BVznN92f4ppPrxIH/rtF+wy+3Yj8jcvL3m7f8FKl5fHdVi6VPUm0hhjli3FKkc3Gc+XUh+liAQHJ+InNMTbY4dhgtG91MF4XYo8pj733ZGjp8x/9x/2/7tp4UYvn8hfUjBs155/rQElasUy4/8V0a39A9YIg0/3kdT4/RfoW87SFSn4w0kWUvnV2bb7q9G97TwOFvs9eASaGL8FYfbAhxkMGYilTUY45hHxKvBJu8TBVzO4NXAZmozQ85uNbhWXYVqgBQS98eG3sc7bxjEw/l063XkMrOvFuIYGt0YJNc47CUy1wyd4FRH6uM42DtinQxIFdZ3HyeoCifO6UOe8l6C3+z6aFlYcKPUQQuURbXcMYq3JLOq+U/nVjqhjjUmE1wgRERHRzS9DAoTTZ87Bw/Xron7dB83ha9euYcz7H6Jv7x5mELBAgfwoUfwOtHm+NVo99SQ+/HiKOZ3o1LE9smXLhh/+t1CN8UdFSDzS/kLi2WGs/GW3PDFjjPEGs+uX37xMvxufzLK9lNte1tPd1kX46fDteKJdKO5Qo+6u3g6PhRZVQ767u40VYHw7Bd9NCzd6+UT+Ml+al/5iy5g5jL3/GP9UfBQNHYF/X+8jKWcGJSu+ijGvyo3Nvj6GA9+oHkrsATSR6JQjkHUYXw75FLskiGgLLJYIG4xusmsH2jKfjP06zbjRS+DlnbpqHIrj+c9UUIUMvvw2PoBOct46jsE6TPtUrpeOvv3x7MBviJS/Hd5Z1hlMLNUa01L9x7es6gG8M/tVVHY/NkkqjoaPys1uGZbaguSH/pU/jlRE6ENezndeI0RERERZQoYECNeu+xNPPO4sWrJv/wFcvHgJte+7V41xql2rJnbstHJwhLRu3DS0Mdb+8aca44/dCH7UeEDe/SneSeqv2OrFo3L5sggub3+BsatovGDaX8qNF8zpxst6k8ZmVkR627JF9klRJN9ejKqDz6348Za5Mu4DLDlqDGydnaiIr/W5s9OfJS4OrOZvq1NQT+Pokqtv0G357ss2O1sdgq7zV9tg4/L53M1qLFHasV6a/8VefV/Q94xHH3IGK3y6j6SGysq6syzqlC1r9Li+xEfN+lT1kXdlUU6CSTrY1ORRt4woHSBxHmvZr5Ll5jU4EvB8/20s8dCjZsDKzK5VQcQmj7hnfnpRyriu5N+lAwO3eL2/dEag+x8TkmAdIyD2X/3MpP7w4fLHEFe8RoiIiIiyhgwJEEpDyRcuXFRDQM6cOVWfB9myQf6zO3/hAq5fS7KxZe/KtkYnyeb5dLrXB2RdzEkebl1eYFwYD8jl5TP1Um6+YFZEt0cquRbFSSeHJRWpeAnAJRjmqQbCu/FY09uNf3dhsy2ot3kjcEfTdnjMU4Bx62x8YXx+Xwcrs29Ch2eNkb7WbrgFSxYXxUuq2POEDsarxeGV+MKPos86o3DC2B54Qr1f3Hf33ea/Evx7bzHwRB9rmrebAj+NdQY/rc9POdfd+FpKQslESUl0X/g3JtELsW/3kVSwB1TqPmoV1wzwYsY+UxlOjoCgefysYK5nuxHjUqJcBRbJAz9+G0vp3+Mh6CRBxIqvopMj4yw5D+CdEY3NvqUDrTruGChMTnGUu1P+tf1xIzkqqOjMgP4XMfKHD/sfQzziNUJERER0s8uQAGGT0EaYF/GtWbRYlCxRHKVLlcSff24yh+3Wr9+IunWdGQUnTp7EoiW/oGkT68UgJeq0l2I23urCcfvreKKHYxsVbJSXcjMYIN/x9n5pLK+vrbLuNKm8/vBKLEQ7ZyBu49ceG/q4IzQU9xn/Our427oZf+J23He35yK9R9R0f25RYbfqd6Od0fnGmHZsO+P/SvWa5rKPHPW/cOWRyNn4SXbTvc8ay5cxEnw8ZQyHOgKbd9x9N+5wBD+PYfNm4/PiDfGYOb1BLZ8oTbndF6z6B+0vxH7cR1LIWmZjNDEDKqrIrC1ryyzyHEB2fdrOcX/12GCC/R7c7lPskroGdXHispWM3wRf+Ldf7esUUPz4bbR+j3dDqhpu0tGXugdt6r5t1m8nRfiFGSj00BCHDiCanfE5Jea+j5z7UGXR6gxo9YeJ4LLesgMD795DRERElFVlSIDwlU4dcPDQIcz675dqDDB86CBM+HgKfl4ciT83bDK7iK/nY8Wq39C712tqKuC90R+gQvlyeCwVAUJn1sJ0rFSjnKy/jktjJmHmw3I7fGIOey4eWOcRYz2Ml/Jpe3Yn8xd110ZK0qQenuIN8ZKut08FwhxBPRd3o+a9wJHNm82ivmbx5OLGOC/Fk+8IfdMRcLSyE/1vHdlZTPhrK4Pv0NGkixm72zrbzASE8er4Uhu34KRjvYxu7ErbfA/hSBomaBF5p1+apfJ+VdTXpXiqf/cR/+mXcGfQq6+sg87acmQKBQ7XRkre9tB4gltDUYkaMTF+E9wannGqiEpmgMu/DCz7OgUan38bdbFXR7Dbf3Xeln0sjWAYA8Y1594QmUsDHJ+1VmMDkb5vJM7uc99H9uOlM6Ali9b1DxOepCBLkYiIiIgypQwJEBbInx8jh76DTz79HL8st0J0NULuwgdjRuDb+QvwZp+B6D9oCDZs3IwZn3+CggUKmNN89Mln2LBpC0YY300tqwXFZYj8RY3QzL+OV0S32baHZVWxt8figWbRPuOlfGnG1rdTs+btxrP+oURBtzuKeX4Nu/tuKeq7BZuPquLFNWs6GjfxqLrKTOzT0MzQ+yK5egS1o5F4r887+OKQbl352RRk8G3B7JlS4O92PNHHlo2o3fustW62zsowJMo4JXS9fxH/mkUnXepO8/c+4i9VZ55ry6OqVVhVzNgM0JBvdJAqUd1sietbs/ZrGhcXz4oy/LexOJ7vaJ3zzvryyIXXujaToep7XLriGyvAmMz3eY0QERERZQ0ZEiAUD9Wvi4/Gj8awkWMxYtT7iN72l5kZOG3KROOFbCGW/jQfo0e+i9tuvRUrf12NN97sj1+WrcT0zyaZxZFTz2pBcddueVp2SlxU0KAejj0XD1RF+2wvkBnBKja8CwtV3X5HIiOTLDZsZRiewp+zk5lOHI3EbF1nYDHje7Z3uzuKW9/TxZWt5doctYKW9zVVrSubxZn9s2WulXWYuI5EVZ+iW1HqI0fVuqpMSSl6vUR/noLlE/lE1/v3qVTI75pR4/99xD+6fkMrq00ri0r2IFfdt+X/5JPieH6wVfVE3/ec9ThGvWdlfroUfTX2q9kq9aftbHXeHcaXr7AOPFfp/Nt44Bt0auy6z63rLmP/WHfzWIdhUrRe/nDRPnH2bNJ0FQafWtdDcg3J8BohIiIiyhIyLEAoHnygNr6LmI1bbsmDbq+/hWYtWqN3v0GYMHEyRo2dgO5v9EHD0BZmf9069yNi7nQEB5dR3069EmEdzRd8B/eK6x3Uw7G3YsZSvMmtSE5irnUQSpe6B+W70a5PQ2DxRLOordUwx5ueGx0xqWLGh13r8EvsGJbMXok/1Xx79pmInw5XxktvqYBf9WZmwyFH1OdfoB1esjc+Xd0a/nOmKgK8pabr5yiqsh9X4j2PjapYGY5CL8PsVAajFH9+u+ntzvkb3XtjZztaMpYGTlyXX8LR0AlR2lL3BWG/Z/h5H3GtO8+X+knXYdqnrlltFt3iri5mHFjc96OnuuiSVKo1pkkW5tKBjnn0XWplgb7jVpxS7vkRr1a01dsmgcTGwL9+LjOL8+23MWWifgNCjevJXnde36VSjHwynncLSLrUr2d0gcR5XQzE0oqvIsLD/hHu+6iBLVAunBnJvhUF5zVCREREdPPLdl2aGE7C3th9KFHca3QpxS5dumTWP7hjZwyOHDmKnLlyovgdd+Dee2qgUcOHkT17hsYuM4WTp86oPsoIhW63irITERERudu1axdCQkLUUMpt37EbIXf51jQSERER0Y1ywwKElBgDhBmLAULKLA5FdEWYZAl6Ym99lyhLkuKoqmEfD6TuTfesTqKMwAAhERERBRIGCDMRBggzFgOERERE5A0DhERERBRIAq8cLxERERERERERETkwQJiJBAXxcGQU7msiIiIiIiIiIgujJJlInty5VR+lN+5rIiIiIiIiIiILA4SZSO7cuXDbrbcwuy0dyb6VfSz7moiIiIiIiIiI2EgJEREREVEibKSEiIiIAglT1YiIiIiIiIiIiAIYA4REREREREREREQBjAFCIiIiIiIiIiKiAMYAIRERERERERERUQBjgJCIiIiIiIiIiCiAMUBIREREREREREQUwBggJCIiIiIiIiIiCmAMEBIREREREREREQUwBgiJiIiIiIiIiIgCGAOEREREREREREREAYwBQiIiIiIiIiIiogCW7bpB9Xu0N3YfCuTPi5MnT+LixYtqLBHRzal06dK8n2USPBaZB49F5sFjkbmEhISovpTbvmM3Qu6qrIaIiIiIMiefAoTlgsuoISIiIiKirC86OpoBQiIiIgoYLGJMREREREREREQUwBggJCIiIiIiIiIiCmAMEBIREREREREREQUwBgiJiIiIiIiIiIgCGAOEREREREREREREAYwBQiIiIiIiIiIiogDGACEREREREREREVEAY4CQiIiIiIiIiIgogDFASEREREREREREFMAYICQiIiIiIiIiIgpgDBASEREREREREREFMAYIiYiIiIiIiIiIAhgDhERERERERERERAGMAUIiIiIiIiIiIqIAxgAhERERERERERFRAAusAOG1BFzZvwMXVn+DC79GmP/KsIwnIiIiIiIiIiIKRNmuG1S/R3tj96FccBk1dHO6fvkCzkVOx4Wo+bged0mNdcqW+xbcVqcV8oV2RLY8t6mxRERERBSooqOjERISooZSbvuO3Qi5q7IaIiIiIsqcbkiA8Nz581i1Ogqbt2zDv/sP4tjx48iZIyeKFi2MO41lPVC7Fuo+cB+CgoLUN1LuysEYnJo1AFdPHFBjvMtRuBRubz8SOUtWUmOSNnT4SBw5ehTjRr+HW265RY0lIiIiopsdA4REREQUSDK8iPHqqD/QpUdfTJ/1JbJly4ZmTRqh35uv4bUuL6HhQ3Vx6vQZjBk/CW++/S7+3Z98UC8pVw7txsnP3/IpOChkOplevkc3n8mfTsWef/5RQ05Lf1mGDyZMxOXLl83hk6dO4eXOXfDlV1+Zw3a/rl6NB+s3cOlknD+OnziBDi91xo6dO9WY5KXkO0REREREREREaSFDA4Sz5n5tBv+eaBqKudM/wdu9e+CJxxqjcKFCKFe2NFo9+TiGvdMPX0wej5LF70CvfuFYs3a9+rZ/rl08g9NfDkPC2eNqjG9kevmefD+jXbp0Cd179HQEpqRfxiVn9py5ju+ENmvuMcjkyzQSpHrmuRfMTvrduQfPZJ52+vv2aeo3bIy3+vZ3BOdOnT6NPv0G4JWu3fF6zzfRu+/baRoU27hpM65cuaKG/CMZoUOGjcTML6Zi7epVZjduzHvmOL+CfceP49y5c2rINyn5Tkokd4zdj6HsExKHMOdF53n94IsROKg+8c7tO/WHYo36xO7g3Jdt07yMOfvVBzZrhtnn0wDhbjFr13kYnYf1c5/Hg8Oi1Cc3oyiEm9vheZ9i9VCXbW0/95D6wMl9n7nvU0peovPO4zmlj5Xu7MfM/Rpx7RIdt/0RaC+f+XT9BS59rfOcJiIiIiJ/ZFiA8MtvfsBPi3/B+NFD0DbsaeTKlQuLf1mB5zt0wRt9BqHLG/3w4qtvYPvOGBQpXMjMKnyjW2eM/uBj/Llxi5qL7y6u+1+KMwHle/L9jCSBwN793jb7V0QuNjsh45IKEkqQbubsOY6gVod2bfHaG2+6BLR8naZ5y1bYf8BztqUEB+3BM1m/qLXrPAaQJKimA2yrVy7D+2NGIU+ePPj777/xVp/+eOSRBvhs8iR8NOEDdOzYHrP/Oxe7YmLUt41jd/Gi6vNPqZIlcPbsOWzYuEmN8Z1s36rfVuPjDz9A1SpV1Fjg4fr1Ebloocu4m1Vyx1iCg692fQ01765hHruFC+Zj85ZoBgnNIEYYJqI75pvndQR6YBJaJRmkUN+pMMZxLawdA/RyCwBKgKXVJKBHhDXN/O7AxDDXaeRlv9eiUIzX84nojr19nS//8nmrScHOzxOtnxWE6fW3Xn/pxqDpor43ZZDQCkr1hXWH9ECCg31jHftU9kfTJWEu22rtd/s+Mw6OsU89BRLJMzmn3Pdh4nNKgoPGsWrmvA7md481rgMdJCyBtjP0953d/O5SzUclNG1QwpxKmEGvsElIuz8nZVHG+d9rkeonIiIiIvJDhgQI9+0/iDkR36J3z64oXy7YHBe5/FfM+fJb9HztFcybMdnMGmz9VAsMHj4We/bGmtM0qF/HHDdx8jRcuXLVHOcLaYjk0uZlRk+S1St6Z3xPvu+pQZOkSMBNion+u2+fIxPQ1+DK+g0bsHNXDF7v3tWsz1A66T906DBi//1XTeVKAjrf//A/M+CnA1itn26FKpUr4Y/1f5rDvkwj2rVtY76YvdatixrjavnylWjwUH3HPPT6bdq8xefsugoVKuCLqVPQ/PFmaozxCmiMq1C+PLbvcM7j1ltvVX3+k+3avGWLWc+lryQA+2XE1y7bl1IShJNz4MDBg+a/cg7IOCH7SbI3dXaMzhBN6jvC/Xv2z3TGnwQ45VzT03jKEEzuGP+8yAq5dOv6qvlvkcKF0fON18wgofu8AsrqqZgYUwk9hoehpDmiBNoO744qMUuw3EO2n2n/KiyW73Sso0YY6v9/e3cCH0V5/w/8g9wIeIBAUiBRGiNNxYsKEkXhF0nUVsX6MwoV5YcoGOUoAmoBLfD3iEcEjVBRUShoFEu1HgmloNjE0CqnCiGiicZwyKFckiDwn+8zz+zOTmZ3Z5cQIft59zXNzuzM7OzMbMx++D7PMwk5GaWYPtsKUIoxM7cUyVlTMbCjuSR+wFSMSCpFwTIrqNqErzYa93XWUPTSS9AxE8OMj1DBEnM/vSZKqDLJ/7wc39A0wHd8OoR5yTp+0RPDJITJX+RegXcMix/wgi1EqqloyWIgY6jvnPquV/4sHbxuwtJFct5t59Q4H5Oz01CSO+u4Ox8/FzPYs993LvdURTnKEPg5iB8wFOlYjPxg1W0VebjX+FykZ79gu4bWfb7M+AzpBeSiGJPGLTbu7SzjHBMRERERRaZOAsLXF/4DF3Y/D93PO0fNy7goM1+YgzuGDFIhYKuWLdHutLa45rcZqk/C2XNfVeuJG6+/VvVVuGjJ+3pJeD9t+xoHdxxZAyTZXvYTKWkmOuGBP6vwTL7MTJpwv34mtLKychVuJXTurJcAbdu2Ve/dHuTZSbNUOZe/6X6BXmIGd+3btVPVfRI+eVknHFlHBmNxkmONi+vgDyP1a51mHLdX+6uqVLPjSLYJpWvXs9TPtWs/VT+9kABWwtnTT0/US6InIZxUULZq1cpXbSnLJGD708QHVVAry2SSa7DgbwuDbiMk+JNqT6lslOVS1SeBrz0kFNJUu0+fS9U6Ut3ZoUN7VQ0YSbD31VdlqnpQgkGLXJcfdu3CuvXr9ZLYU1leDiT1Qx9bWIGOCUiEPchzKC9xrXTq1TfNH6Do8MReJSVh1uldgJJFy3T1n3Ne6NCwi/mPLW7UMSMBp9uP2aFsY6nxvpKN91GfmOemho69ke4LXstR4i9Y9kvtFzq4orDiExz3pNvnRN/3yUFu36LZuShJysKwVL2APCuaMg4Fxrl7ZEDw3w1ERERERMHUSUC4as2n6Nvb/9d+5aYtqk+6bild9RK/s41lX2ws03NA48aNcElqT2Mfn+kl4R3avQOHD1TruejI9rKfSO3avRtDhwyOuBJNwhmnE1u0UEGP23NCRn+W13OSoGvz5i3Yu2+fp3XCkUCxZ48LVRNce7XgY0/k4ONPVug5k7yWVQUn0w03/QHbd7ifx6qqKsx/xQyDzz771+rnkWp54ono0+cy/LvoI1WRF4lEXd16NLgFtRIeW0GgGwn3npr2TED1p1XV5wx3pSpQmkMLq7ozkmAvWAgsIfVJrVurADtWqSCthgQkJwElG4OcFxU22asFDdJ/2rjFesYQJERM7GLsuLQE1qe+10TdZFg+U1OKUTTFbO78yAB7sGhTOBn9VWWivUIukDSxlWaI6UPtVYX1QRz69JMqNqta0KTOme8y9kRGhnHtAqoFpRl2iGbL5IFxDmcZ93dA6NwTk/OygNxM9d+DSYXFmJSZC9iqZgMYn5GZ9fK+PPrMz7S90pmIiIiIKDJ1EhBKMLJ3nz/MaNSokX5Uk1TMyWS3z9j28KFDeq7+kooyCXciIeGNhDiheFknHAmyJKiyh389LrwQ3S84X69hNg2+Ir2fqmCzV7KNHH0PdthCwrLycowcM1b1r3jySSdjzOiRqoq0tiR27owO7duhuHi5XvLzs6ot5fxJVaAXEu5JyGcPFYVU9e3ZszdkuBttsFcbVZT1UpeECL9098Rkq0826/MyAXgkO00/bwld5aeo5sr6sbG/YMGebxAS3f/enBoBotkXoaxj9R03uR5WaUkTZLMvR30+jCm/bx5G2FokS3PVnIzFGK2f75FqXJypxvXSz1MUVFP8mvdm5bJFviC8YJyEsGkYFiTcVuuyejByQZplExERERFFok4Cwkt69cBb7xbgkA752rdri7gO7bD285rVTas//RwXnNdNz8motz/ggw+LcOnFF+klHjRsZLyzwJAxYrK97KcOSQWXlya/dhIgSXVaKF7W8UJCQgn+rOmCC85TVYiWzp06qbDPCjnlZ9bwO1RVYekX/gFjEhMSMO2Jx9QgJWd3+zX+NPEBFH1k79j+yDRu3BiXp/0Pysq/RkVFsE7iajqaVXJyLnKnP6X6X5TmwBJKeOmfUpqsB4SyxiTzUh0Z6pqGqz4NJtL1Y8bG8ihGTZWQ0P95kT4AoZr+2pXjq5C3qFVxlefbj/S9V+AyoIbVR9vywqnABLlXnCP82geE6Id8uZ/q6WiwVj+F1jQ5tWazYv/5kukFDDSuBe/+KFnVsRnZgaGzqmb1D8KjBtBJkmDWbbRus2/IyMN4mjchFyXOc09EREREFKE6CQhv+t/+2LL1O/ztrXf1EmDMiOF4cc4reP/DIqz9bJ2a3n7vnyj+zycYOnigXgvIfW42OnfqqJoZe9WobSec0Ly1nouObC/7qStulVtSISYBXLCqLqkka92qlZ7zk5BHwiEJibysEy0JqCR8DNU0t02bNq6vb0np2hUZ6f3wyYqV2LN3r1565OI6dEBiQme8/e57qNq/Xy91J9V90v9jXYRj0qxYvihLn4PvvJcfNiS090ton8KNrBzu3nGSAFMqWJ28XOP6TjX5rcEMnEL1A+gmoN+/hGS4XcGAdQoX1ai4MivkkkIMqCFBoFTDLcbMoKPymk0/k0tzMTMW+txT/d7JP04EaZYtVJPv4H3jURASDsrowklZWDgx8L/TrgPGvCQhoaP5vdCVsul9vf+3nkyqwtheraxH+ZZ/SJB/KCAiIiIi8qJOAsJWrVpi3KgszH1lAQqL/6uWnXXmLzFx/Gi8t2gJpjySg0dzclVF4RMPP+ALlF76ax4++7wEY0fdqea9anhKBzTpnKLnoiPby37qigQwMlCGfcRiCWec/dbZuQ1iYvUlJ30GSujjZZ1ovfb6G6oZa9ezzIFB3JRs2KAqCCUorGs9e/bAt5Wb8P33P+gl7uQcuPWxGK1goayd9Bco1YRW1ajbNnJe5fwGG6QmFGmeXPHttxEFexImOkcslj4sw13j+k4NvOAcsdh1gJEwdP9qyf16mxVSrgOd6AFI9DrmYCN0pNTAF0hDRtAKK6v/PMdgNBSaFQ4a5zYnYJRsEWTAmGAY0EZtjuMfkJZL9wbG8vRseTzJXImIiIiIKIw6CQjFby44Fw/efw+mz3gez/zlRazf8AU6d/oFHp0yAa/NfQ5/ff4Z3DfmbrRo3hzL/7sCDz70uAoTH3tokmqOHKkTU6/HCc2i69dOtpPt61L3889XVWxP585QgZFM8vjcc7r5KsWk7zqpDrBGsJUBK6695nd4ee48X7Alo+JK5dgVGWZvWl7W8UK2fSJnmp4zj0Uq4GTADHkNeT4t46qA0XUlaJo2PRepF/XEL7t0wcI330TmwJuxctUqvYbx/bKiAu8v+xCdO3dSA4xY5P1njRiF399wky+wcr5/t3XsfhEfj759LjPuJzOUDkWaT/e+OFWNGGwPCZ2v6YVbKCv7kWOVYxZyvBLGhQpyrQFJnnl2ptreIse3YuVKPWeSkY2tcyA/ZXATCSCtgUu8sO6HZ2f8Rf209iP3jxxLzEodalY8TbCa427STfpslVESlEjlzhR/VVTRFHszSr2NGmHUChV7YpiqBJzgW69y/gRML/VXDMYPGKoqAUfb9ivNjmfmlgIZ/dBLmiDL6zqaCqvRTK0A0+XYfMcTMjQ7Tsn7tb/Xwslmv43Zk/yDthjL7E20zfPOAR4iYpzDHlY4WGg7tz5xGDhURu0eh0n2KlWrr0JWChIRERERHVMaHJYStRDKyr9BYkLtNbWVaq4Ff38b+YvfR4vmzXBmUhfEdWiP6qpqVG7egs/WlaB1q5a47pqrkHF5HzRp3FhvGbkf3szB3sIFMkqKXuJBgwYqHDzpmtF6QXAS2kh/chLESNNRCW4kYHpg4v0RBTMWCY9k4A5rZGAZAOTxRx/2VflJSCVhkfV6Fmu5kCapz0x7skbz01DrSBB0x/C7VMWZkzSFlfci7/X5F17yhWdur+O2nxFZwzFwwE16Dvh4xQpMf+ZZNGvaFE1latIU11z9W1xysf98rVy1Gp06dVT7kuat1us437/1etY6GzaUqpBL+iC0SPVizrSn1SAodw67Hc2aNcOOnTsx1rhul1/eFzfecINe02RdUzvrHETCvh+5jtf1vwaFhR+pUNXivI7Obaxrb91X0h+hkHN//XX9Mez223znoNdFPVU/jta5d+7byzUWzvWc+4ldMsCHbSRcaU5pr5iSsET6YPMtl9GGF2F0vrHMkpGN5Y4mmEJGH5VRh01JGJHnHGjA8dqG5Kw82yAkNZ9HjdBGgkTHKL3O93BccHkfmlRLSR9slYV5mDkr1z+wi8s5LTKuV75xvfz7CRZyUTAy4E3gPWdjv9d9VYYWt3vc+hzI4Dku16HGPizu+yLzcwL9maDorV27FikpR9YiRaxb/wVSfnWmniMiIiI6NtV5QGjZX1WF95cVYeNXZfhu23YV6pzWtg1SuiajV4/uqprqSB3+qRo/vPEY9n3yrreQ0HjNFhdciZN+PxYNGjXRC4mOXVagJ1V+UgVJREREtYMBIREREcWSny0grDPG29uz7BXs/ucLOFwVfITgBk2bo9XlQ9Cy900qKKwtbhVpTtFUqFHdclZ3unFWfNYFBoRERERHBwNCIiIiiiX1PyDUDu3bhX0fLcS+T97DwZ2bcPinA2jQqDEanhKHFhdcgRYX9ccJLY5s5GOiusaAkIiI6OhgQEhERESxpM4GKfm5SfjX8n9uQbtxryLu4Q8Q/1iR+inzspzhIBERERERERERxaKYCQiJ6iMZXfiN115h9SARERERERERRY0BIRERERERERERUQxjQEhERERERERERBTDGBASERERERERERHFMAaEREREREREREREMYwBIRERERERERERUQxjQEhERERERERERBTDGBASERERERERERHFMAaEREREREREREREMYwBIRERERERERERUQxjQEhERERERERERBTDGBASERERERERERHFMAaEREREREREREREMYwBIRERERERERERUQxjQEhERERERERERBTDGBASERERERERERHFMAaEREREREREREREMYwBIRERERERERERUQxjQEhERERERERERBTDGBASERERERERERHFMAaEREREREREREREMYwBIRERERERERERUQxjQEhERERERERERBTDGBASERERERERERHFMAaEREREREREREREMYwBIRERERERERERUQxjQEhERERERERERBTDGBASERERERERERHFMAaEREREREREREREMSy2AsJDh3GwsgrV/9mFquIf1E+Zl+VERERERERERESxqMFhg37sqqz8GyQmdNJzx6fDVYdQ9cFOVH+8G4erD+mlfg2anIAm3Vuh6aWnoEFTFlUSERERxbq1a9ciJSVFz0Vv3fovkPKrM/UcERER0bHpZwkId+/Zg2WFxVi95jN8XVGJ77ZtQ+NGjXHaaW1wuvFaF3Y/HxddeAEaNmyot4jewc3V2PfaFhzacUAvCe6EUxujxQ3t0bBDE70ktMlTH8KWrVvx+KMPo3nz5nopERERER3vGBASERFRLKnzcrnC4v9i2IhxmD3nVTRo0AAZl/fB+D/ehbuG/R8uvfgi7Pz+B2Tn5OKP9z2Iryu+1VtF5+CWauybt8lTOChkPVlftqPjz4y/zMKXX32l5/z++a8lePKp6di/f7+a37FzJ4YMHYZXX3tNzdt9WFiIHqm9AyZZFolt27fjlv8bivUlJXpJeNFsQ0RERERERERUG+o0IJwz/3UV/l2Znob5s5/FffeMwJX9+qLNqacisXNH9P/dFZgycTxenJGD+A7tMXr8JBQt/1hvHZnD+w7ix4VbcWj3Qb3EG1lftpPt69qPP/6IrBGjfMGUPJZl4cydN9+3TVrGVa4hk5d1JKT6/Q03qUkeOznDM9mnnbW9fZ3US/tizLh7feHczu+/x9jx9+P24Vm4e9Qfcc+4+2o1FFu5ajUOHPAWCDtJReifpzyEl1+cheWFy9T0ePbDallEYd+2bdi9e7ee8yaabaIR7ho7r6GcExKbMO9W/33d49Y8VOpngnNskzoZRfoZp6Ip5jqT3LLoijwM8u3DbT/O1wm2Xn1SjEmh3mPh5IBzMWj+Jv2EX+X8IQHruJ57CiGC+85xPQI/P9a1dEyOzxivV2jO8+PtdxQRERERkV+dBYSvLngT7xb8CzmP/hkDM69DkyZNUPCv93HjLcMwcuwEDBs5HrfeMRLrSkrRts2pqqpw5J1D8eiTz+CTlWv0XryrXrk76kpA2U62r0sSBN4z/j71+P3FBWoSsixUSCgh3ctz5/lCrVtuHoi7Rv4xINDyus5VV/dHxbfuVZsSDtrDMzm+4uX/cQ2QJFSzArbCD5bgiexH0KxZM2zcuBFjxt6Lyy7rjedm5OLpp57E4MGDMPev87GhtFRvDezbt08/iswv4uOwa9durFi5Si/xTt7fsn8X4plpT+Ks5GS9FLgkNRWL898JWHa8CneNJRy8Y/hdOKfb2eravfPWQqxes5YhoQpCMjEdWVio7us8jEAu+of8Aq636ZLt+ywszwZGpw7BvAq9itDh3+h8Pe+gvvRn5iIxW+/DmHIyFhv7qRnCJGfl+V9LTZPQSz9Xn5hByDiYvyFdSBg1rhwj8qzzkIf0RZnoMaVYr2Duo39uAnJ858q4OOPcg0QKLdx9p65XwPUw7uEuubjXca7Tbfe4ml7KRLx+jtcrNPkHhsDz4+V3FBERERFRoDoJCL+pqMS8vDdwz6jhOCMxQS1bvPRDzHv1DYy663a88tIMVTV4/bW/xQNTH8OXZeVqnd6pPdWy6TOex4EDP6llXshAJAc+22s80AsiZWwn27sNaBKKBG7STPTrb77xVQJ6DVc+XrECJRtKcXfWcNWfoUzyeNOmzSj/+mu9ViAJdP7+5j9U4GcFWNdf1x/JZybhvx9/oua9rCNuHjhAfbG4685hekmgpUs/QO+LU337sI5v1eo1nqvrunTpghdnzcRVV2ToJUCSsazLGWdg3Xr/Plq0aKEfRU7e1+o1a1Q/l15JAPtq3usB7y9aEsLJPfBtZaX6KfeALBNynqR606rwsCpEQ20jnNvZn7Mq/iTglHvNWsetQjDcNX4v34xc7hx+h/rZtk0bjBp5lwoJnfuKKYWzML00CSOmWoFFHAZOzUJy6SIstYd9dhXLUCDbDO6pFxhSJyEnoxTTZ/uDKnTMxBz5Qp9n7E8vsosf8IK6ZpNT9QJDr8Gy7mLkx2gFlXVOFmYl6SWBipYsBjKGYmBHvcC6XvmzdDi7CUsXlSI5a6gtyOqJydlpKMmdVY+rLn8OxZiZW4r07Bds18O4hycuw5wBcXouHF6vcOR8Bgazxj0/NA0I9TuKiIiIiMihTgLC1xf+Axd2Pw/dzztHzcu4KDNfmIM7hgxSIWCrli3R7rS2uOa3GapPwtlzX1XriRuvv1b1Vbhoyft6SXiHth/AoZ3RNTO1yPayn0hJM9EJD/xZhWfyJXbShPv1M6GVlZWrcCuhc2e9BGjbtq167/Ygz06apcq5/E33C/QSM7hr366dqu6T8MnLOuHIOjIYi5Mca1xcB38YqV/rNOO4vdpfVaWaHUeyTShdu56lfq5d+6n66YUEsBLOnn56ol4SPQnhpIKyVatWvmpLWSYB258mPqiCWlkmk1yDBX9bGHQbIcGfVHtKZaMsl6o+CXztIaGQptp9+lyq1pHqzg4d2qtqwEiCva++KlPVgxIMWuS6/LBrF9atX6+XxJ7K8nIgqR/62AIOdExAIkpRsCxIBVN5Cdxi8159jS/t+YuOLNRQr21XjpJSIDHBa+BSn23CVxv1Q7uOvZGeZF0v83zVkNoP6TEcvEbOw31XuAgFSEOGLeCuoaIcZUhCsvlvhy54vaKhfm8hAafbf28RERFwsBplxXmYdHsm0vrobhmMKa3/zch6NA9FFXXUF/2P5SiaPxm3XX8lLtHH0KP3lbjy5lH4f/OLURb+K5pH1ah8ZzKu7G29V0drlghVV6zAgucnGMd9LdJ8+zSm9GvR//YJmLlwRdTHvrt0GeY9OgqZ9nNiTGn9M3Hb/c/jrfU7jHdDREdTnQSEq9Z8ir69/d8QKjdtUX3SdUvpqpf4nW0s+2JjmZ4DGjduZPyC6Gns4zO9JLzDew4CP0VbPqgZ26v9RGjX7t0YOmRwxJVoEs44ndiihQp63J4TMvqzvJ6TBF2bN2/B3n37PK0TjgSKPXtcqJrg2qsFH3siBx9/skLPmeS1rCo4mW646Q/YvmOHfjZQVVUV5r9ihsFnn/1r9fNItTzxRPTpcxn+XfSRqsiLRKKubj0a3IJaCY+tINCNhHtPTXsmoPrTqupzhrtSFSjNoYVV3RlJsBcsBJaQ+qTWrVWAHavKNrqlEwlITgJKNgY5Lyq8cFQLSnPicYv1TPQq589CgUugUjDO/4fUkf7xd/yKQ59+xoXxVQuaiqZkYrrvMvZERoZx7QKqz6RJeIhmyxRUqPvODNeTkejoR7Nm/4HGZyXTvh97E3per4gVTkb/XGfVJRERVX+5EKOvT0PmmFwUfLYJu21p0+6t5fj4rVyMzrwSmdOLEWE39hHYg7Xzx+HK9JsxOncx1m7a4w+9Du7B9i9X4K3ccchMvxaj5392RMdRXb4Mj918Jfo/tBjbj/T9/PAZ5o27FpdkjsJjs5cZx70j8Nj27EDlZ8sw+/FRxrFfiUGRnMOtcpxpSLt1Aqa/tQJl9nNi2L11E9Z+MAf/b8i1+J+bc7C05lcWIqoldRIQSjCyd58/zGjUqJF+VJNUzMlkt8/Y9vChyJr7Ho+kokzCnUhIeCMhTihe1glHgiwJquzhX48LL0T3C87Xa5hNg69I76cq2OyVbCNH34MdtpCwzPjSOHLMWNW/4sknnYwxo0eqKtLakti5Mzq0b4fi4uV6yc/PqraU8ydVgV5IuCchnz1UFFLVt2fP3pDhbrTBXm1UUdZLXRJ8/aF50xOTC7ORnj/O/3mZADySnaafj5bZZDOgCW1FApIz0mz9j0k/hRK4xGZIKE2QF2YhIHDK75uHEbYWydIk0+zL0VrHuDhTjeulnycPPNx3KlwvzUV/ufet9YzPgISK/pAwAYkZVv+eMsm1Cuxnk9fLG2uwI6vPR+/NuImIYkB5Hm67NQdFYcOlapTljUPmuHdQVtsh4cE9WDrlD7gttzh8YHdwB4pyh0d8HNV7NqHknedx75ArccmACVjw5ZHX3G3/52Rc+bvhmF7oXvRRg/E+S4xzmHZ7XthjV/u+3vtxSsh7b+ZwzIvd2gWio6pOAsJLevXAW+8W4JAO+dq3a4u4Du2w9vOa1U2rP/0cF5zXTc/JqLc/4IMPi3DpxRfpJR40bCBJo56Jkmwv+6lDUsHlpcmvnQRIUp0Wipd1vJCQ0PoiKNMFF5ynqhAtnTt1UmGfFXLKz6zhd6iqwtIvvlDLRGJCAqY98ZgapOTsbr/GnyY+gKKPbJVWR6hx48a4PO1/UFb+NSoqvCckR7NKTs5F7vSnVP+L0hxYvsR56Z9SmqwHhLLGJPNSHRnqmoarPg0m0vVjxsbyKDr7l5DQ/3mRQRegmv1Fy6qaSkPORFvfhh3jMHBi4MAQvSZKeBKiCXQ9Z/VTaE2TU2s2UzX7bbOmFzAQ0tSVPPN63yVlYaFtwBGkDlVhbcES/Tu/o/E5mWh73uoz0tF8mNcrPP85mgpMkP9eBBnlm4go1uwpxqS7c1HiC6uaILH/eMxZuNj8vblsMd59Ogu92umnDduLH8XoWW6tSKJXMmsI7s23hWztemLwE3OxeKn+/b34XeQ9MajGcdz5UDG8DZ9ZjKnpmRj00BwsXe+9P/ZQyubejCsftFcgNkH8pYPwyAt5/uM2zt/iBS/gT/0TjGdt1ufizunBWwGW5Q3HtfZ9NzwV3TON67LgXXyo/nu2DB8W/L3GOUH1Z5h+dw7WRhCcEpE3dRIQ3vS//bFl63f421vv6iXAmBHD8eKcV/D+h0VY+9k6Nb393j9R/J9PMHTwQL0WkPvcbHTu1FE1M/bqhFMbo0HzI3trsr3sp664VW5JhZgEcMGquqSSrHWrVnrOT0IeCYckJPKyTrQkoJLwMVTT3DZt2ri+viWla1dkpPfDJytWYs/evXrpkYvr0AGJCZ3x9rvvoWr/fr3UnVT3Sf+PdRGOSbNi+Y+d9Dn4znv5YUNCe7+E9incyMrh7h0nCTClgtXJyzWu7xK7uA2GYQZOyV0iOy+qokqaXOp57/SoyDLwSZ6X0YnNJtCkqX7uwvSXp/qNDNUXHoUXeN+pz05piSPIi8PpXfTDYGr0s+mC1yuEOAx8ScLaxZjJkZ6JKOZVoyhnMgps3XInZ72AvHuuQnI7HWc1bII252ciZ+4MDDa7M1cq5z5Se60xKvLw/+bafiefdRvy8rIxrGcCWlmpWvOWSOx5G3IW/B2T+56qFwLb8yfjsQ9qJ/CLVGL/LKRb3ZO364mxc9/FwoduQ5+z4vzHbZy/VnFJuPqeuXg3Ow3+3syNY18wC2+5dYm+aSGm5n7mb0qccBUeMd537gjjusS19AWNTVqeqs/JXIywXRtsX4hJs2s3wCWiOgoIW7VqiXGjsjD3lQUoLP6vWnbWmb/ExPGj8d6iJZjySA4ezclVFYVPPPyAL1B66a95+OzzEowddaea9+qEkxuhYcemei46sr3sp65IACMDZdhHLJZwxtlvnZ3bICZWX3LSZ6CEPl7WidZrr7+hmrF2Pcv+2zpQyYYNqoJQgsK61rNnD3xbuQnff/+DXuJOzoFbH4vRChbK2kl/gVJNaFWNum0j51XOb7BBakKR5skV334bUbAnYaJzxGLpwzLcNa7v4hOMc+gcDVQPrJDeO4ImfMYfhjPzjT9K+/WOsLmyFQ6ixmiwQalRlMMEYjGkaHYuSkIOlmGc41mLaw5GQ5Fx3HfxvfshGeX4KuDLlTmQTMhwXQ1uEir84/UiIiKPtr6DF/Nt4dqlk5A7IMh/YFqmYNik24z/dllKMf0vywL6w4vWx6++7B/AruH5+NPDg5AYUG5n0/BUpD/4/3C97+vTHhQ8+6qtArIOteyJyU+PRq/ULBVoXn9GsIM2tUqdhMn97V1HrcDHbmNHxvVHzoxBSG4INOlm7Pul8ehTs1bBr2ECBj48Ht2N9S2V+R+4DgpIRNGrk4BQ/OaCc/Hg/fdg+ozn8cxfXsT6DV+gc6df4NEpE/Da3Ofw1+efwX1j7kaL5s2x/L8r8OBDj6sw8bGHJqnmyJFqcuFJaNAsurcn28n2dan7+eerKranc2eowEgmeXzuOd18lWLSd500MbVGsJUBK6695nd4ee48X7Alo+JK5dgVGWbvTF7W8UK2fSJnmp4zj0Uq4GTADHkNeT4t46qA0XUlaJo2PRepF/XEL7t0wcI330TmwJuxctUqvYbxfbKiAu8v+xCdO3dSA4xY5P1njRiF399wky+wcr5/t3XsfhEfj759LjPuJzOUDkWaT/e+OFWNGGwPCZ2v6YVbKCv7kWOVYxZyvBLGhQpyrQFJnnl2ptreIse3YuVKPWeSkY2tcyA/ZXATCSCtgUu8sO6HZ2f8Rf209iP3jxxLzFJNIo0/ECfk6WbGmzBvQi5KAvoB1IMwTPE3lS+aYu8DUG+TlIVHIuoXrBiTUq1wUJrK6sU+8nzg69pfa5j3y19/yLWwn4/CyRidL+fPVnlpLBtkq6yqnD/BrM6cam/qSqF4uu86ZmKY6pfQ39TVOtcqXNefG/u1UPe0DOZj/3zxeoWkrsWt1u8nU9EU6Y4gwn/EICKqh0reehVr9WOpsB48OA0h/xk/4UYM7qsfiyWLsDTKUXl9Dhbjrbf8IWWrqwfh6nBfbxumYOSo3nrGUDEH8+z/2XXl6N5GTzkZ+uloJfRHTnZm8EDTods5/j7qRdm37tXsrVJuQ+6MGZj7jMd9t7sKAy/Xj8Wmj/Cx+66JKEoNDkuJWghl5d8gMaGTnjtyUs214O9vI3/x+2jRvBnOTOqCuA7tUV1VjcrNW/DZuhK0btUS111zFTIu74MmjaNv5vvje9tR/Z8fgEgGNG5ghovNrwgfiEhoI/3JSRAjTUcluJGA6YGJ90cUzFgkPJKBO6yRgWUAkMcffdhX5SchlYRF1utZrOVCmqQ+M+3JGs1PQ60jQdAdw+9SFWdO0hRW3ou81+dfeMkXnrm9jtt+RmQNx8ABN+k54OMVKzD9mWfRrGlTNJWpSVNcc/VvccnF/vO1ctVqdOrUUe1Lmrdar+N8/9brWets2FCqQi7pg9Ai1Ys5055Wg6DcOex2NGvWDDt27sRY47pdfnlf3HjDDXpNk3VN7axzEAn7fuQ6Xtf/GhQWfqRCVYvzOjq3sa69dV9Jf4RCzv311/XHsNtv852DXhf1VP04WufeuW8v11g413PuJ3Zt8lXxKc5+1Qono4eEGr7lxcYX9EUYnW8btTgjG8vtfQcaZFABCa9q0OtWzh+iRiN1JwNESOjlODbh8lr1gwSi7qPXWgFqZWEeZs7KVZVsJmmWHVh5WWRcr3zjevn3Y51L8s77fRd4nzuvR81rmpyVFzDABq9XOC7XgueoVqxduxYpKSl6Lnrr1n+BlF+dqeeIqO44fj/GDcKcBfYKwSCsv+u0Pg8uxiOXe0zH3JTPQeaA531dbqQ/ZPzNcqmeCeXgCjx25SgssLLFKP++C/3f4aPgA+P83e8/f87/rh+J7W+NwpWPmt+VxdVPLMOf6uOfvEQ/kzoPCC37q6rw/rIibPyqDN9t265CndPatkFK12T06tFdVVMdsZ8O48e3t6F69W5vIaGEg+e0QvPftgUa1e0AJUTRsAI9qfKTKkgiIiKqHQwIiY5zPy7GvWmTsVTPtrp+BhaP9vCZjna7YAICxwQMmz8Xgz32AhQQ7rXsj+cLRuNsPetVXQeEJTMzMcjW32KthniO8Na9hQ0RRavOmhg7SQWZVAhm3T5YNT3+09iRuH3wH5Da8ze1Ew6KRg3Q/JrT0OzyNmjQJPRbledlPVm/NsNBqQqzRp8NNsk6dGyzmjO7XT9rsjchJiIiIiKin1HpWnysH4ruv/Y4glvzJCSfoR8bdn/6aUA3DkemCZrY+tELJ9neXHfPp1hzrDep3bMM8xbaDrJlf6T/Rj+uBdX77D1CtoSjC3ciOkI/W0BYZxoATXudhFajO6PZ/5yKE9o09geAxk+Zl+XyvKwn69cmabrp7AfCOUXTHJnqljT1zZ3+lOv1syZ53moOTkREREREP5/qLdthdtAjWqJNe6/NhOMQb29AV1rmGJE/Qq1a2fo9LEVJuX7oQZt29qa5pSiLYNu6Vl3xDibdPAEFvu4WT0X6pKEBA4scqZJP/c2LgTOR/Av9kIhqRf0PCLUGzU9A00tORqu7O+GkCafjpAfPUD9lXpbL80RERERERHT82/advdwuDvGn6odhNUEr+7/5H6xC1ZGMIHxafMCgWmvXBnQaG9LuqqqAQVWO6Dhq2497sH17OT7Oz8OkIVfiksxHUbBVP6dGYn4Rk1PtIxofoYMrkG8fkbrbpUiN4XEUiY4GpmJExzEZXfiN115h/4NERERERDbbtwQGhG0jCJMSu9ibI5ej8kia9sb9GmfbcrLKd9/GWg9BX/WXc5A1cbGtChIoq/j52xhLn4aqi6W0K3Hl1Tcja0ouCtb7g7s252fikQV/x+TLPSeynmx/Z45/wBZD9ysuBfNBotrFgJCIiIiIiIjqld17bGmSsyqwTqUg4ypbWLZ9IcY/VBwQ/DntLs7Bzbc+jxJnkHgsVRDW0ATdb87Gs9lZ6NNOL6otPxZj5vO25sVt+mOY/ZwSUa1gQEhERERERER0lJx9Y2BffNvzx+HKm3Pw1vpN2G2Nu1G9B5Xr38FjQ65E2piFKDsItDkrKaB5MmqxP7/aV42P545DZloa+t+fh7U/6MVHrBpF0x/BW9v1rKHP3UNx9jF9LoiOTwwIiYiIiIiIiI6WdlfhkYfTAprEVn+5EP9vSCbS+ugmu32uRP8hj2KBbq7bpu8kzJ3UL6APwsSO9kFLfh69JupBGpe+i3ffysOcJyZhxNXnI943Bkw1Kj/IxW2/G4KZn9mrOKOzu/ARTH1rh54znJWFEZfXYt+GROTDgJCIiIiIiIjqlTan2cO0Hdhmq0ALp/qgVdZXe1qlTkLec4NwdrjBlBueil5ZM5A3JQ1tjuUquSYt0aZNHJJ7pmHg+KewcPG7eD6rpz8EPViK2cNHYt6RjLxcnoes+xbDd+kaJmHEnzMDqyqJqNYwICQiIiIiIqJ6pVVre5XZbuz+UT/0oLLMnmol4PSO+uERapVyG55f/C7m3D8IfVLi0MZ2iK3apSA9Kxt5BX9HzoAUs3KwvAQl6lmRhOQE/fBY1LAlzh5gHH+2rVLyYCmmP/6OP+CLxJ4VeOy+XFs/jKci/eFpGFhL14KIamJASERERERERPVK24AKwk2otLVSDa06MExs2Sqgme8Ra9gSyVfdhkeey8O7Bbq5rjEtXjgDkwf0RKJtMJXKisCgMv7nb2EcVqvUP2JYTz0jVvwN+RX6sVcHyzFv5CgssL39+Jsfw+RUNi0mOpoYEBIREREREVG90qRToq0p6h5s3+K12XA5vtqgH4ozz/zZmrSWbSjVjwxnJSP5uBiYoyVSLz1fPxalKFrhOZ017EHRQyMxfb2eNUh/jM8PTdJzRHS0MCAkIiIiIiKi+iUhEcn6ofj4U1vYFsr2DVi7ST82tDojMWBwkTpzcAUK/60fG+J/c/5x0/demzan6kem3Xuq9KNw9qBoyh8wOj9wUJJnHzzG+2MkqicYEBIREREREVH90vx89LIVsu0u/MjWn19w1Ss+wsf6seh1YYp+VLeqP/gbFvgGAY5D+v/U9wq6Pfg4Z3hgONjuKuRMy0Qiw0GiOsGAkIiIiIiIiOqZU5Ha1xbubfonltqarbrbg/z8ZfqxoWFv9LG3lq0r0gffHNtxdLsR//tz5IPGcQSM1+JRydrP9CNTYkK4zhPNcDDL3ulgmzTkzB2PXux2kKjOMCAkIiIiIiKieqdN6m/R3Vd9tgmzc8OMqLv+Vcwu1o8Nra6+Dn1sg4YEqkbZwgno36c3evS+EoOmr8Bu/Yy7auz2VQSGVpY3GTN9LaJbIv0PV/0MzZzNvgAzbx2OmZ95PHCxZzFmz7e10Q4bsrqFg70x+flJDAeJ6hgDQiIiIiIiIqp/2l2FYf1t/eGteBSj55bDdbiSPcWYNG4OKvUsGp6PEYOCJ1vVhY/gtseXoVJ2dnAPSvJG4d63gg/GUTZ/ONJ+Nxzzvgw1WIqEjuNwZ66tv8RL/4ixqU30TN3Z/tYEs7lv9WeYffsfMHr+Z9h9UD8ZzA+fYebIyVhqW69N/5tChKxu4WBP/OnpqUhvp+eJqM4wICQiIiIiIqJ66exh9+JqW/ldycybce24PHy8VQd11XtQVvw8Rt88DgW28sLkYaNxdYiQ6uN/L65RMfjxPz9yrVDcXTjZDP2qP8P0m69E5pQ8FH25xxdUVu/ZYRxDHibJc48X+/chzWzvT0MrPVuXmpwUZxsYZAeKcocj7cohmDS/GGXbjWP3hYDV2L2pFEufn4D+1w7HbHsz7jZXYcKwYH047kHRo38IDAebpGDE09m4OkHPE1GdanDYoB+7Kiv/BokJnfQcEREREVH9t3btWqSkHPngBOvWf4GUX52p54joZ1Geh0E356IkXAWc1iYjG3kTe4YM5ooe7Y3Rb+kZy/nj8e7TNZsD7/7gUWROfAfbPb6+0u4qPPLSePQ5Sc+HUjgZPcYt1jPRSMKIvBcwsKOe1aq/XIjxo3JQFLJddhDt0jD5L5OCVAKa4eBoZ8Vly1MR36opcHA3Krd6bNaclIWFL2UeNyM8Ex3LWEFIRERERERE9VdCJuYsCBZW2TVBYmY28u4PHQ6K7hfXrOzrfvlFrn0Ftrp0PP4+fyquP8NLU2HjGPpPxcI8j+HgUdTkjP7IWZCHnMG9Ee+5lbN1/CHOd8U7mOnWHHvPDlRu2uQ9HCSiWsWAkIiIiIiIiOo3qWhbsBh5T2Th6t8koJUt8GrVLgHdrx6N3LfeRd6Inmjla1obXJPUe/H8PTo4a9gSyZlP4ZGrbf0dOjTp2Btj576Ld18Yj8GXpiD+JNsBGNu3OeN8XJ01FXPeNY7R2u+xoEkcet02FQsLjHPz9GgMvvx8JMa1RMDhSeVfSm8MzMpG3uJj7PiJyDM2MSYiIiIicmATYyIiIoolrCAkIiIiIiIiIiKKYQwIiYiIiIiIiIiIYhgDQiIiIiIiIiIiohjGgJCIiIiIiIiIiCiGMSAkIiIiIiIiIiKKYQwIiYiIiIiIiIiIYhgDQiIiIiIiIiIiohjGgJCIiIiIiIiIiCiGMSAkIiIiIiIiIiKKYQwIiYiIiIiIiIiIYhgDQiIiIiIiIiIiohgWUwHh4UPAtm+rsLbwB6xe9r36KfOynIiIiIiIiIiIKBY1OGzQj12VlX+DxIROeu74dKDqEP6TvwPL39uOqh9rpoFNm5+AHle0wYUZp6JxUxZVEhEREcW6tWvXIiUlRc9Fb936L5DyqzP1HBEREdGxqd6nYds3VeOlP5dh2d++cw0HhSyX52U9Wd+ryVMfQtaIUfjxxx/1EiIiIiIiIiIiouPLzxIQ7t6zB+8ULMZDj03DsJHj8fuBQ3DjLcNw9z1/wpNPz8S/P/oPDh48qNeO3o7N1ch7/GvVjNgLWU/Wl+3o+DPjL7Pw5Vdf6Tm/f/5rCZ58ajr279+v5nfs3IkhQ4fh1ddeU/N2HxYWokdq74BJlkVi2/btuOX/hmJ9SYleEl402xARERERERER1YY6DwgLi/+LYSPGYfacV9GgQQNkXN4H4/94F+4a9n+49OKLsPP7H5Cdk4s/3vcgvq74Vm8VOakKfPu5Svyw7YBe4o2sL9sFqzY8mqQSUSoSrWDKa3Xi3HnzfdukZVzlGjJ5WUdCqt/fcJOa5LGTMzyTfdpZ29vXSb20L8aMu9cXzu38/nuMHX8/bh+ehbtH/RH3jLuvVkOxlatW48CByK65RSpC/zzlIbz84iwsL1ympsezH1bLIgr7tm3D7t279Zw30WwTjXDX2HkN5ZyQ2IR5t/rv6x635qFSPxOcY5vUySjSzzgVTTHXmRQ0iy7GJN9+ZBqCeRX6KUvhZNvzxuQ7RudxBE6D5m9Sax1frPMR5Jw6zoXbe6ycPyRgneDnnoJxnsMeU4r1M35e1kFFHgbZ13H7fHlZx2Kt6+lzWj94Os9ERERERCHUaUA4Z/7rKvy7Mj0N82c/i/vuGYEr+/VFm1NPRWLnjuj/uyswZeJ4vDgjB/Ed2mP0+EkoWv6x3joynxf/gMovo2v6K9vJ9nVJgsB7xt+nHr+/uEBNQpaFCgklpHt57jxfqHXLzQNx18g/BgRaXte56ur+qPjWPZSVcNAensnxFS//j2uAJKGaFbAVfrAET2Q/gmbNmmHjxo0YM/ZeXHZZbzw3IxdPP/UkBg8ehLl/nY8NpaV6a2Dfvn36UWR+ER+HXbt2Y8XKVXqJd/L+lv27EM9MexJnJSfrpcAlqalYnP9OwLLjVbhrLOHgHcPvwjndzlbX7p23FmL1mrUMCVXAlonpyMJCdV/nYQRy0T9k+KC36ZLt+ywszwZGO4M9HWSMztfzbtQ641CWlWfbVwKmT/C/vgoHxpVjRJ5+3phyuuTiXhWMxWHgS/7l1rQwK8l4LgnpvePUPo4XZhAyDuZvSBcSDgacizykL8oMCExkH/1zE5DjOx/GxRl3vIalPw8JnZ3nMD1/nO08m8F06HUMcn9n5gK2+1vu3YDPl5d1fIzXnZCL2vtnp2Ofp/NMRERERBRGnQWEry54E+8W/As5j/4ZAzOvQ5MmTVDwr/dV0+KRYyeopsa33jES60pK0bbNqaqqcOSdQ/Hok8/gk5Vr9F68OVB9CJ99tAuhh18JTraT7WU/kZDATZqJfv3NN75KQK/hyscrVqBkQynuzhqO5s2bq0keb9q0GeVff63XCiSBzt/f/IcK/KwA6/rr+iP5zCT89+NP1LyXdcTNAweoLxZ33TlMLwm0dOkH6H1xqm8f1vGtWr3Gc3Vdly5d8OKsmbjqigy9BEgylnU54wysW+/fR4sWLfSjyMn7Wr1mjWrG7pUEsK/mvR7w/qIlIZzcA99WVqqfcg/IMiHnSao3rQoPq0I01DbCuZ39OaviTwJOudesddwqBMNd4/fyzcjlzuF3qJ9t27TBqJF3qZDQua+YUjgL00uTMGJqJuLVgjgMnJqF5NJFWOqs4rNULEOBbDO4p15gSJ2EnIxSTJ9t+9LeMRNz5At9nrE/vcipaHYuSjKyMWeALcgz9rX8Jet4ijEztxTp2S9gYEe1QOk1cVngNnYVebjXZZvjQfyAF9R9bAacNRUtWQxkDLW9L3298mfpcHYTli4qRXLWUPRSz4uemJydhpLcWUGrPCmQGTpPCjiHw+Sa5C/S59AKpkOtYygvgfEbDsNs92qvwY7Pl5d1tMr5E4zPaxpGBLk/6iNP55mIiIiIKIw6CQi/qajEvLw3cM+o4TgjMUEtW7z0Q8x79Q2Muut2vPLSDFU1eP21v8UDUx/Dl2Xlap3eqT3VsukznseBAz+pZV7s2nYA2yu99TsYjGwv+4mUNBOd8MCfVXgmX2InTbhfPxNamfGeJdxK6NxZLwHatm2rmmHbgzw7aZYqg1D/pvsFeokZ3LVv105V90n45GWdcGSdLVu36jk/Oda4uA7+MFK/1mnGcXu1v6pKNTuOZJtQunY9S/1cu/ZT9dMLCWAlnD399ES9JHoSwkkFZatWrXzVlrJMArY/TXxQBbWyTCa5Bgv+tjDoNkKCP6n2lMpGWS5VfRL42kNCIU21+/S5VK0j1Z0dOrRX1YCRBHtffVWmqgclGLTIdflh1y6sW79eL4k9leXG76OkfuhjD9I6JiARpShYFqTiTAUaNfXqmxbhl/Zi5OcD6X1tQaNT4SIUIA0ZqXreAxU6JmVhWATbHB824auN+qFdx95IT7KuVzlK/AXLfqn9kI7FyGdT46jFJ5j/fQ+lxjoJyUh2nnf1+UnA6dZnzss6whd8T0IfvShWebkWRERERER2dRIQvr7wH7iw+3noft45al5CpJkvzMEdQwapELBVy5Zod1pbXPPbDNUn4ey5r6r1xI3XX6tCskVL3tdLwtu3+yB+OhBl+aAm28t+IrVr924MHTI44ko0CWecTmzRQgU9bs+J77ZtU6/nJEHX5s1bsHffPk/rhCOBYs8eF6omuPZqwceeyMHHn6zQcyZ5LasKTqYbbvoDtu/YoZ8NVFVVhfmvmNf67LN/rX4eqZYnnog+fS7Dv4s+UhV5kUjU4fXR4BbUSnhsBYFuJNx7atozAdWfVlWfM9yVqkBpDi2s6s5Igr1gIbCE1Ce1bq0C7FhVttEtTUpAchJQsjHIeVFhk6NaUJpJjlusZzyqKEcZkpCc4OhH0N5cVgWYyUiU/VvPG1PQPvWM9WZK6DjUqkCsT+LQp59UTlnVgqaiKZmY7ruMPZGRYVy7gGpBOb8hmi2TB8Y5nGXc33Iv6iU1uawjVbTZaSgYJ/ftEMwrND8nEvL5KuK8rCP9Umaa1baT613wHSkv14KIiIiIKFCdBISr1nyKvr39f7FXbtqiBq3oltJVL/E721j2xUZ/INa4cSNcktrT2Mdnekn9JRVlEu5EQsIbCXFC8bJOOBJkSVBlD/96XHghul9wvl7DbBp8RXo/VcFmr2QbOfoe7LCFhGXl5Rg5ZqzqX/Hkk07GmNEjVUhcWxI7d0aH9u1QXLxcL/n5WdWWcv6kKtALCfck5LOHikKq+vbs2Rsy3I022KuNKsp6qUtChGFaT0y2+gGzPi8TgEey0/TzHqkqqVJMzzQ2nmpWnjr7F1MBZmku+sv+1fPGpMMUt5Cwctmielo9aJImyAuzYJwzfd6NKb9vHkbYWpxK8+ucjMUYbV2bVDm/xnnVz1MUVFP8MMFzkHVUs3DFuNfHuVe3hlunaMo4FBjLFk4MUW0bK7xcCyIiIiIihzoJCKVyau8+f7VTo0aN9KOapFpQJrt9xraHD3nvD/CEhrIPPRMl2V72U5ekgstLk187CZCkOi0UL+t4ISGhGVCY0wUXnKeqEC2dO3VSYZ8VcsrPrOF3qKrC0i++UMtEYkICpj3xmBqk5Oxuv8afJj6Aoo9qrzP1xo0b4/K0/0FZ+deoqAjWSVxNR7NKTs5F7vSnVP+L0hxYQgkv/VNKk/WAUNaYZF6qI0Nd03DVp8FEun7M2FgexWioEhL6Py/SZyCk2i9iSRiRZ+8r0KV/MQlGfH0SGlKHqkCsYInzc2X2vxd54Hl8sfoptKbJqTWbFUtI6F/HOL+Qak2KilUdG6p6L8g6MmDM6Py0wAE2JPC2jVAddp3Cycbz9n5CY5iXa0FERERE5KJOAsJLevXAW+8W4JAO+dq3a4u4Du2w9vOazR9Xf/o5Ljivm54Ddn7/Az74sAiXXnyRXhLeSW0bo0Wr4CGkF7K97KeuuFVuSYWYBHDBqrqkkqx1q1Z6zk9CHgmHJCTysk60JKCS8DFU09w2bdq4vr4lpWtXZKT3wycrVmLP3r166ZGL69ABiQmd8fa776Fq/3691J1U90n/j3URjkmzYvmCK30OvvNeftiQ0N4voX0KN7JyuHvHSQJMqWB18nKN67vELrbSMx8zcEruEtl5UdV+kTT7U32vlaLEkSva+xdTx1da4gi34nB6F/3QTg2eEqZPw/pINdWWf5wIMWKzqtaU5tx6nryRQEqa9oaq3gu6TpABYyQAxGLMVKNKh1/HrC6USlv/P6T0zzVudBUixtDo1F6uBRERmb8vpcsK73UE9LMyu9oJ2n0OEdWaOgkIb/rf/tiy9Tv87a139RJgzIjheHHOK3j/wyKs/Wydmt5+758o/s8nGDp4oF4LyH1uNjp36qiaGXvV8uRG6HxW9MGXkO1lP3VFAhgZKMM+YrGEM85+6+zcBjGx+pKTPgMl9PGyTrRee/0N1Yy161nmwCBuSjZsUBWEEhTWtZ49e+Dbyk34/vsf9BJ3cg7c+liMVrBQ1k76C5RqQqtq1G0bOa9yfoMNUhOKNE+u+PbbiII9CROdIxZLH5bhrnF9p8I452ipum/A9N4hAicn449R6fsvuV9v71VOanANaZYfGHD4+h00Hsf37odklOOrgD9yzcE6agSYMRqCqUFZQg7kYvXZ5hiMhkKzAinj3ObYK1jtQq4TZMCYAOHXCawGNSc1yrUEZcbjoKN51ydergURESnydwGyptpaZ9CxLQ4Dp2ahbJy/dQERHR11EhC2atUS40ZlYe4rC1BY/F+17Kwzf4mJ40fjvUVLMOWRHDyak6sqCp94+AFfUPLSX/Pw2eclGDvqTjUfifP6noLmLRvqucjIdrJ9Xep+/vmqiu3p3BkqMJJJHp97TjdfpZj0XSeVEdYItjJgxbXX/A4vz53nC7ZkVFypHLsiw+xNy8s6Xsi2T+RM03PmsUgFnAyYIa8hz6dlXBUwuq4ETdOm5yL1op74ZZcuWPjmm8gceDNWrlql1zC+01RU4P1lH6Jz505qgBGLvP+sEaPw+xtu8gVWzvfvto7dL+Lj0bfPZVj+X/OeC0WaT/e+OFWNGGwPCZ2v6YVbKCv7kWOVYxZyvBLGhQpyrQFJnnl2ptreIse3YuVKPWeSkY2tcyA/ZXATCSCtgUu8sO6HZ2f8Rf209iP3jxxLzFLNdUsxfUKebma8CfMmGF/EM4b6/7BU/xLdO2DwkKIp9n+Z1tskZeGRiMIK4w+ioWkoyc30/6up8VoyUqsvaOyYiWEZUj1lb5I5AdNLIwww6wu5FrbrYDY/ReCAFsYye1WZdb7YRDUCxjnsYQVShfbBQmzCrmM2ly/JnRBQxVE5fxYKfAG8l3VinJdrQUREJuPvhJn5aRjm+HtMurOwqtDVgFgB//Dqxqxq828TJLyS39G+ddyr4Iqm2Pfjso71d6Zviq768ai9R4O17+BV+8WY5NuPTP7Xd77/gMn6m079o/li5LOKkOioanBYStRCKCv/BokJnfTckVm5+lM88uTTqslxWp/eSE7qokIRu59++gmfrFyD9/65BN9WbsYD949Bx/jovgAse+M7FP1DqvD0Ag/kcHr9ri16//40vSQ4CW2kPzkJYqTpqAQ3EjA9MPH+iIIZi4RHMnCHNTKwDADy+KMP+6r8JKSSsMh6PYu1XEiT1GemPVmj+WmodSQIumP4XarizEmawsp7kff6/Asv+cIzt9dx28+IrOEYOOAmPQd8vGIFpj/zLJo1bYqmMjVpimuu/i0uudh/vlauWo1OnTqqfUnzVut1nO/fej1rnQ0bSlXIJX0QWqR6MWfa02oQlDuH3Y5mzZphx86dGGtct8sv74sbb7hBr2myrqmddQ4iYd+PXMfr+l+DwsKPVKhqcV5H5zbWtbfuK+mPUMi5v/66/hh2+22+c9Drop6qH0fr3Dv37eUaC+d6zv3ELvkDyTYSrlQm2at05I8/6ffLt7zY+GNnEUbnS9NHLSMbyx3N/uQPIgmvanCua+1fS87Kq1EVFbgvZ7+FJvnjrX9uwnEcIsgfl+6jDadnS1+DxnssNP7wn5WrmlKbap6LIuN85hvn078fBiuRki8M/pGhHdT9mxD4mXGy3+OO+9v1enhZx0bd64v6BX5O66nw14LNjaO1du1apKSk6LnorVv/BVJ+daaeI6Kfk/rvw8ahAb8bzb+PjO8t+u8F53xN+u/CLs7/lpUHbqP/22X9jVJj3mD+/Wb7b5qEgZm5SAyxTfjjq+movUfH32Zuf6Na7wn252Rfs5KD/3fa5X2H3YaIjlidBoRCmnsu+PvbyF/8Plo0b4Yzk7ogrkN7VFdVo3LzFny2rgStW7XEdddchYzL+6CJLeyJ1MGfDmPJq1vxyeIdnkJCCQcvSDsVfW9sh4aN6naAEqJoWIGeVPlJFSQRERHVDgaERPWNGXoV9LOHWGbAVRYQbLmtZ6MCr0VId4RrKuyDFai5BGwGt3WcrxO4jhu3Yw7laL1Hu+DHFP79OAXZlzqmEgzjP+oSHTV10sTY7uSTT8Jttw7EX194BgNuuA6nGPPfVHyLHd9/j04d4zFmxDDMnvkUrr6y3xGFg0JCvssHtkfGrXFo2jz0W5XnZT1ZvzbDQakKcy2Xtk2yDh3brObMbtfPmuxNiImIiIiI6Fgi/do6uqdw7VfaHOytZNEy3b2Mg+rTuaZefdOA/EW6Ga7Zh65zYDg1uJxvHbfXCdKPtJ2Xwdfsjtp79KIY+dLVSwQD5JndiNRsBo6OCUgEmxkTHU11HhBapImpVAhm3T4YD95/D/40diRuH/wHpPb8TY1mx0fE2NW5l52MrJwk9Lu5A07r2BSNmpj7l58yL8vleVlP1q9N0nTT2Xm6c4qmOTLVLWnqmzv9KdfrZ03yvNUcnIiIiIiIjiE6WAsQJAhTQV5pSc31RWo/pMvo+bNtfR5LdZu9Kwy31zKowe9sg8v1mpiHETBH3Zf+9oqmZGI6QvVZXYxJ0u9sUhaGef0KebTeoxc6nExOkIpFW3GFvb/oAMWYKX1tZw11qRJMQLJxyER09PxsAWFdkwrBC9JOwW3/7wyMnXUW7nu5q/op87I8XIUhERERERERHc8ScHqNPvfcloXSE5MLs5GeP84feE0AHslO089bJBjTD4OpWObvN9nYnxpYbahLH3vS/556LbPp7fKI++E7Wu8xDBVOymB6xsZTrcIKvV+3kLBwkXv1oE1ZebCBUIjoSDEVIzqOyejCb7z2CvsfJCIiIiKKir+izzsJ0GytiV7KNHZTrp+zlKLEuSiAWQ0og3dY+1mYlYSCcS6jAadO8q3zCCaowM5tROTgjtZ79MI5YFxPDDPep1tT5aIli43Vk5Go54mobjEgJCIiIiIiohjgCMoSkpGsH9qVbSyNOKgK2Eb1l1dTpQrYdDWfS7Vc/IAXVEhYkjsraD9/sk5OBlAwK8+9/0Cno/UevVCvXTMoNZtaO5n9FSb36x2yOtJz34tEFDEGhERERERERFS/uYV2alkpCpbZK/b0QCFhgqoAFXmYGRBumf3lFSwJbEarAraMfqp/PTMsrANH7T160LE30o3z4GwWrN67M2gMO/iKOfALER09DAiJiIiIiIionpPQzhmUmc1dS3InYJ6uLKycPwHTS22VfTI4h/TBZ+szr2jKEN/6ErbNm2AOHOIfXCQOA4fKiL/j/E2BCydjdH4SRgw2R/SNHzAU6ViM0QF98ZmDdFghYtEU6f/P/loGtR9bUGcd363BKgqP1nv0wjwPJbmZ/vNgvNa9MhBJJEGjUAFiGjI4vifRUdPgsEE/dlVW/g0SEzrpOSIiIiKi+m/t2rVISUnRc9Fbt/4LpPzqTD1HRD+nyvlD0H/jUCyfaIZ0FrVcgjnF0WeeDBAio/cmZWGhGhxERhtehNH5thF9M7Jr7FOxttXSs5dhckDAJaP7ZmK6rTIuOSsPc2whXOCxmQL243uNNOQUTnIZ/dd0NN6j27Epvv1ojvPgfI+KWqfc0V+hjTw/Kzlwv0RUqxgQEhERERE5MCAkqoekUi6zBMNCBGnHp2JMSl2EjHr3vixmkFoy1BmwElFtYhNjIiIiIiIiqv86ZmJYxmLMdI4SfJwrmjIOZVlD62k4aKhYhoJSNi8mOtoYEBIREREREVFM6DU4C7D1x3e8k34KRyO7ZpPdesPs/zAxu75WRxIdO9jEmIiIiIjIgU2MiYiIKJawgpCIiIiIiIiIiCiGMSAkIiIiIiIiIiKKYQwIiYiIiIiIiIiIYhgDQiIiIiIiIiIiohjGgJCIiIiIiIiIiCiGMSAkIiIiIiIiIiKKYQwIiYiIiIiIiIiIYhgDQiIiIiIiIiIiohjGgJCIiIiIiIiIiCiGMSAkIiIiIiIiIiKKYQwIiYiIiIiIiIiIYhgDQiIiIiIiIiIiohjGgJCIiIiIiIiIiCiGMSAkIiIiIiIiIiKKYQwIiYiIiIiIiIiIYhgDQiIiIiIiIiIiohjGgJCIiIiIiIiIiCiGMSAkIiIiIiIiIiKKYQwIiYiIiIiIiIiIYhgDQiIiIiIiIiIiohjGgJCIiIiIiIiIiCiGxVRAeOjwIXy9sxKLSwtRUPKh+inzspyIiIiIiIiIiCgWNThs0I9dlZV/g8SETnru+LT/pyr8bU0B3lj7HvZW/6iX+p3YpDl+f/YVuK5bOpo1aqqXEhEREVGsWrt2LVJSUvRc9Nat/wIpvzpTzxEREREdm36WgHD3nj1YVliM1Ws+w9cVlfhu2zY0btQYp53WBqcbr3Vh9/Nx0YUXoGHDhnqL6FX8sBmT/zldVQqG0/mUeEy6fAQ6ntRBLwlt8tSHsGXrVjz+6MNo3ry5XkpERERExzsGhERERBRL6ryJcWHxfzFsxDjMnvMqGjRogIzL+2D8H+/CXcP+D5defBF2fv8DsnNy8cf7HsTXFd/qraLz7Q+bcf+7j3kKB4WsJ+vLdnT8mfGXWfjyq6/0nN8//7UETz41Hfv371fzO3buxJChw/Dqa6+pebsPCwvRI7V3wCTLIrFt+3bc8n9Dsb6kRC8JL5ptiIiIiIiIiIhqQ50GhHPmv67CvyvT0zB/9rO4754RuLJfX7Q59VQkdu6I/r+7AlMmjseLM3IQ36E9Ro+fhKLlH+utIyNNiR97fxa27tmul3gj68t2bk2Rj7Yff/wRWSNG+YIpeSzLwpk7b75vm7SMq1xDJi/rSEj1+xtuUpM8dnKGZ7JPO2t7+zqpl/bFmHH3+sK5nd9/j7Hj78ftw7Nw96g/4p5x99VqKLZy1WocOHBAz0VGKkL/POUhvPziLCwvXKamx7MfVssiCvu2bcPu3bv1nDfRbBONcNfYeQ3lnJDYhHm3+u/rHrfmIfw/Ozi2SZ2MIv2MU9EUc51Jbll0RR4G+fbhth/n6wRbrz4pxqRQ77FwcsC5GDR/k37Cr3L+kIB1XM89eRbyHg5zvbxcC2v/vmlKsX7GhfWZ8fQ5rU/CfC6IiIiIiEKos4Dw1QVv4t2CfyHn0T9jYOZ1aNKkCQr+9T5uvGUYRo6dgGEjx+PWO0ZiXUkp2rY5VVUVjrxzKB598hl8snKN3ot3H2wsRsl3X+q5yMh2sn1dkiDwnvH3qcfvLy5Qk5BloUJCCelenjvPF2rdcvNA3DXyjwGBltd1rrq6Pyq+da/alHDQHp7J8RUv/49rgCShmhWwFX6wBE9kP4JmzZph48aNGDP2Xlx2WW88NyMXTz/1JAYPHoS5f52PDaWlemtg3759+lFkfhEfh127dmPFylV6iXfy/pb9uxDPTHsSZyUn66XAJampWJz/TsCy41W4ayzh4B3D78I53c5W1+6dtxZi9Zq1DAlVAJeJ6cjCQnVf52EEctE/ZPigt+mS7fssLM8GRqcOwbwKvYrQQcbofD3voIKTzFwkZut9GFNOxmJjPzUDgOSsPP9rqWkSeunn6hMzTBoH8zekCwkHx5VjRJ51HvKQvigzIFCSffTPTUCO71wZF2ece5BIHhjnPOQ9HOJ6hb8WZgA+eqP1+TPXSc8fFyQkNNafkIva+2en40PYzwURERERURh1EhB+U1GJeXlv4J5Rw3FGYoJatnjph5j36hsYddfteOWlGapq8Pprf4sHpj6GL8vK1Tq9U3uqZdNnPI8DB35Sy7yo+qkaS774CGG6VwxKtpPtZT+RkMBNmol+/c03vkpAr+HKxytWoGRDKe7OGq76M5RJHm/atBnlX3+t1wokgc7f3/yHCvysAOv66/oj+cwk/PfjT9S8l3XEzQMHqC9ed905TC8JtHTpB+h9capvH9bxrVq9xnN1XZcuXfDirJm46ooMvQRIMpZ1OeMMrFvv30eLFi30o8jJ+1q9Zo3q59IrCWBfzXs94P1FS0I4uQe+raxUP+UekGVCzpNUb1oVMFaFaKhthHM7+3NWxZ8EnHKvWeu4VQiGu8bv5ZtfLe8cfof62bZNG4waeZcKCZ37iimFszC9NAkjpmYiXi2Iw8CpWUguXYSl9rDPrmIZCmSbwT31AkPqJORklGL6bFuo0TETcyTwyDP2pxfZxQ94QV2zyal6gaHXYFl3MfJjtOLNOicLs5L0kkBFSxYDGUMxsKNeYF2v/Fk6nN2EpYtKkZw11Bag9sTk7DSU5M5i5VXEijFp3GLjfGYhXS+xC329vFwL4/q9ZHxGXrI+f6Inhsn+8hfVuF6V8ycYn9c0jAhyf9RX4T4XRERERETh1ElA+PrCf+DC7ueh+3nnqHkJ4Ga+MAd3DBmkQsBWLVui3Wltcc1vM1SfhLPnvqrWEzdef63qq3DRkvf1kvC27NmGb74/skoQ2V72EylpJjrhgT+r8Ez+WJ804X79TGhlZeUq3Ero3FkvAdq2baveuz3Is5NmqXIuf9P9Ar3EDO7at2unqvskfPKyTjiyjgzG4iTHGhfXwR9G6tc6zThur/ZXValmx5FsE0rXrmepn2vXfqp+eiEBrISzp5+eqJdET0I4qaBs1aqVr9pSlknA9qeJD6qgVpbJJNdgwd8WBt1GSPAn1Z5S2SjLpapPAl97SCikqXafPpeqdaS6s0OH9qoaMJJg76uvylT1oASDFrkuP+zahXXr1+slsaeyvBxI6oc+vsDJ0DEBiShFwbIgv2fKS1wrmHr1TXMNNSKiXtuuHCWlQGJCnJ6PZZvw1Ub90K5jb6QnWdfLPF81pPZDegwHr9EqmjIOBUlZeGSA+Y9/kYn+WpRtNDZMSg78LFTk4d7cUqRnT0IfvYiIiIiIiLypk4Bw1ZpP0be3vwSmctMW1Sddt5Sueonf2cayLzaW6TmgceNGuCS1p7GPz/SS8Hbt34Pqg9H1Q2eR7WU/kdq1ezeGDhkccSWahDNOJ7ZooYIet+eEjP4sr+ckQdfmzVuwd98+T+uEI4Fizx4Xqia49mrBx57IwcefrNBzJnktqwpOphtu+gO279ihnw1UVVWF+a+YYfDZZ/9a/TxSLU88EX36XIZ/F32kKvIikairW48Gt6BWwmMrCHQj4d5T054JqP60qvqc4a5UBUpzaGFVd0YS7AULgSWkPql1axVgxyoVRNSQgOQkoGRjkPOiAg5HtaA0Jx63WM9Er3L+LBQgCcmO27VgnPmZMydHU+aYEYc+/aSyzKoWNBVNycR032XsiYwM49oFVAtKM1Y2z4yUNGsdnW+vro1UdNfCfF0gfaj9dYsxKTMXJRnZARW3RERERETkTZ0EhBKM7N3nDzMaNWqkH9UkFXMy2e0ztj186JCeq7+kokzCnUhIeCMhTihe1glHgiwJquzhX48LL0T3C87Xa5hNg69I76cq2OyVbCNH34MdtpCwrLwcI8eMVf0rnnzSyRgzeqSqIq0tiZ07o0P7diguXq6X/Pysaks5f1IV6IWEexLy2UNFIVV9e/bsDRnuRhvs1UYVZb3UJSHCAKQnJlv9pFmflwnAI9lp+vloFWNmbmlgE9qKBCRnpNn6cJN+CksxPTM2Q0JparkwC8b71+fdmPL75mGEreVlr4lWX47WOsbFmWpcL/08eeCr1nvB1pw7ct6vhdkXoaxj9VloDwKtSsaFE23N+omIiIiIyLM6CQgv6dUDb71bgEM65Gvfri3iOrTD2s9rVjet/vRzXHBeNz0no97+gA8+LMKlF1+kl4TXsMEJNULGSMn2sp+6JBVcXpr82kmAJNVpoXhZxwsJCa0AQqYLLjhPVSFaOnfqpMI+K+SUn1nD71BVhaVffKGWicSEBEx74jE1SMnZ3X6NP018AEUf1d6gMI0bN8blaf+DsvKvUVHhPSE5mlVyci5ypz+l+l+U5sDyJddL/5TSZD0glDUmmZfqyFDXNFz1aTCRrh8zNpZHMRqqhIT+z4v0oQZprhw1q7IqDTn2EKRjHAZODByQpNdECVhCNIGu56z+2KxpcmrNpqwSTPnXeQEDUQ7e/d6pgUBqqVrP27XQfRGqdfohX34fWgMFqUFSjqSSkYiIiIiI6iQBu+l/+2PL1u/wt7fe1UuAMSOG48U5r+D9D4uw9rN1anr7vX+i+D+fYOjggXotIPe52ejcqaNqZuxV+1ZtcVKzVnouOrK97KeuuFVuSYWYBHDBqrqkkqx1q5rvU0IeCYckJPKyTrQkoJLwMVTT3DZt2ri+viWla1dkpPfDJytWYs/evXrpkYvr0AGJCZ3x9rvvoWr/fr3UnVT3Sf+PdRGOSbNi+YIrfQ6+815+2JDQ3i+hfQo3snK4e8dJAkypYHXyco3ru8Qubp3+m4FTcpfIzotrv2meSDgozWSTMCLPy+jEZhNo0irMwClkP42q38iaTbfJXYEErvYKWT2CrtnUveYo2xEJey16YrIM7FOai5mFemAaadJvqxrtL5W2xvP9jcccnZqIiIiIKLw6CQhbtWqJcaOyMPeVBSgs/q9adtaZv8TE8aPx3qIlmPJIDh7NyVUVhU88/IAvUHrpr3n47PMSjB11p5r36tQWJ6Nb3JGNRivby37qigQwMlCGfcRiCWec/dbZuQ1iYvUlJ30GSujjZZ1ovfb6G6oZa9ezzIFB3JRs2KAqCCUorGs9e/bAt5Wb8P33P+gl7uQcuPWxGK1goayd9Bco1YRW1ajbNnJe5fwGG6QmFGmeXPHttxEFexImOkcslj4sw13j+i4+wTiHzhGLVeCUhPTeEQwMUpGHmflAcr/eEVY5WeEgvDfnVKMohwnEYkjR7FyUIA0ZQavdjHM8a3HNwWgoKDX6dsBkNgtOz5bHXkLsYCK/FoEViOakRvOVJsfG4zkD+DkgIiIiIgqnztrQ/uaCc/Hg/fdg+ozn8cxfXsT6DV+gc6df4NEpE/Da3Ofw1+efwX1j7kaL5s2x/L8r8OBDj6sw8bGHJqnmyJG6qmtftG4WXb92sp1sX5e6n3++qmJ7OneGCoxkksfnntPNVykmfddJZYQ1gq0MWHHtNb/Dy3Pn+YItGRVXKseuyDB7cPKyjhey7RM50/SceSxSAScDZshryPNpGVcFjK4rQdO06blIvagnftmlCxa++SYyB96MlatW6TWgmgC/v+xDdO7cSQ0wYpH3nzViFH5/w02+wMr5/t3WsftFfDz69rnMuJ/MUDoUaT7d++JUNWKwPSR0vqYXbqGs7EeOVY5ZyPFKGBcqyLUGJHnm2Zlqe4sc34qVK/WcSUY2ts6B/JTBTSSAtAYu8cK6H56d8Rf109qP3D9yLDErdShGJJVi+gTdnFECDNW80t4PYB4GGfdJjyn+pvJFU+x9AOpt1GivkYQVxZiUaoWDgX2umeT5wNe1v9awWmj+edyRa2E/H6r5qZw/W2hlLLNXlVXOn2BWZ7KJat0Ldy1cPlu+ezxk6EtERERERJFocFhK1EIoK/8GiQmd9NyRk2quBX9/G/mL30eL5s1wZlIXxHVoj+qqalRu3oLP1pWgdauWuO6aq5BxeR80adxYbxm5lz/+G15d9Q9VheeVhDQ3nvs73NL9Or0kOAltpD85CWKk6agENxIwPTDx/oiCGYuERzJwhzUysAwA8vijD/uq/CSkkrDIej2LtVxIk9Rnpj1Zo/lpqHUkCLpj+F2q4sxJmsLKe5H3+vwLL/nCM7fXcdvPiKzhGDjgJj0HfLxiBaY/8yyaNW2KpjI1aYprrv4tLrnYf75WrlqNTp06qn1J81brdZzv33o9a50NG0pVyCV9EFqkejFn2tNqEJQ7h92OZs2aYcfOnRhrXLfLL++LG2+4Qa9psq6pnXUOImHfj1zH6/pfg8LCj1SoanFeR+c21rW37ivpj1DIub/+uv4YdvttvnPQ66Keqh9H69w79+3lGgvnes79xK5Nvio+RSqTXrKFSYWT0UNGKPYtL0bRlEUYnW8btTgjG8sdAygUTemtwqsa9LoyWqtqKulKBiaR0MtxbMLlteoHCUTdR7i1AtTKwjzMnJVrNoFVpFl2YOVlkXG98o3r5d+PdS4peua1QUCQHf56ebsWLvtxfgYd1GdnUb+Q69Qf4c8zRWft2rVISUnRc9Fbt/4LpPzqTD1HREREdGyq84DQsr+qCu8vK8LGr8rw3bbtKtQ5rW0bpHRNRq8e3VVQd6QOHPwJz/8nD299tthTSCiveXVKGm67MBONGwYfaZnoWGEFelLlJ1WQREREVDsYEBIREVEs+dkCwrpy2Phf/vpleH75q9hbHXyE4BObNMdtPW5Exlm90cD4X21xq0hziqZCjeqWs7rTjbPisy4wICQiIjo6GBASERFRLKn3AaFlX/WP+NcXRXhn3VJU7tqK6p+q0aRRE8S3boeruvbB//yyF1o0qbtgh6g2MCAkIiI6OhgQEhERUSyJmYCQqD5iQEhERHR0MCAkIiKiWFJnoxgTUe2T0YXfeO0VhoNEREREREREFDUGhERERERERERERDGMASEREREREREREVEMY0BIREREREREREQUwxgQEhERERERERERxTAGhERERERERERERDGMASEREREREREREZQdu5QAAEgxSURBVFEMY0BIREREREREREQUwxgQEhERERERERERxTAGhERERERERERERDGMASEREREREREREVEMY0BIREREREREREQUwxgQEhERERERERERxTAGhERERERERERERDGMASEREREREREREVEMY0BIREREREREREQUwxgQEhERERERERERxTAGhERERERERERERDGMASEREREREREREVEMY0BIREREREREREQUwxgQEhERERERERERxTAGhERERERERERERDGMASEREREREREREVEMY0BIREREREREREQUwxgQEhERERERERERxTAGhERERERERERERDGMASEREREREREREVEMY0BIREREREREREQUwxgQEhERERERERERxbDYCggPHcKh8m9Q/c+lqM5frH7KvCwnIiIiIiIiIiKKRQ0OG/RjV2Xl3yAxoZOeOz4d3r8f1QveQtXrf8fhvfv0Ur8GJ7ZA0/+9Fk2uvxoNmjXTS4mIiIgoVq1duxYpKSl6Lnrr1n+BlF+dqeeIiIiIjk31voLw0DffYk/WWOx/ab5rOChkuTwv68n6Xk2e+hCyRozCjz/+qJcQEREREREREREdX36WgHD3nj14p2AxHnpsGoaNHI/fDxyCG28Zhrvv+ROefHom/v3Rf3Dw4EG9dvQOVVRi770Pms2IPZD11PrGdnT8mfGXWfjyq6/0nN8//7UETz41Hfv371fzO3buxJChw/Dqa6+pebsPCwvRI7V3wCTLIrFt+3bc8n9Dsb6kRC8JL5ptiIiIiIiIiIhqQ50HhIXF/8WwEeMwe86raNCgATIu74Pxf7wLdw37P1x68UXY+f0PyM7JxR/vexBfV3iv5nOSqsB9j07DoS3f6SXeyPqyXbBqw6NJKhGlItEKprxWJ86dN9+3TVrGVa4hk5d1JKT6/Q03qUkeOznDM9mnnbW9fZ3US/tizLh7feHczu+/x9jx9+P24Vm4e9Qfcc+4+2o1FFu5ajUOHDig5yIjFaF/nvIQXn5xFpYXLlPT49kPq2URhX3btmH37t16zptotolGuGvsvIZyTkhswrxb/fd1j1vzEP6fERzbpE5GkX7GqWiKuc4ktyy6Ig+DfPtw24/zdYKtV58UY1Ko91g4OeBcDJq/ST/hVzl/SMA6rueeQnKeQ+fnwrqvXacpxXotU419OZ6v8Tnw9BmMJdH8jiIiIiIi8qvTgHDO/NdV+Hdlehrmz34W990zAlf264s2p56KxM4d0f93V2DKxPF4cUYO4ju0x+jxk1C0/GO9dWQOLP0QB9dv0HORke1k+7okQeA94+9Tj99fXKAmIctChYQS0r08d54v1Lrl5oG4a+QfAwItr+tcdXV/VHzrHspKOGgPz+T4ipf/xzVAklDNCtgKP1iCJ7IfQbNmzbBx40aMGXsvLrusN56bkYunn3oSgwcPwty/zseG0lK9NbBvX3Th7C/i47Br126sWLlKL/FO3t+yfxfimWlP4qzkZL0UuCQ1FYvz3wlYdrwKd40lHLxj+F04p9vZ6tq989ZCrF6zliGh+uKdienIwkJ1X+dhBHLRP+QXcL1Nl2zfZ2F5NjA6dQjmVehVhA49RufreQcVmmTmIjFb78OYcjIWG/upGYwlZ+X5X0tNk9BLP1efmEHSOJi/IV1IODiuHCPyrPOQh/RFmQGBk+yjf24Ccnznyrg449yDRHIn4V/gOaz5ueg10XrONmWnqefS+/ZUP4XrvjaO84e28jkxPgew3eM5XcJ9BmNJNL+jiIiIiIgC1VlA+OqCN/Fuwb+Q8+ifMTDzOjRp0gQF/3pfNS0eOXaCamp86x0jsa6kFG3bnKqqCkfeORSPPvkMPlm5Ru/Fm8NVVTjwrw+MByHHXwnO2E62l/1EQgI3aSb69Tff+CoBvYYrH69YgZINpbg7aziaN2+uJnm8adNmlH/9tV4rkAQ6f3/zHyrwswKs66/rj+Qzk/Dfjz9R817WETcPHKC+dN115zC9JNDSpR+g98Wpvn1Yx7dq9RrP1XVdunTBi7Nm4qorMvQSIMlY1uWMM7BuvX8fLVq00I8iJ+9r9Zo1qhm7VxLAvpr3esD7i5aEcHIPfFtZqX7KPSDLhJwnqd60KjysCtFQ2wjndvbnrIo/CTjlXrPWcasQDHeN38s3I5c7h9+hfrZt0wajRt6lQkLnvmJK4SxML03CiKmZiFcL4jBwahaSSxdhqT3ss6tYhgLZZrA/BEHqJORklGL6bFtlVMdMzJEv9HnG/vQiu/gBL6hrNjlVLzD0GizrLkZ+jFa8WedkYVaSXhKoaMliIGMoBnbUC6zrlT9Lh7ObsHRRKZKzhtoC1J6YnJ2GktxZ9bjqsnaZ4Z89hDbO89A0INTnQio/xy1WYbbvnq7Iw8x847OS59jXS7b7vrwExm9BDBsQpxfoz0HI14oh0fyOIiIiIiJyqJOA8JuKSszLewP3jBqOMxIT1LLFSz/EvFffwKi7bscrL81QVYPXX/tbPDD1MXxZVq7W6Z3aUy2bPuN5HDjwk1rmxeEt3+Hg10f2V7FsL/uJlDQTnfDAn1V4Jl9iJ024Xz8TWpnxniXcSujcWS8B2rZtq5ph24M8O2mWKoNQ/6b7BXqJGdy1b9dOVfdJ+ORlnXBknS1bt+o5PznWuLgO/jBSv9ZpxnF7tb+qSjU7jmSbULp2PUv9XLv2U/XTCwlgJZw9/fREvSR6EsJJBWWrVq181ZayTAK2P018UAW1skwmuQYL/rYw6DZCgj+p9pTKRlkuVX0S+NpDQiFNtfv0uVStI9WdHTq0V9WAkQR7X31VpqoHJRi0yHX5YdcurFu/Xi+JPZXlxu+jpH7o4wucDB0TkIhSFCwLUnGmAo2aevVNA/IXHVkIpV7brhwlpUBigj88iV2b8NVG/dCuY2+kJ1nXyzxfNaT2Q3oMB6+1QX1WkIDT7Z8Vm8r5s1DgCPoqly1CifPz5ZSQXDMUV5+x4K8VS6L6HUVERERE5FAnAeHrC/+BC7ufh+7nnaPmJUSa+cIc3DFkkAoBW7VsiXantcU1v81QfRLOnvuqWk/ceP21KiRbtOR9vSS8Qz/sAqqr9VyUjO3VfiK0a/duDB0yOOJKNAlnnE5s0UIFPW7Pie+2bVOv5yRB1+bNW7B33z5P64QjgWLPHheqJrj2asHHnsjBx5+s0HMmeS2rCk6mG276A7bv2KGfDVRVVYX5r5jX+uyzf61+HqmWJ56IPn0uw7+LPlIVeZFI1OH10eAW1Ep4bAWBbiTce2raMwHVn1ZVnzPclapAaQ4trOrOSIK9YCGwhNQntW6tAuxYVbbRLU1KQHISULIxyHlRYZOjWlCaSY5brGeiZ4YsSUh23K4F48zPnDk5mjLHjDj06WdcGF+1oKloSiam+y5jT2RkGNcuoFpQmmiGaLZM4RVORv9cZ2WmXTFmujyvPl9dEhAf0G+k4/6VStvsNH2PG88Vmp+l9Oz62Yw+UlH9jiIiIiIicqiTgHDVmk/Rt7e/jVzlpi1q0IpuKV31Er+zjWVfbPQHYo0bN8IlqT2NfXyml9RfUlEm4U4kJLyRECcUL+uEI0GWBFX28K/HhRei+wXn6zXMpsFXpPdTFWz2SraRo+/BDltIWFZejpFjxqr+FU8+6WSMGT1ShcS1JbFzZ3Ro3w7Fxcv1kp+fVW0p50+qAr2QcE9CPnuoKKSqb8+evSHD3WiDvdqooqyXJMDQD73picmF2UjPH+f/vEwAHtH9r0XPDFkCmtBWJCA5I83Wf5v0U1iK6ZmxGRJKE+SFWTDevz7vxpTfNw8jbC2SpXms2ZejtY5xcaYa10s/T975BiLR/T7OsVUHBihcVKN60FfxKZ+TJf189695/QLvX9V0XDHu7XG5KEnKwjBb0/uYF/HvKCIiIiKiQHUSEErl1N59/mqnRo0a6Uc1SbWgTHb7jG0PHzqk58Jr0LCh8X9H+NaM7dV+6pBUcHlp8msnAZJUp4XiZR0vJCS0vsDJdMEF56kqREvnTp1U2GeFnPIza/gdqqqw9Isv1DKRmJCAaU88pgYpObvbr/GniQ+g6CPHiJVHoHHjxrg87X9QVv41Kiq8JyRHs0pOzkXu9KdU/4vSHFi+UHvpn1KarAeEssYk81IdGeqahqs+DSbS9WPGxvIoOvuXkND/eVn+UiagmmBGy6pyS0PORFvfhh3jMHBiYCVVr4kSdsVu80Krn0Jrmpxas1lx4AAaL2AgysG7P3L+8zgVmCC/o9xHllYBX1Kyo3m8lpGN5bZ7On7A0ID7VwaVGZ1vD8GN+7s0F/3r9UjdEYrqdxQRERERkV+dBISX9OqBt94twCEd8rVv1xZxHdph7ec1mz+u/vRzXHBeNz0H7Pz+B3zwYREuvfgivSS8E4x9Nzi5tZ6Ljmwv+6krbpVbUiEmAVywqi6pJGvdqpWe85OQR8IhCYm8rBMtCagkfAzVNLdNmzaur29J6doVGen98MmKldizd69eeuTiOnRAYkJnvP3ue6jav18vdSfVfdL/Y12EY9KsWL7gSp+D77yXHzYktPdLaJ/Cjawc7t5xkgBTKlidvFzj+i6xi9tgGGbglNwlsvOimgIGC0lC0qOUykAEAYM5BGM2LyStwgz/QvbTqPq0q9l0m7ySgUUkmF6MmTVGgy5Gfr7xeenX21HlFofTuxg/aoRb9vs3yKAyEhK6vlbsqc3fUUREREQUu+okILzpf/tjy9bv8Le33tVLgDEjhuPFOa/g/Q+LsPazdWp6+71/ovg/n2Do4IF6LSD3udno3KmjambsVYNTT0GjbkfWp51sL/upKxLAyEAZ9hGLJZxx9ltn5zaIidWXnPQZKKGPl3Wi9drrb6hmrF3PMgcGcVOyYYOqIJSgsK717NkD31Zuwvff/6CXuJNz4NbHYrSChbJ20l+gVBNaVaNu28h5lfMbbJCaUKR5csW330YU7EmY6ByxWPqwDHeN67v4BOMcOkcDVYFTEtJ7RzAwiBqt1S0kCccKB4H07Bdso/OGoEZRDhOIxZCi2blqFNyMoE1SjXM8SyrcwgyWQdEJEdCqgXtKSxzVm/aBd4IMKkM+tfY7ioiIiIhiWp0EhK1atcS4UVmY+8oCFBb/Vy0768xfYuL40Xhv0RJMeSQHj+bkqorCJx5+wBeUvPTXPHz2eQnGjrpTzUeiydUZaNA6dEgTjGwn29el7uefr6rYns6doQIjmeTxued081WKSd910sTUGsFWBqy49prf4eW583zBloyKK5VjV2SYvWl5WccL2faJnGl6zjwWqYCTATPkNeT5tIyrAkbXlaBp2vRcpF7UE7/s0gUL33wTmQNvxspVq/QaxneYigq8v+xDdO7cSQ0wYpH3nzViFH5/w02+wMr5/t3WsftFfDz69rkMy/9r3nOhSPPp3henqhGD7SGh8zW9cAtlZT9yrHLMQo5XwrhQQa41IMkzz85U21vk+FasXKnnTDKysXUO5KcMbiIBpDVwiRfW/fDsjL+on9Z+5P6RY4lZqUMxIqkU0yfk6SqnTZg3IRclAf0A5mGQNAGf4m8qXzTF3oea3iYpC48E66PNVTEmpVrhoDSV1Yt95PnA17W/Vkz20SbXwn4+CidjdL6cP1vlpbFskK3yrHL+BLM6c2om+3HzSN13t1qfCVPRFGkCH2EopT5fizHati9zP1ag2xPDspJQkjshoE9Ca7AeBmAGL7+jiIiIiIjCaHBYStRCKCv/BokJnfTckVm5+lM88uTTqslxWp/eSE7qUqO/wZ9++gmfrFyD9/65BN9WbsYD949Bx/jovgDsnz0fVfNfl04Q9RIPjONpOuB/0Wxw8NFlLRLaSH9yEsRI01EJbiRgemDi/REFMxYJj2TgDmtkYBkA5PFHH/ZV+UlIJWGR9XoWa7mQJqnPTHuyRvPTUOtIEHTH8LtUxZmTNIWV9yLv9fkXXvKFZ26v47afEVnDMXDATXoO+HjFCkx/5lk0a9oUTWVq0hTXXP1bXHKx/3ytXLUanTp1VPuS5q3W6zjfv/V61jobNpSqkEv6ILRI9WLOtKfVICh3DrsdzZo1w46dOzHWuG6XX94XN95wg17TZF1TO+scRMK+H7mO1/W/BoWFH6lQ1eK8js5trGtv3VfSH6GQc3/9df0x7PbbfOeg10U9VT+O1rl37tvLNRbO9Zz7iV3GF25dxackZWHhS7YwSUZglRGKfcuLUTRlEUbn20YtdvSzJmSABwmvatDrSt9rMjKsO+mTTUIvx7EJl9eqHyQQdR9t2ApQKwvzMHNWrqqgNEmz7MDKyyLjeuUb18u/H+tckncu912w86g+HzKISbAKWOe+XPZjfcZ8eM0ChfkdRVFZu3YtUlJS9Fz01q3/Aim/OlPPERERER2b6jQgFNLcc8Hf30b+4vfRonkznJnUBXEd2qO6qhqVm7fgs3UlaN2qJa675ipkXN4HTWxhT8QOHMCPz72M6r+/4y0kbNAATa69Cs1vv0VGutALiY5dVqAnVX5SBUlERES1gwEhERERxZI6Dwgt+6uq8P6yImz8qgzfbduuqr5Oa9sGKV2T0atH9xqVhVEz3l71e//E/r+8hMN79+mFNTU4sQWa3XErmlxxuQoKa4tbRZpTNBVqVLec1Z1unBWfdYEBIRER0dHBgJCIiIhiyc8WENa1w/v24cA/30f12wU4VLkJh6uq0aBpE5wQH4cmv01H48svQ4MjGNGX6OfAgJCIiOjoYEBIREREsaROBik5Fkj41+SaK9Fy1jS0fuc1nLT47+qnzMtyhoNERERERERERBSLYiYgJKqPZHThN157hdWDRERERERERBQ1BoREREREREREREQxjAEhERERERERERFRDGNASEREREREREREFMMYEBIREREREREREcUwBoREREREREREREQxjAEhERERERERERFRDGNASEREREREREREFMMYEBIREREREREREcUwBoREREREREREREQxjAEhERERERERERFRDGNASERERER0lDRu3BhVVdV6joiIiOjYxICQiIiIiOgoOeXk1ti0eStDQiIiIjqmNThs0I9dlZV/g8SETnqOiIiIiKj+W7t2LVJSUvTckdm+fSd2fr8LBw4c0EuIiIiIji0MCImIiIiIHGozICQiIiI61rGJMRERERERERERUQxjQEhERERERERERBTDGBASERERERERERHFMAaEREREREREREREMYwBIRERERERERERUQxjQEhERERERERERBTDGBASERERERERERHFMAaEREREREREREREMYwBIRERERERERERUQxjQEhERERERERERBTDGBASERERERERERHFMAaEREREREREREREMYwBIRERERERERERUQxjQEhERERERERERBTDGBASERERERERERHFsJgKCA8eAr7ccgDvrdiLtz/Zq37KvCwnIiIiIiIiIiKKRQ0OG/RjV2Xl3yAxoZOeOz79WH0YrxftxqJVe7H/QM2326xxA/Q790T8b69WaN6kgV5KRERERLFq7dq1SElJ0XNHbs+evfjpp4N6joiIiOjY8rMEhLv37MGywmKsXvMZvq6oxHfbtqFxo8Y47bQ2ON14rQu7n4+LLrwADRs21FtEr2zrATz+5k5s/v4nvSS4Dic3wj3XnILEdo31ktAmT30IW7ZuxeOPPozmzZvrpURERER0vKutgPCnn37CNxWb0KhRQzRu7O1vTCIiIqK6VucBYWHxf/Hsc7NRVVWNC87rhq5nnYn4uPaorj6AzVu2YvXaz4zpc5ye2BljRgxD546/0FtGrvy7A/h/C3Zgxx7v/1p7asuG+NP1pyLhtPB/wDEgPLbM+MsspPdLwxmnn66XmP75ryXGH/mf4s5ht6NZs2bYsXMnxo67D5df3hc33nCDXsv0YWEh7jGes3s8+2Fckpqq58Lbtn07xoy9F/eNvwdnJSfrpaFFsw0REREdPbUVEFZ8uwnNjb8/5B/CiYiIiI5VddoH4Zz5ryM7JxdXpqdh/uxncd89I3Blv75oc+qpSOzcEf1/dwWmTByPF2fkIL5De4wePwlFyz/WW0dm94+H8PS730cUDgpZX7aT7evajz/+iKwRo9Ajtbea5LEsC2fuvPm+bdIyrsL6khL9jJ+XdSSk+v0NN6lJHjtJeGbtQybZp521vX2d1Ev7Ysy4e7F//361zs7vv8fY8ffj9uFZuHvUH1UY53Ys0Vq5ajUOHDig5yIjge+fpzyEl1+cheWFy9Qk4aAsi+QYt23bht27d+s5b6LZJhrhrrHzGso5IbEJ827139c9bs1DpX4mOMc2qZNRpJ9xKppirjOpUC+ooRiTfPuRaQjmVeinLIWTbc8bk+8YnccROA2av0mtdXyxzkeQc+o4F27vsXL+kIB1gp97CsZ5DntMKdbP+NVYJ+Rnx7pXXe5vnxDrOK577FzTYJ/x4L9zqG7t2bMPbdqcqueIiIiIjk11FhC+uuBNvFvwL+Q8+mcMzLwOTZo0QcG/3seNtwzDyLETMGzkeNx6x0isKylFW+OPqPF/vAsj7xyKR598Bp+sXKP34t2/1u5D+dbogiLZTravSxIE3jPerFx7f3GBmoQsCxUSSkj38tx5vlDrlpsH4q6RfwwItLyuc9XV/VHx7bd6SSAJB+3hmRxf8fL/uAZIEqpZAVvhB0vwRPYjqnJv48aNqkrusst647kZuXj6qScxePAgzP3rfGwoLdVbA/v2RXfufxEfh127dmPFylV6iXfy/pb9uxDPTHsyoIJPKgcX579TL6r6wl1jCQfvGH4Xzul2trp277y1EKvXrGVIqL58Z2I6srBQ3dd5GIFc9A8bdBjbdMn2fRaWZwOjnaFGRR4GGV/kR+freTdqnXEoy8qz7SsB0yf4X1+FMOPKMSJPP29MOV1yca8KxuIw8CX/cmtamJVkPJeE9N5xah/HCzNwGgfzN6QLCYkCzkUe0hdlBoRXso/+uQnI8Z0P4+KMO17D0p+HBFLOc5iePy7gPEvwHbhO6M9O5fwJmO7/T4GroOuo674Y6dn6tbLTUGBc01gKfpPtvyPUNAm99HP085LGOiecwD6uiYiI6NhWJwHhNxWVmJf3Bu4ZNRxnJCaoZYuXfoh5r76BUXfdjldemqGqBq+/9rd4YOpj+LKsXK3TO7WnWjZ9xvM4cCB8H4KW/dWH8VHJjwjZdjoE2U62l/1EQgK3W/5vKL7+5htfJaDXcOXjFStQsqEUd2cNV82VZZLHmzZtRvnXX+u1Akmg8/c3/6ECPyvAuv66/kg+Mwn//fgTNe9lHXHzwAHqC8Vddw7TSwItXfoBel+c6tuHdXyrVq/xXF3XpUsXvDhrJq66IkMvAZKMZV3OOAPr1vv30aJFC/0ocvK+Vq9Zo/q59EoC2FfzXg94f9GSEE7ugW8rK9VPuQdkmZDzJNWbVnWHVSEaahvh3M7+nFXxJwGn3GvWOm4VguGu8Xv5ZuRy5/A71M+2bdpg1Mi7VEjo3FdMKZyF6aVJGDE1E/FqQRwGTs1CcukiLA1W5VSxDAWyzeCeeoEhdRJyMkoxfbatyqpjJubIF/k8Y396kVPR7FyUZGRjzgBbkGfsa/lL1vEUY2ZuKdKzX8DAjmqB0mvissBt7CrycK/LNseD+AEvqPvYDDhrKlqyGMgYantf+nrlz9Lh7CYsXVSK5KyhtvCkJyZnp6EkdxYrrjwyQ2d7ANUTw+Sa5C/ynUO5BwPXMa7F0DTA7bNj3ZNZwT8LwdfZhHmz5LpnY7LVG4T6vAEFS2pWNRIRERERUU11EhC+vvAfuLD7eeh+3jlqXv4ldeYLc3DHkEEqBGzVsiXandYW1/w2AxmX98Hsua+q9cSN11+LBg0aYNGS9/WS8Cp3/oQt3x/ZKHGyvewnUtJMdMIDf1bhmXyJnTThfv1MaGVl5SrcSujcWS8B2rZtq967Pcizk2apci5/0/0CvcQM7tq3a6eq+yR88rJOOLKO9LXoJMcaF9fBH0bq1zrNOG6v9ldVqWbHkWwTSteuZ6mf0uegVxLASjh7+umJekn0JISTCspWrVr5qi1lmQRsf5r4oApqZZlMcg0W/G1h0G2EBH9S7SmVjbJcqvok8LWHhEKaavfpc6laR6o7O3Ror6oBIwn2vvqqTFUPSjBokevyw65dWLd+vV4SeyrLy4GkfuhjD9I6JiARpShYFqTirLwEbrF5r75pAQFKeMXIzwfS+9qCRqfCRShAGjK8d5Npho5JWRgWwTbHh034aqN+aNexN9KTrOtVjhK3CrTUfkjHYuSzqXHU4hPMfwAMRX2ekIDTA4LpTZg3wQzCJ/fWi2oItY55TZ2fk8QugYFl/WW+/8SE46samIiIiIiOLXUSEK5a8yn69vZ/E63ctEX1Sdctpate4ne2seyLjWV6DmjcuBEuSe1p7OMzvSS8H/YdwoGD0dYPmmR72U+kdu3ejaFDBkdciSbhjNOJLVqooMftOSGjP8vrOUnQtXnzFuzdt8/TOuFIoNizx4WqCa69WvCxJ3Lw8Scr9JxJXsuqgpPphpv+gO07duhnA1VVVWH+K2YYfPbZv1Y/j1TLE09Enz6X4d9FH6mKvEgk6urWo8EtqJXw2AoC3Ui499S0ZwKqP62qPme4K1WB1kAqVnVnJMFesBBYQuqTWrdWAXasKtvoliYlIDkJKNkY5LyosMlRLShNhcct1jMeVZSjDElITpAmy+ZnSk325rIqwExGomqK7F8naNNKY72ZEjoOtSoQ65M49OknoZBVLWgqmpJpa5baExkZxrULqBaU8xui2TJ5oKv45F7US2oonIz+uc7qTev6pCFnYvAgPOQ66nNSkxlYluOrYJW+9Yw0qfb9jgjZjyMRERERUU11EhBKMLJ3nz/MaNSokX5Uk1TMyWS3z9j28KG6HzSkrklFmYQ7kZDwRkKcULysE44EWRJU2cO/HhdeiO4XnK/XMJsGX5HeT1Ww2SvZRo6+BztsIWFZeTlGjhmr+lc8+aSTMWb0SFVFWlsSO3dGh/btUFy8XC/5+VnVlnL+pCrQCwn3JOSzh4pCqvr27NkbMtyNNtirjSrKeqlLQoRhWk9Mtvpksz4vE4BHstP08x6pSsRSTM80Np5qVp46+3pTAWZpLvrL/tXzxhSi/7XKZYvqafWgSZogL8yCcc70eTem/L55GGFrkSxNX3MyFmO0dW1S5fwa51U/T1FQTfHdg2drEB6rb8iApu+FkzE6P8lYHqK/PC/rqCBdP4w1FQlIzkiz9fUo97f83mBISERERETe1UlAeEmvHnjr3QIc0iFf+3ZtEdehHdZ+XrO6afWnn+OC87rpORn19gd88GERLr34Ir0kvIbGuzrSrqBle9lPXZIKLi9Nfu0kQJLqtFC8rOOFhITWlw+ZLrjgPFWFaOncqZMK+6yQU35mDb9DVRWWfvGFWiYSExIw7YnH1CAlZ3f7Nf408QEUfVR7/UQ1btwYl6f9D8rKv0ZFhfdvR0ezSk7ORe70p1T/i9IcWL4se+mfUpqsB4SyxiTzUh0Z6pqGqz4NJtL1Y8bG8hADkgQjIaH/8yJ9BkI1r4yUBCP2vgJr9vWGpCws9PVJaEgdqgKxmv2vmf3vRR54Hl+sfgqtaXJqzWbFZv941mScX7hXoZEHVnWsvQ9AG/+5ngpMkN9j1ui6xZhkbJecNTVEX5he1hGlKDl6v8KPbR3jMHBiYHjaa6IE3iG6QSAiIiIicqiTCOym/+2PLVu/w9/eelcvAcaMGI4X57yC9z8swtrP1qnp7ff+ieL/fIKhgwfqtYDc52ajc6eOqpmxV3GnNELLZkf21mR72U9dcavckgoxCeCCVXVJJVnrVq30nJ+EPBIOSUjkZZ1oSUAl4WOoprlt2rRxfX1LSteuyEjvh09WrMSevXv10iMX16EDEhM64+1330PV/v16qTup7pP+H+siHJNmxfJFWfocfOe9/LAhob1fQvsUbmTlcPeOkwSYUsHq5OUa13eqH7MazMApuUtk50VV+4VqgumUkIxkl+DD3tebOr7SEke4FYfTu+iHdmrwlDB9GtZHuglqyD7aVLVmDFehRUvCwUyzT8uFIZoIm2REbQmuFmOmjBit+s+U5t6Z/n8EkX2pqlnjsYx27GUd1SdoTe79HcYKsxsEIiIiIiKv6iQgbNWqJcaNysLcVxagsPi/atlZZ/4SE8ePxnuLlmDKIzl4NCdXVRQ+8fADvkDppb/m4bPPSzB21J1q3qvTWjdEUnwTPRcd2V72U1ckgJGBMuwjFks44+y3zs5tEBOrLznpM1BCHy/rROu1199QzVi7nmUODOKmZMMGVUEoQWFd69mzB76t3ITvv/9BL3En58Ctj8VoBQtl7aS/QKkmtKpG3baR8yrnN9ggNaFI8+SKb7+NKNiTMNE5YrH0YRnuGtd3Koxzjrqq+wZM7x3BoAC677/kfr29V++pwTWkWX5gFZCv30HjcXzvfkiu0c+aOVhHjQAzRkMwNShLyIFcrP7zHIPRUGhWOGic2xx7BatXMhq34x8/zBG9pWrWeCz79LKODsOcFbMqkM/oF6JZcj2m/zGAA5cQERERkVd11oj2NxeciwfvvwfTZzyPZ/7yItZv+AKdO/0Cj06ZgNfmPoe/Pv8M7htzN1o0b47l/12BBx96XIWJjz00STVHjtQV55+IFk2je3uynWxfl7qff76qYns6d4YKjGSSx+ee081XKSZ910n1hDWCrQxYce01v8PLc+f5gi0ZFVcqx67IMHvT8rKOF7LtEznT9Jx5LFIBJwNmyGvI82kZVwWMritB07TpuUi9qCd+2aULFr75JjIH3oyVq1bpNYzvMBUVeH/Zh+jcuZMaYMQi7z9rxCj8/oabfIGV8/27rWP3i/h49O1zmXE/maF0KNJ8uvfFqWrEYHtI6HxNL9xCWdmPHKscs5DjlTAuVJBrDUjyzLMz1fYWOb4VK1fqOZOMbGydA/kpg5tIAGkNXOKFdT88O+Mv6qe1H7l/5FhilmquW4rpE/J0M2NrNNWh/iaPEpRIZZNt8JCiKfb+v/Q2SVl4xN7/WlhxGDg0TVVP+foTNF7rXhnowQoaO2ZimOpvzGq2CVTOn4DppREGmPWFXAvbdTD7rwPSs21NMI1lg6SCTbPO14ipUYRcsco4h2Yln/R959Y3YDEmyWdCKvz0ElE0RQaDqe170/ycIH+c/3Ni9Vs4uP5Xy6rzbL/nbb9v6mtfo0RERERU+xoclhK1EMrKv0FiQic9d+SkmmvB399G/uL30aJ5M5yZ1AVxHdqjuqoalZu34LN1JWjdqiWuu+YqZFzeB00aN9ZbRu7Ff/2A91bsRSTjGUvfgxIO/t//nGQuCEFCG+lPToIYaToqwY0ETA9MvD+iYMYi4ZEM3GGNDCwDgDz+6MO+Kj8JqSQssl7PYi0X0iT1mWlP1mh+GmodCYLuGH6Xqjhzkqaw8l7kvT7/wku+8Mztddz2MyJrOAYOuEnPAR+vWIHpzzyLZk2boqlMTZrimqt/i0su9p+vlatWo1Onjmpf0rzVeh3n+7dez1pnw4ZSFXJJH4QWqV7Mmfa0GgTlzmG3o1mzZtixcyfGGtft8sv74sYbbtBrmqxramedg0jY9yPX8br+16Cw8CMVqlqc19G5jXXtrftK+iMUcu6vv64/ht1+m+8c9Lqop+rH0Tr3zn17ucbCuZ5zP7HL+MJ9q20kXGeffxKWSB9svuXFKJqyCKPzbaMWZ2RjuaMJpgzeIOFVDc51rf1ryVl5gQM9GAL35ey30FQ5fwj65yYECXSOBxI6uY82nJ4tfQ0a77EwDzNn5arqKVPNc1FknM9843z69xMs5KJgZFRt/8jQDr771/G5UcKca1WVuAjpLvevT7B1HJ8T656o/1zOs8vvG4rc2rVrkZKSoueit279F0j51Zl6joiIiOjYVOcBoWV/VRXeX1aEjV+V4btt21Woc1rbNkjpmoxePbrXGMk4Ggd+Oozn/vkD3v90n6eQUF7xsl+3wO2Xn4TGjY789YmONivQkyo/qYIkIiKi2sGAkIiIiGLJzxYQ1hV5c//47x68Vrgb+w8Ef6vNGjfADamt8LvftDziEZDt3CrSnKKpUKO65azudOOs+KwLDAiJiIiODgaEREREFEvqfUBo2bP/EBat2qeqCb/bdRAHDh5G44YN1EAkUjXY79wWRzzyMVFdY0BIRER0dDAgJCIiolgSM4mYhH/X9WyJ6be1wyt/jMOCsfHqp8zLcoaDREREREREREQUi5iKER3HZHThN157hdWDRERERERERBQ1BoREREREREREREQxjAEhERERERERERFRDGNASEREREREREREFMMYEBIREREREREREcUwBoREREREREREREQxjAEhERERERERERFRDGNASEREREREREREFMMYEBIREREREREREcUwBoREREREREREREQxjAEhERERERERERFRDGNASEREREREREREFMMYEBIREREREREREcUwBoREREREREREREQxjAEhERERERERERFRDGNASEREREREREREFMMYEBIREREREREREcUwBoREREREREREREQxrMFhg37sqqz8GyR07qjniIiIiIjqv08//RQpKSl6Lnrr1n+BlF+dqeeIiIiIjk2sICQiIiIiIiIiIophDAiJiIiIiIiIiIhiGANCIiIiIiIiIiKiGMaAkIiIiIiIiIiIKIYxICQiIiIiIiI6KjZh3q290ePWPFTqJcem4+U4iehoYUBIREREREREREdP4WT0SB2CeRV6noiOOZ4CwsOHD3PixIkTJ06cOHHiFDMTERHVnsrycv2IiI5VrCAkIiIiIiIiIiKKYQwIiYiIiIiIiNyoprG90WNKsV4gijFJlun++irnDzHXsaYj7MfPy/6KptieN6ZJhfoJQ6jnvFhq3z7gfQv93oM87/basqx/bqnxbCmmZ8ryySgyVyeiYwgDQiIiIiIiIiI3qUMxIsn4mb/IH2oVLkIBkjBiaibiUYyZuQnIKVyG5TJlpwGlubh3/ia9cqTC7c8cTGR0vvH6eeY6C/PyMCxBng/1nEfGa5X01dtmGW88fxwG+V5bwsFxKMjINo+tMBvp9ucLJxuvDaRnm9svz85W2/SauAw5GbKCdVyT0EtmieiYwoCQiIiIiIiIyFUcBg5NM36W4ys9wEbRksVAUj/06ShzPTHZHnil9kO68aNkY7R97oXZX+EsTC8FkrOmYqB6fSC+Y5yaQj7nVVIWhqWaD+MHDA147cr5s8xgdHBPNS/HmpFhPL9omVlJqfsZLFiiqwpTjfdiTER0fGBASERERERERBSMCulKUbBMKuWKkZ8PJPfrjXjzWcXftHYcCmTBxvIjamYcbn+JCcFDv1DPRSXgta1mwuYkFYOW+AEvmBWP+eP082xKTHQ8YUBIREREREREFJStUk43L07vrUO4ijwMkqBsYxYWWs1uzWei43F/ZeXBmw2Hei4iFeUok59dEmxhqL/5sm96SZpaa6mTzGV5WUjGYow+wv4YiajuMCAkIiIiIiIiCqFXX+kLcBHunWVvXmwoL0GJ8SN9qA7JVIB4BMLtT/eJWJI7AfN0k2cUFpuVeqGe88p4j0utptSzc81j6Ws2EzabHJdi+gRb6FexyfY4D5Os/gg79ka69N2oJXaRmVKURNvymoiOugaHDfqxq7Lyb9Cpo714moiIiIiofvv888+RkpKi56K3bv0XSPnVmXqOiI5feoAO45EMwjFZ99MnpDmwr6ltRjZyMM6sANSVdTIqsRrFVwb3mBi+T75w+zMHI8lU/Q1akrPyMGeAVDWGei4Uc7uCftlIXzTOt73zvdrPg496Xwk1XhdIQ46vP0X7cUkV4gu+fhKJ6NjAgJCIiIiIyIEBIREREcUSBoRERERERA4MCInoaPBVE7rxWGEYjZ/rdYno+MGAkIiIiIjIgQEhERERxRIOUkJERERERERERBTDGBASERERERERERHFMAaEREREREREREREMcxTH4QdfxFuSHQiIiIiovpj3bp17IOQiIiIYgYrCImIiIiIiIiIiGIYA0IiIiIiIiIiIqIYxoCQiIiIiIiIiIgohjEgJCIiIiIiIiIiimEcpISIiIiIyIGDlNQXH2Hr2L+g4ZiX0KaDXkREdWrPe7di15INes7Q9m60Hn8TWhoPv5/dC/vwBOIHX2Q+Jza/gs1PlKDZYw/iZPUZHoOf9FMn9F2ADlfE67kwVj6IyvmL9AzQaEAR2p1nPq5xTAb78+q4PjcfW8f7k32Z3a/M45d97sFDAcenXmfzHfr9VWL7o9ejapv5nGI7F87jFWHfr8s2pn5ooc6fQZ3Pp3FILXfu0zymA92cy+4HbrF+b7r9Ho3mujj3G4Qc78tAS+u8UJ1hQEhERERE5MCAsL44soDQHiKE+gIcfD17IHAmmjqPQ39xP8EWTIig+6sRBgQLAWq+lrnPRP/6iv1LfuA29mNQ7EFGUMH3F/ic7bgNAa+lwxYl1PsVQc5fwHb2/RnswU/AuQ04f47XCUPtc6vz/Njfrz18qhkS2Y/Dfnz2wMrz+fN0nTT7eQoWVDnOXzSvpbbxhWSO8+USBsn6ZtAGda4O9rOfOw8Bk8E8Tvv9Lts+h4bjzXnnMdk5n/Mfj/W5lmtRgCaOe6Tmes59mdfe/34cz8t5X5Ts/fo5yHmtPtfxWVD3dQEa+z6L5n10yHfPmef0gPGose+8Os+zbGP7PVrjc+f1unhcz+WeOBLqHNf43Wdei6C/m2OUpybGkiFy4sSJEydOnDhx4hQrE5F8Sd2jvlQWIf6xBWi85npsXamfswux3p737kdVuyeM5cZzY9Jx4IkH8b35lO9LtlXV47PyQexak47WwV5XAhv1nEzWF17jC7yquNLLBySi6mXjuNRz1hdkW9inWF/63bcRElz5Xivsl/XQ+/t+thlKmM8B+x41n/MFJOp1FqDp1jEe3q8h2PmT45hfZnzhN7dpgTHY/F6lekYClP0drPf0BE5Ycj+2bzaeUPvyn7/Wfcuwb/ZHaptw7IGenbxfDNDHPeZuHFpkP7cSSOjnjMkKlORcqGo6tfwJYP6t5vEZgp0/db/4zp/xfts9bezHfL8hqRAQ+r413nO3AuxT5z34+VP3uu/eNF/L3CYyJw82rjP0th0uRWMUoEq/TwluqtZswAkd5JyU4+C2fmjiC7zi0Wa8lyDnI+xbAuM92AMh2dY+H9xPmzeg0bn+4LDlFS8FhH61qWWHRGBrWcDnrjZ9/97TQN+HbOfsIrQz7kcsec7/u8jQuJvxeX3P4z2v9rnAFkR6vS51z/13X5jfzTGMfRASEREREdEx43DVj/jh709i+wtjwk6ynqzvlXxZ3Gx8Cd7+aC9Ujg39hXDPygIc+lW6DhTi0bTbmfhpVc0v0MHXM4MOX9AgQUjbRai2ApUON6GDhCy/0vPans1lOKHbpTqMM7549+sX8LondEjQj+yML/228Ez2gXaJvkBPAg4JnBrpeZNsY/tSf146Gm0r8VW8CTOk8SrU/j5C9ednovF5en/qOTMUUsfmq+Iyz9+hzf6Ay/39GoKcPxjX46e26Wiqj+Pkc/vh0JoPVABz8mB/GCfH2+RXG3Bwk/FQ7ct//loax3eCx9BG9ikBoPOLdaMOjvsl4NwmoqF1nmwCg6mL0KIvcGClnIvg5w/nPWg7f5U4uNXbdft+1SI0GmB7z8Z1UIFPiPOHDok4wXpdzX6tvLN/TuLRsN0G/T6FPRSUa7QI+8b6g1JPHO8hUvKef5rfyx+MHkVyHeyf1drluG8s6ndRGQ7az+l5t6Pp1r94OM9B9hmWVbG3AVVPGL9/VcAty+R3sTmpfxiwgv9tT2OXscx3DSTQ1uvZf3ebv9NfwVbfc4H3ivvvvjC/m2MYA0IiIiIiIjpmHD5YjaqNK1C1/qPwk7GerB+JQ0v+AtxSZHxpDF9NZA+nVKVPEMHXOxMNfb01SRCiH4Yg2/sCGYMKEHRYJcHfoSXX+74oOwMMqWaT5dLM0R8aebPnvb/gJ1/QaYZVEpJYr+VaPRmCc3+BoVgCGrbVDwOYlV9W+BDu/QZlD1zikt2/9G5+Bfs/t1en+akKKV9IGx0JJqT6Th27auJp3W8SgEnoZZ1bf9gRGCqaIcahzeV6PtT5k+pN2Zc033Q0MXUlQaJxb262hS72ismg50+qz6TaytxGqh1ro7JOhZDW+5Rwz3bfqABWqlH1a0Z6Hwrrc+EMj/C5vj4yWRWZQkJXVWGq7z2P1aRe2T9X+z4/E02vsH1WdTBmPn80q9p0OO4j/xjhvYowclJlKEGdrp6ViuSVz/mr+B5bgIarjGsgYf2AfmbzdWO5ur8kNJSm12o9uR8QUOF7aEkJmujnWvdFjUpod5H/bo4FDAiJiIiIiOiY0aBJCzQ/uw+annVR2EnWk/Uj8qs7PDWFk4DMi+DrSRCkH0bivAdV000rJKju4K9Oa3nFQ2htNU1VzQR1E1lNhSnGcy1xf2DgEY40U5Vm0rZQ8eQrFti+kEtFVQRhhXN/EvSZj0KQaiJpRutvDhnu/bpR1ZNhfYStqg+1miGxaua79W7jtY8k+DIro8zmzBKKbLAFLxehxRh/0+0WUiGnww55v9LE2gyH7ge69VPLw58/qd6U/T0EvOw1SDWOaXO6Pg5/0+6Q50+quHRT7NZ9zwQ+91Jx5oFURH5eoO4vVdloa96rqMDOOE51H0ZYTWgwPxfGe3SG0vbm6zWa0Fvn1Lh+n9uaWUfBWQUrfUua94X0JelomquDMfO4wv8jRvTsAZmmqggdTfyPJnXdjftd/a6KR5vB7t0YqCpte3Aq/WPaK3xtgXLLK+6oUQldU5S/m2MAA0IiIiIiIjpmNGjUGK36DUGbIU+EnWQ9Wf9okGouf/VW8OAk+HpS5WWv0jGbf3phBX0ytetQgkO+iq54tLTChA43oZnVRNbB/JIc2BQ0KNUXnfQ55wgjOhivpR+q4MDZJDEYt/1J01TYt3d+QdfND9s5K9K8vV+7Gn26bTLOn35okmo7sz8/Z6Wd2V9ZIloc6eAImz/AAVghowRNEsD5w7SWtibAJ19xt605s1RZWdf+JTQ1zpkKl8KeP4vZJN1egRqKP4jzN+0Odf7szZJV002p7Iuq4szRxNM4R9KUuHqlfEbcqzoVCc+Ne8DfHDkIexPsI2ZcvwHez6n9d4EI/g8I0oTce1+X0ZHz6nK+5P7c5tbM3bh/bpH+Mj8AglbUBdlnVHQIe4vx2ZPgL8Q/agT0hypTyM9ouN9V0f9uru8YEBIRERERETmofuh0VZMz0JAgyfoyG3w9ez9rBvWlPET44VOJPb4vt3rACNUE0Xhs/wKtmsjqKiBpgmd/TpppBunnLoAvzHNUMcn+7MGFNAV0DRQcgu3PGSoE9BHnDwcDm0WHeL+hOMIhCbX8fTr6w0Fn01hfOFgbVVsS6AUEVBLomYGEvI69QkuaM/sDYNu1N96vDH7TTIeMwc6f7M9e3eatTzt9b/oGTrE17Q5x/pz9Kqow3B4mevT9bON64260sH0WVL9/i2Q03WR/f3FyPwWERl77vzP7b6yKauAJuUcCqxQD76HgzN8FtqpKfc8GO14V5NdWFWYQEkBjiX2gI+P9PSGDjNzufp9LEN/uaVS5DLxjqblPuaZeKjsdTeON66v2ofoAfcJf+efoFkDOa2D1sP13pMF2D8pgPj+FrRKP9ndz/dfgcJhh2srKv8Ev4sP9l4CIiIiIqP5Yv349UlJS9Fz01q3/Aim/OlPPUd3T4RPuRuvxNwESAsmIr17751OB1yL1UCpYrFBJjVy71dynCg2CrCf8o9xK31v+4Kzm6LfS3PBBnGzbl5DmiKrSbbPxpXjTc9jle862v1DP6VAsoMmdNK0cbHyvdi43qNeLq8T3K+/HPt/on/rY9Jw7l9cx+I7fuhaq8s2/PzOYc1RZSTPLWy4Fgr6nEOdPHkrAaY1wrN6reb1rbmOQ568ocxkROfD1XNlfR/Ndf8dz1nnYs/kjVL08Rp8Hg+34As+F8/WDnD/j2v/03vX+9yXnzmMFpP18BNy3Qc6fCDiHHl+rxjV2206/poyOax2HvLeWxj3g+nnwwvFZsr9Ht/tOPS/XqEM59tnvZcc5MO/1AjRx+0wE+/wq5jU82M+2TK0vo0kb+3Jsq9R47eDk2lSf63J+HPdi4O8oOSbpu9J2r6n1pd9M++8QGaHcuY59n0+g5XkX+St+g/Cf937GuTF+GO/XOs/+47Ld69b7D3It1f7WGPPbNuhjsf+uCva7zzyf/nvZw2c9RjAgJCIiIiJyYEBIRER0bFMBYST/6EMhMSAkIiIiInJgQBgL7BVZgSKqUoopsXbOjuf361I9pRyFaim3yjcRQTVjpNwqAJUIKu6OGz/D+Y1O3X9eGBDWLgaEREREREQODAiJiIiObQwIaxcHKSEiIiIiIiIiouOKGk2b4WCtYUBIREREREREREQUwxgQEhERERERERERxTD2QUhERERE5MA+COuLSlTeux0NR5+N9u31IiKqU7sK1mLn0oN6ztCmNU4ZewZaGw+3v7wKe9AOCbfEq6eULV/im5wDaPFIMtqoz/BWHNBPndAnAZ3ST9FzYawuQfkrP+oZoPFN5yL+HPNxjWMy2J9Xx7XOfGwd7wH7Mruu5vHLPn9Ax4DjU6+zuY1+fzux5bFy7N9uPqfYzoXzeEXY9+uyjak5WqrzZ1DncxcOqeXOfZrHVN3NuawCGGT93nT7PRrNdXHuNwg53jnASWP1eaE6w4CQiIiIiMiBAWF9cWQBoT1ECPUFOPh69kCgIZo5j0N/cW9oCyZE0P3VCAOChQA1X8vcZxP/+or9S37gNvZjUOxBRlDB9xf4nO24DQGvpcMWJdT7FUHOX8B29v0Z7MFPwLkNOH+O1wlD7XOr8/wEBij+8KlmSGQ/joBzYTvnwa4HnMsVr8dvHuNB570d4vzZjyPUZ8JObeMLyRznyyUMkvXNoM24LMa5OtjPfu48BEwG8zjt97tsuxWNxprzzmOycz7nPx7rvcp5242mjnNccz3nvsxr738/juflvC9q7OFz5k7Oa9W5js+Cuq/3oonvs+i85uY5rTYeNfGdV+d5lm1sv0drfO68XheP67ncE0dCneMav/vMaxH0d3OMYhNjIiIiIiIiJ+NL6g/qS+W5SHgkAU3WlKNytX7OLsR6uwoqsL9dO2O58dzoE1GdUwJfLqS/ZFtVPT6rS7BzzYk4JdjrSmCjnpPJ+sJrfIFXFVd6+U1NsH/Ol9ilnrO+IDtDJOtLv/s2QgIg32uNDfdlPfT+tr9shhLmc8Cex8znfAGJep0ENNu61cP7NQQ7f3Icr1QbX/jNbVpiK74p2KmekQBlXwfrPbVDw6UV2LLFeELty3/+TulTjT0vV6ptwrEHjnbyfnGTPu7RrXFwkf3cSiChnzMmX6Ak95Lv2hvH3m4X9tjOhdv1aJ1+tn+ZbNNV1msXEFy5k+vlDzD9gp+/UPd6JNrcYlxn42yo99b+FDTBXvwo10HZiR/XHETDDnJOfsTB7c3R1Bd4nYL2Y70EOZXYsxTGe7AHQrJtYKAXzIHNB9H4XH9wKOfYSxAajdYdmgBb9wd87mrT9nxjz3062s5ZPOKN+xFLt/p/FxmadDM+r/ke73m1zwRbEOn1utQ99999YX43xzAGhEREREREdMw4XH0IP767DXvnbQ47yXqyvlfyZfGbgkpseWwVyu8N/YVw16q9ONS1lQ4UTkHzbg1xYFXNL9DB1zODDl/QIEFImx9RZQUq7c9AJx3o2O3aXI0Tup2iwzjji3e/5gGve0KH5vqRnfGl3xaeyT7Qrpkv0DNDpHZorOdNso3tS/05rdB4+4GAwMgMabwKtb9KVK1riCbn6v2p58xQSB2br4rLPH8HN+tAyuD+fg1Bzh9W78aBNieiuT6ONuc2x6E1O1UA0+YWWxhnHG/TrgeN1zIeqn35z1/rc0/ECR5DG9mnBIDOL9aNOzjul4Bz2wSN3MKU9s3QUJ8Xi/1chL0eq0tUZd5JnsIsuV4ShjbU81qI8+f1MxGefdtT0KjdQVSvst6nPRSUa/Qj9ty71gxyvXK8h0jJez7wyip/MHoUbV/1Y8BntXY5PncW9buoGj/Zz+m57dBs63YP5znIPsOyKvYOYn+O8ftX/QOBLJPfxeakwmYr+N9uPGss810Dqa7U69l/d5u/079Epe+5wHvF/XdfmN/NMYwBIRERERERHTt+OoyDZfvxU+m+sJOsJ+tH4tBS46vlIKmACl9NZA+nVKVPEMHXa4iGvt6aJAjRD0OQ7a1ARqgAQYdVEvwdWlru+6LsDDCkmk2WSzNHt6aToewq2I4DvvDHrKKSkMR6rUgrxZz7CwzFmqOh68k3K7+s8CHc+w3KHrh0aOz+pXfLl9i3zl6d5qcqpHwhbXQkmJDqO3Xsqomndb9JACahl3Vu7UG1VHdJNZP5nPTNZwWa4a/HTmxZ9CMa96uFZpkhzp/Xz0QkVAi5WTdplnDPdt+oAFaqUfU5iaZi0fpcOMMjrNPXRyZd0aqck6xCpYbWveexmtQr+3Xcs64hmmXYPqs6GDOfP5pVbToc95F/jPBeRRg5qTKUoE5Xz4417tPVW/1VfI8koOEq4xpIWH+TcY9JE3pjubr/JTSUptdqPbkfEFDhe2jpATTVz53SBzUqod1F/rs5FjAgJCIiIiKiY0aDJieg0a9ORKOkFuEnYz1ZPyJd23hqCieBjBfB15MgSD+MxDnJqmmpFRJUdfBXp7VO74hTrKapqpmgbiKrqTDFeO4kVAQGHuFIs2ZpOmoLFdtkJNi+kEtFVQRhhXN/W/Yj/NmUaiJphuxvDhnu/bpR1ZNhVaJS9aFWMySWiiTvVXjBmJVRZnNmCUUO2oKXeLQc7W8q3FIq5KywQ6qkdFNnVdm3zl/RFfZ6bNmJarRGS5fAMxKhzp/Xz0TEpKJ03W71fiQQtzfvVVRgZ73vCKsJDebnIgHNnBfb3nx9rDNYNSss1fVbZ2tmHQVnFaz0R2neF9JXpKNprg7GzOMK/48Y0bMHZNo5UkXoaOJ/NKnrvlX/rjoF7W9xD7dV5ao9OJX+Me0VvrZAuXV6mxqV0DVF+bs5BjAgJCIiIiKiY0ejBmh22Sk4cWCHsJOsJ+sfDdJE1FfVZAgWnARfT6rk7FU6O/HTVv0wDCvokym+wwEc8lV0nYLWVpjQ/gy0sJrIOphfkgObqgYloZTqc84RRrS3VdBJcOBskhiM2/6k6Szs2zu/oOvmh+38FXMmb+/XrkafbpuN86cfmsy+96Q/xIDBHAxmf2VN0HLsEVbh6bDODBklaJK+Ff1hX2vj3FraZLT2NWdW4ZgOLVXTSKmcs4LFcNdD3mctNFUNdf68fibCczTx1E2Jq1bLZ8S9qlOR8Ny4B/zNkYOwNWE/csb1u8nfzDoc+/kRwUPVeLSMoK/L6JjN6GucL7k/t7s1cz8F7QdJf5nG+kEr6oLsMyo6hB0E/CDBX4h/1Ajof1OmsaE+o+F+V0X/u7m+Y0BIRERERETkoPqh01VNzkBDgiTry2zw9Rx9tKkv5SHCD5+d2OX7cqsHjFBNEI3H9i/QqomsrgKSJnj256SZZrB+7ux8YZ6jikn2Zw8upCmga6DgEGx/zlAhoI84fzgY2Cw6xPsNxREOSejm79PRHw46B53whYO1UbVVoy9BCUTNQEJex16hJc2ZrWDP2W+hCuAkrPNwPWTdoP01RiLE+Qv1mYjE9peN623s0V7tqPr9WySj6Tb29xcn91NAaOS1/zsJ34D9UQ08IfdIYJVi4D0UnHl+bP346Xs22PGqIN++/lEgATSW2geTMd5fjnFGgw1kI0F8u13Y7zLwjqXmPuWaeqnsdHQtYFxftQ/VB2g7f+Wfo1m7nNfA6mH770iDLdCWwYEOhK0Sj/Z3c/3X4LBBP3ZVVv4NfhEf7rcwEREREVH9sX79eqSkpOi56K1b/wVSfnWmnqO6p8Mn46v9KWPPACQEkhFzvfbPpwIvsyJIKlisUEmNXLvV3KcKDYKsJ/yj3ErfW/7grObot9LcMBltbPsS0hxRVboZX2J3bd6Knb7nbPsL9ZwOxQKa3EnTyluMZ1xGsVWv12Entq+qwB7f6J/62PScO5fXMfiO37oWKrHx788M5hxVVtLMcpBxDoO+pxDnTx5KoGaNcKzeq3m9a25jkOcz9ruMiBz4eq7sr6P5rr/jOes87NpSiR/nSMCnn7Adnwg4RjkPY417zLi+4a6HbCdNmp3BZzA1z7v9fnI/f0qIez2YGq9lvS9zzqRfU0bHtfa5y3jfrY17wPXz4IXjs2Q/Xrf7Tj1/rvFce+kj0nYvO8+Butd3o6nbZyLY51cxPwMH+9mWqfXh+tlXarx2cHIPVJ3rcn4c92LgdZNjqgAG2e51tb70m2n/HSIjlDvXse+zHU46N95f8RuE/7w3N86N8cN4v9Z59h+X7XeF9f6DXEu1vzXG/PaD+ljsn41gv/vM8+n/rHn4rMcIBoRERERERA4MCImIiI5tKiCM5B99KCQGhEREREREDgwIY4G9oi1QRFVKMSXWztnx/H7dKzqPSrWUW+WbcKsUrCVuFYBKBBV3x42f4fxGp+4/LwwIaxcDQiIiIiIiBwaERERExzYGhLWLg5QQEREREREREdFxRY32zXCwlgD/HxHgK2ny9ng+AAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: gsnqjucp\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: smzsgkjb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 27271\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251212_181901-smzsgkjb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/smzsgkjb' target=\"_blank\">bumbling-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/smzsgkjb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/smzsgkjb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251212_181909_743', 'my_seed': 27271, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 216.0\n",
      "lif layer 1 self.abs_max_v: 216.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 277.0\n",
      "lif layer 2 self.abs_max_v: 277.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 62.0\n",
      "fc layer 1 self.abs_max_out: 280.0\n",
      "lif layer 1 self.abs_max_v: 343.0\n",
      "lif layer 2 self.abs_max_v: 299.5\n",
      "fc layer 3 self.abs_max_out: 73.0\n",
      "lif layer 1 self.abs_max_v: 359.5\n",
      "fc layer 2 self.abs_max_out: 285.0\n",
      "lif layer 2 self.abs_max_v: 322.5\n",
      "fc layer 1 self.abs_max_out: 349.0\n",
      "lif layer 1 self.abs_max_v: 395.0\n",
      "fc layer 2 self.abs_max_out: 329.0\n",
      "lif layer 2 self.abs_max_v: 467.5\n",
      "fc layer 3 self.abs_max_out: 104.0\n",
      "lif layer 1 self.abs_max_v: 396.5\n",
      "fc layer 2 self.abs_max_out: 336.0\n",
      "fc layer 1 self.abs_max_out: 362.0\n",
      "fc layer 2 self.abs_max_out: 339.0\n",
      "fc layer 1 self.abs_max_out: 392.0\n",
      "fc layer 2 self.abs_max_out: 343.0\n",
      "lif layer 2 self.abs_max_v: 538.0\n",
      "fc layer 3 self.abs_max_out: 128.0\n",
      "fc layer 1 self.abs_max_out: 397.0\n",
      "lif layer 1 self.abs_max_v: 397.0\n",
      "lif layer 2 self.abs_max_v: 575.0\n",
      "fc layer 1 self.abs_max_out: 514.0\n",
      "lif layer 1 self.abs_max_v: 514.0\n",
      "fc layer 2 self.abs_max_out: 374.0\n",
      "fc layer 3 self.abs_max_out: 131.0\n",
      "fc layer 3 self.abs_max_out: 141.0\n",
      "fc layer 3 self.abs_max_out: 184.0\n",
      "lif layer 1 self.abs_max_v: 544.5\n",
      "fc layer 3 self.abs_max_out: 185.0\n",
      "fc layer 1 self.abs_max_out: 586.0\n",
      "lif layer 1 self.abs_max_v: 586.0\n",
      "fc layer 2 self.abs_max_out: 452.0\n",
      "fc layer 1 self.abs_max_out: 617.0\n",
      "lif layer 1 self.abs_max_v: 617.0\n",
      "lif layer 2 self.abs_max_v: 618.0\n",
      "fc layer 2 self.abs_max_out: 456.0\n",
      "fc layer 1 self.abs_max_out: 622.0\n",
      "lif layer 1 self.abs_max_v: 622.0\n",
      "lif layer 2 self.abs_max_v: 643.0\n",
      "fc layer 3 self.abs_max_out: 216.0\n",
      "fc layer 1 self.abs_max_out: 623.0\n",
      "lif layer 1 self.abs_max_v: 623.0\n",
      "fc layer 1 self.abs_max_out: 814.0\n",
      "lif layer 1 self.abs_max_v: 814.0\n",
      "lif layer 2 self.abs_max_v: 655.0\n",
      "fc layer 1 self.abs_max_out: 853.0\n",
      "lif layer 1 self.abs_max_v: 853.0\n",
      "lif layer 2 self.abs_max_v: 668.5\n",
      "lif layer 2 self.abs_max_v: 711.5\n",
      "fc layer 2 self.abs_max_out: 500.0\n",
      "lif layer 2 self.abs_max_v: 790.5\n",
      "fc layer 2 self.abs_max_out: 505.0\n",
      "fc layer 1 self.abs_max_out: 931.0\n",
      "lif layer 1 self.abs_max_v: 931.0\n",
      "fc layer 2 self.abs_max_out: 537.0\n",
      "lif layer 2 self.abs_max_v: 810.5\n",
      "lif layer 2 self.abs_max_v: 815.0\n",
      "lif layer 2 self.abs_max_v: 820.0\n",
      "fc layer 2 self.abs_max_out: 557.0\n",
      "lif layer 2 self.abs_max_v: 848.0\n",
      "fc layer 1 self.abs_max_out: 940.0\n",
      "lif layer 1 self.abs_max_v: 940.0\n",
      "fc layer 2 self.abs_max_out: 570.0\n",
      "fc layer 2 self.abs_max_out: 575.0\n",
      "fc layer 2 self.abs_max_out: 624.0\n",
      "lif layer 2 self.abs_max_v: 1016.0\n",
      "lif layer 2 self.abs_max_v: 1043.0\n",
      "fc layer 2 self.abs_max_out: 698.0\n",
      "lif layer 2 self.abs_max_v: 1219.5\n",
      "lif layer 1 self.abs_max_v: 962.0\n",
      "lif layer 2 self.abs_max_v: 1224.0\n",
      "lif layer 2 self.abs_max_v: 1289.0\n",
      "fc layer 3 self.abs_max_out: 219.0\n",
      "fc layer 3 self.abs_max_out: 247.0\n",
      "fc layer 2 self.abs_max_out: 714.0\n",
      "fc layer 2 self.abs_max_out: 715.0\n",
      "lif layer 1 self.abs_max_v: 1157.0\n",
      "fc layer 1 self.abs_max_out: 1203.0\n",
      "lif layer 1 self.abs_max_v: 1394.5\n",
      "fc layer 1 self.abs_max_out: 1404.0\n",
      "lif layer 1 self.abs_max_v: 1419.5\n",
      "fc layer 2 self.abs_max_out: 750.0\n",
      "fc layer 2 self.abs_max_out: 841.0\n",
      "fc layer 3 self.abs_max_out: 251.0\n",
      "fc layer 3 self.abs_max_out: 286.0\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "fc layer 2 self.abs_max_out: 912.0\n",
      "lif layer 2 self.abs_max_v: 1401.5\n",
      "lif layer 1 self.abs_max_v: 1455.5\n",
      "lif layer 1 self.abs_max_v: 1503.5\n",
      "fc layer 2 self.abs_max_out: 1073.0\n",
      "lif layer 2 self.abs_max_v: 1626.5\n",
      "lif layer 1 self.abs_max_v: 1582.0\n",
      "fc layer 3 self.abs_max_out: 317.0\n",
      "lif layer 1 self.abs_max_v: 1666.0\n",
      "lif layer 1 self.abs_max_v: 1685.0\n",
      "lif layer 1 self.abs_max_v: 1797.0\n",
      "lif layer 1 self.abs_max_v: 1857.0\n",
      "lif layer 1 self.abs_max_v: 1892.5\n",
      "fc layer 3 self.abs_max_out: 321.0\n",
      "lif layer 1 self.abs_max_v: 1992.0\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "fc layer 1 self.abs_max_out: 1546.0\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "fc layer 2 self.abs_max_out: 1119.0\n",
      "fc layer 1 self.abs_max_out: 1599.0\n",
      "fc layer 2 self.abs_max_out: 1182.0\n",
      "lif layer 1 self.abs_max_v: 2037.0\n",
      "lif layer 1 self.abs_max_v: 2099.5\n",
      "lif layer 1 self.abs_max_v: 2153.0\n",
      "lif layer 1 self.abs_max_v: 2199.5\n",
      "lif layer 1 self.abs_max_v: 2307.0\n",
      "fc layer 1 self.abs_max_out: 1677.0\n",
      "lif layer 1 self.abs_max_v: 2325.0\n",
      "lif layer 1 self.abs_max_v: 2410.5\n",
      "lif layer 1 self.abs_max_v: 2646.0\n",
      "lif layer 1 self.abs_max_v: 2838.0\n",
      "lif layer 1 self.abs_max_v: 2863.0\n",
      "lif layer 1 self.abs_max_v: 2930.0\n",
      "lif layer 1 self.abs_max_v: 3002.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 14.195413/ 83.339409, val:  33.75%, val_best:  33.75%, tr:  95.81%, tr_best:  95.81%, epoch time: 73.70 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0954%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.5427%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6736%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2258  23.064%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 1639.5\n",
      "fc layer 3 self.abs_max_out: 423.0\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "fc layer 1 self.abs_max_out: 1711.0\n",
      "fc layer 1 self.abs_max_out: 1989.0\n",
      "lif layer 1 self.abs_max_v: 3036.0\n",
      "lif layer 1 self.abs_max_v: 3049.5\n",
      "lif layer 1 self.abs_max_v: 3183.0\n",
      "lif layer 1 self.abs_max_v: 3293.5\n",
      "fc layer 1 self.abs_max_out: 1993.0\n",
      "lif layer 2 self.abs_max_v: 1643.0\n",
      "lif layer 1 self.abs_max_v: 3297.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.644292/ 37.659283, val:  54.17%, val_best:  54.17%, tr:  99.39%, tr_best:  99.39%, epoch time: 70.17 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.0514%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9914%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3872  19.775%\n",
      "fc layer 1 self.abs_max_out: 2013.0\n",
      "fc layer 1 self.abs_max_out: 2023.0\n",
      "lif layer 2 self.abs_max_v: 1722.5\n",
      "lif layer 1 self.abs_max_v: 3357.5\n",
      "fc layer 1 self.abs_max_out: 2068.0\n",
      "fc layer 2 self.abs_max_out: 1237.0\n",
      "lif layer 2 self.abs_max_v: 1749.0\n",
      "fc layer 1 self.abs_max_out: 2166.0\n",
      "lif layer 1 self.abs_max_v: 3442.5\n",
      "lif layer 1 self.abs_max_v: 3478.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.736392/ 38.657307, val:  50.42%, val_best:  54.17%, tr:  99.18%, tr_best:  99.39%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0681%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.2704%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1389%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5376  18.304%\n",
      "lif layer 2 self.abs_max_v: 1794.5\n",
      "fc layer 2 self.abs_max_out: 1242.0\n",
      "fc layer 2 self.abs_max_out: 1251.0\n",
      "fc layer 1 self.abs_max_out: 2225.0\n",
      "lif layer 1 self.abs_max_v: 3615.0\n",
      "lif layer 1 self.abs_max_v: 4016.5\n",
      "fc layer 2 self.abs_max_out: 1258.0\n",
      "fc layer 2 self.abs_max_out: 1297.0\n",
      "fc layer 2 self.abs_max_out: 1327.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.131640/ 56.631580, val:  37.92%, val_best:  54.17%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0418%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1199%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5430%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6811  17.393%\n",
      "fc layer 2 self.abs_max_out: 1330.0\n",
      "fc layer 2 self.abs_max_out: 1378.0\n",
      "fc layer 2 self.abs_max_out: 1435.0\n",
      "fc layer 1 self.abs_max_out: 2247.0\n",
      "lif layer 2 self.abs_max_v: 1859.5\n",
      "lif layer 2 self.abs_max_v: 1948.0\n",
      "lif layer 2 self.abs_max_v: 2007.5\n",
      "lif layer 2 self.abs_max_v: 2170.0\n",
      "lif layer 2 self.abs_max_v: 2225.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  8.553332/ 51.348904, val:  49.17%, val_best:  54.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4611%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7857%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 8185  16.721%\n",
      "fc layer 1 self.abs_max_out: 2272.0\n",
      "fc layer 1 self.abs_max_out: 2292.0\n",
      "lif layer 2 self.abs_max_v: 2256.0\n",
      "lif layer 2 self.abs_max_v: 2349.0\n",
      "fc layer 1 self.abs_max_out: 2443.0\n",
      "fc layer 1 self.abs_max_out: 2754.0\n",
      "lif layer 1 self.abs_max_v: 4473.5\n",
      "lif layer 1 self.abs_max_v: 4522.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  8.482160/ 56.651340, val:  48.33%, val_best:  54.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0985%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8920%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0530%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9537  16.236%\n",
      "fc layer 3 self.abs_max_out: 439.0\n",
      "lif layer 2 self.abs_max_v: 2384.5\n",
      "fc layer 2 self.abs_max_out: 1440.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  7.825878/ 51.116211, val:  51.25%, val_best:  54.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0375%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10814  15.780%\n",
      "fc layer 2 self.abs_max_out: 1458.0\n",
      "fc layer 3 self.abs_max_out: 444.0\n",
      "fc layer 2 self.abs_max_out: 1474.0\n",
      "fc layer 2 self.abs_max_out: 1508.0\n",
      "lif layer 2 self.abs_max_v: 2532.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  7.469670/ 44.986053, val:  54.17%, val_best:  54.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0458%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0213%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 12081  15.425%\n",
      "fc layer 3 self.abs_max_out: 449.0\n",
      "fc layer 1 self.abs_max_out: 2881.0\n",
      "lif layer 1 self.abs_max_v: 4642.0\n",
      "lif layer 1 self.abs_max_v: 4659.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  7.866411/ 58.347645, val:  50.83%, val_best:  54.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6144%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1123%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13336  15.136%\n",
      "fc layer 2 self.abs_max_out: 1520.0\n",
      "fc layer 3 self.abs_max_out: 455.0\n",
      "lif layer 1 self.abs_max_v: 4678.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  7.561532/ 50.575138, val:  51.67%, val_best:  54.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6156%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3952%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14503  14.814%\n",
      "fc layer 3 self.abs_max_out: 458.0\n",
      "fc layer 3 self.abs_max_out: 462.0\n",
      "fc layer 1 self.abs_max_out: 2946.0\n",
      "lif layer 1 self.abs_max_v: 4816.5\n",
      "lif layer 1 self.abs_max_v: 4847.5\n",
      "fc layer 2 self.abs_max_out: 1533.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  7.457866/ 62.636284, val:  47.50%, val_best:  54.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0859%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4381%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9251%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15691  14.571%\n",
      "fc layer 3 self.abs_max_out: 468.0\n",
      "fc layer 3 self.abs_max_out: 474.0\n",
      "fc layer 3 self.abs_max_out: 477.0\n",
      "lif layer 2 self.abs_max_v: 2540.5\n",
      "lif layer 2 self.abs_max_v: 2614.5\n",
      "fc layer 1 self.abs_max_out: 3012.0\n",
      "lif layer 1 self.abs_max_v: 4992.5\n",
      "lif layer 1 self.abs_max_v: 5301.5\n",
      "lif layer 2 self.abs_max_v: 2668.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.930536/ 47.207291, val:  52.92%, val_best:  54.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1052%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9155%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6404%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16946  14.425%\n",
      "fc layer 2 self.abs_max_out: 1556.0\n",
      "lif layer 2 self.abs_max_v: 2678.0\n",
      "fc layer 3 self.abs_max_out: 491.0\n",
      "lif layer 2 self.abs_max_v: 2699.0\n",
      "lif layer 2 self.abs_max_v: 2712.5\n",
      "fc layer 2 self.abs_max_out: 1563.0\n",
      "fc layer 2 self.abs_max_out: 1564.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.180192/ 75.229866, val:  53.75%, val_best:  54.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0689%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6253%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5099%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 18080  14.206%\n",
      "fc layer 2 self.abs_max_out: 1574.0\n",
      "fc layer 2 self.abs_max_out: 1639.0\n",
      "lif layer 2 self.abs_max_v: 2715.5\n",
      "lif layer 2 self.abs_max_v: 2725.0\n",
      "fc layer 1 self.abs_max_out: 3094.0\n",
      "fc layer 2 self.abs_max_out: 1674.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.081318/ 32.128162, val:  63.75%, val_best:  63.75%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3371%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8123%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19193  14.003%\n",
      "lif layer 2 self.abs_max_v: 2769.0\n",
      "lif layer 2 self.abs_max_v: 2774.5\n",
      "lif layer 2 self.abs_max_v: 2782.5\n",
      "fc layer 1 self.abs_max_out: 3201.0\n",
      "lif layer 1 self.abs_max_v: 5344.5\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.342425/ 50.265648, val:  55.42%, val_best:  63.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9751%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4414%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20379  13.877%\n",
      "fc layer 3 self.abs_max_out: 505.0\n",
      "fc layer 3 self.abs_max_out: 513.0\n",
      "lif layer 2 self.abs_max_v: 2899.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  6.724144/ 68.356552, val:  46.25%, val_best:  63.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0377%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8476%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21470  13.707%\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  6.838942/ 49.103180, val:  57.08%, val_best:  63.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22575  13.564%\n",
      "fc layer 1 self.abs_max_out: 3241.0\n",
      "lif layer 1 self.abs_max_v: 5430.5\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.005612/ 33.369534, val:  65.83%, val_best:  65.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1037%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2098%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7884%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23719  13.460%\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  6.683433/ 46.429615, val:  59.17%, val_best:  65.83%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0941%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9582%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4484%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24826  13.347%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  6.578386/ 36.027313, val:  66.25%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0884%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25938  13.247%\n",
      "fc layer 1 self.abs_max_out: 3304.0\n",
      "lif layer 1 self.abs_max_v: 5454.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  6.132346/ 42.090778, val:  62.92%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.51 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9043%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26977  13.122%\n",
      "fc layer 3 self.abs_max_out: 531.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.041822/ 41.168945, val:  65.42%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1122%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9769%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2956%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27982  12.992%\n",
      "fc layer 2 self.abs_max_out: 1690.0\n",
      "fc layer 2 self.abs_max_out: 1707.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  5.548000/ 46.195824, val:  60.83%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.76 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0536%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0813%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28960  12.861%\n",
      "fc layer 2 self.abs_max_out: 1885.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  5.818493/ 41.604382, val:  63.33%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6651%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8907%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29944  12.744%\n",
      "fc layer 2 self.abs_max_out: 1927.0\n",
      "fc layer 2 self.abs_max_out: 1940.0\n",
      "fc layer 2 self.abs_max_out: 1961.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.463917/ 36.070904, val:  64.17%, val_best:  66.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9467%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0240%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30999  12.666%\n",
      "lif layer 1 self.abs_max_v: 5537.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  5.609517/ 52.595097, val:  62.92%, val_best:  66.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0934%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9898%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31962  12.557%\n",
      "fc layer 2 self.abs_max_out: 1964.0\n",
      "lif layer 1 self.abs_max_v: 5559.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  5.607891/ 41.663834, val:  66.25%, val_best:  66.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0704%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2849%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32915  12.452%\n",
      "fc layer 1 self.abs_max_out: 3390.0\n",
      "lif layer 1 self.abs_max_v: 5564.0\n",
      "lif layer 1 self.abs_max_v: 5679.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  5.577270/ 37.791794, val:  73.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1160%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2409%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8619%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33878  12.359%\n",
      "fc layer 1 self.abs_max_out: 3454.0\n",
      "lif layer 1 self.abs_max_v: 5684.5\n",
      "lif layer 1 self.abs_max_v: 5711.5\n",
      "lif layer 2 self.abs_max_v: 2919.0\n",
      "lif layer 2 self.abs_max_v: 2984.5\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  5.616814/ 43.305695, val:  67.08%, val_best:  73.75%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0929%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7193%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2160%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 34815  12.263%\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  5.063062/ 41.126141, val:  68.33%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0135%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9899%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6877%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35702  12.156%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  5.780076/ 36.439186, val:  70.42%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1336%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4382%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0064%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 36671  12.083%\n",
      "fc layer 1 self.abs_max_out: 3499.0\n",
      "lif layer 1 self.abs_max_v: 5755.0\n",
      "lif layer 1 self.abs_max_v: 5784.5\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  5.247787/ 43.079975, val:  67.50%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0500%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1263%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8594%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 37579  11.995%\n",
      "lif layer 1 self.abs_max_v: 5785.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.301964/ 34.355968, val:  76.67%, val_best:  76.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4716%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1817%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 38490  11.914%\n",
      "fc layer 3 self.abs_max_out: 532.0\n",
      "fc layer 1 self.abs_max_out: 3526.0\n",
      "lif layer 1 self.abs_max_v: 5804.0\n",
      "lif layer 1 self.abs_max_v: 5911.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.131882/ 38.354698, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0941%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4060%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0654%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 39349  11.821%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  4.777057/ 36.051910, val:  75.00%, val_best:  77.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6138%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2182%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 40154  11.719%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.380775/ 38.540520, val:  71.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0773%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8592%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 41055  11.649%\n",
      "fc layer 3 self.abs_max_out: 536.0\n",
      "lif layer 2 self.abs_max_v: 3006.0\n",
      "lif layer 2 self.abs_max_v: 3109.0\n",
      "lif layer 1 self.abs_max_v: 6016.5\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.171652/ 52.878342, val:  59.17%, val_best:  77.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5607%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2145%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 41889  11.564%\n",
      "lif layer 1 self.abs_max_v: 6161.0\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  4.686628/ 29.786097, val:  82.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3511%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9817%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 42680  11.473%\n",
      "fc layer 1 self.abs_max_out: 3570.0\n",
      "fc layer 3 self.abs_max_out: 542.0\n",
      "fc layer 3 self.abs_max_out: 571.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  4.657458/ 47.777092, val:  63.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0505%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3947%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4191%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 43483  11.389%\n",
      "fc layer 1 self.abs_max_out: 3586.0\n",
      "lif layer 1 self.abs_max_v: 6251.5\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  4.456403/ 37.705708, val:  73.33%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1546%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 44248  11.299%\n",
      "fc layer 1 self.abs_max_out: 3620.0\n",
      "fc layer 1 self.abs_max_out: 3688.0\n",
      "lif layer 1 self.abs_max_v: 6465.5\n",
      "lif layer 2 self.abs_max_v: 3139.5\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  4.971997/ 47.894783, val:  67.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0399%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7233%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7271%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 45049  11.223%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  4.820754/ 36.455055, val:  76.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0758%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0685%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 45846  11.150%\n",
      "fc layer 1 self.abs_max_out: 3918.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  4.961418/ 36.880196, val:  77.08%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0723%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0421%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1521%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 46619  11.074%\n",
      "fc layer 1 self.abs_max_out: 3945.0\n",
      "fc layer 3 self.abs_max_out: 599.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  4.613035/ 55.278477, val:  58.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3737%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7447%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 47368  10.996%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.495560/ 45.896381, val:  64.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9771%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 48134  10.926%\n",
      "lif layer 2 self.abs_max_v: 3193.5\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.318679/ 42.713062, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0509%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7212%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7029%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 48859  10.849%\n",
      "fc layer 2 self.abs_max_out: 1983.0\n",
      "fc layer 2 self.abs_max_out: 2000.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.896125/ 48.420677, val:  75.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 49610  10.782%\n",
      "fc layer 1 self.abs_max_out: 4040.0\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.204881/ 39.119953, val:  81.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1003%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8487%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 50309  10.706%\n",
      "lif layer 1 self.abs_max_v: 6487.5\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.836662/ 35.507980, val:  75.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1139%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0593%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3516%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 51056  10.643%\n",
      "fc layer 2 self.abs_max_out: 2008.0\n",
      "lif layer 1 self.abs_max_v: 6547.5\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.331564/ 37.368622, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1407%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0116%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 51769  10.576%\n",
      "fc layer 3 self.abs_max_out: 619.0\n",
      "lif layer 1 self.abs_max_v: 6682.5\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.010111/ 43.290062, val:  79.17%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7223%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 52444  10.504%\n",
      "fc layer 3 self.abs_max_out: 639.0\n",
      "fc layer 1 self.abs_max_out: 4211.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.394636/ 41.467476, val:  72.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5366%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 53166  10.444%\n",
      "fc layer 2 self.abs_max_out: 2062.0\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.066281/ 30.073948, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 53839  10.376%\n",
      "lif layer 1 self.abs_max_v: 6915.5\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.245864/ 36.528656, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0936%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7553%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6446%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 54523  10.313%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  3.963424/ 40.309551, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0421%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9616%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7665%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 55167  10.246%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  3.792239/ 44.265621, val:  74.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1059%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0472%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6092%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 55781  10.175%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.121852/ 37.472881, val:  78.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0733%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8827%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8893%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 56421  10.111%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.071668/ 34.843323, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0574%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0947%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3179%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 57059  10.049%\n",
      "lif layer 1 self.abs_max_v: 6954.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  3.827789/ 40.981884, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0720%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8709%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3884%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 57674   9.985%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  3.686228/ 37.434425, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1016%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9281%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8886%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 58289   9.923%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  3.848670/ 36.266159, val:  79.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0382%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7540%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 58912   9.865%\n",
      "lif layer 1 self.abs_max_v: 7121.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  3.582582/ 38.516930, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0478%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8658%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 59540   9.809%\n",
      "fc layer 1 self.abs_max_out: 4372.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  3.742662/ 39.643867, val:  80.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0472%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9501%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7396%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 60173   9.756%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  3.469754/ 43.978760, val:  78.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2271%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8440%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 60756   9.697%\n",
      "fc layer 1 self.abs_max_out: 4438.0\n",
      "lif layer 1 self.abs_max_v: 7264.5\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.831273/ 39.249218, val:  78.33%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1013%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1297%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 61340   9.639%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.699106/ 46.068020, val:  75.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5457%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6549%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 61940   9.586%\n",
      "fc layer 2 self.abs_max_out: 2102.0\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  3.092223/ 44.429192, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0987%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8301%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 62471   9.524%\n",
      "fc layer 3 self.abs_max_out: 659.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.337061/ 48.100731, val:  74.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1032%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9423%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 63034   9.469%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.356827/ 35.302135, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9855%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3044%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 63553   9.408%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.450331/ 36.282970, val:  83.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6498%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1521%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 64121   9.357%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.373381/ 49.814926, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1201%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3334%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9136%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 64689   9.307%\n",
      "lif layer 1 self.abs_max_v: 7327.0\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.519978/ 36.117165, val:  82.08%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1258%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6155%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1071%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 65221   9.253%\n",
      "fc layer 1 self.abs_max_out: 4585.0\n",
      "lif layer 1 self.abs_max_v: 7354.0\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.430076/ 43.253616, val:  72.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3085%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9915%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 65788   9.205%\n",
      "lif layer 1 self.abs_max_v: 7614.0\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.293437/ 43.490623, val:  76.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7690%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3139%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 66332   9.156%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.458224/ 42.802185, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8665%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 66879   9.108%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.508774/ 39.180988, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5748%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3002%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 67410   9.060%\n",
      "fc layer 2 self.abs_max_out: 2112.0\n",
      "lif layer 2 self.abs_max_v: 3199.5\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.130637/ 37.818863, val:  82.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0763%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7466%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6517%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 67923   9.010%\n",
      "fc layer 2 self.abs_max_out: 2117.0\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.001016/ 33.902203, val:  81.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1070%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8760%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7891%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 68414   8.959%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.097192/ 35.555592, val:  82.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 68909   8.910%\n",
      "lif layer 2 self.abs_max_v: 3241.5\n",
      "lif layer 2 self.abs_max_v: 3332.0\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.024699/ 28.981676, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0479%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6080%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1552%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 69411   8.862%\n",
      "lif layer 2 self.abs_max_v: 3348.5\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  2.926649/ 42.725407, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0687%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5334%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 69882   8.812%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  2.945384/ 35.145260, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0689%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7049%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3986%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 70389   8.768%\n",
      "lif layer 1 self.abs_max_v: 7663.0\n",
      "lif layer 1 self.abs_max_v: 7777.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  2.902742/ 34.541866, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6653%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6593%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 70876   8.722%\n",
      "lif layer 1 self.abs_max_v: 7886.0\n",
      "fc layer 1 self.abs_max_out: 4665.0\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  2.992395/ 32.571823, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0426%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5730%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8949%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 71357   8.677%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.199704/ 31.796778, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0946%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4470%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6664%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 71874   8.637%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  2.872713/ 31.702404, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5910%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3183%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 72347   8.593%\n",
      "fc layer 3 self.abs_max_out: 669.0\n",
      "fc layer 1 self.abs_max_out: 4780.0\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  2.992714/ 42.611759, val:  79.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0340%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 72813   8.549%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  2.683089/ 42.377918, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0876%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5278%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6010%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 73261   8.504%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  2.932987/ 37.608356, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2999%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1440%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 73739   8.463%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  2.805417/ 36.866425, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0933%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3693%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8210%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 74200   8.421%\n",
      "fc layer 1 self.abs_max_out: 4848.0\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.043093/ 45.501583, val:  72.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9429%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 74683   8.383%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  2.769505/ 41.446533, val:  77.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8549%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4512%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 75124   8.341%\n",
      "fc layer 1 self.abs_max_out: 4877.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  2.278497/ 38.070179, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1646%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 75514   8.294%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  2.735553/ 33.454430, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0621%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3944%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9495%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 75958   8.254%\n",
      "fc layer 1 self.abs_max_out: 4885.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.410120/ 40.623867, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0476%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3004%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9346%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 76379   8.212%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.562097/ 33.369370, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0589%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2988%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0352%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 76802   8.172%\n",
      "fc layer 1 self.abs_max_out: 4902.0\n",
      "fc layer 3 self.abs_max_out: 690.0\n",
      "fc layer 3 self.abs_max_out: 709.0\n",
      "fc layer 2 self.abs_max_out: 2142.0\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.873668/ 35.233284, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1096%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3767%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 77245   8.134%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.810939/ 36.550865, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0780%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4696%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7482%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 77675   8.096%\n",
      "fc layer 2 self.abs_max_out: 2151.0\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.365284/ 32.624844, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4174%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 78077   8.056%\n",
      "fc layer 2 self.abs_max_out: 2218.0\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.531142/ 41.441650, val:  79.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0776%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7384%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 78488   8.017%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.272489/ 45.468662, val:  77.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5783%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0531%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 78860   7.975%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.398983/ 34.200794, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0473%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3273%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1188%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 79248   7.936%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.227531/ 37.502220, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0923%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6751%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8488%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 79612   7.895%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.562723/ 39.393921, val:  82.08%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3689%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6734%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 80016   7.859%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.749567/ 39.947071, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1058%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3433%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5409%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 80468   7.828%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.496100/ 33.849625, val:  85.42%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1929%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2346%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 80866   7.793%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.375849/ 40.976780, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0676%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3876%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7789%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 81266   7.758%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.513011/ 36.592735, val:  86.67%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4624%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9987%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 81662   7.723%\n",
      "fc layer 2 self.abs_max_out: 2237.0\n",
      "fc layer 3 self.abs_max_out: 714.0\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.363856/ 35.047287, val:  85.42%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3967%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5064%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 82061   7.690%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.419307/ 39.905159, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3906%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7356%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 82449   7.656%\n",
      "lif layer 1 self.abs_max_v: 7921.5\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.312927/ 40.061478, val:  83.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3791%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9237%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 82810   7.620%\n",
      "fc layer 1 self.abs_max_out: 4958.0\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.495041/ 37.312645, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.00 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0240%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6324%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 83199   7.588%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  1.987550/ 36.162281, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1237%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2119%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6905%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 83532   7.551%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.412478/ 38.130531, val:  85.42%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1008%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4461%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7329%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 83906   7.518%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.212385/ 36.650715, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6999%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2067%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 84272   7.485%\n",
      "fc layer 2 self.abs_max_out: 2267.0\n",
      "fc layer 1 self.abs_max_out: 5057.0\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  1.727430/ 34.069885, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0477%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6476%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4976%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 84576   7.447%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.098645/ 42.828342, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1055%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2995%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2716%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 84948   7.416%\n",
      "fc layer 2 self.abs_max_out: 2273.0\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.424847/ 43.200848, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1010%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2518%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2657%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 85335   7.387%\n",
      "lif layer 1 self.abs_max_v: 8065.0\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.228057/ 45.276539, val:  81.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0900%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5628%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3656%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 85687   7.355%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  1.987044/ 37.734180, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0298%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 86027   7.323%\n",
      "fc layer 2 self.abs_max_out: 2275.0\n",
      "lif layer 2 self.abs_max_v: 3351.5\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.118576/ 38.871117, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0285%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4458%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1808%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 86375   7.292%\n",
      "fc layer 1 self.abs_max_out: 5072.0\n",
      "lif layer 2 self.abs_max_v: 3364.5\n",
      "lif layer 2 self.abs_max_v: 3418.0\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  1.648194/ 45.561981, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1027%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5130%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 86665   7.256%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  1.864624/ 39.059277, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1064%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4786%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5991%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 86980   7.223%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  1.996032/ 36.632812, val:  87.08%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0880%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5109%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4843%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 87289   7.190%\n",
      "fc layer 3 self.abs_max_out: 720.0\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.301397/ 36.360615, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0728%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4296%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 87653   7.163%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  1.978521/ 34.771603, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4310%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5626%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 87984   7.133%\n",
      "fc layer 1 self.abs_max_out: 5084.0\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  1.991204/ 38.953075, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4921%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9733%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 88319   7.103%\n",
      "fc layer 1 self.abs_max_out: 5102.0\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.052202/ 37.445423, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0423%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2906%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3701%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 88656   7.075%\n",
      "fc layer 3 self.abs_max_out: 728.0\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  1.870041/ 39.658989, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0466%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5847%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5389%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 88959   7.044%\n",
      "lif layer 2 self.abs_max_v: 3500.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  1.979665/ 39.357002, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4515%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 89267   7.014%\n",
      "lif layer 2 self.abs_max_v: 3507.0\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  1.892410/ 35.576973, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3858%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 89570   6.984%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  1.761588/ 34.484367, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4324%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 89865   6.954%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.017813/ 44.469696, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.95 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0531%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6542%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 90195   6.927%\n",
      "fc layer 3 self.abs_max_out: 753.0\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  1.862892/ 41.350742, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0325%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3184%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 90493   6.898%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  1.875287/ 35.687515, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0383%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6059%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8785%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 90797   6.870%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  1.889754/ 32.321861, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6452%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1323%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 91105   6.843%\n",
      "lif layer 2 self.abs_max_v: 3533.0\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  1.657552/ 40.364880, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4453%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7813%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 91385   6.814%\n",
      "fc layer 1 self.abs_max_out: 5142.0\n",
      "fc layer 2 self.abs_max_out: 2299.0\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.051040/ 40.905758, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1017%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 91713   6.788%\n",
      "fc layer 2 self.abs_max_out: 2333.0\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  2.013693/ 42.187447, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.62 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1523%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8408%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 92057   6.765%\n",
      "lif layer 1 self.abs_max_v: 8170.0\n",
      "lif layer 2 self.abs_max_v: 3553.0\n",
      "lif layer 2 self.abs_max_v: 3625.5\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.232525/ 43.785679, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1034%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2211%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7065%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 92415   6.743%\n",
      "lif layer 2 self.abs_max_v: 3669.0\n",
      "lif layer 2 self.abs_max_v: 3797.0\n",
      "lif layer 2 self.abs_max_v: 3828.5\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  1.925196/ 36.937180, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4639%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0742%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 92740   6.718%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  1.515427/ 33.309994, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0845%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 93012   6.691%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  1.726085/ 36.208439, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0872%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9841%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2803%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 93308   6.665%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  1.743157/ 41.832207, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0686%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1372%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 93595   6.639%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  1.670501/ 40.571026, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1549%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3589%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 93880   6.613%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.632540/ 37.590458, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.14 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0959%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 94157   6.587%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.662535/ 33.894806, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2137%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 94437   6.562%\n",
      "lif layer 1 self.abs_max_v: 8398.5\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.834986/ 33.297100, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0459%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9995%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 94723   6.537%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.606252/ 43.698376, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8713%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5591%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 94988   6.512%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.416602/ 36.778427, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0885%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0954%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9285%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 95259   6.487%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.732687/ 36.677368, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0861%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2649%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7199%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 95541   6.463%\n",
      "fc layer 3 self.abs_max_out: 768.0\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.555911/ 36.361153, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0474%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3050%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 95793   6.437%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.826400/ 34.338722, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2649%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5708%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 96088   6.415%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.752178/ 42.525711, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0888%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3283%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2996%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 96350   6.391%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.652954/ 37.347626, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1921%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4857%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 96620   6.367%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.234484/ 37.220341, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0796%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1365%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0701%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 96834   6.340%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.750374/ 36.404263, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3857%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2829%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 97115   6.318%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.482049/ 37.661743, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0745%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2928%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4826%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 97358   6.294%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.764137/ 41.340919, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3390%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0444%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 97657   6.274%\n",
      "fc layer 1 self.abs_max_out: 5155.0\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.292929/ 38.929787, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1091%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5131%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1181%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 97877   6.249%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.683133/ 32.024628, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0801%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6230%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 98140   6.226%\n",
      "fc layer 1 self.abs_max_out: 5247.0\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.300314/ 35.664841, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0704%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2193%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5349%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 98363   6.202%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.296439/ 36.895390, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.36 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1313%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 98606   6.179%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.731807/ 40.100403, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0353%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 98875   6.158%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.499987/ 45.686653, val:  81.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3050%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 99118   6.136%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.670731/ 40.625156, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0274%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8882%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 99393   6.116%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.559573/ 41.910507, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1013%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2452%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3261%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 99652   6.095%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.476309/ 37.506989, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4237%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7748%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 99895   6.074%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.536781/ 40.846031, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0951%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3558%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 100142   6.053%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.650216/ 38.009239, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0780%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9833%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 100413   6.033%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.411061/ 46.190296, val:  81.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0788%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1346%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0693%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 100640   6.012%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.289681/ 42.492645, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3013%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0200%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 100866   5.990%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.761267/ 40.222507, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4832%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 101152   5.972%\n",
      "fc layer 2 self.abs_max_out: 2347.0\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.296412/ 36.415516, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0927%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9644%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1948%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 101395   5.952%\n",
      "fc layer 3 self.abs_max_out: 783.0\n",
      "fc layer 2 self.abs_max_out: 2359.0\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.256876/ 38.164154, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.55 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0524%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2379%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 101614   5.931%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.295527/ 38.915974, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0267%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2313%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 101817   5.909%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.350007/ 34.577824, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1197%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1568%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 102041   5.889%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.206538/ 41.075668, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0751%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2137%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3101%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 102250   5.868%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.305373/ 35.684814, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.15 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0884%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3496%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4279%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 102455   5.847%\n",
      "fc layer 3 self.abs_max_out: 798.0\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.163671/ 37.499229, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4580%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2563%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 102640   5.825%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.378391/ 39.774986, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0818%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2963%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2972%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 102861   5.805%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.287139/ 35.533005, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0855%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9971%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1657%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 103059   5.784%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.281046/ 36.695286, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 103276   5.765%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.413735/ 35.979351, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0294%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0906%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1621%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 103500   5.746%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.149928/ 40.745327, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2931%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 103709   5.726%\n",
      "fc layer 2 self.abs_max_out: 2457.0\n",
      "fc layer 3 self.abs_max_out: 811.0\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.371106/ 36.692387, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0910%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3823%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 103923   5.707%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.199065/ 39.794434, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1032%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1569%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5305%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 104130   5.688%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.306208/ 40.303398, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2144%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3603%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 104349   5.670%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.290166/ 37.599537, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0519%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2580%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4519%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 104556   5.651%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.279280/ 39.492233, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0321%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2918%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 104767   5.632%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.463434/ 42.663021, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1600%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6079%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 104996   5.615%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.020752/ 37.023415, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0874%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2067%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6687%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 105169   5.595%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.366747/ 36.464165, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0560%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7443%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 105387   5.578%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.091175/ 38.068199, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0560%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0618%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 105572   5.559%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.088626/ 36.658779, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0465%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3402%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 105772   5.541%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.265642/ 39.655205, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1272%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1613%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2409%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 106000   5.524%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.008905/ 37.712402, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0973%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 106190   5.506%\n",
      "lif layer 2 self.abs_max_v: 3835.0\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.447143/ 41.095188, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0974%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9948%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5116%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 106429   5.491%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.152681/ 41.087112, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0648%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5916%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 106624   5.473%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.069822/ 38.985500, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0407%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6189%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f465f2d65744b60813c98d2b5b78842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñá‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÇ‚ñÜ‚ñÑ‚ñà‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.06982</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>38.9855</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bumbling-sweep-1</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/smzsgkjb' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/smzsgkjb</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251212_181901-smzsgkjb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w5g2agby with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 1985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251212_223846-w5g2agby</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w5g2agby' target=\"_blank\">charmed-sweep-8</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w5g2agby' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w5g2agby</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251212_223856_458', 'my_seed': 1985, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 283.0\n",
      "lif layer 1 self.abs_max_v: 283.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 280.0\n",
      "lif layer 2 self.abs_max_v: 280.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 105.0\n",
      "fc layer 1 self.abs_max_out: 387.0\n",
      "lif layer 1 self.abs_max_v: 433.0\n",
      "fc layer 2 self.abs_max_out: 454.0\n",
      "lif layer 2 self.abs_max_v: 454.0\n",
      "lif layer 1 self.abs_max_v: 490.0\n",
      "lif layer 2 self.abs_max_v: 500.5\n",
      "lif layer 1 self.abs_max_v: 495.0\n",
      "fc layer 3 self.abs_max_out: 145.0\n",
      "lif layer 1 self.abs_max_v: 540.0\n",
      "fc layer 1 self.abs_max_out: 516.0\n",
      "lif layer 1 self.abs_max_v: 624.0\n",
      "lif layer 2 self.abs_max_v: 621.5\n",
      "fc layer 1 self.abs_max_out: 542.0\n",
      "lif layer 1 self.abs_max_v: 779.5\n",
      "lif layer 2 self.abs_max_v: 699.0\n",
      "fc layer 1 self.abs_max_out: 604.0\n",
      "lif layer 1 self.abs_max_v: 792.5\n",
      "fc layer 3 self.abs_max_out: 149.0\n",
      "fc layer 1 self.abs_max_out: 613.0\n",
      "fc layer 3 self.abs_max_out: 185.0\n",
      "fc layer 2 self.abs_max_out: 477.0\n",
      "fc layer 2 self.abs_max_out: 534.0\n",
      "fc layer 1 self.abs_max_out: 615.0\n",
      "lif layer 1 self.abs_max_v: 870.0\n",
      "fc layer 2 self.abs_max_out: 549.0\n",
      "lif layer 2 self.abs_max_v: 720.5\n",
      "fc layer 1 self.abs_max_out: 816.0\n",
      "lif layer 1 self.abs_max_v: 1034.0\n",
      "fc layer 2 self.abs_max_out: 566.0\n",
      "lif layer 2 self.abs_max_v: 807.5\n",
      "lif layer 2 self.abs_max_v: 938.0\n",
      "fc layer 1 self.abs_max_out: 983.0\n",
      "lif layer 2 self.abs_max_v: 941.0\n",
      "lif layer 1 self.abs_max_v: 1078.5\n",
      "fc layer 3 self.abs_max_out: 234.0\n",
      "fc layer 2 self.abs_max_out: 580.0\n",
      "fc layer 3 self.abs_max_out: 239.0\n",
      "fc layer 2 self.abs_max_out: 608.0\n",
      "lif layer 2 self.abs_max_v: 1010.0\n",
      "fc layer 2 self.abs_max_out: 611.0\n",
      "fc layer 2 self.abs_max_out: 652.0\n",
      "fc layer 2 self.abs_max_out: 653.0\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "fc layer 2 self.abs_max_out: 681.0\n",
      "fc layer 2 self.abs_max_out: 784.0\n",
      "fc layer 3 self.abs_max_out: 249.0\n",
      "fc layer 3 self.abs_max_out: 259.0\n",
      "fc layer 3 self.abs_max_out: 314.0\n",
      "lif layer 1 self.abs_max_v: 1156.0\n",
      "lif layer 2 self.abs_max_v: 1027.5\n",
      "lif layer 2 self.abs_max_v: 1081.0\n",
      "lif layer 2 self.abs_max_v: 1103.5\n",
      "lif layer 2 self.abs_max_v: 1191.0\n",
      "lif layer 2 self.abs_max_v: 1214.5\n",
      "fc layer 1 self.abs_max_out: 1196.0\n",
      "lif layer 1 self.abs_max_v: 1196.0\n",
      "lif layer 1 self.abs_max_v: 1227.0\n",
      "fc layer 1 self.abs_max_out: 1263.0\n",
      "lif layer 1 self.abs_max_v: 1263.0\n",
      "lif layer 1 self.abs_max_v: 1320.5\n",
      "lif layer 1 self.abs_max_v: 1479.5\n",
      "lif layer 2 self.abs_max_v: 1267.0\n",
      "lif layer 1 self.abs_max_v: 1617.0\n",
      "lif layer 2 self.abs_max_v: 1296.5\n",
      "lif layer 1 self.abs_max_v: 1717.5\n",
      "lif layer 2 self.abs_max_v: 1345.5\n",
      "fc layer 1 self.abs_max_out: 1281.0\n",
      "fc layer 2 self.abs_max_out: 794.0\n",
      "fc layer 1 self.abs_max_out: 1291.0\n",
      "fc layer 2 self.abs_max_out: 795.0\n",
      "fc layer 2 self.abs_max_out: 805.0\n",
      "fc layer 3 self.abs_max_out: 326.0\n",
      "fc layer 2 self.abs_max_out: 823.0\n",
      "fc layer 2 self.abs_max_out: 860.0\n",
      "fc layer 1 self.abs_max_out: 1294.0\n",
      "fc layer 1 self.abs_max_out: 1311.0\n",
      "lif layer 1 self.abs_max_v: 1805.5\n",
      "lif layer 2 self.abs_max_v: 1356.5\n",
      "lif layer 1 self.abs_max_v: 1821.0\n",
      "lif layer 2 self.abs_max_v: 1364.5\n",
      "lif layer 1 self.abs_max_v: 1950.5\n",
      "fc layer 3 self.abs_max_out: 348.0\n",
      "lif layer 2 self.abs_max_v: 1432.0\n",
      "fc layer 1 self.abs_max_out: 1316.0\n",
      "fc layer 2 self.abs_max_out: 914.0\n",
      "fc layer 2 self.abs_max_out: 931.0\n",
      "fc layer 2 self.abs_max_out: 968.0\n",
      "fc layer 2 self.abs_max_out: 990.0\n",
      "lif layer 2 self.abs_max_v: 1442.5\n",
      "fc layer 3 self.abs_max_out: 353.0\n",
      "fc layer 3 self.abs_max_out: 368.0\n",
      "fc layer 1 self.abs_max_out: 1414.0\n",
      "lif layer 1 self.abs_max_v: 1963.5\n",
      "fc layer 1 self.abs_max_out: 1517.0\n",
      "fc layer 3 self.abs_max_out: 375.0\n",
      "fc layer 2 self.abs_max_out: 1052.0\n",
      "lif layer 1 self.abs_max_v: 2170.0\n",
      "fc layer 3 self.abs_max_out: 378.0\n",
      "fc layer 3 self.abs_max_out: 410.0\n",
      "fc layer 3 self.abs_max_out: 413.0\n",
      "fc layer 2 self.abs_max_out: 1066.0\n",
      "lif layer 1 self.abs_max_v: 2384.5\n",
      "lif layer 1 self.abs_max_v: 2616.5\n",
      "fc layer 1 self.abs_max_out: 1565.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 14.037681/ 68.078537, val:  40.00%, val_best:  40.00%, tr:  96.12%, tr_best:  96.12%, epoch time: 78.81 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1027%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0612%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9918%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2150  21.961%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 1140.0\n",
      "lif layer 2 self.abs_max_v: 1487.0\n",
      "lif layer 2 self.abs_max_v: 1563.5\n",
      "lif layer 2 self.abs_max_v: 1594.5\n",
      "lif layer 2 self.abs_max_v: 1597.5\n",
      "fc layer 1 self.abs_max_out: 1577.0\n",
      "fc layer 1 self.abs_max_out: 1626.0\n",
      "fc layer 3 self.abs_max_out: 466.0\n",
      "fc layer 1 self.abs_max_out: 1737.0\n",
      "lif layer 2 self.abs_max_v: 1663.5\n",
      "lif layer 1 self.abs_max_v: 2687.0\n",
      "fc layer 1 self.abs_max_out: 1756.0\n",
      "fc layer 2 self.abs_max_out: 1147.0\n",
      "fc layer 2 self.abs_max_out: 1165.0\n",
      "fc layer 1 self.abs_max_out: 1762.0\n",
      "lif layer 1 self.abs_max_v: 2847.5\n",
      "lif layer 1 self.abs_max_v: 2886.0\n",
      "fc layer 1 self.abs_max_out: 1822.0\n",
      "fc layer 1 self.abs_max_out: 1905.0\n",
      "fc layer 2 self.abs_max_out: 1199.0\n",
      "lif layer 1 self.abs_max_v: 3030.0\n",
      "lif layer 1 self.abs_max_v: 3108.0\n",
      "lif layer 1 self.abs_max_v: 3164.0\n",
      "fc layer 1 self.abs_max_out: 2165.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.895492/ 74.479019, val:  40.00%, val_best:  40.00%, tr:  99.49%, tr_best:  99.49%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6353%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3688  18.836%\n",
      "lif layer 2 self.abs_max_v: 1684.0\n",
      "lif layer 2 self.abs_max_v: 1687.5\n",
      "fc layer 2 self.abs_max_out: 1366.0\n",
      "lif layer 2 self.abs_max_v: 1706.5\n",
      "lif layer 2 self.abs_max_v: 1833.0\n",
      "lif layer 2 self.abs_max_v: 1836.5\n",
      "fc layer 3 self.abs_max_out: 497.0\n",
      "fc layer 3 self.abs_max_out: 499.0\n",
      "fc layer 1 self.abs_max_out: 2169.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.800326/ 56.217316, val:  49.58%, val_best:  49.58%, tr:  99.59%, tr_best:  99.59%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0688%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7523%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4776%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5085  17.314%\n",
      "lif layer 1 self.abs_max_v: 3352.5\n",
      "lif layer 2 self.abs_max_v: 1884.0\n",
      "lif layer 2 self.abs_max_v: 1949.0\n",
      "fc layer 1 self.abs_max_out: 2185.0\n",
      "fc layer 3 self.abs_max_out: 536.0\n",
      "lif layer 1 self.abs_max_v: 3395.0\n",
      "lif layer 1 self.abs_max_v: 3425.5\n",
      "lif layer 1 self.abs_max_v: 3509.0\n",
      "lif layer 1 self.abs_max_v: 3633.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.813611/ 52.485035, val:  51.25%, val_best:  51.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 78.76 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2091%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5836%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6501  16.601%\n",
      "fc layer 2 self.abs_max_out: 1399.0\n",
      "fc layer 3 self.abs_max_out: 594.0\n",
      "fc layer 1 self.abs_max_out: 2241.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  9.493320/ 63.874962, val:  44.17%, val_best:  51.25%, tr:  99.49%, tr_best:  99.80%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6077%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7825  15.986%\n",
      "fc layer 1 self.abs_max_out: 2248.0\n",
      "fc layer 1 self.abs_max_out: 2377.0\n",
      "fc layer 3 self.abs_max_out: 612.0\n",
      "fc layer 2 self.abs_max_out: 1404.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  9.317109/ 64.506569, val:  47.50%, val_best:  51.25%, tr:  99.59%, tr_best:  99.80%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0842%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6997%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4465%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9115  15.518%\n",
      "fc layer 2 self.abs_max_out: 1512.0\n",
      "fc layer 2 self.abs_max_out: 1562.0\n",
      "fc layer 1 self.abs_max_out: 2437.0\n",
      "lif layer 1 self.abs_max_v: 3782.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  9.201788/ 56.636246, val:  50.42%, val_best:  51.25%, tr:  99.69%, tr_best:  99.80%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0679%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10377  15.142%\n",
      "fc layer 1 self.abs_max_out: 2516.0\n",
      "lif layer 1 self.abs_max_v: 3964.0\n",
      "lif layer 1 self.abs_max_v: 4005.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.871146/ 47.187546, val:  57.08%, val_best:  57.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0523%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0430%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5815%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11657  14.884%\n",
      "lif layer 2 self.abs_max_v: 2013.0\n",
      "fc layer 1 self.abs_max_out: 2523.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.625854/ 53.349689, val:  57.50%, val_best:  57.50%, tr:  99.59%, tr_best:  99.90%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0289%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2207%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 12841  14.574%\n",
      "fc layer 1 self.abs_max_out: 2582.0\n",
      "lif layer 2 self.abs_max_v: 2172.5\n",
      "fc layer 1 self.abs_max_out: 2595.0\n",
      "lif layer 1 self.abs_max_v: 4164.0\n",
      "lif layer 1 self.abs_max_v: 4434.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.634575/ 52.399799, val:  57.50%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.46 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3585%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14070  14.372%\n",
      "fc layer 1 self.abs_max_out: 2771.0\n",
      "lif layer 1 self.abs_max_v: 4485.0\n",
      "lif layer 1 self.abs_max_v: 4787.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  7.880951/ 57.036629, val:  52.92%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 78.44 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0961%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0015%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15235  14.147%\n",
      "fc layer 3 self.abs_max_out: 613.0\n",
      "lif layer 2 self.abs_max_v: 2194.5\n",
      "lif layer 2 self.abs_max_v: 2223.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.946439/ 42.026394, val:  52.50%, val_best:  57.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0915%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5973%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1595%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16412  13.970%\n",
      "fc layer 2 self.abs_max_out: 1590.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  8.101991/ 42.015461, val:  60.42%, val_best:  60.42%, tr:  99.69%, tr_best:  99.90%, epoch time: 78.85 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7371%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17543  13.784%\n",
      "lif layer 2 self.abs_max_v: 2312.0\n",
      "lif layer 2 self.abs_max_v: 2486.0\n",
      "lif layer 2 self.abs_max_v: 2549.0\n",
      "lif layer 1 self.abs_max_v: 4955.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.874777/ 54.279232, val:  55.00%, val_best:  60.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1143%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18677  13.627%\n",
      "fc layer 1 self.abs_max_out: 2783.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.722717/ 48.529331, val:  57.92%, val_best:  60.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 19798  13.482%\n",
      "fc layer 1 self.abs_max_out: 2874.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.773800/ 67.212334, val:  49.58%, val_best:  60.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0585%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2445%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8203%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 20933  13.364%\n",
      "fc layer 3 self.abs_max_out: 614.0\n",
      "fc layer 2 self.abs_max_out: 1627.0\n",
      "lif layer 2 self.abs_max_v: 2580.5\n",
      "lif layer 2 self.abs_max_v: 2703.5\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.569437/ 44.011318, val:  67.50%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6421%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5571%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22027  13.235%\n",
      "fc layer 1 self.abs_max_out: 2880.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.683010/ 56.473953, val:  59.58%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0909%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2612%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9803%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23141  13.132%\n",
      "fc layer 2 self.abs_max_out: 1643.0\n",
      "fc layer 3 self.abs_max_out: 645.0\n",
      "fc layer 1 self.abs_max_out: 3047.0\n",
      "fc layer 2 self.abs_max_out: 1679.0\n",
      "lif layer 1 self.abs_max_v: 5193.5\n",
      "lif layer 1 self.abs_max_v: 5205.0\n",
      "lif layer 1 self.abs_max_v: 5317.5\n",
      "lif layer 1 self.abs_max_v: 5381.0\n",
      "lif layer 1 self.abs_max_v: 5693.5\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.196284/ 46.948589, val:  64.17%, val_best:  67.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0977%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9588%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8949%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24187  13.003%\n",
      "fc layer 1 self.abs_max_out: 3067.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.292062/ 41.424622, val:  68.75%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1149%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1097%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5227%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25263  12.902%\n",
      "fc layer 2 self.abs_max_out: 1682.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  7.261547/ 38.195873, val:  74.17%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1025%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0146%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5030%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26317  12.801%\n",
      "fc layer 1 self.abs_max_out: 3108.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.762786/ 60.909439, val:  56.25%, val_best:  74.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1577%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3934%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27318  12.684%\n",
      "fc layer 2 self.abs_max_out: 1686.0\n",
      "fc layer 1 self.abs_max_out: 3202.0\n",
      "fc layer 2 self.abs_max_out: 1742.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.804641/ 47.500813, val:  62.50%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0579%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9393%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28313  12.574%\n",
      "fc layer 3 self.abs_max_out: 650.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.159875/ 53.354298, val:  64.17%, val_best:  74.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0962%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9423%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29244  12.446%\n",
      "fc layer 2 self.abs_max_out: 1752.0\n",
      "lif layer 2 self.abs_max_v: 2723.5\n",
      "lif layer 2 self.abs_max_v: 2806.0\n",
      "lif layer 2 self.abs_max_v: 2929.0\n",
      "fc layer 2 self.abs_max_out: 1805.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.848657/ 30.484415, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6456%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1803%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30234  12.353%\n",
      "fc layer 1 self.abs_max_out: 3315.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.345520/ 40.366650, val:  75.42%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0611%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9005%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5552%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31193  12.255%\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.304075/ 73.271011, val:  47.08%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0581%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8635%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3404%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32150  12.163%\n",
      "fc layer 3 self.abs_max_out: 660.0\n",
      "lif layer 1 self.abs_max_v: 5726.0\n",
      "lif layer 1 self.abs_max_v: 6078.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.058129/ 27.073580, val:  82.50%, val_best:  82.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0995%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4937%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1946%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33022  12.047%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  6.049648/ 64.469429, val:  57.08%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0769%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 33934  11.952%\n",
      "fc layer 1 self.abs_max_out: 3320.0\n",
      "fc layer 2 self.abs_max_out: 1877.0\n",
      "lif layer 2 self.abs_max_v: 2953.0\n",
      "lif layer 2 self.abs_max_v: 2967.5\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.474236/ 47.251289, val:  61.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0411%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7868%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 34865  11.871%\n",
      "lif layer 2 self.abs_max_v: 3011.0\n",
      "fc layer 2 self.abs_max_out: 1881.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  6.433845/ 31.773987, val:  76.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0978%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5292%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 35797  11.795%\n",
      "fc layer 3 self.abs_max_out: 674.0\n",
      "fc layer 3 self.abs_max_out: 682.0\n",
      "fc layer 1 self.abs_max_out: 3405.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  5.918148/ 46.327274, val:  66.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4627%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1155%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 36707  11.717%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.676509/ 43.487434, val:  70.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0854%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8742%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3025%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 37577  11.631%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.637430/ 35.984818, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0850%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9848%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4463%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 38433  11.546%\n",
      "fc layer 1 self.abs_max_out: 3533.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.244578/ 32.410965, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3079%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 39253  11.456%\n",
      "fc layer 2 self.abs_max_out: 1909.0\n",
      "fc layer 1 self.abs_max_out: 3551.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.742042/ 46.282784, val:  66.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8088%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 40082  11.373%\n",
      "fc layer 1 self.abs_max_out: 3618.0\n",
      "fc layer 3 self.abs_max_out: 689.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.360743/ 56.981937, val:  65.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0694%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6628%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 40859  11.280%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.160960/ 56.493835, val:  62.08%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.50 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2319%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0923%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 41654  11.197%\n",
      "fc layer 3 self.abs_max_out: 694.0\n",
      "lif layer 1 self.abs_max_v: 6256.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.427725/ 46.888775, val:  76.25%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.73 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0625%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2879%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1354%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 42455  11.119%\n",
      "fc layer 3 self.abs_max_out: 702.0\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.787108/ 31.014206, val:  85.42%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0957%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4508%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3740%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 43304  11.058%\n",
      "fc layer 3 self.abs_max_out: 716.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.475215/ 38.331490, val:  74.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0536%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6119%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5944%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 44106  10.988%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.336314/ 35.575661, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2482%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6740%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 44869  10.912%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.172217/ 34.906395, val:  81.67%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.60 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0448%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 45660  10.846%\n",
      "fc layer 1 self.abs_max_out: 3742.0\n",
      "lif layer 2 self.abs_max_v: 3055.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  4.929791/ 47.804878, val:  77.08%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0924%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5917%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9953%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 46404  10.773%\n",
      "lif layer 2 self.abs_max_v: 3084.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.844412/ 39.400982, val:  79.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 47135  10.699%\n",
      "lif layer 2 self.abs_max_v: 3142.5\n",
      "fc layer 3 self.abs_max_out: 735.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.874595/ 38.110214, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0971%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6899%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5489%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 47883  10.633%\n",
      "lif layer 1 self.abs_max_v: 6327.5\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.888342/ 41.943951, val:  73.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0913%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6542%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 48621  10.567%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  5.118781/ 43.736668, val:  77.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1160%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7954%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6861%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 49377  10.508%\n",
      "lif layer 1 self.abs_max_v: 6594.5\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.667768/ 46.735783, val:  65.42%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6706%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1393%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 50086  10.441%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.932658/ 40.763241, val:  74.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0696%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8228%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 50843  10.387%\n",
      "fc layer 1 self.abs_max_out: 3974.0\n",
      "lif layer 1 self.abs_max_v: 6646.5\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.841657/ 41.135864, val:  69.17%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0800%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2395%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7835%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 51555  10.326%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.576440/ 38.500072, val:  72.50%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4611%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3888%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 52273  10.268%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.799098/ 34.210808, val:  81.67%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0965%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8809%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4281%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 52976  10.210%\n",
      "fc layer 3 self.abs_max_out: 750.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.574026/ 32.011246, val:  80.83%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0892%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7313%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0924%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 53649  10.148%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.412316/ 49.035789, val:  68.75%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0856%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4793%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0607%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 54329  10.090%\n",
      "lif layer 1 self.abs_max_v: 6737.5\n",
      "lif layer 1 self.abs_max_v: 6872.0\n",
      "lif layer 1 self.abs_max_v: 7218.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.251398/ 39.255753, val:  80.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0181%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 54967  10.026%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.583548/ 47.288654, val:  72.08%, val_best:  85.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0504%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 55640   9.971%\n",
      "fc layer 3 self.abs_max_out: 756.0\n",
      "fc layer 3 self.abs_max_out: 760.0\n",
      "fc layer 3 self.abs_max_out: 767.0\n",
      "fc layer 3 self.abs_max_out: 774.0\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.503165/ 41.557579, val:  76.25%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0573%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0810%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0080%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 56307   9.916%\n",
      "fc layer 2 self.abs_max_out: 2001.0\n",
      "fc layer 3 self.abs_max_out: 814.0\n",
      "fc layer 3 self.abs_max_out: 819.0\n",
      "fc layer 3 self.abs_max_out: 830.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.241065/ 33.211090, val:  85.00%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3792%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 56958   9.861%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.317574/ 34.611485, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0997%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1307%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6162%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 57589   9.804%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.499829/ 47.663681, val:  69.58%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0531%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9493%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 58272   9.758%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.485314/ 36.983608, val:  82.92%, val_best:  85.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0927%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9555%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 58932   9.709%\n",
      "fc layer 1 self.abs_max_out: 3994.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.109580/ 33.768974, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0736%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3360%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0100%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 59549   9.655%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  3.914066/ 44.053501, val:  77.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1687%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0607%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 60123   9.596%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.833707/ 48.453026, val:  77.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3443%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 60708   9.540%\n",
      "lif layer 2 self.abs_max_v: 3226.5\n",
      "lif layer 2 self.abs_max_v: 3230.5\n",
      "fc layer 2 self.abs_max_out: 2012.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.840300/ 43.731007, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1888%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9342%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 61291   9.486%\n",
      "lif layer 2 self.abs_max_v: 3236.0\n",
      "fc layer 2 self.abs_max_out: 2017.0\n",
      "fc layer 2 self.abs_max_out: 2033.0\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.189213/ 37.063671, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1006%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0078%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4896%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 61908   9.438%\n",
      "fc layer 2 self.abs_max_out: 2074.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.460152/ 37.934402, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1455%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 62456   9.382%\n",
      "fc layer 1 self.abs_max_out: 4167.0\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.058325/ 37.598145, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 63045   9.333%\n",
      "fc layer 1 self.abs_max_out: 4219.0\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.824344/ 36.316196, val:  80.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2365%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 63616   9.283%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.842545/ 44.328053, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2148%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1329%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 64184   9.234%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.869622/ 42.117344, val:  77.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2318%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9341%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 64741   9.185%\n",
      "fc layer 2 self.abs_max_out: 2091.0\n",
      "fc layer 1 self.abs_max_out: 4235.0\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.956221/ 43.554302, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1076%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9849%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 65328   9.141%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.823201/ 34.610313, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3675%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0023%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 65876   9.093%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.706097/ 47.471870, val:  76.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0586%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 66415   9.045%\n",
      "lif layer 2 self.abs_max_v: 3301.5\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.625642/ 34.450645, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0326%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1143%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7578%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 66978   9.002%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.879088/ 38.958862, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.78 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0897%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2103%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2472%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 67549   8.961%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.311294/ 40.012138, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0702%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2922%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 68056   8.912%\n",
      "fc layer 3 self.abs_max_out: 843.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.465075/ 30.602207, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 68589   8.868%\n",
      "fc layer 3 self.abs_max_out: 856.0\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.402498/ 38.559795, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1793%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2353%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 69093   8.822%\n",
      "fc layer 1 self.abs_max_out: 4443.0\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.513825/ 34.858154, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0343%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4475%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 69597   8.777%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.621347/ 44.391819, val:  76.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1007%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0361%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6093%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 70151   8.739%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.444810/ 43.638645, val:  76.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0680%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9319%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3661%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 70665   8.696%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.115353/ 36.574120, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0473%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9501%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 71118   8.648%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.026564/ 35.024403, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0783%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8944%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1327%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 71568   8.600%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  2.831315/ 38.801567, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7834%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 72028   8.555%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.307497/ 38.770290, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.46 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0946%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8650%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8702%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 72521   8.515%\n",
      "fc layer 3 self.abs_max_out: 916.0\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.086391/ 47.892345, val:  76.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7110%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 72985   8.472%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.126351/ 40.413639, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.16 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8249%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9729%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 73456   8.431%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.178273/ 47.662994, val:  77.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1065%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8027%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 73924   8.390%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.346787/ 48.613472, val:  75.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0499%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6472%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1435%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 74421   8.354%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.381810/ 36.836662, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0952%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6397%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9393%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 74912   8.317%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  2.734246/ 35.666431, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.93 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7385%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2132%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 75345   8.275%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.180936/ 50.979053, val:  74.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.05 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3873%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6398%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 75830   8.240%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  3.220737/ 32.431435, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8444%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 76307   8.205%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.858779/ 40.428307, val:  80.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1023%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5762%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9626%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 76715   8.163%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  3.051986/ 37.105347, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 77168   8.126%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.681876/ 36.541367, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3842%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3717%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 77602   8.088%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  3.102292/ 43.111065, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1015%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4313%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4052%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 78050   8.053%\n",
      "lif layer 2 self.abs_max_v: 3360.0\n",
      "lif layer 2 self.abs_max_v: 3412.0\n",
      "lif layer 2 self.abs_max_v: 3463.0\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.635084/ 45.809067, val:  80.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3377%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5925%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 78477   8.016%\n",
      "lif layer 2 self.abs_max_v: 3472.0\n",
      "lif layer 2 self.abs_max_v: 3501.5\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.913042/ 41.538010, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0895%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6180%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6882%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 78895   7.979%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.595097/ 36.940701, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1175%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3317%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4390%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 79316   7.943%\n",
      "lif layer 2 self.abs_max_v: 3518.5\n",
      "lif layer 2 self.abs_max_v: 3578.5\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  3.020420/ 35.907890, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0140%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6011%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9403%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 79766   7.910%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.718635/ 44.265812, val:  80.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1238%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5355%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7957%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 80184   7.875%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.713506/ 36.466297, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0432%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7701%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7738%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 80599   7.841%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.658588/ 41.310898, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5383%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9015%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 81011   7.806%\n",
      "lif layer 2 self.abs_max_v: 3619.0\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.787997/ 53.998547, val:  81.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0957%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6812%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9943%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 81426   7.773%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.723607/ 35.445518, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0571%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0283%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6904%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 81826   7.739%\n",
      "lif layer 2 self.abs_max_v: 3684.5\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.525316/ 39.138847, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9407%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9770%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 82218   7.705%\n",
      "lif layer 2 self.abs_max_v: 3716.5\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.720123/ 37.326611, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9567%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 82603   7.670%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.155360/ 36.545906, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0550%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8272%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6857%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 82961   7.634%\n",
      "lif layer 2 self.abs_max_v: 3787.5\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.679742/ 33.018913, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0958%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7832%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 83380   7.604%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.431190/ 39.133400, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0404%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7781%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 83771   7.572%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.348044/ 38.120785, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6919%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2285%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 84152   7.540%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.351261/ 41.545143, val:  84.58%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0333%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 84543   7.509%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.411007/ 43.402740, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6326%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4347%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 84904   7.476%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.863303/ 36.712883, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0676%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4507%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9193%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 85316   7.448%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.676931/ 41.498936, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0845%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5255%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8820%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 85700   7.419%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.563683/ 34.156601, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0583%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5989%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 86077   7.389%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.236764/ 39.775105, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8131%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2261%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 86449   7.359%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.345816/ 38.176338, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7718%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4811%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 86821   7.329%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.318851/ 44.772316, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8039%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 87181   7.299%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.188661/ 34.827667, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0333%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6617%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0299%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 87537   7.269%\n",
      "fc layer 2 self.abs_max_out: 2100.0\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.137255/ 39.756172, val:  82.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0985%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6451%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3969%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 87860   7.237%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.159767/ 41.725899, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5197%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9822%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 88175   7.205%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.096113/ 40.377434, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1027%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3878%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7495%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 88513   7.176%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.105344/ 42.940285, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1522%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 88859   7.147%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.281021/ 42.469925, val:  80.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0850%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6783%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4180%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 89230   7.121%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.249028/ 39.359581, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1220%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7522%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2661%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 89582   7.093%\n",
      "fc layer 1 self.abs_max_out: 4489.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.112311/ 41.617702, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7813%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0190%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 89914   7.065%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.143859/ 38.744198, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4257%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9710%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 90261   7.038%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.040624/ 41.960159, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1012%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6890%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9831%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 90585   7.010%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  1.998653/ 37.756771, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7204%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2209%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 90900   6.981%\n",
      "fc layer 1 self.abs_max_out: 4583.0\n",
      "fc layer 2 self.abs_max_out: 2123.0\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.198284/ 39.881504, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0939%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2374%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1249%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 91206   6.952%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.202080/ 36.070759, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0944%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3316%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9245%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 91541   6.926%\n",
      "fc layer 2 self.abs_max_out: 2156.0\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  1.962976/ 41.342617, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0728%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4523%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7090%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 91854   6.899%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  1.783293/ 40.167404, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0482%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6632%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1378%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 92146   6.870%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.093061/ 36.787308, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0620%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4505%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 92466   6.844%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  1.818009/ 41.813629, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6526%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1987%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 92762   6.817%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  1.903284/ 35.671707, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0934%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5896%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2570%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 93056   6.789%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  2.267235/ 37.538380, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0607%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3830%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3110%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 93391   6.766%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  1.867646/ 42.356506, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1079%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4785%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 93687   6.739%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  1.901481/ 39.520855, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0554%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2820%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 93988   6.714%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  1.749138/ 46.835297, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0741%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3168%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 94275   6.687%\n",
      "fc layer 1 self.abs_max_out: 4641.0\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  1.968391/ 37.119091, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1083%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4657%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 94585   6.663%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.781851/ 39.059139, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0472%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2908%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 94867   6.637%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.538367/ 39.611118, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0364%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4238%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6786%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 95107   6.609%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  2.111492/ 40.929878, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1583%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8585%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 95430   6.586%\n",
      "fc layer 2 self.abs_max_out: 2201.0\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.981851/ 50.813999, val:  80.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2380%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1497%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 95734   6.563%\n",
      "fc layer 1 self.abs_max_out: 4700.0\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  2.074808/ 42.379517, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0904%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3941%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7226%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 96053   6.541%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.873177/ 50.520058, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.78 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1132%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4300%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4381%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 96368   6.519%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.782372/ 43.549461, val:  81.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5106%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 96663   6.496%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  2.094925/ 39.129047, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1287%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6559%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7525%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 96985   6.475%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.870222/ 46.745308, val:  82.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5302%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7785%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 97283   6.453%\n",
      "fc layer 2 self.abs_max_out: 2208.0\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.673857/ 40.086426, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0704%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4505%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 97536   6.428%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.677230/ 38.280010, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0733%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0319%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 97797   6.404%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.356191/ 36.516022, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0614%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7051%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 98036   6.378%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.810729/ 41.228790, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0788%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6429%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3971%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 98304   6.355%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.919823/ 44.562466, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.17 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1058%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4317%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9119%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 98610   6.335%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.776534/ 51.113785, val:  81.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0836%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5884%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 98890   6.313%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.965567/ 43.462799, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1276%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2629%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2981%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 99200   6.294%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.674765/ 41.484726, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0874%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2204%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 99460   6.271%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.594736/ 35.516529, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0617%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3872%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5940%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 99710   6.248%\n",
      "fc layer 2 self.abs_max_out: 2210.0\n",
      "lif layer 1 self.abs_max_v: 7254.5\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.570394/ 33.082306, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3746%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6862%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 99924   6.224%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.689148/ 41.334976, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3656%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6400%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 100176   6.202%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.908364/ 47.692902, val:  79.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0811%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4294%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 100480   6.183%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.891814/ 40.319500, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1533%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 100788   6.165%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.916706/ 39.490246, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6555%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3146%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 101084   6.146%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.652022/ 39.038010, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5821%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 101372   6.127%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.455539/ 40.069038, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0686%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8938%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 101618   6.106%\n",
      "fc layer 1 self.abs_max_out: 4755.0\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.674484/ 37.877380, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1341%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 101876   6.085%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.720028/ 36.655437, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2645%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6992%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 102132   6.065%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.685992/ 36.806503, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.75 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1265%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4214%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8441%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 102399   6.046%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.306399/ 35.612171, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6052%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 102629   6.025%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.271247/ 41.382870, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0416%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9416%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 102845   6.003%\n",
      "fc layer 2 self.abs_max_out: 2235.0\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.328792/ 40.285725, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0592%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8639%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9410%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 103068   5.982%\n",
      "fc layer 1 self.abs_max_out: 4775.0\n",
      "lif layer 1 self.abs_max_v: 7271.0\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.727849/ 43.498909, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7358%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7231%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 103344   5.964%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.838765/ 37.097351, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1012%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4807%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 103615   5.946%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.347659/ 40.343967, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0974%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4548%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 103844   5.926%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.221227/ 39.080402, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5184%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0868%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 104066   5.905%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.689523/ 39.270859, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0498%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4230%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1979%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 104320   5.887%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.421110/ 38.036068, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2960%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3833%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 104551   5.868%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.334445/ 45.944317, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8487%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 104774   5.848%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.360421/ 40.605015, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9694%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 104975   5.828%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.479888/ 42.026882, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1130%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3039%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8470%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 105206   5.809%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.459899/ 39.581177, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2084%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0211%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 105429   5.790%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.752032/ 36.034237, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0845%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8925%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 105703   5.774%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.204546/ 43.470783, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0970%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2146%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0517%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 105908   5.754%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.540278/ 42.771324, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5649%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2005%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 106151   5.737%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.074650/ 41.945377, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0921%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 106326   5.716%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.261477/ 40.912170, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5960%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 106529   5.697%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.373345/ 36.790222, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1172%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5067%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5394%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 106752   5.679%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.490509/ 36.726162, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3790%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 106996   5.663%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.193295/ 45.183262, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6156%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7764%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 107197   5.644%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.330511/ 36.009903, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1085%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7255%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0112%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 107412   5.626%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.284173/ 41.732140, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1227%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8124%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3420%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 107622   5.609%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.136065/ 43.528175, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0558%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6388%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 107817   5.590%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.439389/ 39.339149, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4970%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4222%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 108029   5.573%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.506405/ 40.108807, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1060%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3847%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8489%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 108255   5.557%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.258787/ 42.349598, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2993%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3498%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0977e1f0174c43f5b84d7c61c13d5b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÉ‚ñá‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.25879</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.8625</td></tr><tr><td>val_loss</td><td>42.3496</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">charmed-sweep-8</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w5g2agby' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/w5g2agby</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251212_223846-w5g2agby/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ht67ipil with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 39472\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251213_025809-ht67ipil</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ht67ipil' target=\"_blank\">breezy-sweep-15</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ht67ipil' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ht67ipil</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251213_025819_469', 'my_seed': 39472, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 226.0\n",
      "lif layer 1 self.abs_max_v: 226.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 202.0\n",
      "lif layer 2 self.abs_max_v: 202.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 72.0\n",
      "lif layer 1 self.abs_max_v: 262.0\n",
      "lif layer 1 self.abs_max_v: 267.0\n",
      "fc layer 2 self.abs_max_out: 215.0\n",
      "lif layer 2 self.abs_max_v: 232.5\n",
      "lif layer 1 self.abs_max_v: 273.0\n",
      "lif layer 2 self.abs_max_v: 258.5\n",
      "fc layer 3 self.abs_max_out: 82.0\n",
      "fc layer 1 self.abs_max_out: 230.0\n",
      "lif layer 1 self.abs_max_v: 362.5\n",
      "fc layer 2 self.abs_max_out: 247.0\n",
      "lif layer 2 self.abs_max_v: 264.0\n",
      "fc layer 1 self.abs_max_out: 251.0\n",
      "lif layer 2 self.abs_max_v: 332.0\n",
      "fc layer 1 self.abs_max_out: 265.0\n",
      "lif layer 1 self.abs_max_v: 433.0\n",
      "fc layer 3 self.abs_max_out: 86.0\n",
      "fc layer 3 self.abs_max_out: 104.0\n",
      "fc layer 2 self.abs_max_out: 255.0\n",
      "fc layer 2 self.abs_max_out: 258.0\n",
      "fc layer 2 self.abs_max_out: 294.0\n",
      "fc layer 1 self.abs_max_out: 278.0\n",
      "fc layer 2 self.abs_max_out: 350.0\n",
      "lif layer 2 self.abs_max_v: 401.5\n",
      "fc layer 3 self.abs_max_out: 116.0\n",
      "lif layer 1 self.abs_max_v: 443.0\n",
      "fc layer 1 self.abs_max_out: 390.0\n",
      "lif layer 2 self.abs_max_v: 427.5\n",
      "lif layer 2 self.abs_max_v: 520.0\n",
      "fc layer 1 self.abs_max_out: 419.0\n",
      "lif layer 1 self.abs_max_v: 570.0\n",
      "fc layer 2 self.abs_max_out: 424.0\n",
      "lif layer 2 self.abs_max_v: 565.5\n",
      "fc layer 3 self.abs_max_out: 188.0\n",
      "lif layer 1 self.abs_max_v: 698.0\n",
      "lif layer 2 self.abs_max_v: 589.0\n",
      "fc layer 3 self.abs_max_out: 214.0\n",
      "fc layer 1 self.abs_max_out: 455.0\n",
      "lif layer 1 self.abs_max_v: 743.5\n",
      "fc layer 2 self.abs_max_out: 451.0\n",
      "lif layer 2 self.abs_max_v: 680.5\n",
      "fc layer 2 self.abs_max_out: 498.0\n",
      "fc layer 1 self.abs_max_out: 572.0\n",
      "lif layer 1 self.abs_max_v: 748.5\n",
      "fc layer 1 self.abs_max_out: 670.0\n",
      "lif layer 1 self.abs_max_v: 860.5\n",
      "fc layer 1 self.abs_max_out: 741.0\n",
      "lif layer 1 self.abs_max_v: 870.5\n",
      "fc layer 2 self.abs_max_out: 542.0\n",
      "fc layer 3 self.abs_max_out: 296.0\n",
      "lif layer 2 self.abs_max_v: 699.0\n",
      "fc layer 2 self.abs_max_out: 544.0\n",
      "lif layer 2 self.abs_max_v: 700.0\n",
      "lif layer 2 self.abs_max_v: 727.0\n",
      "lif layer 2 self.abs_max_v: 743.5\n",
      "lif layer 2 self.abs_max_v: 747.5\n",
      "lif layer 2 self.abs_max_v: 772.5\n",
      "lif layer 2 self.abs_max_v: 815.0\n",
      "lif layer 2 self.abs_max_v: 835.5\n",
      "fc layer 2 self.abs_max_out: 590.0\n",
      "lif layer 2 self.abs_max_v: 1008.0\n",
      "fc layer 1 self.abs_max_out: 940.0\n",
      "lif layer 1 self.abs_max_v: 940.0\n",
      "lif layer 1 self.abs_max_v: 994.5\n",
      "fc layer 1 self.abs_max_out: 999.0\n",
      "lif layer 1 self.abs_max_v: 999.0\n",
      "fc layer 2 self.abs_max_out: 641.0\n",
      "lif layer 1 self.abs_max_v: 1010.5\n",
      "lif layer 1 self.abs_max_v: 1115.5\n",
      "fc layer 1 self.abs_max_out: 1000.0\n",
      "lif layer 2 self.abs_max_v: 1056.5\n",
      "fc layer 2 self.abs_max_out: 650.0\n",
      "fc layer 2 self.abs_max_out: 659.0\n",
      "fc layer 2 self.abs_max_out: 663.0\n",
      "fc layer 2 self.abs_max_out: 712.0\n",
      "lif layer 2 self.abs_max_v: 1101.5\n",
      "lif layer 1 self.abs_max_v: 1174.5\n",
      "fc layer 2 self.abs_max_out: 725.0\n",
      "lif layer 1 self.abs_max_v: 1224.5\n",
      "lif layer 1 self.abs_max_v: 1249.5\n",
      "lif layer 2 self.abs_max_v: 1156.5\n",
      "fc layer 2 self.abs_max_out: 767.0\n",
      "lif layer 1 self.abs_max_v: 1267.0\n",
      "lif layer 1 self.abs_max_v: 1271.5\n",
      "fc layer 1 self.abs_max_out: 1028.0\n",
      "fc layer 1 self.abs_max_out: 1109.0\n",
      "fc layer 2 self.abs_max_out: 794.0\n",
      "lif layer 2 self.abs_max_v: 1171.5\n",
      "fc layer 1 self.abs_max_out: 1191.0\n",
      "lif layer 1 self.abs_max_v: 1351.0\n",
      "lif layer 1 self.abs_max_v: 1403.5\n",
      "fc layer 1 self.abs_max_out: 1327.0\n",
      "fc layer 2 self.abs_max_out: 819.0\n",
      "fc layer 2 self.abs_max_out: 828.0\n",
      "fc layer 2 self.abs_max_out: 844.0\n",
      "lif layer 2 self.abs_max_v: 1194.0\n",
      "lif layer 1 self.abs_max_v: 1419.5\n",
      "lif layer 1 self.abs_max_v: 1612.0\n",
      "lif layer 1 self.abs_max_v: 1667.0\n",
      "lif layer 1 self.abs_max_v: 1717.5\n",
      "fc layer 2 self.abs_max_out: 884.0\n",
      "lif layer 2 self.abs_max_v: 1229.0\n",
      "lif layer 2 self.abs_max_v: 1270.5\n",
      "lif layer 2 self.abs_max_v: 1289.5\n",
      "lif layer 2 self.abs_max_v: 1349.0\n",
      "lif layer 2 self.abs_max_v: 1397.5\n",
      "lif layer 1 self.abs_max_v: 1802.0\n",
      "fc layer 2 self.abs_max_out: 910.0\n",
      "fc layer 2 self.abs_max_out: 936.0\n",
      "lif layer 1 self.abs_max_v: 1805.0\n",
      "lif layer 1 self.abs_max_v: 1876.5\n",
      "lif layer 1 self.abs_max_v: 1996.0\n",
      "lif layer 1 self.abs_max_v: 2031.0\n",
      "fc layer 2 self.abs_max_out: 941.0\n",
      "fc layer 3 self.abs_max_out: 299.0\n",
      "fc layer 1 self.abs_max_out: 1331.0\n",
      "fc layer 1 self.abs_max_out: 1510.0\n",
      "fc layer 2 self.abs_max_out: 952.0\n",
      "fc layer 3 self.abs_max_out: 304.0\n",
      "fc layer 2 self.abs_max_out: 1057.0\n",
      "lif layer 2 self.abs_max_v: 1428.5\n",
      "lif layer 2 self.abs_max_v: 1519.5\n",
      "fc layer 1 self.abs_max_out: 1548.0\n",
      "fc layer 1 self.abs_max_out: 1551.0\n",
      "fc layer 2 self.abs_max_out: 1103.0\n",
      "fc layer 2 self.abs_max_out: 1104.0\n",
      "fc layer 2 self.abs_max_out: 1123.0\n",
      "fc layer 2 self.abs_max_out: 1136.0\n",
      "lif layer 2 self.abs_max_v: 1521.0\n",
      "lif layer 2 self.abs_max_v: 1539.0\n",
      "fc layer 3 self.abs_max_out: 321.0\n",
      "lif layer 2 self.abs_max_v: 1623.0\n",
      "lif layer 2 self.abs_max_v: 1643.0\n",
      "lif layer 2 self.abs_max_v: 1666.5\n",
      "lif layer 1 self.abs_max_v: 2119.5\n",
      "lif layer 1 self.abs_max_v: 2125.5\n",
      "lif layer 1 self.abs_max_v: 2157.5\n",
      "lif layer 1 self.abs_max_v: 2280.0\n",
      "lif layer 1 self.abs_max_v: 2366.0\n",
      "lif layer 1 self.abs_max_v: 2433.5\n",
      "lif layer 2 self.abs_max_v: 1702.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 14.008505/ 45.399818, val:  39.58%, val_best:  39.58%, tr:  96.94%, tr_best:  96.94%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1065%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6110%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2251  22.993%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 1570.0\n",
      "fc layer 3 self.abs_max_out: 344.0\n",
      "fc layer 1 self.abs_max_out: 1704.0\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "lif layer 2 self.abs_max_v: 1743.0\n",
      "lif layer 2 self.abs_max_v: 1772.5\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "fc layer 2 self.abs_max_out: 1142.0\n",
      "fc layer 2 self.abs_max_out: 1160.0\n",
      "fc layer 3 self.abs_max_out: 374.0\n",
      "fc layer 2 self.abs_max_out: 1189.0\n",
      "lif layer 1 self.abs_max_v: 2487.0\n",
      "lif layer 1 self.abs_max_v: 2542.5\n",
      "fc layer 2 self.abs_max_out: 1232.0\n",
      "lif layer 2 self.abs_max_v: 1774.5\n",
      "lif layer 1 self.abs_max_v: 2873.0\n",
      "fc layer 1 self.abs_max_out: 1734.0\n",
      "lif layer 1 self.abs_max_v: 3170.5\n",
      "fc layer 2 self.abs_max_out: 1242.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  9.890784/ 57.805511, val:  39.58%, val_best:  39.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0982%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4803%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5986%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3816  19.489%\n",
      "fc layer 3 self.abs_max_out: 376.0\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "lif layer 2 self.abs_max_v: 1810.5\n",
      "fc layer 1 self.abs_max_out: 1788.0\n",
      "fc layer 1 self.abs_max_out: 1806.0\n",
      "fc layer 1 self.abs_max_out: 1838.0\n",
      "lif layer 2 self.abs_max_v: 1833.0\n",
      "lif layer 2 self.abs_max_v: 1900.5\n",
      "fc layer 1 self.abs_max_out: 1905.0\n",
      "fc layer 2 self.abs_max_out: 1244.0\n",
      "fc layer 1 self.abs_max_out: 1985.0\n",
      "fc layer 2 self.abs_max_out: 1268.0\n",
      "fc layer 2 self.abs_max_out: 1279.0\n",
      "lif layer 1 self.abs_max_v: 3192.5\n",
      "lif layer 1 self.abs_max_v: 3515.5\n",
      "lif layer 1 self.abs_max_v: 3609.0\n",
      "lif layer 1 self.abs_max_v: 3636.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.708368/ 54.924492, val:  45.00%, val_best:  45.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4714%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3996%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5345  18.199%\n",
      "fc layer 2 self.abs_max_out: 1285.0\n",
      "fc layer 3 self.abs_max_out: 438.0\n",
      "fc layer 3 self.abs_max_out: 440.0\n",
      "fc layer 2 self.abs_max_out: 1288.0\n",
      "fc layer 1 self.abs_max_out: 1992.0\n",
      "lif layer 2 self.abs_max_v: 2052.0\n",
      "lif layer 2 self.abs_max_v: 2119.5\n",
      "fc layer 2 self.abs_max_out: 1323.0\n",
      "fc layer 2 self.abs_max_out: 1336.0\n",
      "lif layer 2 self.abs_max_v: 2140.0\n",
      "fc layer 2 self.abs_max_out: 1383.0\n",
      "fc layer 1 self.abs_max_out: 2053.0\n",
      "fc layer 1 self.abs_max_out: 2097.0\n",
      "fc layer 1 self.abs_max_out: 2125.0\n",
      "lif layer 1 self.abs_max_v: 3886.0\n",
      "fc layer 2 self.abs_max_out: 1401.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  8.337895/ 50.603317, val:  45.00%, val_best:  45.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0956%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9268%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3119%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6642  16.961%\n",
      "lif layer 2 self.abs_max_v: 2147.5\n",
      "lif layer 2 self.abs_max_v: 2185.0\n",
      "lif layer 2 self.abs_max_v: 2250.5\n",
      "lif layer 2 self.abs_max_v: 2374.5\n",
      "fc layer 2 self.abs_max_out: 1406.0\n",
      "fc layer 3 self.abs_max_out: 449.0\n",
      "fc layer 3 self.abs_max_out: 466.0\n",
      "fc layer 3 self.abs_max_out: 472.0\n",
      "fc layer 1 self.abs_max_out: 2148.0\n",
      "fc layer 2 self.abs_max_out: 1453.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  8.197489/ 46.984196, val:  50.83%, val_best:  50.83%, tr:  99.49%, tr_best:  99.59%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0597%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6907%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1540%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7933  16.206%\n",
      "fc layer 2 self.abs_max_out: 1481.0\n",
      "fc layer 2 self.abs_max_out: 1489.0\n",
      "fc layer 1 self.abs_max_out: 2206.0\n",
      "lif layer 1 self.abs_max_v: 3947.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  8.345759/ 53.966259, val:  47.50%, val_best:  50.83%, tr:  99.28%, tr_best:  99.59%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9754%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9203  15.667%\n",
      "fc layer 2 self.abs_max_out: 1561.0\n",
      "fc layer 1 self.abs_max_out: 2381.0\n",
      "fc layer 2 self.abs_max_out: 1645.0\n",
      "fc layer 3 self.abs_max_out: 493.0\n",
      "fc layer 2 self.abs_max_out: 1669.0\n",
      "fc layer 2 self.abs_max_out: 1701.0\n",
      "fc layer 2 self.abs_max_out: 1776.0\n",
      "fc layer 1 self.abs_max_out: 2405.0\n",
      "lif layer 1 self.abs_max_v: 4222.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  8.726801/ 54.179535, val:  52.92%, val_best:  52.92%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1226%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10463  15.268%\n",
      "fc layer 2 self.abs_max_out: 1793.0\n",
      "fc layer 3 self.abs_max_out: 527.0\n",
      "fc layer 3 self.abs_max_out: 547.0\n",
      "fc layer 3 self.abs_max_out: 555.0\n",
      "fc layer 3 self.abs_max_out: 557.0\n",
      "fc layer 2 self.abs_max_out: 1816.0\n",
      "fc layer 2 self.abs_max_out: 1817.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.414660/ 40.071503, val:  56.67%, val_best:  56.67%, tr:  99.59%, tr_best:  99.80%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7391%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5108%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11685  14.920%\n",
      "fc layer 1 self.abs_max_out: 2420.0\n",
      "fc layer 1 self.abs_max_out: 2430.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.533089/ 63.472343, val:  45.83%, val_best:  56.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1098%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9642%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 12932  14.677%\n",
      "fc layer 3 self.abs_max_out: 559.0\n",
      "lif layer 1 self.abs_max_v: 4347.0\n",
      "lif layer 1 self.abs_max_v: 4466.5\n",
      "lif layer 1 self.abs_max_v: 4529.5\n",
      "fc layer 1 self.abs_max_out: 2480.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  7.800805/ 62.325626, val:  50.42%, val_best:  56.67%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0773%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4904%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7317%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14102  14.404%\n",
      "lif layer 2 self.abs_max_v: 2444.0\n",
      "lif layer 2 self.abs_max_v: 2506.0\n",
      "fc layer 1 self.abs_max_out: 2582.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.021648/ 44.097946, val:  65.42%, val_best:  65.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0618%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0397%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15290  14.198%\n",
      "fc layer 3 self.abs_max_out: 571.0\n",
      "fc layer 3 self.abs_max_out: 602.0\n",
      "lif layer 1 self.abs_max_v: 4690.0\n",
      "lif layer 1 self.abs_max_v: 4717.0\n",
      "fc layer 1 self.abs_max_out: 2632.0\n",
      "lif layer 1 self.abs_max_v: 4852.0\n",
      "fc layer 1 self.abs_max_out: 2743.0\n",
      "lif layer 1 self.abs_max_v: 4952.0\n",
      "fc layer 1 self.abs_max_out: 2892.0\n",
      "lif layer 1 self.abs_max_v: 5176.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.873861/ 64.786118, val:  49.58%, val_best:  65.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4936%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7698%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16477  14.025%\n",
      "fc layer 2 self.abs_max_out: 1837.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  8.065116/ 72.996956, val:  47.50%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1038%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5034%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3495%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17648  13.867%\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.977148/ 63.496548, val:  57.50%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0884%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4699%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18844  13.749%\n",
      "lif layer 1 self.abs_max_v: 5190.0\n",
      "fc layer 1 self.abs_max_out: 2912.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.857384/ 60.345036, val:  56.67%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5169%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6166%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 19971  13.600%\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.494678/ 84.722740, val:  42.08%, val_best:  65.42%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0774%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4034%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6916%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21067  13.449%\n",
      "lif layer 2 self.abs_max_v: 2552.0\n",
      "lif layer 2 self.abs_max_v: 2652.5\n",
      "lif layer 2 self.abs_max_v: 2676.0\n",
      "lif layer 2 self.abs_max_v: 2871.0\n",
      "lif layer 2 self.abs_max_v: 2901.0\n",
      "fc layer 1 self.abs_max_out: 2922.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.583947/ 44.617294, val:  62.50%, val_best:  65.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8914%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4885%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22153  13.311%\n",
      "fc layer 3 self.abs_max_out: 606.0\n",
      "fc layer 1 self.abs_max_out: 2993.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.396346/ 38.999718, val:  71.25%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5965%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8247%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23272  13.206%\n",
      "fc layer 2 self.abs_max_out: 1849.0\n",
      "fc layer 2 self.abs_max_out: 1889.0\n",
      "lif layer 1 self.abs_max_v: 5217.5\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.041100/ 41.231853, val:  64.17%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0676%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7055%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9413%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24306  13.067%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  6.936356/ 35.960846, val:  69.17%, val_best:  71.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1238%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1414%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9526%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25354  12.949%\n",
      "fc layer 2 self.abs_max_out: 1944.0\n",
      "lif layer 2 self.abs_max_v: 2933.5\n",
      "lif layer 2 self.abs_max_v: 2949.5\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  6.719912/ 37.983631, val:  70.00%, val_best:  71.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 79.12 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1389%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0563%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26371  12.827%\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.735139/ 47.598057, val:  65.42%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0916%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4010%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27415  12.729%\n",
      "fc layer 1 self.abs_max_out: 3042.0\n",
      "fc layer 2 self.abs_max_out: 1950.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.431868/ 44.333717, val:  66.67%, val_best:  71.25%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0729%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4102%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28408  12.616%\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  7.114322/ 49.712914, val:  57.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0685%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2926%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5562%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29449  12.534%\n",
      "fc layer 2 self.abs_max_out: 2008.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.331087/ 37.982143, val:  67.08%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0596%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4772%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1731%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30421  12.429%\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.736387/ 53.277702, val:  60.42%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9370%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8853%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31416  12.342%\n",
      "fc layer 2 self.abs_max_out: 2034.0\n",
      "fc layer 1 self.abs_max_out: 3091.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.336505/ 35.321297, val:  74.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6285%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32368  12.245%\n",
      "fc layer 1 self.abs_max_out: 3177.0\n",
      "fc layer 2 self.abs_max_out: 2099.0\n",
      "fc layer 1 self.abs_max_out: 3203.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.204525/ 37.663643, val:  71.67%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1055%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2910%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7868%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33320  12.155%\n",
      "fc layer 1 self.abs_max_out: 3354.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  6.141626/ 51.117626, val:  57.92%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5341%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4508%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 34256  12.066%\n",
      "fc layer 1 self.abs_max_out: 3619.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  5.997274/ 48.542496, val:  66.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1194%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3993%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5312%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35169  11.974%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  6.121110/ 38.952084, val:  68.33%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0587%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3198%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2027%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 36066  11.884%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.020461/ 45.196098, val:  67.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1496%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8092%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 36972  11.802%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.721120/ 39.449539, val:  67.50%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0298%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2870%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 37843  11.714%\n",
      "lif layer 1 self.abs_max_v: 5290.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  6.319977/ 45.055904, val:  68.33%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0381%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7934%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5568%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 38774  11.649%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.688362/ 56.396080, val:  64.58%, val_best:  74.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0300%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1365%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2559%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 39653  11.572%\n",
      "lif layer 1 self.abs_max_v: 5368.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.225489/ 33.471855, val:  77.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9948%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 40490  11.488%\n",
      "lif layer 1 self.abs_max_v: 5382.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.514400/ 41.207573, val:  71.67%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.67 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0988%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7954%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 41347  11.415%\n",
      "fc layer 3 self.abs_max_out: 610.0\n",
      "lif layer 1 self.abs_max_v: 5427.0\n",
      "lif layer 1 self.abs_max_v: 5606.5\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.008301/ 42.762348, val:  74.17%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0688%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7036%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2852%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 42137  11.327%\n",
      "fc layer 3 self.abs_max_out: 619.0\n",
      "fc layer 3 self.abs_max_out: 626.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.734480/ 39.520000, val:  73.75%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7793%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4201%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 42997  11.261%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.234116/ 52.423656, val:  62.08%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0531%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2302%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 43808  11.187%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  4.931909/ 41.709473, val:  75.42%, val_best:  77.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0245%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4267%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3792%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 44599  11.111%\n",
      "fc layer 3 self.abs_max_out: 628.0\n",
      "fc layer 3 self.abs_max_out: 634.0\n",
      "fc layer 3 self.abs_max_out: 645.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.110321/ 36.054016, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1049%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3865%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7074%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 45372  11.035%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.003598/ 46.556686, val:  70.00%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.03 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0946%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1204%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4235%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 46116  10.955%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.096832/ 27.925653, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9137%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6554%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 46868  10.880%\n",
      "fc layer 3 self.abs_max_out: 652.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.886676/ 59.008011, val:  63.33%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0563%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9438%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8680%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 47613  10.808%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.951416/ 35.633610, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0552%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3825%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 48378  10.743%\n",
      "fc layer 2 self.abs_max_out: 2163.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.201580/ 38.275230, val:  79.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0702%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4843%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 49065  10.663%\n",
      "fc layer 3 self.abs_max_out: 655.0\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.297187/ 39.188137, val:  74.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0747%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0388%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7146%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 49741  10.585%\n",
      "fc layer 1 self.abs_max_out: 3692.0\n",
      "lif layer 1 self.abs_max_v: 5724.5\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.368594/ 52.594006, val:  70.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0969%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9428%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 50421  10.511%\n",
      "fc layer 3 self.abs_max_out: 666.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.602525/ 32.425869, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0398%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3448%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1753%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 51125  10.444%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  3.905512/ 39.227760, val:  77.08%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2502%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 51749  10.365%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.459688/ 42.844418, val:  77.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0265%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1807%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7392%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 52443  10.302%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.397153/ 30.014437, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0699%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5531%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 53116  10.237%\n",
      "fc layer 3 self.abs_max_out: 676.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.490737/ 46.348927, val:  73.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2335%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7820%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 53807  10.178%\n",
      "fc layer 1 self.abs_max_out: 3787.0\n",
      "fc layer 3 self.abs_max_out: 679.0\n",
      "fc layer 3 self.abs_max_out: 691.0\n",
      "lif layer 1 self.abs_max_v: 5888.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  3.965847/ 46.302349, val:  70.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3523%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 54455  10.113%\n",
      "fc layer 3 self.abs_max_out: 720.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.141145/ 38.118137, val:  79.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7156%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6868%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 55094  10.049%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.064614/ 34.274776, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0516%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7831%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8710%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 55737   9.988%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  3.982991/ 34.580616, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.38 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6209%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 56393   9.931%\n",
      "lif layer 1 self.abs_max_v: 5918.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.323451/ 28.890572, val:  82.92%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0493%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5035%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 57037   9.875%\n",
      "fc layer 3 self.abs_max_out: 722.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.302322/ 28.447210, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3174%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1121%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 57704   9.824%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.185498/ 33.316486, val:  85.00%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7373%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1176%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 58347   9.770%\n",
      "lif layer 2 self.abs_max_v: 2966.0\n",
      "fc layer 1 self.abs_max_out: 3793.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  3.634219/ 37.035511, val:  80.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0936%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3612%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5419%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 58934   9.709%\n",
      "fc layer 3 self.abs_max_out: 770.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  3.577912/ 40.502304, val:  76.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.34 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3049%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5115%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 59491   9.646%\n",
      "lif layer 2 self.abs_max_v: 2990.0\n",
      "lif layer 2 self.abs_max_v: 3043.0\n",
      "lif layer 1 self.abs_max_v: 6002.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  3.935265/ 46.346138, val:  80.00%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.94 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6482%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 60090   9.590%\n",
      "lif layer 2 self.abs_max_v: 3071.5\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.463943/ 36.723896, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9890%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6346%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 60668   9.534%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.944461/ 46.784927, val:  81.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.55 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0597%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2670%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1258%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 61262   9.481%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.104500/ 42.699207, val:  74.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.62 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 61886   9.435%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  4.102840/ 30.119968, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1041%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4618%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4308%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 62495   9.388%\n",
      "lif layer 2 self.abs_max_v: 3147.5\n",
      "lif layer 2 self.abs_max_v: 3268.0\n",
      "lif layer 2 self.abs_max_v: 3273.0\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.848300/ 35.256447, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0796%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4158%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 63064   9.336%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.534719/ 39.940891, val:  75.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0496%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7283%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1360%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 63646   9.287%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.532753/ 36.909950, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4601%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0109%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 64214   9.238%\n",
      "fc layer 2 self.abs_max_out: 2232.0\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.355220/ 38.395157, val:  81.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 64739   9.184%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.275802/ 33.469635, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0436%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8346%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4678%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 65269   9.133%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.792385/ 35.647968, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0612%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9665%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4602%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 65845   9.089%\n",
      "fc layer 1 self.abs_max_out: 3831.0\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.112193/ 39.428986, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0630%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7476%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 66359   9.038%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.477010/ 48.418232, val:  77.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1297%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0527%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 66905   8.992%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.196808/ 34.350399, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9417%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6947%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 67404   8.942%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.324503/ 32.690521, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8841%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3662%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 67911   8.893%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.352656/ 31.493572, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7411%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2982%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 68414   8.846%\n",
      "fc layer 1 self.abs_max_out: 3847.0\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.138631/ 32.345562, val:  87.92%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0858%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8303%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1227%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 68911   8.799%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.126134/ 50.017818, val:  80.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 69422   8.754%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.319703/ 44.179852, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0884%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4382%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6393%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 69912   8.709%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.111211/ 47.102715, val:  75.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1865%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5331%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 70378   8.661%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.695604/ 40.627018, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0601%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3620%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5337%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 70907   8.622%\n",
      "fc layer 1 self.abs_max_out: 3915.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.257713/ 38.138527, val:  83.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3564%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1476%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 71440   8.585%\n",
      "fc layer 1 self.abs_max_out: 3981.0\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.114008/ 43.775661, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0564%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1700%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7921%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 71926   8.543%\n",
      "fc layer 1 self.abs_max_out: 4117.0\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.446306/ 34.993004, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0835%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1147%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4778%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 72443   8.505%\n",
      "fc layer 2 self.abs_max_out: 2352.0\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.152709/ 37.555206, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1121%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7557%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 72930   8.465%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.379210/ 29.403006, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5852%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 73431   8.428%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  2.592879/ 34.204792, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6237%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1175%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 73858   8.382%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  2.889697/ 32.778416, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2615%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4925%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 74326   8.343%\n",
      "lif layer 1 self.abs_max_v: 6005.5\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.356433/ 44.152420, val:  80.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6749%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 74827   8.308%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.039345/ 35.577530, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.95 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0545%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4554%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8377%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 75313   8.272%\n",
      "lif layer 1 self.abs_max_v: 6059.5\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  2.961119/ 34.590397, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3505%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1361%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 75775   8.234%\n",
      "lif layer 1 self.abs_max_v: 6064.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.714891/ 38.670738, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0588%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1597%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 76209   8.194%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.778571/ 44.650845, val:  83.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6831%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 76643   8.155%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.752290/ 35.417683, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0910%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5896%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5568%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 77052   8.114%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.678630/ 32.208637, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4133%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1190%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 77486   8.076%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.714185/ 39.574154, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0729%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5739%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9928%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 77904   8.038%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  3.105127/ 36.230518, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0875%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7445%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 78361   8.004%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.956503/ 35.796108, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0671%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2371%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8122%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 78804   7.970%\n",
      "lif layer 2 self.abs_max_v: 3366.5\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.415303/ 36.427460, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0530%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5393%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6404%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 79197   7.931%\n",
      "lif layer 1 self.abs_max_v: 6165.5\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.643034/ 33.740257, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1108%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8622%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 79613   7.895%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.695968/ 33.758358, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1057%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.5710%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 80036   7.861%\n",
      "lif layer 2 self.abs_max_v: 3408.0\n",
      "lif layer 1 self.abs_max_v: 6273.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.797090/ 32.995445, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9885%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2887%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 80479   7.829%\n",
      "lif layer 1 self.abs_max_v: 6389.5\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.545640/ 36.532692, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0617%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2423%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4948%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 80872   7.793%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  3.025351/ 34.210487, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1061%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1434%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6630%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 81309   7.762%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.484280/ 36.134892, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 81693   7.726%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.433132/ 33.351162, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0768%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0518%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9290%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 82084   7.692%\n",
      "fc layer 2 self.abs_max_out: 2381.0\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.484592/ 35.887341, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0266%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8039%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 82456   7.657%\n",
      "fc layer 1 self.abs_max_out: 4272.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.747043/ 33.660980, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0354%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8622%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6285%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 82891   7.628%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.183408/ 41.046818, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0966%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6299%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 83233   7.591%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.665884/ 36.096207, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8903%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 83634   7.560%\n",
      "fc layer 1 self.abs_max_out: 4286.0\n",
      "lif layer 1 self.abs_max_v: 6393.5\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.632233/ 34.719654, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9140%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1714%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 84014   7.528%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.478702/ 33.420006, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0689%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8786%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2779%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 84375   7.494%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.251539/ 37.557968, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8627%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9876%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 84726   7.461%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.588003/ 40.692501, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0842%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7414%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6744%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 85106   7.430%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.303810/ 36.960049, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9681%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2155%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 85471   7.399%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.064246/ 36.265892, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0532%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0749%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5693%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 85796   7.364%\n",
      "fc layer 3 self.abs_max_out: 783.0\n",
      "fc layer 3 self.abs_max_out: 799.0\n",
      "fc layer 3 self.abs_max_out: 800.0\n",
      "fc layer 3 self.abs_max_out: 810.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.439480/ 39.404633, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0991%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1596%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2224%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 86194   7.337%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.093231/ 38.862354, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0490%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 86525   7.304%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.493597/ 40.942936, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0731%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8134%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3322%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 86881   7.274%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.130457/ 39.838890, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7890%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6353%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 87220   7.243%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.347776/ 36.008175, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0591%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8162%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7525%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 87579   7.214%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.391991/ 38.585220, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.43 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0858%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8930%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 87962   7.188%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.078366/ 37.044853, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1009%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1327%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5028%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 88297   7.158%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.065922/ 44.504303, val:  82.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0574%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2326%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3629%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 88626   7.128%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.012663/ 39.504082, val:  83.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1096%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0959%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 88953   7.099%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.131058/ 36.840576, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2088%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9978%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 89302   7.071%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  1.812837/ 33.065845, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0979%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0426%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1824%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 89606   7.041%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  1.913814/ 33.024124, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.61 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1136%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0378%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 89917   7.011%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  1.818198/ 45.081322, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7764%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7645%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 90223   6.982%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  1.855149/ 36.322155, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9706%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1132%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 90519   6.952%\n",
      "fc layer 2 self.abs_max_out: 2389.0\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.047448/ 44.702206, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0944%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7194%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 90833   6.924%\n",
      "fc layer 3 self.abs_max_out: 819.0\n",
      "lif layer 1 self.abs_max_v: 6683.0\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  1.981272/ 34.249592, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0741%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8435%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 91152   6.897%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.165270/ 47.261803, val:  82.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9544%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6273%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 91472   6.870%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  1.833224/ 33.707611, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0976%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2114%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 91782   6.843%\n",
      "fc layer 1 self.abs_max_out: 4304.0\n",
      "fc layer 3 self.abs_max_out: 823.0\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  1.990540/ 33.955952, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.45 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1328%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1263%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 92075   6.815%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  1.955107/ 31.679779, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0228%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3707%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4658%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 92379   6.789%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  1.952895/ 37.212200, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.71 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1018%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3064%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4378%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 92677   6.762%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  1.841132/ 48.017612, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0888%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1983%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1213%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 92981   6.736%\n",
      "fc layer 3 self.abs_max_out: 826.0\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  1.788356/ 35.243942, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0889%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9242%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3054%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 93269   6.709%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  1.936279/ 36.951126, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9834%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 93574   6.684%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  1.961753/ 40.171371, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8045%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4012%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 93879   6.659%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  1.866723/ 35.014305, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0702%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8999%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 94173   6.634%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.888335/ 41.518848, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1193%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0517%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4146%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 94487   6.611%\n",
      "fc layer 1 self.abs_max_out: 4470.0\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.897155/ 41.401249, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0588%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1011%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6757%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 94775   6.586%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.553688/ 39.096519, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2512%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5166%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 95029   6.559%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.776995/ 34.684444, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0989%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0450%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 95314   6.534%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.958691/ 38.885899, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8594%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6555%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 95621   6.511%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.731538/ 37.836987, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0747%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2188%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 95901   6.487%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.586753/ 42.663425, val:  85.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0835%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 96183   6.464%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.926357/ 42.209240, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0490%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8382%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 96469   6.440%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.885935/ 46.442242, val:  83.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0858%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9078%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2959%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 96743   6.417%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.659464/ 38.457539, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.64 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0530%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9393%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4160%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 97016   6.393%\n",
      "fc layer 1 self.abs_max_out: 4517.0\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.844574/ 37.409653, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0872%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6831%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7023%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 97317   6.372%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.708714/ 47.028328, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0858%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5137%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6975%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 97585   6.349%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.655587/ 37.902668, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7636%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8417%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 97853   6.326%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.447456/ 39.967621, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8264%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6766%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 98096   6.302%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.445223/ 35.398224, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0453%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2201%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 98333   6.278%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.916876/ 34.942368, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0948%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6857%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 98622   6.257%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.697455/ 36.453197, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1033%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7179%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 98898   6.236%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.669303/ 37.366600, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1263%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9654%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5697%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 99164   6.214%\n",
      "fc layer 1 self.abs_max_out: 4547.0\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.614993/ 32.868546, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6058%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9049%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 99435   6.193%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.569740/ 41.201488, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.54 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1127%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4760%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8156%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 99689   6.171%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.398674/ 33.275333, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0396%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9344%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 99905   6.147%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.861495/ 36.362995, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.03 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0158%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6115%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5957%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 100182   6.128%\n",
      "fc layer 1 self.abs_max_out: 4597.0\n",
      "lif layer 1 self.abs_max_v: 6698.5\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.757541/ 40.040596, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.07 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0962%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9890%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4178%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 100460   6.108%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.446140/ 34.809414, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.58 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0966%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8743%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7680%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 100706   6.087%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.378451/ 40.176926, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.70 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0600%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5531%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8329%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 100946   6.065%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.619658/ 36.370407, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.30 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6148%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 101206   6.045%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.557012/ 36.708973, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0522%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5952%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3684%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 101450   6.025%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.382122/ 40.009796, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1054%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6596%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1682%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 101684   6.004%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.329141/ 41.010052, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0436%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5028%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 101906   5.982%\n",
      "lif layer 2 self.abs_max_v: 3481.0\n",
      "lif layer 2 self.abs_max_v: 3507.5\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.478192/ 36.659130, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.02 seconds, 1.20 minutes\n",
      "layer   1  Sparsity: 91.1086%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7491%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1635%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 102126   5.961%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.500545/ 39.997723, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0607%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7077%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3919%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 102366   5.941%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.515800/ 38.196651, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0944%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8069%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9405%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 102593   5.921%\n",
      "lif layer 1 self.abs_max_v: 6795.0\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.485037/ 35.989178, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8829%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 102827   5.901%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.533074/ 36.809250, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9678%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4745%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 103065   5.881%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.221466/ 46.391068, val:  83.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0933%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7506%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5846%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 103276   5.861%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.444767/ 43.347885, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5265%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6537%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 103515   5.842%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.164273/ 39.858852, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1221%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4758%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7653%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 103712   5.821%\n",
      "lif layer 1 self.abs_max_v: 6951.5\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.142035/ 38.280163, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0985%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5743%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0407%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 103911   5.800%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.260749/ 39.834564, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0347%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5429%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4484%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 104133   5.781%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.448630/ 40.410297, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7934%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 104360   5.762%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.219260/ 35.347240, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1044%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9843%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4680%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 104564   5.742%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.116343/ 34.798565, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0557%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9793%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0807%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 104764   5.723%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.154951/ 40.021641, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0712%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2314%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 104953   5.702%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.294205/ 40.082638, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0718%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6429%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2070%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 105173   5.684%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.280220/ 42.955837, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0557%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6461%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 105374   5.665%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.346308/ 38.449814, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0446%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4568%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 105610   5.648%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.159379/ 37.240593, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0526%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3163%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1195%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 105809   5.629%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.219936/ 41.092510, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.5121%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6487%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 106008   5.610%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.458782/ 43.008591, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0877%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7305%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6616%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 106242   5.594%\n",
      "fc layer 3 self.abs_max_out: 828.0\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.397312/ 37.758301, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0469%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8306%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9952%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 106468   5.577%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.283422/ 38.591999, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0676%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7340%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9697%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 106667   5.559%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.335128/ 36.695328, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0796%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.6105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8693%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 106912   5.543%\n",
      "fc layer 3 self.abs_max_out: 831.0\n",
      "fc layer 3 self.abs_max_out: 834.0\n",
      "fc layer 3 self.abs_max_out: 849.0\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.142135/ 37.757500, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0833%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8652%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1817%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 107100   5.525%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.015211/ 36.843918, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1109%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2369%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 107275   5.506%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.327657/ 41.810467, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7851%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1249%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63bb6cf69d2e48b2947de7f4dbd72762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÉ‚ñÅ‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.32766</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>41.81047</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">breezy-sweep-15</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ht67ipil' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ht67ipil</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251213_025809-ht67ipil/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lswze1lx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 39530\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251213_071704-lswze1lx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lswze1lx' target=\"_blank\">gentle-sweep-22</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lswze1lx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lswze1lx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251213_071713_932', 'my_seed': 39530, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 352.0\n",
      "lif layer 1 self.abs_max_v: 352.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 348.0\n",
      "lif layer 2 self.abs_max_v: 348.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 146.0\n",
      "fc layer 1 self.abs_max_out: 453.0\n",
      "lif layer 1 self.abs_max_v: 453.0\n",
      "fc layer 2 self.abs_max_out: 415.0\n",
      "lif layer 2 self.abs_max_v: 415.0\n",
      "lif layer 1 self.abs_max_v: 500.5\n",
      "lif layer 2 self.abs_max_v: 454.0\n",
      "lif layer 1 self.abs_max_v: 617.5\n",
      "fc layer 2 self.abs_max_out: 476.0\n",
      "lif layer 2 self.abs_max_v: 493.0\n",
      "lif layer 2 self.abs_max_v: 545.5\n",
      "fc layer 1 self.abs_max_out: 464.0\n",
      "fc layer 1 self.abs_max_out: 503.0\n",
      "fc layer 1 self.abs_max_out: 558.0\n",
      "fc layer 1 self.abs_max_out: 681.0\n",
      "lif layer 1 self.abs_max_v: 681.0\n",
      "fc layer 1 self.abs_max_out: 754.0\n",
      "lif layer 1 self.abs_max_v: 754.0\n",
      "lif layer 2 self.abs_max_v: 605.5\n",
      "fc layer 2 self.abs_max_out: 494.0\n",
      "lif layer 2 self.abs_max_v: 746.0\n",
      "fc layer 3 self.abs_max_out: 148.0\n",
      "lif layer 2 self.abs_max_v: 767.0\n",
      "fc layer 2 self.abs_max_out: 536.0\n",
      "fc layer 3 self.abs_max_out: 161.0\n",
      "fc layer 2 self.abs_max_out: 558.0\n",
      "fc layer 3 self.abs_max_out: 173.0\n",
      "fc layer 3 self.abs_max_out: 177.0\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "fc layer 2 self.abs_max_out: 570.0\n",
      "fc layer 2 self.abs_max_out: 585.0\n",
      "lif layer 2 self.abs_max_v: 820.0\n",
      "fc layer 2 self.abs_max_out: 609.0\n",
      "lif layer 2 self.abs_max_v: 831.0\n",
      "lif layer 2 self.abs_max_v: 876.0\n",
      "fc layer 1 self.abs_max_out: 829.0\n",
      "lif layer 1 self.abs_max_v: 829.0\n",
      "fc layer 2 self.abs_max_out: 764.0\n",
      "lif layer 2 self.abs_max_v: 1036.0\n",
      "fc layer 1 self.abs_max_out: 935.0\n",
      "lif layer 1 self.abs_max_v: 935.0\n",
      "fc layer 1 self.abs_max_out: 1018.0\n",
      "lif layer 1 self.abs_max_v: 1018.0\n",
      "fc layer 3 self.abs_max_out: 224.0\n",
      "fc layer 1 self.abs_max_out: 1040.0\n",
      "lif layer 1 self.abs_max_v: 1040.0\n",
      "lif layer 2 self.abs_max_v: 1072.5\n",
      "fc layer 3 self.abs_max_out: 256.0\n",
      "lif layer 2 self.abs_max_v: 1086.0\n",
      "fc layer 3 self.abs_max_out: 273.0\n",
      "lif layer 1 self.abs_max_v: 1072.5\n",
      "lif layer 2 self.abs_max_v: 1087.5\n",
      "lif layer 2 self.abs_max_v: 1137.0\n",
      "fc layer 1 self.abs_max_out: 1136.0\n",
      "lif layer 1 self.abs_max_v: 1136.0\n",
      "fc layer 1 self.abs_max_out: 1211.0\n",
      "lif layer 1 self.abs_max_v: 1211.0\n",
      "fc layer 1 self.abs_max_out: 1466.0\n",
      "lif layer 1 self.abs_max_v: 1466.0\n",
      "fc layer 3 self.abs_max_out: 283.0\n",
      "fc layer 2 self.abs_max_out: 778.0\n",
      "fc layer 2 self.abs_max_out: 814.0\n",
      "lif layer 2 self.abs_max_v: 1152.5\n",
      "fc layer 1 self.abs_max_out: 1534.0\n",
      "lif layer 1 self.abs_max_v: 1534.0\n",
      "lif layer 2 self.abs_max_v: 1208.5\n",
      "fc layer 3 self.abs_max_out: 291.0\n",
      "fc layer 3 self.abs_max_out: 292.0\n",
      "lif layer 2 self.abs_max_v: 1234.5\n",
      "fc layer 2 self.abs_max_out: 816.0\n",
      "lif layer 2 self.abs_max_v: 1248.0\n",
      "fc layer 2 self.abs_max_out: 827.0\n",
      "fc layer 2 self.abs_max_out: 856.0\n",
      "lif layer 2 self.abs_max_v: 1252.0\n",
      "lif layer 2 self.abs_max_v: 1273.0\n",
      "fc layer 3 self.abs_max_out: 296.0\n",
      "fc layer 3 self.abs_max_out: 308.0\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "fc layer 3 self.abs_max_out: 314.0\n",
      "lif layer 1 self.abs_max_v: 1538.5\n",
      "lif layer 1 self.abs_max_v: 1542.0\n",
      "lif layer 1 self.abs_max_v: 1549.5\n",
      "lif layer 2 self.abs_max_v: 1339.0\n",
      "lif layer 1 self.abs_max_v: 1570.0\n",
      "lif layer 2 self.abs_max_v: 1340.0\n",
      "fc layer 2 self.abs_max_out: 864.0\n",
      "fc layer 2 self.abs_max_out: 880.0\n",
      "lif layer 2 self.abs_max_v: 1351.5\n",
      "fc layer 3 self.abs_max_out: 325.0\n",
      "lif layer 2 self.abs_max_v: 1361.0\n",
      "lif layer 2 self.abs_max_v: 1437.0\n",
      "lif layer 2 self.abs_max_v: 1539.5\n",
      "fc layer 2 self.abs_max_out: 945.0\n",
      "lif layer 1 self.abs_max_v: 1590.0\n",
      "lif layer 1 self.abs_max_v: 1629.0\n",
      "lif layer 1 self.abs_max_v: 1674.0\n",
      "lif layer 1 self.abs_max_v: 1688.5\n",
      "lif layer 1 self.abs_max_v: 1734.5\n",
      "lif layer 1 self.abs_max_v: 1859.0\n",
      "fc layer 2 self.abs_max_out: 979.0\n",
      "fc layer 2 self.abs_max_out: 1045.0\n",
      "fc layer 2 self.abs_max_out: 1078.0\n",
      "lif layer 2 self.abs_max_v: 1554.0\n",
      "fc layer 3 self.abs_max_out: 327.0\n",
      "lif layer 2 self.abs_max_v: 1597.5\n",
      "lif layer 2 self.abs_max_v: 1768.5\n",
      "fc layer 3 self.abs_max_out: 329.0\n",
      "fc layer 3 self.abs_max_out: 333.0\n",
      "lif layer 1 self.abs_max_v: 1862.5\n",
      "lif layer 1 self.abs_max_v: 1924.5\n",
      "fc layer 3 self.abs_max_out: 335.0\n",
      "lif layer 1 self.abs_max_v: 1940.0\n",
      "lif layer 1 self.abs_max_v: 2024.0\n",
      "lif layer 1 self.abs_max_v: 2040.5\n",
      "lif layer 1 self.abs_max_v: 2168.0\n",
      "fc layer 3 self.abs_max_out: 349.0\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "fc layer 3 self.abs_max_out: 358.0\n",
      "fc layer 1 self.abs_max_out: 1573.0\n",
      "lif layer 1 self.abs_max_v: 2526.0\n",
      "lif layer 1 self.abs_max_v: 2533.5\n",
      "lif layer 1 self.abs_max_v: 2600.5\n",
      "lif layer 1 self.abs_max_v: 2697.0\n",
      "lif layer 1 self.abs_max_v: 2766.5\n",
      "lif layer 1 self.abs_max_v: 2783.5\n",
      "fc layer 3 self.abs_max_out: 389.0\n",
      "fc layer 1 self.abs_max_out: 1577.0\n",
      "fc layer 1 self.abs_max_out: 1761.0\n",
      "lif layer 1 self.abs_max_v: 2946.5\n",
      "lif layer 1 self.abs_max_v: 3016.0\n",
      "fc layer 2 self.abs_max_out: 1091.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 13.520793/ 60.409630, val:  41.67%, val_best:  41.67%, tr:  96.53%, tr_best:  96.53%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0671%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1299%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5422%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2168  22.145%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "lif layer 2 self.abs_max_v: 1807.5\n",
      "fc layer 2 self.abs_max_out: 1095.0\n",
      "fc layer 1 self.abs_max_out: 1831.0\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "fc layer 2 self.abs_max_out: 1099.0\n",
      "fc layer 2 self.abs_max_out: 1124.0\n",
      "fc layer 2 self.abs_max_out: 1191.0\n",
      "fc layer 1 self.abs_max_out: 1861.0\n",
      "fc layer 1 self.abs_max_out: 2143.0\n",
      "lif layer 1 self.abs_max_v: 3615.0\n",
      "lif layer 1 self.abs_max_v: 3793.5\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.209084/ 58.578365, val:  42.92%, val_best:  42.92%, tr:  99.39%, tr_best:  99.39%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0468%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9188%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4364%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3709  18.943%\n",
      "fc layer 3 self.abs_max_out: 422.0\n",
      "fc layer 3 self.abs_max_out: 442.0\n",
      "fc layer 2 self.abs_max_out: 1203.0\n",
      "fc layer 3 self.abs_max_out: 447.0\n",
      "fc layer 1 self.abs_max_out: 2320.0\n",
      "fc layer 1 self.abs_max_out: 2375.0\n",
      "lif layer 1 self.abs_max_v: 4001.5\n",
      "lif layer 1 self.abs_max_v: 4119.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.721278/ 73.977669, val:  36.67%, val_best:  42.92%, tr:  99.49%, tr_best:  99.49%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1623%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5774%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5209  17.736%\n",
      "lif layer 2 self.abs_max_v: 1822.0\n",
      "fc layer 3 self.abs_max_out: 469.0\n",
      "lif layer 2 self.abs_max_v: 1827.5\n",
      "lif layer 2 self.abs_max_v: 1870.5\n",
      "lif layer 2 self.abs_max_v: 1887.0\n",
      "lif layer 2 self.abs_max_v: 1952.5\n",
      "lif layer 2 self.abs_max_v: 1986.5\n",
      "lif layer 2 self.abs_max_v: 1993.5\n",
      "fc layer 3 self.abs_max_out: 485.0\n",
      "lif layer 2 self.abs_max_v: 2000.5\n",
      "fc layer 1 self.abs_max_out: 2389.0\n",
      "fc layer 1 self.abs_max_out: 2438.0\n",
      "lif layer 1 self.abs_max_v: 4252.0\n",
      "lif layer 2 self.abs_max_v: 2112.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.262064/ 68.220551, val:  40.42%, val_best:  42.92%, tr:  99.49%, tr_best:  99.49%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2762%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6331%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6570  16.777%\n",
      "fc layer 2 self.abs_max_out: 1259.0\n",
      "fc layer 2 self.abs_max_out: 1281.0\n",
      "fc layer 3 self.abs_max_out: 493.0\n",
      "fc layer 2 self.abs_max_out: 1287.0\n",
      "lif layer 2 self.abs_max_v: 2116.0\n",
      "fc layer 2 self.abs_max_out: 1292.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  9.198373/ 65.449234, val:  45.42%, val_best:  45.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1002%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8470%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7924  16.188%\n",
      "lif layer 1 self.abs_max_v: 4254.5\n",
      "fc layer 2 self.abs_max_out: 1325.0\n",
      "fc layer 2 self.abs_max_out: 1340.0\n",
      "fc layer 1 self.abs_max_out: 2518.0\n",
      "lif layer 1 self.abs_max_v: 4348.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  8.652857/ 43.717625, val:  54.17%, val_best:  54.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2967%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4944%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9213  15.684%\n",
      "fc layer 2 self.abs_max_out: 1352.0\n",
      "fc layer 3 self.abs_max_out: 521.0\n",
      "fc layer 3 self.abs_max_out: 556.0\n",
      "fc layer 3 self.abs_max_out: 569.0\n",
      "fc layer 2 self.abs_max_out: 1449.0\n",
      "fc layer 1 self.abs_max_out: 2561.0\n",
      "lif layer 1 self.abs_max_v: 4363.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  8.958770/ 59.999355, val:  47.08%, val_best:  54.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0363%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8052%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10524  15.357%\n",
      "fc layer 1 self.abs_max_out: 2631.0\n",
      "lif layer 1 self.abs_max_v: 4384.0\n",
      "lif layer 1 self.abs_max_v: 4471.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  9.139137/ 81.665237, val:  33.75%, val_best:  54.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0858%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0586%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4126%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11827  15.101%\n",
      "fc layer 2 self.abs_max_out: 1552.0\n",
      "fc layer 1 self.abs_max_out: 2711.0\n",
      "lif layer 1 self.abs_max_v: 4508.5\n",
      "lif layer 1 self.abs_max_v: 4718.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.706874/ 55.184246, val:  47.92%, val_best:  54.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9179%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13089  14.855%\n",
      "fc layer 1 self.abs_max_out: 2757.0\n",
      "lif layer 1 self.abs_max_v: 4785.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.616694/ 38.996193, val:  59.17%, val_best:  59.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 76.74 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0880%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4962%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7952%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14316  14.623%\n",
      "fc layer 1 self.abs_max_out: 2862.0\n",
      "lif layer 1 self.abs_max_v: 5017.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  7.700477/ 59.624783, val:  49.58%, val_best:  59.17%, tr:  99.39%, tr_best:  99.90%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0989%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2032%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15484  14.378%\n",
      "fc layer 2 self.abs_max_out: 1573.0\n",
      "lif layer 2 self.abs_max_v: 2155.0\n",
      "fc layer 1 self.abs_max_out: 3074.0\n",
      "lif layer 1 self.abs_max_v: 5020.0\n",
      "lif layer 1 self.abs_max_v: 5187.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.892639/ 49.384830, val:  51.67%, val_best:  59.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0358%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0541%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7186%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16683  14.201%\n",
      "lif layer 2 self.abs_max_v: 2157.0\n",
      "lif layer 2 self.abs_max_v: 2210.5\n",
      "lif layer 2 self.abs_max_v: 2265.0\n",
      "lif layer 2 self.abs_max_v: 2280.5\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  8.166186/ 55.992489, val:  50.42%, val_best:  59.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0876%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9173%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3156%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17905  14.069%\n",
      "fc layer 3 self.abs_max_out: 580.0\n",
      "lif layer 2 self.abs_max_v: 2324.0\n",
      "fc layer 1 self.abs_max_out: 3160.0\n",
      "lif layer 1 self.abs_max_v: 5396.5\n",
      "fc layer 2 self.abs_max_out: 1593.0\n",
      "fc layer 2 self.abs_max_out: 1621.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  8.424233/ 50.493683, val:  51.67%, val_best:  59.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0916%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0661%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2340%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19099  13.935%\n",
      "fc layer 2 self.abs_max_out: 1665.0\n",
      "fc layer 2 self.abs_max_out: 1699.0\n",
      "fc layer 2 self.abs_max_out: 1703.0\n",
      "fc layer 2 self.abs_max_out: 1717.0\n",
      "fc layer 1 self.abs_max_out: 3195.0\n",
      "lif layer 1 self.abs_max_v: 5403.5\n",
      "lif layer 1 self.abs_max_v: 5451.5\n",
      "lif layer 2 self.abs_max_v: 2372.5\n",
      "lif layer 2 self.abs_max_v: 2415.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  8.121884/ 44.898762, val:  64.58%, val_best:  64.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7229%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0669%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20263  13.798%\n",
      "fc layer 3 self.abs_max_out: 615.0\n",
      "fc layer 1 self.abs_max_out: 3254.0\n",
      "lif layer 1 self.abs_max_v: 5486.5\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.873767/ 53.465439, val:  58.75%, val_best:  64.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0629%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7281%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1952%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21389  13.655%\n",
      "fc layer 1 self.abs_max_out: 3378.0\n",
      "lif layer 1 self.abs_max_v: 5525.0\n",
      "lif layer 1 self.abs_max_v: 5776.5\n",
      "fc layer 2 self.abs_max_out: 1742.0\n",
      "fc layer 2 self.abs_max_out: 1750.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.848282/ 40.020535, val:  60.00%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5490%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5415%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22497  13.517%\n",
      "fc layer 2 self.abs_max_out: 1768.0\n",
      "fc layer 1 self.abs_max_out: 3385.0\n",
      "fc layer 2 self.abs_max_out: 1778.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.745949/ 38.071980, val:  67.92%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0800%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0073%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4609%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23608  13.397%\n",
      "fc layer 2 self.abs_max_out: 1792.0\n",
      "fc layer 2 self.abs_max_out: 1850.0\n",
      "fc layer 1 self.abs_max_out: 3418.0\n",
      "lif layer 1 self.abs_max_v: 5880.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.286978/ 47.096989, val:  61.67%, val_best:  67.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0560%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4873%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24667  13.261%\n",
      "fc layer 2 self.abs_max_out: 1856.0\n",
      "fc layer 1 self.abs_max_out: 3436.0\n",
      "fc layer 2 self.abs_max_out: 1869.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.611068/ 58.848251, val:  63.75%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0921%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8048%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2340%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25739  13.146%\n",
      "fc layer 2 self.abs_max_out: 1934.0\n",
      "fc layer 3 self.abs_max_out: 638.0\n",
      "fc layer 2 self.abs_max_out: 2020.0\n",
      "fc layer 3 self.abs_max_out: 646.0\n",
      "fc layer 2 self.abs_max_out: 2037.0\n",
      "fc layer 1 self.abs_max_out: 3593.0\n",
      "lif layer 1 self.abs_max_v: 6087.5\n",
      "fc layer 2 self.abs_max_out: 2120.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  7.243906/ 74.531555, val:  49.58%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8669%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26778  13.025%\n",
      "fc layer 1 self.abs_max_out: 3672.0\n",
      "lif layer 1 self.abs_max_v: 6119.5\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  7.413237/ 39.540974, val:  62.92%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0462%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8940%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27769  12.893%\n",
      "lif layer 2 self.abs_max_v: 2485.0\n",
      "lif layer 2 self.abs_max_v: 2613.0\n",
      "lif layer 2 self.abs_max_v: 2762.5\n",
      "lif layer 2 self.abs_max_v: 2763.5\n",
      "lif layer 1 self.abs_max_v: 6122.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.566373/ 64.555565, val:  56.25%, val_best:  67.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0258%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5111%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28723  12.756%\n",
      "fc layer 3 self.abs_max_out: 654.0\n",
      "fc layer 3 self.abs_max_out: 664.0\n",
      "fc layer 3 self.abs_max_out: 675.0\n",
      "fc layer 3 self.abs_max_out: 676.0\n",
      "fc layer 3 self.abs_max_out: 701.0\n",
      "fc layer 1 self.abs_max_out: 3740.0\n",
      "lif layer 1 self.abs_max_v: 6259.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  7.091709/ 35.836304, val:  73.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5885%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29715  12.647%\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  7.009138/ 57.434898, val:  61.67%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1018%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4825%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30711  12.548%\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  7.185894/ 65.155754, val:  63.75%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2671%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4572%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31680  12.446%\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  7.218495/ 57.356544, val:  59.58%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1131%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8696%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7665%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32682  12.364%\n",
      "fc layer 1 self.abs_max_out: 3808.0\n",
      "lif layer 1 self.abs_max_v: 6417.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.923492/ 49.145836, val:  70.00%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0586%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8632%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3154%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33645  12.274%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  5.875063/ 52.075008, val:  68.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0904%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9747%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6295%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 34540  12.166%\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.596300/ 51.522678, val:  58.33%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8981%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9963%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35465  12.075%\n",
      "fc layer 1 self.abs_max_out: 3810.0\n",
      "lif layer 1 self.abs_max_v: 6421.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  5.504949/ 38.317066, val:  75.00%, val_best:  75.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6889%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2830%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 36316  11.966%\n",
      "fc layer 3 self.abs_max_out: 713.0\n",
      "fc layer 3 self.abs_max_out: 719.0\n",
      "fc layer 3 self.abs_max_out: 767.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.097430/ 44.222923, val:  74.17%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0186%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4144%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6061%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 37187  11.870%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  6.105436/ 60.462704, val:  63.33%, val_best:  75.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1015%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5300%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4918%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 38039  11.774%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  6.202961/ 32.152428, val:  82.08%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1000%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5164%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3065%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 38896  11.685%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.499006/ 46.221241, val:  68.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1204%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 39724  11.593%\n",
      "fc layer 1 self.abs_max_out: 3852.0\n",
      "lif layer 1 self.abs_max_v: 6505.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.595958/ 35.475502, val:  80.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 40569  11.511%\n",
      "fc layer 1 self.abs_max_out: 3858.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.543482/ 43.611542, val:  73.33%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0977%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3096%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8029%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 41384  11.425%\n",
      "fc layer 2 self.abs_max_out: 2228.0\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  6.134755/ 55.720333, val:  69.17%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0385%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5777%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5829%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 42248  11.356%\n",
      "fc layer 2 self.abs_max_out: 2306.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.658857/ 41.495201, val:  72.50%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0485%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9476%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6289%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 43067  11.280%\n",
      "lif layer 2 self.abs_max_v: 2768.0\n",
      "lif layer 2 self.abs_max_v: 2778.5\n",
      "lif layer 2 self.abs_max_v: 3068.5\n",
      "lif layer 2 self.abs_max_v: 3090.0\n",
      "lif layer 2 self.abs_max_v: 3217.0\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.325463/ 47.104851, val:  69.17%, val_best:  82.08%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0915%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4901%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4123%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 43842  11.196%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.501110/ 57.162842, val:  60.00%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5312%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7014%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 44627  11.118%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  4.713742/ 35.500137, val:  78.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7805%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8519%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 45348  11.029%\n",
      "fc layer 1 self.abs_max_out: 3868.0\n",
      "lif layer 1 self.abs_max_v: 6556.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.006386/ 44.718960, val:  67.92%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0729%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5223%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7873%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 46094  10.949%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.138210/ 52.930851, val:  62.92%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3660%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 46849  10.876%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  5.046924/ 37.326920, val:  81.25%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0776%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9305%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 47573  10.799%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.420749/ 37.372177, val:  75.83%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0289%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0859%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6340%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 48251  10.714%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.958734/ 40.517235, val:  75.42%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9466%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1313%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 48951  10.639%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.925500/ 41.393955, val:  79.17%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6378%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 49676  10.571%\n",
      "fc layer 2 self.abs_max_out: 2308.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.752831/ 39.915951, val:  76.67%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8711%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 50354  10.497%\n",
      "fc layer 3 self.abs_max_out: 806.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  5.358468/ 54.393234, val:  72.50%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9266%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 51114  10.442%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  5.200035/ 40.329777, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4305%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 51832  10.381%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.789207/ 43.007336, val:  73.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0581%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8709%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4941%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 52502  10.313%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.515186/ 31.777172, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0513%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6654%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4079%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 53175  10.248%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.714798/ 32.473663, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0347%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 53835  10.183%\n",
      "fc layer 1 self.abs_max_out: 3893.0\n",
      "lif layer 1 self.abs_max_v: 6593.5\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.439415/ 34.985668, val:  82.08%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0767%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7654%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3190%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 54500  10.122%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.499822/ 41.990578, val:  75.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7310%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6557%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 55129  10.056%\n",
      "fc layer 1 self.abs_max_out: 3932.0\n",
      "lif layer 1 self.abs_max_v: 6618.0\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.739686/ 34.398399, val:  83.75%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3939%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 55827  10.004%\n",
      "fc layer 1 self.abs_max_out: 3967.0\n",
      "lif layer 1 self.abs_max_v: 6629.0\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.375556/ 36.420036, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9735%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3392%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 56479   9.947%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.215823/ 39.379112, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1044%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7202%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3490%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 57092   9.884%\n",
      "lif layer 1 self.abs_max_v: 6655.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.329509/ 39.384590, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1256%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7048%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8924%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 57713   9.825%\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.143082/ 32.781384, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0901%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7168%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6250%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 58323   9.766%\n",
      "fc layer 1 self.abs_max_out: 3991.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.185231/ 45.732407, val:  80.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0546%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 58944   9.711%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.231789/ 40.479630, val:  78.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0542%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6472%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1530%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 59541   9.654%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.006879/ 42.655701, val:  76.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0747%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5664%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 60137   9.598%\n",
      "fc layer 1 self.abs_max_out: 4123.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.789777/ 38.101189, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9304%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 60717   9.541%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.804312/ 39.323505, val:  82.50%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 61292   9.486%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.004381/ 37.008156, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8368%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2565%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 61880   9.434%\n",
      "fc layer 1 self.abs_max_out: 4156.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.457570/ 41.845398, val:  82.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7477%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7555%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 62402   9.374%\n",
      "fc layer 1 self.abs_max_out: 4247.0\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.399013/ 40.487667, val:  83.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4450%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6072%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 62933   9.316%\n",
      "lif layer 1 self.abs_max_v: 6724.5\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.633631/ 40.877991, val:  78.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0699%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5597%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9809%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 63469   9.261%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.516671/ 40.107178, val:  79.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1221%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4663%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 64004   9.208%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.325781/ 33.998772, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5823%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1232%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 64515   9.153%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.695481/ 34.826492, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7326%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0431%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 65061   9.104%\n",
      "lif layer 1 self.abs_max_v: 6729.0\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.498379/ 44.964725, val:  76.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6434%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3816%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 65590   9.054%\n",
      "fc layer 3 self.abs_max_out: 810.0\n",
      "fc layer 3 self.abs_max_out: 817.0\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.597764/ 41.172901, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5355%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7633%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 66130   9.006%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.222312/ 36.782372, val:  84.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.33 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2374%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7931%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 66618   8.954%\n",
      "lif layer 2 self.abs_max_v: 3283.0\n",
      "lif layer 2 self.abs_max_v: 3352.5\n",
      "lif layer 2 self.abs_max_v: 3390.5\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.706580/ 47.645184, val:  75.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0801%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5369%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0358%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 67184   8.912%\n",
      "lif layer 2 self.abs_max_v: 3493.5\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.415268/ 41.833504, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0928%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4849%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 67725   8.869%\n",
      "lif layer 1 self.abs_max_v: 6913.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.259499/ 40.292278, val:  79.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0743%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7644%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0959%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 68234   8.822%\n",
      "fc layer 3 self.abs_max_out: 831.0\n",
      "fc layer 3 self.abs_max_out: 841.0\n",
      "fc layer 1 self.abs_max_out: 4468.0\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.442242/ 37.648575, val:  82.50%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5979%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 68752   8.778%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.662227/ 35.090038, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7341%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 69286   8.737%\n",
      "fc layer 3 self.abs_max_out: 845.0\n",
      "fc layer 3 self.abs_max_out: 863.0\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.221847/ 33.502705, val:  85.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0428%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8281%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 69775   8.692%\n",
      "fc layer 3 self.abs_max_out: 869.0\n",
      "fc layer 3 self.abs_max_out: 875.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.624744/ 40.849674, val:  82.92%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6890%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0293%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 70310   8.653%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.330846/ 37.260010, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0578%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4895%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2702%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 70815   8.611%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.309455/ 34.676098, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0935%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8214%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7996%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 71282   8.566%\n",
      "fc layer 3 self.abs_max_out: 899.0\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.526904/ 37.318428, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6630%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9610%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 71802   8.528%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.182484/ 36.889420, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.84 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0363%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4498%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 72269   8.485%\n",
      "lif layer 1 self.abs_max_v: 6979.0\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.095838/ 42.534042, val:  78.75%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2450%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 72731   8.442%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.230480/ 35.400364, val:  86.25%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2753%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1048%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 73214   8.403%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.216800/ 35.420704, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0633%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3782%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7547%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 73671   8.361%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.544329/ 34.941818, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1000%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9311%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 74167   8.325%\n",
      "lif layer 1 self.abs_max_v: 7127.0\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  2.494510/ 40.310715, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1110%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1683%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 74573   8.280%\n",
      "lif layer 1 self.abs_max_v: 7323.0\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  2.341438/ 49.776836, val:  77.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4290%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 74950   8.232%\n",
      "lif layer 1 self.abs_max_v: 7696.0\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.302172/ 34.657074, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0997%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3070%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9798%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 75416   8.195%\n",
      "fc layer 3 self.abs_max_out: 910.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  3.055852/ 31.926760, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1010%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8039%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 75857   8.156%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.834798/ 43.355507, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0836%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1184%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 76265   8.115%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.548058/ 38.332703, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5669%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 76661   8.073%\n",
      "fc layer 1 self.abs_max_out: 4547.0\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.883463/ 45.210709, val:  81.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2587%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0486%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 77100   8.036%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.873424/ 39.143112, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0504%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9304%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 77531   7.999%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.599771/ 32.943459, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2741%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8124%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 77947   7.962%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.606808/ 39.705109, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3146%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5847%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 78340   7.923%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.562300/ 38.065987, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2084%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0128%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 78740   7.885%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.892414/ 36.990471, val:  86.25%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2944%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3485%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 79163   7.851%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.436948/ 34.399918, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0880%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2673%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3901%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 79545   7.813%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.928644/ 38.880474, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0961%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6907%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 79986   7.781%\n",
      "fc layer 1 self.abs_max_out: 4607.0\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.382900/ 39.028858, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0703%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4539%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5069%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 80363   7.744%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.434286/ 48.430168, val:  78.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.51 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0926%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0603%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4249%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 80748   7.708%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.487127/ 33.041859, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0612%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1506%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6515%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 81122   7.672%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.810660/ 37.897053, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0694%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0683%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8405%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 81509   7.638%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.583342/ 39.611015, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0510%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1887%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7178%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 81885   7.604%\n",
      "fc layer 1 self.abs_max_out: 4623.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.401149/ 42.855671, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3618%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 82262   7.570%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.580630/ 37.703773, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1008%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3836%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5335%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 82666   7.539%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.442722/ 50.845264, val:  78.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0572%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3815%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 83014   7.504%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.660279/ 37.080280, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1670%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2076%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 83412   7.474%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.680047/ 42.876114, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.34 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0597%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2726%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 83804   7.444%\n",
      "fc layer 1 self.abs_max_out: 4710.0\n",
      "fc layer 2 self.abs_max_out: 2402.0\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.246778/ 36.506187, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1021%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8057%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 84136   7.409%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.412189/ 47.508156, val:  80.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0366%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0621%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6647%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 84507   7.378%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.365504/ 39.494595, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1076%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2491%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2953%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 84866   7.346%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.757489/ 32.704956, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4173%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4999%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 85278   7.320%\n",
      "fc layer 3 self.abs_max_out: 916.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.697794/ 40.291622, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0415%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3373%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 85676   7.293%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.085882/ 38.777809, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0342%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0918%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7557%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 86014   7.261%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.211305/ 37.423016, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0703%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0593%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6527%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 86354   7.230%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.409746/ 39.325817, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0718%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3355%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8378%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 86723   7.202%\n",
      "fc layer 3 self.abs_max_out: 920.0\n",
      "fc layer 2 self.abs_max_out: 2429.0\n",
      "fc layer 3 self.abs_max_out: 938.0\n",
      "fc layer 3 self.abs_max_out: 991.0\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.332808/ 42.369305, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0990%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8904%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 87076   7.173%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.047390/ 36.050236, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0874%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3202%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4360%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 87384   7.141%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.220894/ 41.239731, val:  82.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0407%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2137%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 87708   7.110%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.141298/ 47.816792, val:  82.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1657%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9784%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 88042   7.081%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.170184/ 42.765282, val:  83.75%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1052%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2220%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1640%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 88374   7.052%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.115036/ 41.132919, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0555%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1512%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4952%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 88703   7.024%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  1.995076/ 43.657486, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0409%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3466%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 89002   6.993%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.348295/ 35.739071, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2160%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8836%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 89359   6.968%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.483211/ 35.316170, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0756%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0498%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4187%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 89699   6.941%\n",
      "fc layer 1 self.abs_max_out: 4768.0\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.186740/ 36.717838, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0699%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2873%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0212%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 90038   6.915%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.246789/ 41.375828, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.12 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0418%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4022%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9384%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 90361   6.888%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  1.874367/ 41.474377, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2265%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 90669   6.860%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.069974/ 38.514484, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9117%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 90970   6.832%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  1.949142/ 36.415428, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0782%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2292%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 91273   6.805%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  1.910722/ 43.611973, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1456%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2545%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2330%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 91579   6.779%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  1.858074/ 37.472336, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0686%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4194%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3018%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 91871   6.751%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  1.991409/ 45.786892, val:  82.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5339%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 92161   6.724%\n",
      "fc layer 1 self.abs_max_out: 4772.0\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  1.798088/ 39.204552, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1012%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4093%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7146%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 92443   6.697%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  1.751127/ 49.200375, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3920%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7949%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 92698   6.668%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  2.024361/ 41.204247, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1163%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7336%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 93008   6.644%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  1.854154/ 37.838528, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0952%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3613%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 93290   6.617%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  1.923495/ 37.674992, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1186%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2376%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 93578   6.592%\n",
      "fc layer 1 self.abs_max_out: 4864.0\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.766417/ 38.057373, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9134%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2632%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 93857   6.566%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  2.001133/ 35.329529, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.72 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4121%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0454%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 94164   6.543%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  2.134354/ 39.374535, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0463%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5911%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9129%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 94478   6.521%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.711294/ 44.091290, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0900%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6976%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 94759   6.496%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.796682/ 39.697392, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1263%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6672%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 95039   6.472%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.498374/ 37.399445, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0528%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3960%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5878%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 95291   6.446%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.985869/ 44.130436, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3961%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7543%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 95587   6.424%\n",
      "lif layer 2 self.abs_max_v: 3575.0\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.585614/ 42.022881, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0546%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2909%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3079%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 95843   6.399%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.800983/ 37.924683, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.39 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0398%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4063%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4997%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 96119   6.375%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.681149/ 46.150520, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 96380   6.351%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.628147/ 38.055103, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.13 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0949%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1026%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8719%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 96644   6.328%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.752627/ 39.512577, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0600%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3320%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4146%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 96898   6.304%\n",
      "fc layer 2 self.abs_max_out: 2430.0\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.960230/ 35.774647, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0820%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2443%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3128%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 97174   6.282%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.557014/ 37.779305, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.11 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2289%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1337%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 97420   6.258%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.811680/ 46.354538, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0302%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7732%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 97720   6.239%\n",
      "lif layer 2 self.abs_max_v: 3652.5\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.797409/ 37.041370, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1842%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 97987   6.217%\n",
      "lif layer 2 self.abs_max_v: 3712.0\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.767570/ 41.688164, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0833%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2620%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6985%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 98255   6.195%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.522404/ 49.561596, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.89 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0754%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3041%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1825%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 98497   6.172%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.562596/ 37.930828, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0743%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2388%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 98721   6.149%\n",
      "fc layer 2 self.abs_max_out: 2442.0\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.513586/ 38.578197, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1242%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2411%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7467%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 98952   6.126%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.751379/ 35.233467, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.35 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0830%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4678%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 99228   6.106%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.432644/ 39.964062, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8498%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3883%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 99481   6.085%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.583259/ 39.129673, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8739%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1827%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 99709   6.062%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.632206/ 42.596748, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.64 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9905%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8457%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 99952   6.041%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.633104/ 41.470356, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1162%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0615%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 100215   6.021%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.778223/ 35.518841, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3123%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9066%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 100462   6.001%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.461560/ 38.957474, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4676%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 100697   5.980%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.668668/ 38.554455, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1006%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1103%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3413%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 100936   5.960%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.390192/ 40.670132, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1705%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2421%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 101174   5.939%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.463949/ 38.042782, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0951%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2929%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2476%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 101415   5.919%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.368897/ 36.275406, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3737%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 101627   5.898%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.501775/ 45.857597, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6362%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7522%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 101862   5.878%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.238530/ 35.347610, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3189%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6663%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 102065   5.857%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.557551/ 40.356823, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1023%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9570%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2525%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 102305   5.838%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.592605/ 43.476395, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0992%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8608%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2680%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 102547   5.819%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.502156/ 50.948750, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1050%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5888%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 102782   5.800%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.473511/ 40.811893, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9376%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 103029   5.782%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.535157/ 44.731258, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7575%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2829%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 103287   5.765%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.263869/ 42.095806, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7980%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 103489   5.745%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.283982/ 38.775883, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1052%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8197%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8135%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 103692   5.725%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.312331/ 43.633492, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7205%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3163%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 103904   5.706%\n",
      "fc layer 2 self.abs_max_out: 2458.0\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.251563/ 40.706631, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0851%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7696%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2298%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 104104   5.686%\n",
      "fc layer 2 self.abs_max_out: 2523.0\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.233931/ 39.778709, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0615%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7681%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 104305   5.667%\n",
      "lif layer 2 self.abs_max_v: 3815.0\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.424103/ 37.569691, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1168%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.8292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 104513   5.648%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.202205/ 39.622787, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0689%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0714%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6413%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 104707   5.629%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.201917/ 47.827187, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1384%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7127%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 104913   5.611%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.184584/ 41.448322, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2962%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9074%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 105124   5.593%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.300573/ 42.528854, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2305%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 105318   5.574%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.296354/ 42.920986, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1901%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 105526   5.556%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.087898/ 43.477917, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0962%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2513%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9204%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 105692   5.536%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.261981/ 37.928299, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1853%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9278%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 105895   5.519%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.043141/ 44.678955, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0473%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0852%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0704%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 106041   5.498%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.301521/ 43.509647, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.62 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2153%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3198%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 106225   5.480%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.173666/ 47.248013, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.41 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0886%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2233%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3085%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 106409   5.462%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.227830/ 41.863838, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.96 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0752%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1997%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7093%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70a4ac520c44778852db978b3655642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.22783</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.87917</td></tr><tr><td>val_loss</td><td>41.86384</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-sweep-22</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lswze1lx' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/lswze1lx</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251213_071704-lswze1lx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zppmq4cw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 8822\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251213_113543-zppmq4cw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zppmq4cw' target=\"_blank\">amber-sweep-30</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zppmq4cw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zppmq4cw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251213_113552_432', 'my_seed': 8822, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 243.0\n",
      "lif layer 1 self.abs_max_v: 243.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 251.0\n",
      "lif layer 2 self.abs_max_v: 251.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 53.0\n",
      "lif layer 1 self.abs_max_v: 304.5\n",
      "fc layer 2 self.abs_max_out: 299.0\n",
      "lif layer 2 self.abs_max_v: 341.5\n",
      "fc layer 3 self.abs_max_out: 106.0\n",
      "fc layer 1 self.abs_max_out: 303.0\n",
      "lif layer 1 self.abs_max_v: 425.5\n",
      "lif layer 2 self.abs_max_v: 401.0\n",
      "fc layer 1 self.abs_max_out: 322.0\n",
      "lif layer 1 self.abs_max_v: 515.0\n",
      "fc layer 2 self.abs_max_out: 338.0\n",
      "lif layer 2 self.abs_max_v: 444.5\n",
      "lif layer 2 self.abs_max_v: 458.5\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "fc layer 2 self.abs_max_out: 357.0\n",
      "lif layer 2 self.abs_max_v: 473.5\n",
      "fc layer 1 self.abs_max_out: 347.0\n",
      "lif layer 2 self.abs_max_v: 536.0\n",
      "lif layer 2 self.abs_max_v: 594.5\n",
      "fc layer 3 self.abs_max_out: 124.0\n",
      "fc layer 1 self.abs_max_out: 360.0\n",
      "fc layer 3 self.abs_max_out: 158.0\n",
      "fc layer 2 self.abs_max_out: 371.0\n",
      "fc layer 3 self.abs_max_out: 191.0\n",
      "fc layer 1 self.abs_max_out: 386.0\n",
      "fc layer 2 self.abs_max_out: 428.0\n",
      "fc layer 1 self.abs_max_out: 413.0\n",
      "fc layer 1 self.abs_max_out: 579.0\n",
      "lif layer 1 self.abs_max_v: 579.0\n",
      "fc layer 2 self.abs_max_out: 546.0\n",
      "lif layer 1 self.abs_max_v: 582.0\n",
      "lif layer 2 self.abs_max_v: 622.0\n",
      "lif layer 1 self.abs_max_v: 704.0\n",
      "lif layer 2 self.abs_max_v: 675.0\n",
      "fc layer 3 self.abs_max_out: 202.0\n",
      "lif layer 1 self.abs_max_v: 723.0\n",
      "fc layer 1 self.abs_max_out: 876.0\n",
      "lif layer 1 self.abs_max_v: 876.0\n",
      "fc layer 1 self.abs_max_out: 883.0\n",
      "lif layer 1 self.abs_max_v: 883.0\n",
      "lif layer 2 self.abs_max_v: 684.5\n",
      "fc layer 2 self.abs_max_out: 558.0\n",
      "fc layer 3 self.abs_max_out: 223.0\n",
      "lif layer 2 self.abs_max_v: 691.0\n",
      "fc layer 3 self.abs_max_out: 255.0\n",
      "fc layer 2 self.abs_max_out: 663.0\n",
      "lif layer 2 self.abs_max_v: 693.0\n",
      "lif layer 2 self.abs_max_v: 703.0\n",
      "lif layer 2 self.abs_max_v: 705.0\n",
      "lif layer 2 self.abs_max_v: 739.5\n",
      "lif layer 2 self.abs_max_v: 751.0\n",
      "lif layer 2 self.abs_max_v: 773.0\n",
      "lif layer 2 self.abs_max_v: 917.0\n",
      "fc layer 2 self.abs_max_out: 691.0\n",
      "lif layer 1 self.abs_max_v: 909.5\n",
      "lif layer 2 self.abs_max_v: 931.0\n",
      "lif layer 1 self.abs_max_v: 995.5\n",
      "lif layer 2 self.abs_max_v: 1023.5\n",
      "lif layer 2 self.abs_max_v: 1067.0\n",
      "lif layer 2 self.abs_max_v: 1151.5\n",
      "fc layer 3 self.abs_max_out: 268.0\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "fc layer 3 self.abs_max_out: 357.0\n",
      "fc layer 1 self.abs_max_out: 901.0\n",
      "fc layer 2 self.abs_max_out: 695.0\n",
      "fc layer 2 self.abs_max_out: 820.0\n",
      "fc layer 1 self.abs_max_out: 991.0\n",
      "fc layer 1 self.abs_max_out: 1078.0\n",
      "lif layer 1 self.abs_max_v: 1078.0\n",
      "fc layer 3 self.abs_max_out: 364.0\n",
      "lif layer 1 self.abs_max_v: 1137.0\n",
      "lif layer 1 self.abs_max_v: 1243.0\n",
      "lif layer 1 self.abs_max_v: 1512.5\n",
      "fc layer 1 self.abs_max_out: 1096.0\n",
      "fc layer 2 self.abs_max_out: 824.0\n",
      "fc layer 2 self.abs_max_out: 843.0\n",
      "fc layer 2 self.abs_max_out: 847.0\n",
      "lif layer 2 self.abs_max_v: 1215.5\n",
      "fc layer 2 self.abs_max_out: 868.0\n",
      "fc layer 2 self.abs_max_out: 917.0\n",
      "lif layer 2 self.abs_max_v: 1279.0\n",
      "lif layer 2 self.abs_max_v: 1323.0\n",
      "lif layer 2 self.abs_max_v: 1428.5\n",
      "fc layer 1 self.abs_max_out: 1106.0\n",
      "fc layer 1 self.abs_max_out: 1162.0\n",
      "fc layer 1 self.abs_max_out: 1220.0\n",
      "lif layer 1 self.abs_max_v: 1523.5\n",
      "lif layer 1 self.abs_max_v: 1550.0\n",
      "fc layer 3 self.abs_max_out: 381.0\n",
      "lif layer 1 self.abs_max_v: 1578.5\n",
      "lif layer 1 self.abs_max_v: 1593.5\n",
      "lif layer 1 self.abs_max_v: 1661.0\n",
      "lif layer 1 self.abs_max_v: 1756.0\n",
      "fc layer 3 self.abs_max_out: 386.0\n",
      "fc layer 3 self.abs_max_out: 388.0\n",
      "fc layer 3 self.abs_max_out: 410.0\n",
      "fc layer 3 self.abs_max_out: 419.0\n",
      "fc layer 3 self.abs_max_out: 421.0\n",
      "fc layer 3 self.abs_max_out: 426.0\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "fc layer 2 self.abs_max_out: 918.0\n",
      "fc layer 2 self.abs_max_out: 925.0\n",
      "fc layer 1 self.abs_max_out: 1382.0\n",
      "lif layer 2 self.abs_max_v: 1480.5\n",
      "lif layer 1 self.abs_max_v: 1803.5\n",
      "fc layer 2 self.abs_max_out: 979.0\n",
      "lif layer 1 self.abs_max_v: 1814.5\n",
      "lif layer 1 self.abs_max_v: 1895.0\n",
      "lif layer 1 self.abs_max_v: 1941.5\n",
      "lif layer 1 self.abs_max_v: 2045.0\n",
      "lif layer 1 self.abs_max_v: 2146.0\n",
      "fc layer 1 self.abs_max_out: 1395.0\n",
      "fc layer 1 self.abs_max_out: 1408.0\n",
      "fc layer 1 self.abs_max_out: 1513.0\n",
      "lif layer 1 self.abs_max_v: 2218.0\n",
      "fc layer 1 self.abs_max_out: 1565.0\n",
      "lif layer 1 self.abs_max_v: 2644.5\n",
      "lif layer 1 self.abs_max_v: 2807.5\n",
      "fc layer 2 self.abs_max_out: 988.0\n",
      "fc layer 2 self.abs_max_out: 1060.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 15.713155/ 83.562897, val:  38.75%, val_best:  38.75%, tr:  97.14%, tr_best:  97.14%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0460%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4853%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7891%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2257  23.054%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 464.0\n",
      "fc layer 2 self.abs_max_out: 1086.0\n",
      "fc layer 2 self.abs_max_out: 1102.0\n",
      "fc layer 2 self.abs_max_out: 1121.0\n",
      "fc layer 2 self.abs_max_out: 1186.0\n",
      "lif layer 2 self.abs_max_v: 1519.0\n",
      "lif layer 2 self.abs_max_v: 1565.5\n",
      "lif layer 2 self.abs_max_v: 1642.0\n",
      "lif layer 2 self.abs_max_v: 1660.0\n",
      "lif layer 2 self.abs_max_v: 1696.0\n",
      "fc layer 1 self.abs_max_out: 1619.0\n",
      "fc layer 1 self.abs_max_out: 1626.0\n",
      "fc layer 1 self.abs_max_out: 1659.0\n",
      "fc layer 1 self.abs_max_out: 1689.0\n",
      "fc layer 1 self.abs_max_out: 1769.0\n",
      "lif layer 1 self.abs_max_v: 2964.5\n",
      "lif layer 1 self.abs_max_v: 3178.5\n",
      "fc layer 1 self.abs_max_out: 1775.0\n",
      "lif layer 1 self.abs_max_v: 3183.5\n",
      "lif layer 1 self.abs_max_v: 3261.0\n",
      "lif layer 1 self.abs_max_v: 3307.5\n",
      "fc layer 1 self.abs_max_out: 1818.0\n",
      "lif layer 1 self.abs_max_v: 3472.0\n",
      "lif layer 1 self.abs_max_v: 3473.5\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.691254/ 71.042061, val:  39.17%, val_best:  39.17%, tr:  99.08%, tr_best:  99.08%, epoch time: 79.62 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.9904%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7786%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3851  19.668%\n",
      "fc layer 2 self.abs_max_out: 1229.0\n",
      "fc layer 3 self.abs_max_out: 477.0\n",
      "fc layer 1 self.abs_max_out: 1884.0\n",
      "fc layer 3 self.abs_max_out: 504.0\n",
      "fc layer 3 self.abs_max_out: 505.0\n",
      "lif layer 2 self.abs_max_v: 1703.5\n",
      "lif layer 2 self.abs_max_v: 1757.0\n",
      "fc layer 1 self.abs_max_out: 2037.0\n",
      "fc layer 2 self.abs_max_out: 1242.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 10.421771/ 79.194084, val:  35.42%, val_best:  39.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0897%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4290%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6595%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5377  18.308%\n",
      "lif layer 2 self.abs_max_v: 1778.5\n",
      "lif layer 2 self.abs_max_v: 1818.5\n",
      "lif layer 2 self.abs_max_v: 1924.5\n",
      "fc layer 2 self.abs_max_out: 1252.0\n",
      "fc layer 2 self.abs_max_out: 1303.0\n",
      "lif layer 2 self.abs_max_v: 1932.0\n",
      "lif layer 2 self.abs_max_v: 1951.0\n",
      "fc layer 1 self.abs_max_out: 2210.0\n",
      "lif layer 1 self.abs_max_v: 3549.5\n",
      "lif layer 1 self.abs_max_v: 3704.0\n",
      "lif layer 1 self.abs_max_v: 3809.0\n",
      "lif layer 1 self.abs_max_v: 3831.5\n",
      "lif layer 1 self.abs_max_v: 3917.0\n",
      "lif layer 1 self.abs_max_v: 4151.5\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.598127/ 51.139675, val:  53.75%, val_best:  53.75%, tr:  99.49%, tr_best:  99.69%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0328%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9832%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8898%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6753  17.245%\n",
      "lif layer 2 self.abs_max_v: 1972.0\n",
      "lif layer 2 self.abs_max_v: 2016.5\n",
      "fc layer 3 self.abs_max_out: 529.0\n",
      "fc layer 1 self.abs_max_out: 2238.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  9.427364/ 65.919479, val:  45.83%, val_best:  53.75%, tr:  99.39%, tr_best:  99.69%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3827%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7557%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 8102  16.552%\n",
      "fc layer 2 self.abs_max_out: 1306.0\n",
      "fc layer 2 self.abs_max_out: 1308.0\n",
      "fc layer 2 self.abs_max_out: 1344.0\n",
      "fc layer 1 self.abs_max_out: 2252.0\n",
      "fc layer 3 self.abs_max_out: 551.0\n",
      "fc layer 3 self.abs_max_out: 559.0\n",
      "fc layer 3 self.abs_max_out: 577.0\n",
      "fc layer 1 self.abs_max_out: 2342.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  9.557159/ 85.290787, val:  38.75%, val_best:  53.75%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0449%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.5236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1105%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9441  16.073%\n",
      "fc layer 2 self.abs_max_out: 1357.0\n",
      "lif layer 1 self.abs_max_v: 4159.0\n",
      "fc layer 1 self.abs_max_out: 2526.0\n",
      "lif layer 1 self.abs_max_v: 4354.0\n",
      "fc layer 2 self.abs_max_out: 1403.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  9.480294/ 52.333588, val:  56.25%, val_best:  56.25%, tr:  99.59%, tr_best:  99.69%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.4277%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0212%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10729  15.656%\n",
      "fc layer 2 self.abs_max_out: 1570.0\n",
      "fc layer 1 self.abs_max_out: 2721.0\n",
      "lif layer 1 self.abs_max_v: 4657.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.724137/ 66.821571, val:  48.33%, val_best:  56.25%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1126%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.2854%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9135%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11920  15.220%\n",
      "fc layer 1 self.abs_max_out: 2734.0\n",
      "lif layer 1 self.abs_max_v: 4693.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.719638/ 38.782757, val:  60.00%, val_best:  60.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0638%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.6611%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7509%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13113  14.883%\n",
      "fc layer 2 self.abs_max_out: 1588.0\n",
      "fc layer 3 self.abs_max_out: 602.0\n",
      "fc layer 1 self.abs_max_out: 2803.0\n",
      "lif layer 1 self.abs_max_v: 4715.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  9.197780/ 53.756798, val:  57.50%, val_best:  60.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0782%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3999%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.1769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14352  14.660%\n",
      "fc layer 2 self.abs_max_out: 1607.0\n",
      "fc layer 2 self.abs_max_out: 1648.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.695275/ 64.279610, val:  52.50%, val_best:  60.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0413%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.5730%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6385%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15566  14.454%\n",
      "lif layer 2 self.abs_max_v: 2043.5\n",
      "lif layer 2 self.abs_max_v: 2068.5\n",
      "lif layer 2 self.abs_max_v: 2082.0\n",
      "lif layer 2 self.abs_max_v: 2160.0\n",
      "lif layer 2 self.abs_max_v: 2185.0\n",
      "lif layer 2 self.abs_max_v: 2201.0\n",
      "lif layer 2 self.abs_max_v: 2274.5\n",
      "fc layer 2 self.abs_max_out: 1699.0\n",
      "fc layer 1 self.abs_max_out: 2947.0\n",
      "lif layer 1 self.abs_max_v: 4761.0\n",
      "lif layer 1 self.abs_max_v: 4800.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  8.190209/ 57.792076, val:  48.33%, val_best:  60.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7957%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16739  14.248%\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  8.513122/ 53.446331, val:  59.17%, val_best:  60.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8696%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17916  14.077%\n",
      "lif layer 2 self.abs_max_v: 2338.0\n",
      "lif layer 1 self.abs_max_v: 4832.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  8.256362/ 47.658085, val:  65.00%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0937%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7929%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4961%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19091  13.929%\n",
      "lif layer 1 self.abs_max_v: 4935.0\n",
      "fc layer 1 self.abs_max_out: 2954.0\n",
      "lif layer 1 self.abs_max_v: 5421.5\n",
      "lif layer 1 self.abs_max_v: 5487.0\n",
      "fc layer 2 self.abs_max_out: 1720.0\n",
      "fc layer 1 self.abs_max_out: 3216.0\n",
      "fc layer 2 self.abs_max_out: 1875.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.477450/ 53.767414, val:  57.92%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6688%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20181  13.743%\n",
      "fc layer 1 self.abs_max_out: 3219.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.619919/ 51.496037, val:  55.00%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4199%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4241%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21259  13.572%\n",
      "fc layer 3 self.abs_max_out: 612.0\n",
      "fc layer 3 self.abs_max_out: 621.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  8.012169/ 62.976639, val:  52.08%, val_best:  65.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0796%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4107%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9008%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22362  13.436%\n",
      "fc layer 1 self.abs_max_out: 3239.0\n",
      "lif layer 1 self.abs_max_v: 5560.0\n",
      "lif layer 1 self.abs_max_v: 5777.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.988037/ 52.539623, val:  45.83%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0768%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2184%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2631%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23470  13.319%\n",
      "fc layer 3 self.abs_max_out: 622.0\n",
      "fc layer 3 self.abs_max_out: 647.0\n",
      "fc layer 1 self.abs_max_out: 3241.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.409468/ 41.402359, val:  58.75%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9304%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1181%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24530  13.187%\n",
      "fc layer 1 self.abs_max_out: 3438.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.223017/ 53.261703, val:  60.42%, val_best:  65.00%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0681%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7344%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5605%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25570  13.059%\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  7.017533/ 56.745506, val:  57.50%, val_best:  65.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6782%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26602  12.939%\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  7.467930/ 44.092796, val:  65.83%, val_best:  65.83%, tr:  99.80%, tr_best:  99.90%, epoch time: 78.59 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5909%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27661  12.843%\n",
      "lif layer 2 self.abs_max_v: 2391.0\n",
      "lif layer 2 self.abs_max_v: 2431.5\n",
      "fc layer 3 self.abs_max_out: 669.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  7.188272/ 41.857029, val:  70.83%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0555%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28683  12.738%\n",
      "fc layer 1 self.abs_max_out: 3486.0\n",
      "lif layer 1 self.abs_max_v: 5909.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.636660/ 48.066158, val:  64.17%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.88 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7576%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29630  12.611%\n",
      "fc layer 1 self.abs_max_out: 3624.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  7.156645/ 41.181629, val:  69.17%, val_best:  70.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0855%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8083%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7846%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30671  12.532%\n",
      "lif layer 1 self.abs_max_v: 5911.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.550673/ 36.999973, val:  72.92%, val_best:  72.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1629%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6348%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31651  12.435%\n",
      "lif layer 2 self.abs_max_v: 2467.5\n",
      "lif layer 2 self.abs_max_v: 2473.5\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  7.056635/ 43.033363, val:  73.75%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7930%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6294%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32615  12.339%\n",
      "fc layer 2 self.abs_max_out: 1878.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.489769/ 45.196110, val:  70.42%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6290%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5861%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33529  12.232%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  6.518378/ 35.474628, val:  71.67%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8463%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 34428  12.126%\n",
      "fc layer 1 self.abs_max_out: 3644.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.122017/ 55.741539, val:  60.83%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6692%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6168%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35323  12.027%\n",
      "lif layer 2 self.abs_max_v: 2504.0\n",
      "lif layer 2 self.abs_max_v: 2531.5\n",
      "lif layer 2 self.abs_max_v: 2642.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  5.909217/ 46.862446, val:  63.33%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1006%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6381%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3659%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 36187  11.924%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.269827/ 39.287132, val:  73.75%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2465%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5127%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 37067  11.832%\n",
      "fc layer 3 self.abs_max_out: 697.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  6.020498/ 42.162849, val:  71.67%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4129%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7956%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 37910  11.734%\n",
      "fc layer 1 self.abs_max_out: 3656.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.959356/ 37.219074, val:  75.83%, val_best:  75.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0806%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5364%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8340%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 38792  11.654%\n",
      "fc layer 1 self.abs_max_out: 3763.0\n",
      "lif layer 1 self.abs_max_v: 5968.5\n",
      "lif layer 1 self.abs_max_v: 6126.5\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  6.227960/ 37.950294, val:  78.33%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0892%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5658%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6085%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 39668  11.577%\n",
      "fc layer 1 self.abs_max_out: 3780.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.757608/ 43.481472, val:  68.75%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5884%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9553%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 40499  11.491%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.781664/ 45.646362, val:  73.33%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6094%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6064%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 41333  11.411%\n",
      "fc layer 3 self.abs_max_out: 709.0\n",
      "fc layer 3 self.abs_max_out: 712.0\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.707453/ 46.412685, val:  77.92%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0378%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1620%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 42166  11.334%\n",
      "fc layer 3 self.abs_max_out: 721.0\n",
      "fc layer 2 self.abs_max_out: 1883.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.980272/ 33.062981, val:  79.17%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0466%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 43005  11.263%\n",
      "fc layer 3 self.abs_max_out: 739.0\n",
      "fc layer 3 self.abs_max_out: 749.0\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.901184/ 41.134968, val:  75.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0548%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0846%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.1233%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 43800  11.185%\n",
      "fc layer 2 self.abs_max_out: 1910.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.255315/ 40.608711, val:  75.00%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0536%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9658%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3023%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 44574  11.105%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.674706/ 35.515659, val:  80.42%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8344%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 45348  11.029%\n",
      "fc layer 1 self.abs_max_out: 3787.0\n",
      "fc layer 1 self.abs_max_out: 3882.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.222163/ 54.642895, val:  67.50%, val_best:  80.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3011%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5341%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 46105  10.952%\n",
      "fc layer 2 self.abs_max_out: 1927.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.347194/ 34.650833, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.89 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0475%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4552%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8138%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 46869  10.881%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.974771/ 45.940586, val:  72.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4811%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 47611  10.807%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  5.203150/ 41.607883, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1142%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4789%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5781%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 48368  10.740%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.905375/ 33.781895, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0383%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2213%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2148%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 49108  10.673%\n",
      "fc layer 3 self.abs_max_out: 756.0\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.932564/ 44.438347, val:  77.92%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0398%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1450%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 49843  10.607%\n",
      "lif layer 2 self.abs_max_v: 2670.0\n",
      "lif layer 2 self.abs_max_v: 2679.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  5.248002/ 35.184540, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4470%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 50582  10.544%\n",
      "fc layer 2 self.abs_max_out: 1946.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  5.459165/ 33.011818, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0875%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2575%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7999%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 51345  10.489%\n",
      "fc layer 2 self.abs_max_out: 1983.0\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.552639/ 36.854420, val:  79.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0854%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0406%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 52015  10.418%\n",
      "fc layer 1 self.abs_max_out: 3888.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  5.228577/ 51.196243, val:  79.58%, val_best:  87.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1034%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9613%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6925%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 52749  10.362%\n",
      "fc layer 1 self.abs_max_out: 3897.0\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.830149/ 39.589439, val:  79.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0556%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8553%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 53450  10.301%\n",
      "lif layer 1 self.abs_max_v: 6151.5\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  5.236677/ 48.437664, val:  67.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.57 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0552%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7585%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8245%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 54168  10.246%\n",
      "fc layer 1 self.abs_max_out: 3957.0\n",
      "fc layer 2 self.abs_max_out: 2008.0\n",
      "lif layer 2 self.abs_max_v: 2765.5\n",
      "lif layer 1 self.abs_max_v: 6163.5\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.123043/ 41.668266, val:  77.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8828%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2649%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 54810  10.179%\n",
      "fc layer 1 self.abs_max_out: 3963.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.641193/ 53.244225, val:  74.58%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6270%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 55469  10.118%\n",
      "fc layer 1 self.abs_max_out: 4041.0\n",
      "lif layer 2 self.abs_max_v: 2859.0\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  5.128674/ 60.573589, val:  70.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.85 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0542%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6272%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9358%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 56158  10.064%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.393529/ 46.084949, val:  85.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5915%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1767%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 56788  10.001%\n",
      "fc layer 3 self.abs_max_out: 780.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.083470/ 36.859554, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1118%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6854%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6742%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 57412   9.940%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.634508/ 39.548412, val:  80.83%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9268%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4756%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 58097   9.891%\n",
      "fc layer 1 self.abs_max_out: 4052.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.399497/ 41.369976, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0949%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9086%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5271%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 58716   9.832%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.461118/ 35.411674, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0704%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7337%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 59330   9.775%\n",
      "fc layer 2 self.abs_max_out: 2054.0\n",
      "fc layer 1 self.abs_max_out: 4074.0\n",
      "lif layer 1 self.abs_max_v: 6168.0\n",
      "lif layer 1 self.abs_max_v: 6388.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.408669/ 37.447235, val:  79.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0583%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7978%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4042%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 59946   9.719%\n",
      "fc layer 1 self.abs_max_out: 4083.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.265277/ 52.259789, val:  70.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0562%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5635%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 60574   9.668%\n",
      "fc layer 2 self.abs_max_out: 2057.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.812055/ 36.801338, val:  84.17%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.30 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9600%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 61138   9.608%\n",
      "fc layer 1 self.abs_max_out: 4132.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.710192/ 56.856976, val:  65.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0585%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9504%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7291%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 61701   9.549%\n",
      "fc layer 2 self.abs_max_out: 2076.0\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.137802/ 47.577782, val:  76.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1025%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8527%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2059%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 62312   9.500%\n",
      "fc layer 2 self.abs_max_out: 2092.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  4.581212/ 50.818653, val:  76.25%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6839%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0495%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 62939   9.454%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.918522/ 44.462929, val:  83.33%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0528%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3861%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 63528   9.404%\n",
      "fc layer 2 self.abs_max_out: 2133.0\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  4.255053/ 34.913479, val:  82.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.61 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2961%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 64134   9.359%\n",
      "fc layer 2 self.abs_max_out: 2142.0\n",
      "fc layer 2 self.abs_max_out: 2191.0\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  4.019313/ 36.138603, val:  86.67%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7877%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9216%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 64704   9.309%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  4.022039/ 38.649429, val:  85.42%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9643%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 65284   9.262%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.605581/ 37.114960, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0845%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8342%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3607%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 65799   9.207%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.952198/ 61.687637, val:  69.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6470%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2178%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 66348   9.158%\n",
      "fc layer 2 self.abs_max_out: 2209.0\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  4.119324/ 40.453548, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0823%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5118%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9993%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 66932   9.116%\n",
      "lif layer 1 self.abs_max_v: 6451.0\n",
      "lif layer 1 self.abs_max_v: 6468.5\n",
      "fc layer 2 self.abs_max_out: 2218.0\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.750881/ 55.814346, val:  72.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0389%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7636%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5517%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 67470   9.068%\n",
      "fc layer 1 self.abs_max_out: 4215.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.503308/ 44.431858, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0909%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7109%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 67997   9.020%\n",
      "fc layer 1 self.abs_max_out: 4228.0\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.759975/ 38.483166, val:  83.33%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0365%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6341%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4861%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 68531   8.974%\n",
      "fc layer 2 self.abs_max_out: 2245.0\n",
      "lif layer 1 self.abs_max_v: 6547.5\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.644078/ 35.531754, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.99 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2033%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7212%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 69073   8.931%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.780562/ 39.287689, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0845%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7059%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4358%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 69617   8.889%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.635885/ 53.173538, val:  74.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.97 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4808%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2441%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 70156   8.847%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.229869/ 41.327457, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0774%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5091%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1256%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 70647   8.800%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.698101/ 36.434242, val:  83.75%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1028%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5485%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 71154   8.757%\n",
      "fc layer 1 self.abs_max_out: 4262.0\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.184131/ 40.433208, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0821%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6261%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 71631   8.710%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.566230/ 36.472038, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0525%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7153%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6751%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 72152   8.671%\n",
      "lif layer 1 self.abs_max_v: 6551.5\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.516842/ 41.383286, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.91 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0471%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4632%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7578%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 72646   8.628%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.293088/ 37.451202, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0929%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6477%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9313%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 73134   8.587%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.281180/ 40.283356, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.38 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7129%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6584%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 73608   8.544%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.387882/ 46.068874, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4623%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3838%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 74102   8.505%\n",
      "lif layer 1 self.abs_max_v: 6620.0\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.299394/ 38.233170, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.67 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0615%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6020%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4285%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 74572   8.464%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.474456/ 37.284267, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1105%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6724%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6327%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 75042   8.423%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  2.808378/ 38.621368, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.83 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6771%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2386%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 75498   8.382%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.196083/ 43.788670, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0420%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5137%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6113%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 75965   8.343%\n",
      "fc layer 3 self.abs_max_out: 827.0\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  2.928708/ 40.972046, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0694%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5861%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7978%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 76430   8.305%\n",
      "fc layer 2 self.abs_max_out: 2273.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  3.343794/ 34.183929, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7330%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0344%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 76895   8.268%\n",
      "fc layer 1 self.abs_max_out: 4413.0\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  3.300235/ 42.939636, val:  85.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0958%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6619%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8016%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 77379   8.233%\n",
      "fc layer 2 self.abs_max_out: 2285.0\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.845840/ 39.579647, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0677%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5828%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9762%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 77813   8.194%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  3.082534/ 43.659332, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1216%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5224%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 78262   8.157%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.967667/ 37.349308, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.16 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0886%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3981%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6254%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 78717   8.122%\n",
      "lif layer 2 self.abs_max_v: 2865.0\n",
      "fc layer 1 self.abs_max_out: 4539.0\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  3.058654/ 39.615974, val:  87.92%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3371%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6048%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 79164   8.086%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  3.157860/ 45.157085, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4204%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1362%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 79619   8.052%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  3.220189/ 45.220917, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.13 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4147%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4522%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 80070   8.018%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  3.064427/ 39.952152, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.21 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3822%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8172%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 80526   7.986%\n",
      "lif layer 2 self.abs_max_v: 2900.0\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  3.131920/ 40.709877, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.37 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0788%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9062%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9025%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 80948   7.950%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  3.013361/ 39.795029, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.13 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0258%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0683%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4275%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 81377   7.916%\n",
      "fc layer 3 self.abs_max_out: 856.0\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.694693/ 42.231224, val:  85.00%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.07 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1275%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3241%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 81783   7.881%\n",
      "fc layer 2 self.abs_max_out: 2305.0\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.790578/ 38.826424, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.61 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0965%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1941%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7138%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 82200   7.847%\n",
      "fc layer 2 self.abs_max_out: 2364.0\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.649131/ 39.699402, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.34 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0894%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 82606   7.813%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.849045/ 38.201481, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.13 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0203%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6591%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 83036   7.781%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.867770/ 44.328148, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5200%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 83471   7.751%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.777691/ 36.284962, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0970%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3560%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8341%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 83898   7.721%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.638879/ 38.351456, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.88 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2502%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4195%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 84282   7.687%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.662860/ 39.668892, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0939%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7425%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 84674   7.654%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.626111/ 41.826210, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0992%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3142%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 85069   7.622%\n",
      "fc layer 3 self.abs_max_out: 860.0\n",
      "fc layer 2 self.abs_max_out: 2393.0\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.714104/ 37.126945, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0706%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1461%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1658%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 85478   7.592%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.481338/ 41.625851, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.35 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0932%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2460%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0040%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 85856   7.560%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.465815/ 36.363373, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0273%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4510%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8380%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 86224   7.528%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.655934/ 36.843163, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1039%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4101%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6574%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 86611   7.497%\n",
      "lif layer 2 self.abs_max_v: 2934.5\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.531466/ 39.170052, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0563%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2120%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1990%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 87011   7.469%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.434053/ 41.550770, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0676%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2435%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4104%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 87384   7.438%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.548237/ 43.314896, val:  84.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5965%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4041%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 87766   7.409%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.313330/ 43.512775, val:  85.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0460%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5970%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5133%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 88139   7.379%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.482989/ 43.457729, val:  85.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4543%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5306%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 88499   7.349%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.310122/ 44.855705, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.07 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0811%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3687%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9504%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 88834   7.318%\n",
      "fc layer 1 self.abs_max_out: 4581.0\n",
      "lif layer 1 self.abs_max_v: 6956.5\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.486934/ 42.295918, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1182%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7374%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0539%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 89209   7.290%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.490355/ 40.087532, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0736%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5772%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0174%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 89548   7.259%\n",
      "fc layer 2 self.abs_max_out: 2419.0\n",
      "lif layer 2 self.abs_max_v: 2946.0\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.196758/ 43.395500, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4483%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 89881   7.229%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.387030/ 45.648407, val:  83.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1065%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7089%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7912%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 90253   7.202%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.356493/ 57.714260, val:  80.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4628%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 90611   7.175%\n",
      "lif layer 2 self.abs_max_v: 3003.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.424232/ 46.972870, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4365%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8915%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 90954   7.147%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.476983/ 42.469147, val:  85.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5528%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8141%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 91282   7.118%\n",
      "fc layer 1 self.abs_max_out: 4606.0\n",
      "lif layer 1 self.abs_max_v: 7118.0\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.372152/ 39.040112, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0979%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4604%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7734%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 91624   7.090%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.539493/ 36.244766, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1044%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2315%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3606%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 91982   7.064%\n",
      "lif layer 2 self.abs_max_v: 3030.0\n",
      "lif layer 2 self.abs_max_v: 3039.0\n",
      "fc layer 2 self.abs_max_out: 2434.0\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  1.888394/ 40.411106, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.58 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1451%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5119%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 92295   7.035%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.209431/ 48.701359, val:  83.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4650%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4351%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 92617   7.008%\n",
      "fc layer 1 self.abs_max_out: 4716.0\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.361411/ 45.423996, val:  83.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.60 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.1112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5094%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2977%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 92925   6.979%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  2.226780/ 42.770634, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0585%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5395%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 93254   6.953%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.280941/ 40.683167, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.65 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0854%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3751%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6941%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 93568   6.926%\n",
      "lif layer 2 self.abs_max_v: 3045.5\n",
      "fc layer 3 self.abs_max_out: 864.0\n",
      "lif layer 2 self.abs_max_v: 3086.0\n",
      "lif layer 2 self.abs_max_v: 3149.5\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  2.188878/ 35.669296, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1004%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5431%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9166%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 93880   6.899%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.245892/ 42.174721, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9877%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 94224   6.875%\n",
      "fc layer 2 self.abs_max_out: 2454.0\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  2.344279/ 38.128254, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.39 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0330%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4355%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6226%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 94567   6.851%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.267354/ 46.289707, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0592%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3401%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8851%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 94896   6.826%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  2.332976/ 40.504929, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0552%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1632%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2281%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 95217   6.801%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  1.820187/ 42.588577, val:  85.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0599%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3103%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 95498   6.774%\n",
      "fc layer 3 self.abs_max_out: 866.0\n",
      "lif layer 2 self.abs_max_v: 3170.0\n",
      "lif layer 2 self.abs_max_v: 3171.0\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  2.212167/ 36.290333, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1884%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3535%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 95814   6.750%\n",
      "fc layer 3 self.abs_max_out: 912.0\n",
      "lif layer 2 self.abs_max_v: 3202.5\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  2.150438/ 41.190964, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0780%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3682%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5774%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 96132   6.726%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.938166/ 40.251472, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0531%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7622%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 96441   6.701%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.951432/ 42.152611, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1141%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2040%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1818%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 96725   6.676%\n",
      "fc layer 2 self.abs_max_out: 2461.0\n",
      "fc layer 2 self.abs_max_out: 2488.0\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.995169/ 50.060146, val:  85.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0517%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2800%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6873%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 97032   6.652%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  2.336939/ 45.901909, val:  84.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0909%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7753%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 97376   6.631%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  2.112405/ 45.779945, val:  83.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0783%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4303%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6865%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 97694   6.609%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  2.083030/ 48.740448, val:  82.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.34 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0732%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2201%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 98005   6.586%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  2.364177/ 40.903976, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.13 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3319%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 98345   6.566%\n",
      "fc layer 2 self.abs_max_out: 2515.0\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  2.048852/ 49.020115, val:  82.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.57 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0474%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4777%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4229%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 98636   6.542%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  2.094615/ 40.054741, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.11 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1039%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5636%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 98912   6.518%\n",
      "lif layer 2 self.abs_max_v: 3229.5\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.687318/ 54.115704, val:  82.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.09 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0768%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7291%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1387%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 99179   6.494%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.886546/ 40.658077, val:  86.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5360%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 99449   6.470%\n",
      "lif layer 2 self.abs_max_v: 3261.5\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.936859/ 41.720955, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.81 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4621%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3394%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 99736   6.448%\n",
      "fc layer 1 self.abs_max_out: 4727.0\n",
      "lif layer 2 self.abs_max_v: 3278.5\n",
      "lif layer 2 self.abs_max_v: 3337.5\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.676923/ 48.876740, val:  84.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0586%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3565%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7953%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 100004   6.424%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.977521/ 42.943722, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0811%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3450%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1578%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 100291   6.403%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.982986/ 46.327717, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0728%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5340%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 100577   6.381%\n",
      "fc layer 1 self.abs_max_out: 4731.0\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.850306/ 38.439590, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3455%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3290%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 100846   6.359%\n",
      "fc layer 2 self.abs_max_out: 2516.0\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.844119/ 43.671803, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.59 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5142%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2169%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 101107   6.336%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.766765/ 41.952007, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7004%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4258%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 101386   6.315%\n",
      "fc layer 1 self.abs_max_out: 4771.0\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  2.174639/ 40.761753, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0829%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4049%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5516%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 101703   6.296%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  2.005744/ 43.799335, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0737%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3567%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7301%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 102003   6.277%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.699781/ 37.176331, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0929%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3173%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8327%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 102245   6.254%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.681427/ 41.642326, val:  86.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0823%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3572%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9922%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 102511   6.233%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.336315/ 43.927822, val:  85.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2644%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0158%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 102749   6.210%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.666291/ 45.495480, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0829%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3357%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 103009   6.189%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.619627/ 49.253071, val:  82.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0514%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9904%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0791%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 103267   6.169%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.418675/ 42.206013, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1006%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 103463   6.144%\n",
      "fc layer 1 self.abs_max_out: 4778.0\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.699534/ 37.733753, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0985%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3592%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 103728   6.124%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.528274/ 44.298817, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0720%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0520%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2893%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 103980   6.104%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.571201/ 44.883545, val:  86.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.52 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0565%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8270%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 104212   6.083%\n",
      "fc layer 2 self.abs_max_out: 2531.0\n",
      "fc layer 1 self.abs_max_out: 4789.0\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.726607/ 39.106827, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7703%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5626%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 104465   6.063%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.692949/ 42.930099, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0574%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0020%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7054%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 104708   6.043%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.688325/ 42.536392, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1252%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0092%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 104981   6.024%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.512697/ 48.711628, val:  85.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8673%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4161%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 105223   6.004%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.462664/ 42.754559, val:  86.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0591%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7791%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0981%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 105462   5.985%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.444711/ 38.827419, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8218%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 105693   5.965%\n",
      "fc layer 1 self.abs_max_out: 4805.0\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.801371/ 44.106785, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0514%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0347%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7147%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 105932   5.945%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.593959/ 51.439034, val:  85.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6216%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 106173   5.926%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.321233/ 48.248798, val:  87.92%, val_best:  91.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 80.34 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0036%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8970%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 106376   5.905%\n",
      "lif layer 2 self.abs_max_v: 3343.0\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.684764/ 43.425476, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.91 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0919%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7176%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6867%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 106624   5.887%\n",
      "fc layer 1 self.abs_max_out: 4840.0\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.376020/ 43.352547, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6696%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 106830   5.867%\n",
      "lif layer 2 self.abs_max_v: 3371.0\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.841376/ 42.302189, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.32 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4362%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 107101   5.850%\n",
      "fc layer 3 self.abs_max_out: 914.0\n",
      "fc layer 3 self.abs_max_out: 916.0\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.690564/ 44.970795, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0924%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8380%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9189%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 107352   5.833%\n",
      "lif layer 2 self.abs_max_v: 3491.5\n",
      "fc layer 3 self.abs_max_out: 929.0\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.543814/ 48.684486, val:  83.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0955%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9310%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0815%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 107599   5.815%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.691624/ 47.331558, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0167%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 107836   5.797%\n",
      "fc layer 3 self.abs_max_out: 949.0\n",
      "lif layer 2 self.abs_max_v: 3496.5\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.712746/ 46.357151, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0788%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0307%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1369%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 108083   5.780%\n",
      "fc layer 3 self.abs_max_out: 994.0\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.511803/ 41.383408, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1068%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0728%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9111%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 108315   5.762%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.427248/ 41.774853, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1204%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 108556   5.745%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.345466/ 43.525604, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.77 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9727%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3673%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 108764   5.727%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.131585/ 45.352928, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9590%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 108965   5.708%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.434263/ 45.750877, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0357%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1847%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 109177   5.690%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.391088/ 48.942299, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9735%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4474%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 109398   5.672%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.423436/ 42.526337, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1006%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8966%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2391%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 109615   5.655%\n",
      "lif layer 1 self.abs_max_v: 7190.5\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.463530/ 46.879589, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0465%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9941%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9661%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 109848   5.638%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.431670/ 51.652637, val:  82.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0510%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8006%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6cbf0ea18d4a02a715dec5229fbc10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñÜ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.43167</td></tr><tr><td>val_acc_best</td><td>0.91667</td></tr><tr><td>val_acc_now</td><td>0.82083</td></tr><tr><td>val_loss</td><td>51.65264</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-sweep-30</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zppmq4cw' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/zppmq4cw</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251213_113543-zppmq4cw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nb7wkky2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 35155\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251213_155613-nb7wkky2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nb7wkky2' target=\"_blank\">woven-sweep-37</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nb7wkky2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nb7wkky2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251213_155623_317', 'my_seed': 35155, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 307.0\n",
      "lif layer 1 self.abs_max_v: 307.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 333.0\n",
      "lif layer 2 self.abs_max_v: 333.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 119.0\n",
      "lif layer 1 self.abs_max_v: 386.5\n",
      "fc layer 2 self.abs_max_out: 371.0\n",
      "lif layer 2 self.abs_max_v: 371.0\n",
      "fc layer 3 self.abs_max_out: 167.0\n",
      "lif layer 1 self.abs_max_v: 429.0\n",
      "lif layer 2 self.abs_max_v: 438.5\n",
      "fc layer 1 self.abs_max_out: 373.0\n",
      "lif layer 1 self.abs_max_v: 586.0\n",
      "fc layer 2 self.abs_max_out: 384.0\n",
      "lif layer 2 self.abs_max_v: 443.0\n",
      "lif layer 1 self.abs_max_v: 639.0\n",
      "lif layer 2 self.abs_max_v: 474.5\n",
      "lif layer 2 self.abs_max_v: 475.5\n",
      "fc layer 1 self.abs_max_out: 393.0\n",
      "lif layer 1 self.abs_max_v: 657.5\n",
      "fc layer 2 self.abs_max_out: 396.0\n",
      "lif layer 2 self.abs_max_v: 516.0\n",
      "fc layer 1 self.abs_max_out: 501.0\n",
      "fc layer 1 self.abs_max_out: 619.0\n",
      "lif layer 1 self.abs_max_v: 701.5\n",
      "lif layer 2 self.abs_max_v: 543.0\n",
      "lif layer 2 self.abs_max_v: 572.0\n",
      "lif layer 2 self.abs_max_v: 605.0\n",
      "lif layer 2 self.abs_max_v: 666.0\n",
      "fc layer 2 self.abs_max_out: 437.0\n",
      "lif layer 2 self.abs_max_v: 675.5\n",
      "fc layer 2 self.abs_max_out: 476.0\n",
      "lif layer 2 self.abs_max_v: 800.5\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "fc layer 2 self.abs_max_out: 520.0\n",
      "fc layer 3 self.abs_max_out: 200.0\n",
      "fc layer 1 self.abs_max_out: 657.0\n",
      "fc layer 1 self.abs_max_out: 688.0\n",
      "fc layer 1 self.abs_max_out: 727.0\n",
      "lif layer 1 self.abs_max_v: 727.0\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "fc layer 1 self.abs_max_out: 778.0\n",
      "lif layer 1 self.abs_max_v: 778.0\n",
      "fc layer 3 self.abs_max_out: 216.0\n",
      "fc layer 2 self.abs_max_out: 592.0\n",
      "lif layer 2 self.abs_max_v: 890.5\n",
      "fc layer 3 self.abs_max_out: 225.0\n",
      "fc layer 2 self.abs_max_out: 632.0\n",
      "fc layer 3 self.abs_max_out: 231.0\n",
      "fc layer 2 self.abs_max_out: 742.0\n",
      "lif layer 2 self.abs_max_v: 913.0\n",
      "lif layer 1 self.abs_max_v: 799.5\n",
      "lif layer 1 self.abs_max_v: 806.0\n",
      "lif layer 1 self.abs_max_v: 939.0\n",
      "lif layer 1 self.abs_max_v: 946.0\n",
      "fc layer 1 self.abs_max_out: 878.0\n",
      "lif layer 1 self.abs_max_v: 978.0\n",
      "lif layer 1 self.abs_max_v: 993.0\n",
      "lif layer 2 self.abs_max_v: 943.0\n",
      "fc layer 3 self.abs_max_out: 268.0\n",
      "fc layer 1 self.abs_max_out: 968.0\n",
      "fc layer 2 self.abs_max_out: 760.0\n",
      "fc layer 3 self.abs_max_out: 273.0\n",
      "lif layer 2 self.abs_max_v: 944.0\n",
      "fc layer 1 self.abs_max_out: 984.0\n",
      "lif layer 1 self.abs_max_v: 1095.5\n",
      "fc layer 2 self.abs_max_out: 853.0\n",
      "lif layer 2 self.abs_max_v: 976.5\n",
      "lif layer 2 self.abs_max_v: 983.0\n",
      "lif layer 2 self.abs_max_v: 1012.5\n",
      "lif layer 2 self.abs_max_v: 1107.5\n",
      "lif layer 2 self.abs_max_v: 1211.0\n",
      "fc layer 1 self.abs_max_out: 1018.0\n",
      "lif layer 1 self.abs_max_v: 1170.5\n",
      "lif layer 1 self.abs_max_v: 1221.5\n",
      "fc layer 3 self.abs_max_out: 274.0\n",
      "fc layer 1 self.abs_max_out: 1049.0\n",
      "fc layer 1 self.abs_max_out: 1090.0\n",
      "fc layer 1 self.abs_max_out: 1099.0\n",
      "fc layer 1 self.abs_max_out: 1216.0\n",
      "fc layer 3 self.abs_max_out: 278.0\n",
      "lif layer 1 self.abs_max_v: 1237.5\n",
      "lif layer 1 self.abs_max_v: 1277.0\n",
      "lif layer 1 self.abs_max_v: 1330.5\n",
      "lif layer 1 self.abs_max_v: 1473.0\n",
      "fc layer 1 self.abs_max_out: 1258.0\n",
      "lif layer 1 self.abs_max_v: 1497.0\n",
      "fc layer 2 self.abs_max_out: 889.0\n",
      "lif layer 1 self.abs_max_v: 1518.0\n",
      "lif layer 1 self.abs_max_v: 1540.0\n",
      "lif layer 1 self.abs_max_v: 1565.0\n",
      "lif layer 1 self.abs_max_v: 1656.0\n",
      "lif layer 1 self.abs_max_v: 1976.0\n",
      "fc layer 3 self.abs_max_out: 316.0\n",
      "lif layer 2 self.abs_max_v: 1307.0\n",
      "fc layer 2 self.abs_max_out: 909.0\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "fc layer 1 self.abs_max_out: 1284.0\n",
      "fc layer 2 self.abs_max_out: 914.0\n",
      "fc layer 2 self.abs_max_out: 995.0\n",
      "fc layer 2 self.abs_max_out: 1012.0\n",
      "lif layer 2 self.abs_max_v: 1390.5\n",
      "lif layer 2 self.abs_max_v: 1430.5\n",
      "fc layer 3 self.abs_max_out: 337.0\n",
      "fc layer 1 self.abs_max_out: 1324.0\n",
      "fc layer 1 self.abs_max_out: 1425.0\n",
      "lif layer 1 self.abs_max_v: 1982.5\n",
      "fc layer 1 self.abs_max_out: 1432.0\n",
      "fc layer 1 self.abs_max_out: 1547.0\n",
      "lif layer 2 self.abs_max_v: 1477.0\n",
      "lif layer 2 self.abs_max_v: 1513.5\n",
      "lif layer 2 self.abs_max_v: 1645.0\n",
      "fc layer 2 self.abs_max_out: 1036.0\n",
      "lif layer 1 self.abs_max_v: 2084.0\n",
      "lif layer 1 self.abs_max_v: 2218.0\n",
      "lif layer 1 self.abs_max_v: 2246.5\n",
      "lif layer 1 self.abs_max_v: 2310.0\n",
      "fc layer 1 self.abs_max_out: 1553.0\n",
      "fc layer 3 self.abs_max_out: 338.0\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "fc layer 3 self.abs_max_out: 356.0\n",
      "fc layer 3 self.abs_max_out: 363.0\n",
      "fc layer 1 self.abs_max_out: 1627.0\n",
      "fc layer 1 self.abs_max_out: 1643.0\n",
      "lif layer 1 self.abs_max_v: 2320.5\n",
      "lif layer 1 self.abs_max_v: 2473.5\n",
      "lif layer 1 self.abs_max_v: 2489.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 13.926214/ 83.164062, val:  34.17%, val_best:  34.17%, tr:  96.42%, tr_best:  96.42%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 79.3947%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.1473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2318  23.677%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 1701.0\n",
      "fc layer 2 self.abs_max_out: 1048.0\n",
      "fc layer 3 self.abs_max_out: 374.0\n",
      "fc layer 2 self.abs_max_out: 1051.0\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "fc layer 1 self.abs_max_out: 1744.0\n",
      "fc layer 2 self.abs_max_out: 1137.0\n",
      "lif layer 1 self.abs_max_v: 2517.0\n",
      "fc layer 2 self.abs_max_out: 1210.0\n",
      "fc layer 1 self.abs_max_out: 1785.0\n",
      "lif layer 1 self.abs_max_v: 2528.0\n",
      "lif layer 1 self.abs_max_v: 2635.5\n",
      "lif layer 2 self.abs_max_v: 1695.0\n",
      "fc layer 2 self.abs_max_out: 1219.0\n",
      "fc layer 1 self.abs_max_out: 1876.0\n",
      "lif layer 1 self.abs_max_v: 2988.5\n",
      "lif layer 1 self.abs_max_v: 3201.5\n",
      "lif layer 2 self.abs_max_v: 1696.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.688178/ 59.733395, val:  37.50%, val_best:  37.50%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.9921%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6384%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 4006  20.460%\n",
      "fc layer 2 self.abs_max_out: 1227.0\n",
      "lif layer 2 self.abs_max_v: 1730.5\n",
      "lif layer 2 self.abs_max_v: 1766.5\n",
      "lif layer 2 self.abs_max_v: 1803.5\n",
      "fc layer 2 self.abs_max_out: 1250.0\n",
      "fc layer 3 self.abs_max_out: 402.0\n",
      "fc layer 3 self.abs_max_out: 403.0\n",
      "fc layer 3 self.abs_max_out: 433.0\n",
      "fc layer 2 self.abs_max_out: 1282.0\n",
      "fc layer 1 self.abs_max_out: 2033.0\n",
      "lif layer 1 self.abs_max_v: 3219.5\n",
      "lif layer 1 self.abs_max_v: 3463.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.454145/ 43.702137, val:  50.00%, val_best:  50.00%, tr:  99.18%, tr_best:  99.59%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1016%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3529%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8476%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5525  18.812%\n",
      "fc layer 2 self.abs_max_out: 1321.0\n",
      "fc layer 3 self.abs_max_out: 460.0\n",
      "fc layer 2 self.abs_max_out: 1326.0\n",
      "fc layer 2 self.abs_max_out: 1335.0\n",
      "fc layer 2 self.abs_max_out: 1363.0\n",
      "fc layer 2 self.abs_max_out: 1454.0\n",
      "lif layer 2 self.abs_max_v: 1809.0\n",
      "lif layer 2 self.abs_max_v: 1891.5\n",
      "lif layer 2 self.abs_max_v: 1901.5\n",
      "lif layer 2 self.abs_max_v: 1973.0\n",
      "lif layer 2 self.abs_max_v: 1994.0\n",
      "fc layer 1 self.abs_max_out: 2360.0\n",
      "lif layer 1 self.abs_max_v: 3730.0\n",
      "lif layer 1 self.abs_max_v: 3983.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.310664/ 80.102745, val:  34.58%, val_best:  50.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0302%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7530%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8252%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6965  17.786%\n",
      "fc layer 1 self.abs_max_out: 2559.0\n",
      "lif layer 1 self.abs_max_v: 4049.5\n",
      "lif layer 1 self.abs_max_v: 4317.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  8.624109/ 29.630693, val:  58.33%, val_best:  58.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.6192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9712%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 8295  16.946%\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "fc layer 3 self.abs_max_out: 490.0\n",
      "fc layer 3 self.abs_max_out: 493.0\n",
      "fc layer 3 self.abs_max_out: 506.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  8.550846/ 47.751450, val:  43.75%, val_best:  58.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0391%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3632%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0891%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9603  16.348%\n",
      "fc layer 2 self.abs_max_out: 1465.0\n",
      "fc layer 1 self.abs_max_out: 2576.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  8.569843/ 30.908598, val:  56.67%, val_best:  58.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3143%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10879  15.875%\n",
      "lif layer 2 self.abs_max_v: 1998.0\n",
      "lif layer 2 self.abs_max_v: 2050.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  7.995608/ 62.113766, val:  52.08%, val_best:  58.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1273%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.3478%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9864%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 12106  15.457%\n",
      "lif layer 2 self.abs_max_v: 2166.0\n",
      "fc layer 2 self.abs_max_out: 1481.0\n",
      "lif layer 2 self.abs_max_v: 2172.0\n",
      "fc layer 3 self.abs_max_out: 507.0\n",
      "fc layer 3 self.abs_max_out: 521.0\n",
      "fc layer 3 self.abs_max_out: 546.0\n",
      "fc layer 1 self.abs_max_out: 2814.0\n",
      "lif layer 1 self.abs_max_v: 4417.0\n",
      "lif layer 1 self.abs_max_v: 4651.5\n",
      "fc layer 2 self.abs_max_out: 1515.0\n",
      "fc layer 2 self.abs_max_out: 1569.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.469004/ 67.332962, val:  46.25%, val_best:  58.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0354%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5943%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4743%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13355  15.157%\n",
      "fc layer 1 self.abs_max_out: 2917.0\n",
      "lif layer 1 self.abs_max_v: 4888.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.240028/ 52.176365, val:  50.83%, val_best:  58.33%, tr:  99.59%, tr_best: 100.00%, epoch time: 78.87 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6237%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4440%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14556  14.868%\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  7.985460/ 52.768806, val:  55.00%, val_best:  58.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.18 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0770%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5725%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0122%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15739  14.615%\n",
      "lif layer 2 self.abs_max_v: 2266.5\n",
      "lif layer 2 self.abs_max_v: 2321.5\n",
      "lif layer 2 self.abs_max_v: 2344.0\n",
      "lif layer 2 self.abs_max_v: 2463.5\n",
      "fc layer 1 self.abs_max_out: 2986.0\n",
      "lif layer 1 self.abs_max_v: 4997.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.761457/ 52.162800, val:  50.42%, val_best:  58.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 76.60 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1030%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0948%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0911%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16917  14.400%\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.859827/ 60.115955, val:  54.58%, val_best:  58.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 78.02 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6368%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4448%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 18126  14.242%\n",
      "fc layer 3 self.abs_max_out: 557.0\n",
      "fc layer 3 self.abs_max_out: 559.0\n",
      "fc layer 2 self.abs_max_out: 1582.0\n",
      "fc layer 1 self.abs_max_out: 3156.0\n",
      "lif layer 1 self.abs_max_v: 5035.5\n",
      "lif layer 1 self.abs_max_v: 5342.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.981718/ 50.877907, val:  55.00%, val_best:  58.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 78.01 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0681%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5583%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5575%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19322  14.097%\n",
      "fc layer 3 self.abs_max_out: 571.0\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "fc layer 3 self.abs_max_out: 598.0\n",
      "fc layer 1 self.abs_max_out: 3161.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.493459/ 64.236938, val:  48.33%, val_best:  58.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1033%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0539%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2847%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20456  13.930%\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.798279/ 34.833138, val:  63.33%, val_best:  63.33%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1457%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7112%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21634  13.811%\n",
      "fc layer 3 self.abs_max_out: 627.0\n",
      "fc layer 1 self.abs_max_out: 3190.0\n",
      "lif layer 1 self.abs_max_v: 5372.5\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.819910/ 35.175514, val:  62.92%, val_best:  63.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0769%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7258%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8627%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22771  13.682%\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.254486/ 28.538618, val:  72.50%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0935%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6713%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23908  13.567%\n",
      "fc layer 1 self.abs_max_out: 3291.0\n",
      "lif layer 1 self.abs_max_v: 5545.5\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.370244/ 62.150055, val:  52.92%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0922%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7601%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 25023  13.453%\n",
      "fc layer 2 self.abs_max_out: 1583.0\n",
      "fc layer 1 self.abs_max_out: 3416.0\n",
      "lif layer 1 self.abs_max_v: 5756.0\n",
      "lif layer 2 self.abs_max_v: 2474.5\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  6.996815/ 59.880306, val:  48.75%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0753%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5118%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0054%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 26123  13.342%\n",
      "fc layer 2 self.abs_max_out: 1600.0\n",
      "fc layer 2 self.abs_max_out: 1602.0\n",
      "fc layer 2 self.abs_max_out: 1631.0\n",
      "fc layer 2 self.abs_max_out: 1646.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  6.235581/ 53.340710, val:  57.08%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0472%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4321%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9053%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 27116  13.189%\n",
      "fc layer 2 self.abs_max_out: 1651.0\n",
      "fc layer 3 self.abs_max_out: 639.0\n",
      "fc layer 1 self.abs_max_out: 3496.0\n",
      "lif layer 1 self.abs_max_v: 5831.5\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.667861/ 45.162514, val:  59.58%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0961%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1102%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 28176  13.082%\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.487288/ 45.047100, val:  59.17%, val_best:  72.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6507%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9146%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 29192  12.964%\n",
      "fc layer 2 self.abs_max_out: 1653.0\n",
      "fc layer 1 self.abs_max_out: 3575.0\n",
      "lif layer 1 self.abs_max_v: 5900.0\n",
      "lif layer 1 self.abs_max_v: 5973.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  5.901763/ 40.342125, val:  70.00%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1037%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0096%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2773%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 30169  12.840%\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.031618/ 46.393295, val:  63.33%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0899%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6669%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3918%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 31137  12.722%\n",
      "fc layer 2 self.abs_max_out: 1667.0\n",
      "fc layer 2 self.abs_max_out: 1717.0\n",
      "fc layer 2 self.abs_max_out: 1797.0\n",
      "lif layer 2 self.abs_max_v: 2626.5\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.179633/ 57.813435, val:  55.42%, val_best:  72.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.22 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0953%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3821%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4278%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 32129  12.622%\n",
      "fc layer 2 self.abs_max_out: 1831.0\n",
      "fc layer 2 self.abs_max_out: 1868.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.601667/ 48.328789, val:  67.50%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.62 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3152%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2822%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 33140  12.537%\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  5.971262/ 40.310970, val:  62.08%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1624%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0049%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 34105  12.442%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  5.718393/ 44.324802, val:  69.17%, val_best:  72.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2323%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 35045  12.344%\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.149348/ 38.117462, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5985%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35994  12.255%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  5.551608/ 40.691113, val:  67.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4275%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 36905  12.160%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.052159/ 56.618729, val:  54.58%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3784%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1669%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 37843  12.080%\n",
      "fc layer 2 self.abs_max_out: 1923.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.583928/ 47.831188, val:  55.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0970%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6541%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 38715  11.983%\n",
      "fc layer 1 self.abs_max_out: 3605.0\n",
      "lif layer 1 self.abs_max_v: 5990.5\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.618044/ 35.608402, val:  76.67%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0826%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5337%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9754%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 39595  11.895%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.702886/ 42.644672, val:  62.50%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5586%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3100%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 40490  11.817%\n",
      "lif layer 2 self.abs_max_v: 2647.5\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.424889/ 29.497427, val:  80.00%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0452%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1753%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3223%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 41340  11.730%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.263926/ 36.308743, val:  78.75%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.93 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2360%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8847%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 42185  11.646%\n",
      "fc layer 1 self.abs_max_out: 3653.0\n",
      "lif layer 1 self.abs_max_v: 6111.0\n",
      "lif layer 1 self.abs_max_v: 6137.5\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.466004/ 38.269650, val:  71.67%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.65 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0894%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3570%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1982%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 43049  11.572%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.308993/ 38.025913, val:  70.42%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0824%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5976%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 43881  11.493%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  4.835914/ 34.075420, val:  78.75%, val_best:  80.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2304%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9426%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 44659  11.404%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.305276/ 40.477364, val:  75.42%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6533%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 45475  11.329%\n",
      "fc layer 1 self.abs_max_out: 3782.0\n",
      "lif layer 1 self.abs_max_v: 6345.5\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.347034/ 39.562771, val:  71.25%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1691%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5372%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 46297  11.260%\n",
      "lif layer 2 self.abs_max_v: 2757.0\n",
      "lif layer 2 self.abs_max_v: 2802.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.152409/ 39.804138, val:  69.58%, val_best:  80.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.29 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0563%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 47116  11.192%\n",
      "fc layer 1 self.abs_max_out: 3866.0\n",
      "lif layer 1 self.abs_max_v: 6534.0\n",
      "lif layer 2 self.abs_max_v: 2879.5\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.090588/ 30.985434, val:  82.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7607%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0782%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 47946  11.131%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.929440/ 33.411865, val:  80.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0665%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8874%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 48695  11.053%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.757524/ 32.683731, val:  82.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.63 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9325%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9510%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 49467  10.984%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.906132/ 42.426998, val:  67.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0520%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1564%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1231%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 50200  10.910%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.919220/ 43.463203, val:  74.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0786%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3832%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7723%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 50945  10.841%\n",
      "lif layer 1 self.abs_max_v: 6542.0\n",
      "lif layer 1 self.abs_max_v: 6756.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.538410/ 39.815704, val:  78.33%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0570%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3216%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9057%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 51707  10.779%\n",
      "fc layer 3 self.abs_max_out: 654.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.095555/ 38.652695, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0531%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7692%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9742%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 52407  10.706%\n",
      "lif layer 2 self.abs_max_v: 2964.0\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.390829/ 41.857059, val:  75.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0999%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7392%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 53090  10.633%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.397303/ 37.480320, val:  80.42%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.68 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0576%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9250%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9543%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 53793  10.567%\n",
      "fc layer 3 self.abs_max_out: 659.0\n",
      "fc layer 3 self.abs_max_out: 661.0\n",
      "fc layer 3 self.abs_max_out: 670.0\n",
      "fc layer 1 self.abs_max_out: 3913.0\n",
      "lif layer 1 self.abs_max_v: 6768.5\n",
      "lif layer 2 self.abs_max_v: 2996.0\n",
      "lif layer 2 self.abs_max_v: 3031.0\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.335632/ 47.911652, val:  67.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8815%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 54491  10.502%\n",
      "fc layer 3 self.abs_max_out: 695.0\n",
      "lif layer 2 self.abs_max_v: 3041.5\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.424817/ 35.868759, val:  83.75%, val_best:  83.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0595%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7660%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9631%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 55190  10.440%\n",
      "fc layer 1 self.abs_max_out: 3944.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.586096/ 32.359028, val:  78.75%, val_best:  83.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2904%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9486%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 55868  10.376%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.426017/ 35.418346, val:  80.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0501%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2482%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 56560  10.317%\n",
      "lif layer 2 self.abs_max_v: 3082.5\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.105638/ 47.005241, val:  70.00%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1006%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6569%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0876%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 57211  10.252%\n",
      "lif layer 2 self.abs_max_v: 3133.0\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  3.801574/ 38.822544, val:  83.75%, val_best:  83.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0518%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5560%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 57858  10.189%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.190455/ 29.148615, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8095%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7997%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 58543  10.135%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  3.920393/ 36.198257, val:  81.67%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.99 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4110%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 59149  10.070%\n",
      "fc layer 1 self.abs_max_out: 4066.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.017273/ 36.808983, val:  75.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1157%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9406%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7188%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 59819  10.017%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  3.634208/ 62.293232, val:  66.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.40 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6199%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9325%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 60433   9.956%\n",
      "fc layer 1 self.abs_max_out: 4190.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  3.977625/ 36.646076, val:  83.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5362%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9563%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 61054   9.899%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  3.728710/ 39.570564, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8270%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5873%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 61667   9.842%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.422438/ 37.763798, val:  76.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1034%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7788%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 62251   9.783%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.844541/ 36.855396, val:  78.75%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.26 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0922%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3343%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 62896   9.734%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  3.884095/ 35.147850, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0938%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6205%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2557%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 63509   9.682%\n",
      "fc layer 1 self.abs_max_out: 4221.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.880703/ 36.117634, val:  83.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.48 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6010%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 64125   9.632%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.782054/ 41.881062, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0212%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3366%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4184%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 64736   9.583%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.654963/ 41.237225, val:  75.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4935%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 65305   9.529%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.676641/ 28.617382, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2519%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1343%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 65898   9.480%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.428460/ 39.101006, val:  78.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1065%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4039%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 66497   9.434%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.539998/ 39.383335, val:  75.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0612%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5577%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9212%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 67071   9.385%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.410411/ 30.246017, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0858%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4644%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3880%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 67652   9.338%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.204869/ 40.437313, val:  78.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.91 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0383%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3532%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3402%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 68177   9.285%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.373407/ 32.279636, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.27 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0623%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1838%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 68725   9.237%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.539935/ 35.827309, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2980%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.0646%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 69291   9.192%\n",
      "fc layer 1 self.abs_max_out: 4410.0\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.425692/ 35.675789, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5002%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4362%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 69860   9.149%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.468004/ 31.386295, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.42 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6119%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8612%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 70427   9.106%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.388038/ 47.882267, val:  79.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0477%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9095%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 70944   9.058%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.114688/ 32.232613, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0484%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6539%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8572%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 71465   9.012%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.036265/ 38.496113, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0435%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3593%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6424%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 71978   8.966%\n",
      "lif layer 1 self.abs_max_v: 6803.5\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  2.833698/ 36.487019, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.76 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0445%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5600%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5242%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 72476   8.919%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  2.861919/ 31.004002, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0913%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7395%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 72989   8.876%\n",
      "fc layer 3 self.abs_max_out: 697.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.180283/ 43.982155, val:  77.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0598%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8607%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 73509   8.834%\n",
      "lif layer 1 self.abs_max_v: 6937.5\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.111432/ 37.488831, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0272%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1271%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 74025   8.792%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.177157/ 31.107550, val:  86.67%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.89 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1125%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1485%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5329%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 74533   8.751%\n",
      "fc layer 1 self.abs_max_out: 4425.0\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.024532/ 30.973915, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.80 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1026%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9779%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5534%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 75029   8.709%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.174362/ 32.187950, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1060%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7981%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8531%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 75531   8.669%\n",
      "fc layer 3 self.abs_max_out: 720.0\n",
      "fc layer 3 self.abs_max_out: 734.0\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  2.808358/ 33.488564, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.08 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0884%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8175%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6891%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 76026   8.629%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.050005/ 35.688660, val:  81.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0976%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5905%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8102%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 76512   8.588%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  2.931413/ 35.542881, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.71 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7149%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6712%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 76989   8.548%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  2.772015/ 36.911198, val:  81.67%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.09 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6613%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4106%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 77441   8.506%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  2.736013/ 32.850311, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0717%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0643%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 77915   8.467%\n",
      "lif layer 1 self.abs_max_v: 6946.0\n",
      "lif layer 1 self.abs_max_v: 7237.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.942400/ 36.750168, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1082%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0904%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1805%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 78407   8.430%\n",
      "fc layer 3 self.abs_max_out: 772.0\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.675012/ 36.739559, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1080%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8555%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 78857   8.390%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.869313/ 40.300419, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.11 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0745%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6608%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5665%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 79324   8.353%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.822479/ 32.068871, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1294%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7221%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7192%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 79786   8.316%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.754018/ 29.840395, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0922%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5655%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 80241   8.279%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.693556/ 37.006786, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.69 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0286%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6427%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6030%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 80716   8.245%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.405324/ 35.568188, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1027%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7241%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6763%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 81147   8.207%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.651345/ 37.355148, val:  82.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0508%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3944%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8264%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 81574   8.169%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.976811/ 35.877449, val:  83.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.04 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5587%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3656%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 82051   8.137%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.460627/ 36.467056, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.41 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5693%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 82503   8.103%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.639802/ 32.063072, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.21 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0720%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5846%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.6445%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 82947   8.069%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.539356/ 37.532715, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.54 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0823%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6325%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 83361   8.033%\n",
      "fc layer 1 self.abs_max_out: 4739.0\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.477017/ 33.943264, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0855%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4289%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 83769   7.997%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.483917/ 31.431816, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1808%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8867%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 84175   7.961%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.896424/ 35.278965, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0322%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2993%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2631%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 84620   7.930%\n",
      "fc layer 1 self.abs_max_out: 4773.0\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.588548/ 37.893867, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0882%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3980%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2044%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 85036   7.896%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.578437/ 33.851585, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.59 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0567%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3871%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6562%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 85457   7.864%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.172457/ 28.511051, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.06 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0389%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9816%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 85850   7.830%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.473258/ 35.675900, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.33 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0842%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5024%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 86240   7.796%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.628678/ 32.902653, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.96 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0609%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2472%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1616%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 86696   7.768%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  1.972989/ 41.447704, val:  81.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1029%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2849%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 87036   7.731%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.133111/ 44.744873, val:  76.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0695%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0819%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2475%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 87397   7.696%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.264306/ 38.132992, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.25 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0743%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3274%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5810%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 87792   7.665%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.416497/ 33.427841, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.66 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0621%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3712%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1747%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 88206   7.635%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.078328/ 33.485050, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.53 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0192%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2251%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4153%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 88588   7.604%\n",
      "fc layer 1 self.abs_max_out: 4812.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.206194/ 38.590607, val:  82.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2700%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5187%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 88956   7.572%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.126592/ 34.337643, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1959%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6775%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 89317   7.540%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.263505/ 35.442932, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9900%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9490%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 89694   7.510%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.080823/ 38.302891, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0631%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4067%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 90055   7.479%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.187614/ 38.831833, val:  82.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0621%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5241%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 90417   7.448%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.451531/ 36.121014, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7912%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 90818   7.421%\n",
      "fc layer 1 self.abs_max_out: 4816.0\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.093550/ 36.068314, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5522%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7777%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 91189   7.392%\n",
      "fc layer 1 self.abs_max_out: 4875.0\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.283859/ 37.805717, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5543%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 91563   7.364%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.289234/ 39.628826, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3330%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5105%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 91936   7.337%\n",
      "fc layer 2 self.abs_max_out: 1947.0\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.198319/ 37.443924, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8288%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4283%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 92298   7.308%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.233468/ 39.572823, val:  81.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0613%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7710%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9684%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 92687   7.283%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.171100/ 44.740704, val:  80.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.01 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1044%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5416%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2455%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 93047   7.255%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  1.873926/ 33.730152, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.19 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0264%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4962%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9743%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 93380   7.226%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.212723/ 36.284222, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0607%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4576%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3872%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 93744   7.200%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.019300/ 39.309536, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0562%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5757%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7110%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 94082   7.172%\n",
      "fc layer 2 self.abs_max_out: 1952.0\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.001204/ 33.619709, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.75 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0752%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3852%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0136%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 94437   7.145%\n",
      "fc layer 2 self.abs_max_out: 1967.0\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  1.808148/ 34.335705, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0942%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8957%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 94775   7.118%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  1.877603/ 41.631783, val:  83.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.80 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0963%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4031%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6935%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 95088   7.090%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.208983/ 30.260954, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.70 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0935%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5669%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4982%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 95432   7.064%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  1.941872/ 32.751553, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0774%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5370%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5567%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 95767   7.037%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.030777/ 36.542999, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0361%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5420%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3734%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 96116   7.013%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  1.831406/ 33.934002, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5263%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5237%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 96425   6.985%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.047467/ 32.404785, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0603%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3466%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3199%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 96757   6.960%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  2.236768/ 36.080101, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0482%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2267%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3837%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 97108   6.936%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  1.797531/ 45.358006, val:  80.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0568%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1532%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7673%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 97417   6.910%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  1.772486/ 40.683430, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3913%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0635%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 97734   6.885%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.818779/ 33.908188, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.97 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0519%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3002%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9116%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 98038   6.859%\n",
      "fc layer 1 self.abs_max_out: 4966.0\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.798200/ 38.034756, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.50 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0591%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1368%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 98358   6.835%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  2.262720/ 44.865334, val:  81.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0494%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2253%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6596%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 98724   6.814%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.784662/ 41.535637, val:  78.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0911%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1767%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6925%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 99045   6.790%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  2.226244/ 37.884357, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0630%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4184%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 99400   6.769%\n",
      "fc layer 2 self.abs_max_out: 1974.0\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.630782/ 36.106895, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1040%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2763%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4364%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 99674   6.743%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.703103/ 36.126137, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.74 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0845%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3660%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5401%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 99952   6.717%\n",
      "fc layer 2 self.abs_max_out: 2029.0\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.851595/ 34.025795, val:  89.58%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5366%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 100266   6.694%\n",
      "fc layer 1 self.abs_max_out: 5067.0\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.585974/ 37.968807, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0517%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6015%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7252%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 100553   6.669%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.503108/ 37.375523, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.63 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0752%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4786%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8425%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 100840   6.645%\n",
      "fc layer 1 self.abs_max_out: 5088.0\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.754910/ 34.386639, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0913%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0565%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 101131   6.622%\n",
      "fc layer 2 self.abs_max_out: 2085.0\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.533907/ 34.781044, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.27 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2032%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 101391   6.597%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.538143/ 37.456444, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0753%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4769%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9691%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 101660   6.572%\n",
      "fc layer 2 self.abs_max_out: 2161.0\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.465428/ 38.774551, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.20 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0724%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2682%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8443%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 101924   6.548%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.516304/ 35.372929, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.83 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1868%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 102195   6.524%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.657330/ 36.294159, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.05 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0279%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7735%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 102475   6.501%\n",
      "fc layer 3 self.abs_max_out: 779.0\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.400912/ 37.767998, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.66 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2618%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4236%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 102724   6.477%\n",
      "fc layer 3 self.abs_max_out: 783.0\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.565529/ 34.773376, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.72 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0779%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2563%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 102996   6.454%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.398768/ 40.999546, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.77 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0539%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3394%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2064%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 103265   6.432%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.747617/ 36.443371, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.83 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3099%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3589%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 103550   6.410%\n",
      "lif layer 2 self.abs_max_v: 3141.5\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.619532/ 44.304363, val:  82.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1983%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5801%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 103821   6.388%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.561952/ 34.515495, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.30 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0729%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6417%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 104095   6.367%\n",
      "fc layer 3 self.abs_max_out: 785.0\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.637308/ 37.757317, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1133%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4791%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7286%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 104370   6.346%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.525988/ 43.622898, val:  85.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.38 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7913%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 104628   6.324%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.421562/ 39.002346, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.99 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0341%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4536%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5650%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 104879   6.302%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.656676/ 43.872799, val:  86.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.42 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2272%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 105138   6.280%\n",
      "lif layer 2 self.abs_max_v: 3222.5\n",
      "lif layer 2 self.abs_max_v: 3239.5\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.494527/ 38.958687, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0739%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3180%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.2080%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 105404   6.260%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.450924/ 37.453815, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.42 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4317%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7783%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 105655   6.238%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.528858/ 36.122257, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8165%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 105911   6.217%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.454530/ 41.513233, val:  86.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.86 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0836%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3835%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8115%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 106172   6.197%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.615000/ 37.470562, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3817%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0401%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 106434   6.177%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.293650/ 44.091587, val:  85.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4939%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.4465%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 106674   6.156%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.178495/ 40.919434, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.43 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0861%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2989%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9943%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 106906   6.135%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.486606/ 35.086735, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0239%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0356%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 107148   6.114%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.391702/ 39.966091, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.78 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0574%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1681%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2478%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 107377   6.093%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.288578/ 39.890015, val:  86.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2651%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.3720%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 107586   6.071%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.268558/ 35.276417, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9494%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6244%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 107822   6.051%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.324610/ 37.817829, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.13 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0150%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4954%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 108052   6.031%\n",
      "lif layer 1 self.abs_max_v: 7276.5\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.195241/ 36.526615, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.46 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1065%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2506%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6272%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 108263   6.010%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.392145/ 39.735310, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.45 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1257%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3935%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 108508   5.991%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.308400/ 38.151417, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0918%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3351%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9998%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 108752   5.972%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.658047/ 39.851757, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1179%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1727%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9114%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 109003   5.954%\n",
      "fc layer 2 self.abs_max_out: 2223.0\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.548151/ 40.727413, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0631%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0355%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0337%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 109265   5.937%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.409675/ 40.474869, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.10 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0573%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0906%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0402%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 109493   5.918%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.374387/ 43.038689, val:  85.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0630%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0835%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1108%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 109734   5.899%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.352477/ 38.945297, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.56 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1289%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2080%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 109958   5.880%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.393699/ 39.981804, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2707%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 110178   5.862%\n",
      "fc layer 3 self.abs_max_out: 796.0\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.019274/ 40.573906, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0911%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1421%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7499%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 110360   5.841%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.233151/ 38.850079, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1055%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0085%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1005%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 110588   5.823%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.238134/ 37.798042, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.87 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1014%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9414%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1307%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 110819   5.805%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.236426/ 38.280781, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.36 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9685%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.3490%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 111025   5.786%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.301580/ 35.715256, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0351%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0567%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.3777%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 111280   5.770%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.358843/ 39.773277, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.46 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9906%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1679%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 111514   5.753%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.395447/ 36.039387, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1270%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.5323%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 111756   5.736%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.171656/ 37.536697, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.66 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0570%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0991%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6406%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9a6a2ffd914d41b04188859c7055cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñà‚ñÜ‚ñà‚ñÜ‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñá‚ñÖ‚ñÜ‚ñÇ‚ñÑ‚ñá‚ñá‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.17166</td></tr><tr><td>val_acc_best</td><td>0.92083</td></tr><tr><td>val_acc_now</td><td>0.875</td></tr><tr><td>val_loss</td><td>37.5367</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">woven-sweep-37</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nb7wkky2' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/nb7wkky2</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251213_155613-nb7wkky2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 54a6ge5z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 11207\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251213_201521-54a6ge5z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/54a6ge5z' target=\"_blank\">crisp-sweep-44</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/54a6ge5z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/54a6ge5z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251213_201530_886', 'my_seed': 11207, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 329.0\n",
      "lif layer 1 self.abs_max_v: 329.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 369.0\n",
      "lif layer 2 self.abs_max_v: 369.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 120.0\n",
      "fc layer 1 self.abs_max_out: 354.0\n",
      "lif layer 1 self.abs_max_v: 389.5\n",
      "fc layer 2 self.abs_max_out: 435.0\n",
      "lif layer 2 self.abs_max_v: 435.0\n",
      "fc layer 3 self.abs_max_out: 127.0\n",
      "lif layer 1 self.abs_max_v: 406.0\n",
      "fc layer 3 self.abs_max_out: 133.0\n",
      "lif layer 1 self.abs_max_v: 442.5\n",
      "lif layer 2 self.abs_max_v: 477.0\n",
      "fc layer 3 self.abs_max_out: 138.0\n",
      "lif layer 1 self.abs_max_v: 467.5\n",
      "lif layer 2 self.abs_max_v: 534.0\n",
      "fc layer 1 self.abs_max_out: 406.0\n",
      "lif layer 1 self.abs_max_v: 496.5\n",
      "lif layer 2 self.abs_max_v: 546.5\n",
      "fc layer 3 self.abs_max_out: 150.0\n",
      "fc layer 3 self.abs_max_out: 158.0\n",
      "lif layer 2 self.abs_max_v: 577.5\n",
      "fc layer 3 self.abs_max_out: 166.0\n",
      "lif layer 2 self.abs_max_v: 584.0\n",
      "lif layer 2 self.abs_max_v: 697.5\n",
      "lif layer 2 self.abs_max_v: 714.0\n",
      "fc layer 3 self.abs_max_out: 181.0\n",
      "fc layer 1 self.abs_max_out: 428.0\n",
      "fc layer 2 self.abs_max_out: 486.0\n",
      "fc layer 1 self.abs_max_out: 474.0\n",
      "lif layer 1 self.abs_max_v: 531.5\n",
      "lif layer 1 self.abs_max_v: 600.0\n",
      "fc layer 1 self.abs_max_out: 535.0\n",
      "fc layer 2 self.abs_max_out: 496.0\n",
      "fc layer 2 self.abs_max_out: 500.0\n",
      "fc layer 1 self.abs_max_out: 539.0\n",
      "lif layer 1 self.abs_max_v: 683.5\n",
      "lif layer 1 self.abs_max_v: 707.5\n",
      "lif layer 2 self.abs_max_v: 823.5\n",
      "fc layer 1 self.abs_max_out: 540.0\n",
      "fc layer 1 self.abs_max_out: 681.0\n",
      "fc layer 3 self.abs_max_out: 195.0\n",
      "fc layer 1 self.abs_max_out: 849.0\n",
      "lif layer 1 self.abs_max_v: 849.0\n",
      "fc layer 3 self.abs_max_out: 232.0\n",
      "fc layer 2 self.abs_max_out: 599.0\n",
      "fc layer 2 self.abs_max_out: 605.0\n",
      "fc layer 1 self.abs_max_out: 854.0\n",
      "lif layer 1 self.abs_max_v: 854.0\n",
      "fc layer 3 self.abs_max_out: 235.0\n",
      "fc layer 1 self.abs_max_out: 944.0\n",
      "lif layer 1 self.abs_max_v: 944.0\n",
      "fc layer 2 self.abs_max_out: 651.0\n",
      "lif layer 2 self.abs_max_v: 861.5\n",
      "fc layer 3 self.abs_max_out: 237.0\n",
      "fc layer 3 self.abs_max_out: 242.0\n",
      "lif layer 2 self.abs_max_v: 878.5\n",
      "fc layer 2 self.abs_max_out: 716.0\n",
      "lif layer 2 self.abs_max_v: 1111.0\n",
      "lif layer 1 self.abs_max_v: 1053.5\n",
      "lif layer 1 self.abs_max_v: 1072.0\n",
      "lif layer 1 self.abs_max_v: 1111.5\n",
      "fc layer 1 self.abs_max_out: 960.0\n",
      "lif layer 1 self.abs_max_v: 1220.5\n",
      "lif layer 1 self.abs_max_v: 1244.5\n",
      "lif layer 1 self.abs_max_v: 1329.5\n",
      "fc layer 3 self.abs_max_out: 247.0\n",
      "fc layer 3 self.abs_max_out: 251.0\n",
      "fc layer 2 self.abs_max_out: 799.0\n",
      "fc layer 1 self.abs_max_out: 1017.0\n",
      "fc layer 1 self.abs_max_out: 1200.0\n",
      "fc layer 2 self.abs_max_out: 803.0\n",
      "lif layer 1 self.abs_max_v: 1371.0\n",
      "fc layer 2 self.abs_max_out: 814.0\n",
      "fc layer 3 self.abs_max_out: 289.0\n",
      "lif layer 2 self.abs_max_v: 1169.5\n",
      "fc layer 2 self.abs_max_out: 830.0\n",
      "fc layer 2 self.abs_max_out: 855.0\n",
      "fc layer 2 self.abs_max_out: 929.0\n",
      "fc layer 1 self.abs_max_out: 1308.0\n",
      "fc layer 1 self.abs_max_out: 1338.0\n",
      "lif layer 2 self.abs_max_v: 1208.0\n",
      "lif layer 2 self.abs_max_v: 1285.5\n",
      "lif layer 2 self.abs_max_v: 1306.5\n",
      "lif layer 2 self.abs_max_v: 1321.5\n",
      "fc layer 2 self.abs_max_out: 956.0\n",
      "fc layer 2 self.abs_max_out: 998.0\n",
      "lif layer 1 self.abs_max_v: 1417.0\n",
      "lif layer 1 self.abs_max_v: 1557.5\n",
      "lif layer 2 self.abs_max_v: 1388.0\n",
      "lif layer 2 self.abs_max_v: 1472.0\n",
      "lif layer 2 self.abs_max_v: 1485.0\n",
      "fc layer 1 self.abs_max_out: 1630.0\n",
      "lif layer 1 self.abs_max_v: 1630.0\n",
      "fc layer 2 self.abs_max_out: 1006.0\n",
      "fc layer 2 self.abs_max_out: 1016.0\n",
      "fc layer 3 self.abs_max_out: 346.0\n",
      "fc layer 2 self.abs_max_out: 1039.0\n",
      "fc layer 2 self.abs_max_out: 1047.0\n",
      "fc layer 2 self.abs_max_out: 1113.0\n",
      "fc layer 2 self.abs_max_out: 1136.0\n",
      "lif layer 2 self.abs_max_v: 1542.5\n",
      "lif layer 2 self.abs_max_v: 1583.5\n",
      "fc layer 2 self.abs_max_out: 1172.0\n",
      "fc layer 2 self.abs_max_out: 1222.0\n",
      "lif layer 1 self.abs_max_v: 1657.0\n",
      "fc layer 3 self.abs_max_out: 358.0\n",
      "fc layer 3 self.abs_max_out: 382.0\n",
      "lif layer 2 self.abs_max_v: 1624.5\n",
      "lif layer 1 self.abs_max_v: 1666.0\n",
      "lif layer 1 self.abs_max_v: 1669.5\n",
      "fc layer 1 self.abs_max_out: 1657.0\n",
      "lif layer 1 self.abs_max_v: 1776.5\n",
      "lif layer 1 self.abs_max_v: 1961.0\n",
      "lif layer 2 self.abs_max_v: 1782.0\n",
      "lif layer 1 self.abs_max_v: 1976.5\n",
      "lif layer 2 self.abs_max_v: 1852.0\n",
      "fc layer 2 self.abs_max_out: 1234.0\n",
      "fc layer 1 self.abs_max_out: 1670.0\n",
      "fc layer 3 self.abs_max_out: 386.0\n",
      "fc layer 3 self.abs_max_out: 401.0\n",
      "fc layer 2 self.abs_max_out: 1238.0\n",
      "fc layer 2 self.abs_max_out: 1306.0\n",
      "fc layer 2 self.abs_max_out: 1314.0\n",
      "lif layer 1 self.abs_max_v: 1985.5\n",
      "lif layer 2 self.abs_max_v: 1884.0\n",
      "lif layer 1 self.abs_max_v: 2081.5\n",
      "lif layer 1 self.abs_max_v: 2317.0\n",
      "fc layer 1 self.abs_max_out: 1724.0\n",
      "fc layer 1 self.abs_max_out: 1750.0\n",
      "fc layer 1 self.abs_max_out: 1762.0\n",
      "fc layer 1 self.abs_max_out: 1808.0\n",
      "fc layer 1 self.abs_max_out: 1849.0\n",
      "lif layer 2 self.abs_max_v: 1920.5\n",
      "lif layer 2 self.abs_max_v: 1971.0\n",
      "lif layer 2 self.abs_max_v: 2032.5\n",
      "lif layer 2 self.abs_max_v: 2095.5\n",
      "fc layer 1 self.abs_max_out: 1856.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 13.763844/ 67.033737, val:  40.83%, val_best:  40.83%, tr:  97.24%, tr_best:  97.24%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0893%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6467%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2092%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2066  21.103%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 407.0\n",
      "lif layer 1 self.abs_max_v: 2363.5\n",
      "fc layer 1 self.abs_max_out: 1880.0\n",
      "fc layer 1 self.abs_max_out: 1973.0\n",
      "fc layer 2 self.abs_max_out: 1355.0\n",
      "fc layer 3 self.abs_max_out: 423.0\n",
      "fc layer 2 self.abs_max_out: 1369.0\n",
      "fc layer 2 self.abs_max_out: 1377.0\n",
      "lif layer 2 self.abs_max_v: 2105.5\n",
      "fc layer 2 self.abs_max_out: 1465.0\n",
      "fc layer 2 self.abs_max_out: 1477.0\n",
      "lif layer 2 self.abs_max_v: 2127.5\n",
      "lif layer 2 self.abs_max_v: 2194.0\n",
      "lif layer 2 self.abs_max_v: 2221.0\n",
      "lif layer 2 self.abs_max_v: 2242.5\n",
      "lif layer 2 self.abs_max_v: 2292.5\n",
      "lif layer 2 self.abs_max_v: 2451.5\n",
      "lif layer 1 self.abs_max_v: 2409.5\n",
      "lif layer 1 self.abs_max_v: 2415.0\n",
      "lif layer 1 self.abs_max_v: 2531.0\n",
      "lif layer 1 self.abs_max_v: 2875.5\n",
      "lif layer 1 self.abs_max_v: 3168.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.134200/ 71.888458, val:  37.92%, val_best:  40.83%, tr:  99.39%, tr_best:  99.39%, epoch time: 78.10 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1154%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8290%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6302%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3586  18.315%\n",
      "fc layer 3 self.abs_max_out: 428.0\n",
      "fc layer 3 self.abs_max_out: 432.0\n",
      "fc layer 3 self.abs_max_out: 463.0\n",
      "fc layer 1 self.abs_max_out: 1975.0\n",
      "lif layer 2 self.abs_max_v: 2486.5\n",
      "fc layer 3 self.abs_max_out: 496.0\n",
      "fc layer 1 self.abs_max_out: 2029.0\n",
      "lif layer 1 self.abs_max_v: 3296.5\n",
      "fc layer 1 self.abs_max_out: 2099.0\n",
      "lif layer 2 self.abs_max_v: 2498.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.544219/ 57.089245, val:  51.25%, val_best:  51.25%, tr:  99.80%, tr_best:  99.80%, epoch time: 77.14 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0868%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0738%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9930%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5014  17.072%\n",
      "fc layer 1 self.abs_max_out: 2218.0\n",
      "lif layer 2 self.abs_max_v: 2609.5\n",
      "lif layer 1 self.abs_max_v: 3420.5\n",
      "lif layer 1 self.abs_max_v: 3633.5\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.460767/ 42.566544, val:  57.92%, val_best:  57.92%, tr:  99.39%, tr_best:  99.80%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1174%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0412%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6422  16.399%\n",
      "fc layer 2 self.abs_max_out: 1498.0\n",
      "lif layer 1 self.abs_max_v: 3772.5\n",
      "fc layer 1 self.abs_max_out: 2251.0\n",
      "lif layer 1 self.abs_max_v: 3871.5\n",
      "lif layer 1 self.abs_max_v: 3908.0\n",
      "fc layer 1 self.abs_max_out: 2302.0\n",
      "lif layer 1 self.abs_max_v: 4121.0\n",
      "fc layer 1 self.abs_max_out: 2365.0\n",
      "lif layer 1 self.abs_max_v: 4153.5\n",
      "lif layer 1 self.abs_max_v: 4189.0\n",
      "lif layer 1 self.abs_max_v: 4239.5\n",
      "fc layer 1 self.abs_max_out: 2366.0\n",
      "lif layer 1 self.abs_max_v: 4486.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  8.651879/ 70.896904, val:  38.33%, val_best:  57.92%, tr:  99.49%, tr_best:  99.80%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2361%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8891%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7758  15.849%\n",
      "fc layer 1 self.abs_max_out: 2397.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  8.220065/ 60.707466, val:  39.17%, val_best:  57.92%, tr:  99.69%, tr_best:  99.80%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9103%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7708%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9047  15.402%\n",
      "fc layer 2 self.abs_max_out: 1502.0\n",
      "fc layer 2 self.abs_max_out: 1538.0\n",
      "fc layer 1 self.abs_max_out: 2581.0\n",
      "lif layer 1 self.abs_max_v: 4556.5\n",
      "lif layer 1 self.abs_max_v: 4570.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  8.259202/ 47.012032, val:  51.25%, val_best:  57.92%, tr:  99.80%, tr_best:  99.80%, epoch time: 76.50 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0573%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6525%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1668%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10332  15.077%\n",
      "fc layer 3 self.abs_max_out: 513.0\n",
      "fc layer 3 self.abs_max_out: 523.0\n",
      "fc layer 2 self.abs_max_out: 1551.0\n",
      "fc layer 2 self.abs_max_out: 1567.0\n",
      "fc layer 3 self.abs_max_out: 553.0\n",
      "fc layer 1 self.abs_max_out: 2599.0\n",
      "lif layer 1 self.abs_max_v: 4709.0\n",
      "lif layer 1 self.abs_max_v: 4738.5\n",
      "fc layer 2 self.abs_max_out: 1675.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  7.846676/ 58.769775, val:  50.00%, val_best:  57.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0674%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2774%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0699%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11551  14.748%\n",
      "fc layer 3 self.abs_max_out: 565.0\n",
      "fc layer 1 self.abs_max_out: 2804.0\n",
      "lif layer 1 self.abs_max_v: 4747.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.132525/ 48.003761, val:  51.25%, val_best:  57.92%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0732%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9254%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 12786  14.511%\n",
      "lif layer 2 self.abs_max_v: 2708.0\n",
      "fc layer 1 self.abs_max_out: 2890.0\n",
      "lif layer 1 self.abs_max_v: 5050.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  7.870992/ 58.259186, val:  50.83%, val_best:  57.92%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0915%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9005%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5076%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14014  14.315%\n",
      "fc layer 2 self.abs_max_out: 1730.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  7.496764/ 59.472122, val:  54.17%, val_best:  57.92%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.29 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0485%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5119%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15221  14.134%\n",
      "fc layer 1 self.abs_max_out: 2893.0\n",
      "fc layer 1 self.abs_max_out: 2897.0\n",
      "lif layer 1 self.abs_max_v: 5089.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.578973/ 39.988853, val:  62.50%, val_best:  62.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.71 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0548%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0218%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4934%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16450  14.002%\n",
      "fc layer 3 self.abs_max_out: 581.0\n",
      "lif layer 2 self.abs_max_v: 2799.5\n",
      "fc layer 1 self.abs_max_out: 3013.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  6.893201/ 29.877689, val:  63.75%, val_best:  63.75%, tr:  99.90%, tr_best:  99.90%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1048%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0116%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17562  13.799%\n",
      "fc layer 1 self.abs_max_out: 3127.0\n",
      "lif layer 1 self.abs_max_v: 5141.5\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.364168/ 48.083187, val:  59.58%, val_best:  63.75%, tr:  99.69%, tr_best:  99.90%, epoch time: 77.17 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0576%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8292%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18710  13.651%\n",
      "fc layer 2 self.abs_max_out: 1791.0\n",
      "fc layer 2 self.abs_max_out: 1857.0\n",
      "fc layer 1 self.abs_max_out: 3214.0\n",
      "lif layer 1 self.abs_max_v: 5260.0\n",
      "lif layer 1 self.abs_max_v: 5282.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.151315/ 60.220783, val:  52.08%, val_best:  63.75%, tr:  99.80%, tr_best:  99.90%, epoch time: 76.95 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0382%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5513%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 19861  13.525%\n",
      "fc layer 1 self.abs_max_out: 3301.0\n",
      "lif layer 1 self.abs_max_v: 5465.0\n",
      "lif layer 1 self.abs_max_v: 5490.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.050982/ 46.385456, val:  64.17%, val_best:  64.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 77.18 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1057%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21010  13.413%\n",
      "fc layer 1 self.abs_max_out: 3370.0\n",
      "lif layer 1 self.abs_max_v: 5586.5\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  6.801212/ 45.393688, val:  54.58%, val_best:  64.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6226%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1635%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22088  13.272%\n",
      "fc layer 3 self.abs_max_out: 589.0\n",
      "lif layer 2 self.abs_max_v: 2818.5\n",
      "fc layer 1 self.abs_max_out: 3414.0\n",
      "lif layer 1 self.abs_max_v: 5695.5\n",
      "lif layer 1 self.abs_max_v: 5794.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.067099/ 63.627171, val:  43.33%, val_best:  64.17%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.90 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5389%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23196  13.163%\n",
      "lif layer 2 self.abs_max_v: 2849.5\n",
      "lif layer 2 self.abs_max_v: 2865.5\n",
      "lif layer 2 self.abs_max_v: 2889.0\n",
      "lif layer 2 self.abs_max_v: 3162.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  6.657893/ 32.494572, val:  74.58%, val_best:  74.58%, tr:  99.80%, tr_best:  99.90%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0763%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5788%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24249  13.036%\n",
      "fc layer 1 self.abs_max_out: 3470.0\n",
      "lif layer 1 self.abs_max_v: 5848.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  6.344729/ 31.817366, val:  67.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0812%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6816%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25265  12.903%\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  6.744039/ 49.701019, val:  60.42%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0532%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.0591%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5327%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26306  12.795%\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  7.140111/ 57.660404, val:  55.00%, val_best:  74.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 77.62 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2901%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7274%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27376  12.711%\n",
      "fc layer 1 self.abs_max_out: 3490.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.238367/ 35.921719, val:  76.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0894%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28357  12.594%\n",
      "fc layer 3 self.abs_max_out: 593.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.305363/ 69.709053, val:  51.25%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.70 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0960%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2422%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1383%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29361  12.496%\n",
      "lif layer 2 self.abs_max_v: 3163.5\n",
      "fc layer 2 self.abs_max_out: 1859.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.113962/ 33.516586, val:  75.83%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0854%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1377%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3534%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30338  12.396%\n",
      "fc layer 2 self.abs_max_out: 1869.0\n",
      "fc layer 2 self.abs_max_out: 1930.0\n",
      "fc layer 2 self.abs_max_out: 1933.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.226693/ 29.865503, val:  72.50%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.82 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0534%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.9984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1075%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31283  12.290%\n",
      "lif layer 2 self.abs_max_v: 3168.0\n",
      "fc layer 2 self.abs_max_out: 1945.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  5.887363/ 37.883400, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7604%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32229  12.193%\n",
      "lif layer 2 self.abs_max_v: 3197.5\n",
      "lif layer 2 self.abs_max_v: 3263.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.131033/ 40.179996, val:  72.08%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 78.17 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0861%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7317%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0045%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33213  12.116%\n",
      "lif layer 2 self.abs_max_v: 3299.5\n",
      "fc layer 2 self.abs_max_out: 1948.0\n",
      "fc layer 2 self.abs_max_out: 2018.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  5.689664/ 44.693707, val:  67.08%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.33 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0592%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.7077%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0627%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 34148  12.028%\n",
      "fc layer 1 self.abs_max_out: 3729.0\n",
      "lif layer 1 self.abs_max_v: 6224.0\n",
      "lif layer 1 self.abs_max_v: 6304.5\n",
      "lif layer 1 self.abs_max_v: 6532.5\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  5.699908/ 39.120338, val:  71.25%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1065%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8748%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35058  11.937%\n",
      "fc layer 3 self.abs_max_out: 597.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  5.509046/ 40.923336, val:  75.42%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.81 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1085%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4340%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 35932  11.840%\n",
      "lif layer 2 self.abs_max_v: 3304.5\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  5.426914/ 46.416332, val:  67.08%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0783%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3008%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3233%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 36806  11.749%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.690042/ 33.838493, val:  75.83%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.1897%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5188%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 37679  11.663%\n",
      "lif layer 2 self.abs_max_v: 3386.5\n",
      "fc layer 2 self.abs_max_out: 2020.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.357427/ 58.565498, val:  57.92%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.08 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0492%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.2062%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2822%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 38579  11.590%\n",
      "lif layer 2 self.abs_max_v: 3405.5\n",
      "lif layer 2 self.abs_max_v: 3414.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.468923/ 54.576477, val:  67.50%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.3745%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4205%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 39439  11.510%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.272952/ 39.187702, val:  69.58%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.03 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.6240%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1433%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 40270  11.426%\n",
      "lif layer 2 self.abs_max_v: 3564.0\n",
      "lif layer 2 self.abs_max_v: 3578.0\n",
      "lif layer 2 self.abs_max_v: 3664.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  4.933086/ 50.999409, val:  64.58%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.28 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0880%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.4649%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.7664%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 41087  11.343%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.358079/ 34.880939, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8249%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2440%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 41925  11.270%\n",
      "fc layer 3 self.abs_max_out: 604.0\n",
      "fc layer 3 self.abs_max_out: 613.0\n",
      "fc layer 3 self.abs_max_out: 615.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  4.853900/ 39.112492, val:  78.33%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.06 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0851%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9541%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9053%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 42717  11.188%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  4.861222/ 39.840347, val:  79.58%, val_best:  79.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.43 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9848%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1542%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 43490  11.106%\n",
      "fc layer 1 self.abs_max_out: 3769.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  4.851477/ 29.947798, val:  81.25%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.68 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0554%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 44264  11.028%\n",
      "fc layer 1 self.abs_max_out: 3792.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  4.998754/ 39.629379, val:  68.75%, val_best:  81.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.9999%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 45052  10.957%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  4.606872/ 32.090805, val:  77.08%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.04 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0654%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0220%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5127%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 45796  10.879%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  4.968271/ 30.331568, val:  81.25%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.73 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1171%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 69.0937%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.5900%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 46590  10.816%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.518635/ 33.944824, val:  78.75%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.98 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0636%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.8073%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.8589%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 47312  10.739%\n",
      "fc layer 3 self.abs_max_out: 631.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.541939/ 51.868015, val:  68.33%, val_best:  81.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0850%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.3500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1764%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 48052  10.670%\n",
      "fc layer 2 self.abs_max_out: 2024.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.749178/ 38.174831, val:  69.58%, val_best:  81.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.58 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4743%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6288%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 48807  10.607%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.279543/ 26.264685, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0549%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4660%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3153%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 49492  10.532%\n",
      "fc layer 3 self.abs_max_out: 644.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.435336/ 32.408726, val:  81.67%, val_best:  85.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0765%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.7470%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 50233  10.472%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.284568/ 36.227623, val:  78.75%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.75 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0476%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4101%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 50952  10.409%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.038933/ 37.100128, val:  79.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.93 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8797%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5114%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 51633  10.341%\n",
      "fc layer 2 self.abs_max_out: 2032.0\n",
      "fc layer 2 self.abs_max_out: 2053.0\n",
      "fc layer 2 self.abs_max_out: 2087.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.281409/ 33.038952, val:  81.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.35 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.1671%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.9333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 52329  10.279%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.078910/ 29.817781, val:  84.58%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.85 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0610%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.4791%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.4971%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 53025  10.219%\n",
      "fc layer 2 self.abs_max_out: 2142.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.322256/ 32.800827, val:  82.08%, val_best:  85.00%, tr:  99.80%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 68.2536%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7954%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 53717  10.161%\n",
      "fc layer 1 self.abs_max_out: 3933.0\n",
      "lif layer 1 self.abs_max_v: 6873.0\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  3.899464/ 36.075283, val:  80.42%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.19 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.9441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1551%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 54378  10.099%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.282633/ 26.051432, val:  85.00%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0626%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6503%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7745%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 55070  10.045%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  3.606075/ 31.394489, val:  84.17%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0921%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6932%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.1306%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 55699   9.981%\n",
      "fc layer 2 self.abs_max_out: 2183.0\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.020246/ 25.700779, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7961%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7423%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 56346   9.923%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.156695/ 35.514782, val:  75.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7337%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.3852%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 57037   9.875%\n",
      "fc layer 3 self.abs_max_out: 648.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  3.945376/ 33.874950, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.05 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6212%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.2657%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 57687   9.821%\n",
      "fc layer 3 self.abs_max_out: 653.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  3.590693/ 32.648132, val:  83.75%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.94 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1127%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 58277   9.759%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  3.513306/ 44.612041, val:  75.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.54 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2838%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8564%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 58883   9.701%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  3.345972/ 32.002422, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3956%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9213%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 59470   9.642%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  3.737531/ 29.592113, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0818%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3150%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.8355%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 60062   9.586%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.272267/ 30.707497, val:  84.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.12 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7842%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.5707%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 60635   9.529%\n",
      "fc layer 3 self.abs_max_out: 661.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.447946/ 39.967487, val:  78.33%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.98 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0569%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5180%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.7084%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 61212   9.473%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  3.462146/ 31.726431, val:  83.33%, val_best:  85.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.26 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0320%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3088%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3113%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 61776   9.418%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.306098/ 28.807413, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.56 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0737%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7923%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6025%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 62320   9.361%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.453908/ 35.451237, val:  79.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.27 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0499%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3431%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 62916   9.314%\n",
      "fc layer 1 self.abs_max_out: 3948.0\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.324990/ 32.996315, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.10 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9433%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 63477   9.263%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.157566/ 31.637884, val:  82.50%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.77 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0694%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3740%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5808%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 63998   9.207%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.181400/ 32.137466, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.68 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0761%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4496%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8375%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 64533   9.155%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.012707/ 31.178774, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.25 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0439%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7274%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6909%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 65014   9.097%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.173490/ 27.172131, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.91 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6853%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 65533   9.046%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.261677/ 36.770168, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.63 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2293%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5796%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 66109   9.004%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.016460/ 33.068161, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.95 seconds, 1.37 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3353%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8982%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 66633   8.956%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.247102/ 40.429333, val:  78.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.81 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.1022%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0191%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 67166   8.910%\n",
      "lif layer 2 self.abs_max_v: 3743.0\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  2.874402/ 33.423954, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.77 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.1116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6793%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5780%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 67678   8.863%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.101445/ 27.983438, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.82 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0642%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4814%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3694%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 68224   8.821%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.263444/ 29.649242, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.88 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0429%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2791%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.6129%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 68771   8.781%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  2.966160/ 26.857460, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.52 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0167%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.3309%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 69302   8.739%\n",
      "fc layer 2 self.abs_max_out: 2188.0\n",
      "fc layer 2 self.abs_max_out: 2197.0\n",
      "lif layer 2 self.abs_max_v: 3774.0\n",
      "fc layer 3 self.abs_max_out: 677.0\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  2.842638/ 33.639778, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.28 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5447%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4041%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 69787   8.693%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.202084/ 29.357618, val:  84.58%, val_best:  90.00%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.79 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0280%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5806%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9790%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 70342   8.657%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  2.844188/ 31.139692, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3688%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1901%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 70843   8.615%\n",
      "lif layer 2 self.abs_max_v: 3836.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.115859/ 27.247147, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.18 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4691%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2528%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 71363   8.576%\n",
      "fc layer 3 self.abs_max_out: 682.0\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.081150/ 34.843639, val:  83.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.44 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0936%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4048%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 71874   8.537%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  2.884872/ 38.358311, val:  79.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.75 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4616%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.4164%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 72376   8.498%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  2.755217/ 43.270042, val:  71.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.87 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0875%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.7966%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2762%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 72849   8.456%\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  2.848068/ 26.321106, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.39 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5543%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1128%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 73331   8.416%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  2.545450/ 28.651985, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.98 seconds, 1.32 minutes\n",
      "layer   1  Sparsity: 91.0211%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4607%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1299%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 73815   8.378%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  2.601669/ 37.081432, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.83 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0731%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5239%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7824%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 74291   8.339%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  2.626634/ 29.581551, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.86 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.8420%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8635%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 74721   8.296%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.074598/ 30.375174, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.98 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0860%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4843%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8418%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 75228   8.263%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  2.650443/ 42.066505, val:  78.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.21 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0945%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.6734%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.4963%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 75684   8.224%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.624694/ 38.988720, val:  81.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.02 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4642%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.7782%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 76137   8.186%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.724705/ 27.446846, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.06 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0723%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3900%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2550%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 76622   8.153%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.803971/ 32.017345, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0629%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2909%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 77088   8.118%\n",
      "lif layer 2 self.abs_max_v: 3859.0\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.819706/ 30.219128, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0669%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 77561   8.084%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.607697/ 31.250954, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.75 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1000%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0888%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9295%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 77992   8.047%\n",
      "fc layer 1 self.abs_max_out: 3965.0\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.205721/ 30.575026, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.69 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0511%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2358%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2466%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 78409   8.009%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.582063/ 28.184034, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.54 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0924%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2987%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.5967%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 78864   7.976%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.486336/ 30.037485, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.20 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1029%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1543%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6921%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 79309   7.942%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.280896/ 33.133015, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.71 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2613%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.3349%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 79733   7.907%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.199822/ 28.990459, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.39 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.4114%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 80128   7.870%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.211862/ 30.064566, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0588%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2336%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0739%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 80527   7.834%\n",
      "lif layer 2 self.abs_max_v: 3899.0\n",
      "fc layer 1 self.abs_max_out: 4003.0\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  1.959219/ 30.677158, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3224%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.5202%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 80895   7.795%\n",
      "fc layer 2 self.abs_max_out: 2243.0\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.054963/ 29.325941, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4170%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 81286   7.760%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.116442/ 33.256058, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1998%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.3811%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 81672   7.724%\n",
      "fc layer 1 self.abs_max_out: 4125.0\n",
      "fc layer 1 self.abs_max_out: 4161.0\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.200571/ 34.559952, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.49 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0515%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2151%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2995%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 82072   7.691%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.415649/ 38.457066, val:  80.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.00 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0723%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1180%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0016%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 82478   7.659%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.450511/ 28.989162, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.92 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1841%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 82878   7.627%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.055986/ 37.971230, val:  80.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0603%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0388%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 83266   7.594%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.229986/ 38.125256, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.16 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1235%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3579%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.3171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 83673   7.564%\n",
      "fc layer 3 self.abs_max_out: 702.0\n",
      "fc layer 3 self.abs_max_out: 703.0\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.285940/ 31.898148, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0499%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4202%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2744%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 84079   7.534%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.078148/ 32.051208, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.0226%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 84454   7.501%\n",
      "fc layer 3 self.abs_max_out: 704.0\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.249997/ 36.047318, val:  82.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0683%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.7199%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 84845   7.471%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.330128/ 29.811852, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.07 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0858%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0595%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1689%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 85259   7.443%\n",
      "fc layer 3 self.abs_max_out: 728.0\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.222323/ 32.086288, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0626%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1830%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6572%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 85654   7.415%\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.209578/ 51.985397, val:  81.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.00 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0348%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1221%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.7406%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 86019   7.384%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  1.996121/ 32.540249, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4817%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6307%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 86381   7.353%\n",
      "fc layer 1 self.abs_max_out: 4173.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.009003/ 30.696571, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.77 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4321%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.7857%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 86728   7.321%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  1.593781/ 35.008465, val:  82.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.72 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0561%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3919%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 87044   7.288%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.058752/ 33.000557, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.79 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0829%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2332%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6052%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 87409   7.259%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  1.677336/ 36.247566, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0614%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0806%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.3496%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 87725   7.226%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.029821/ 37.965748, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0555%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.9500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6152%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 88083   7.198%\n",
      "fc layer 1 self.abs_max_out: 4337.0\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.138068/ 31.640940, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3556%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.9412%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 88456   7.171%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  1.822709/ 33.413380, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.73 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0256%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1773%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.0663%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 88780   7.141%\n",
      "fc layer 2 self.abs_max_out: 2265.0\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  1.754280/ 34.564682, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1886%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.1290%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 89099   7.110%\n",
      "fc layer 1 self.abs_max_out: 4347.0\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  1.642511/ 38.778061, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.96 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0543%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.9601%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.0318%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 89400   7.079%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.089889/ 41.716171, val:  80.83%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0323%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0071%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.5439%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 89768   7.053%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  1.495247/ 39.458427, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0761%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3411%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.9744%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 90047   7.021%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  1.844777/ 36.109550, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.76 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3225%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3732%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 90362   6.992%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  1.887003/ 32.874756, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0588%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.7426%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4112%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 90677   6.964%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  1.824569/ 30.668800, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.47 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.9007%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.2811%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 91022   6.938%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  1.851689/ 34.337360, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.84 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0616%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.8932%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3582%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 91351   6.912%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  1.885285/ 34.415405, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.63 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0874%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1044%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4404%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 91695   6.887%\n",
      "lif layer 1 self.abs_max_v: 7069.5\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  1.965205/ 31.273092, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.23 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0585%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1817%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5761%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 92050   6.863%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  1.743272/ 34.081238, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.92 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1158%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 92382   6.838%\n",
      "fc layer 3 self.abs_max_out: 736.0\n",
      "fc layer 3 self.abs_max_out: 747.0\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  1.783277/ 33.095417, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.52 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0838%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3720%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 92689   6.811%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  1.743371/ 33.634598, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.48 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1258%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9225%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 93005   6.786%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  1.678083/ 34.777580, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2007%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.8432%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 93308   6.760%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  1.612370/ 30.022673, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0425%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.2629%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 93616   6.734%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  1.637837/ 29.356167, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.41 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0392%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1877%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.2214%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 93886   6.706%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  1.735529/ 34.657829, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.94 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.9185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.1764%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 94181   6.681%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  1.805627/ 30.441694, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0469%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5042%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 94512   6.658%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.590794/ 35.428596, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.87 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.1099%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1911%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 94791   6.632%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.684121/ 29.551643, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.68 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0884%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.9201%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 95098   6.608%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.671113/ 32.837368, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.23 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0638%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4430%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 95427   6.586%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.320836/ 31.879919, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6783%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 95711   6.561%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.616525/ 28.525749, val:  90.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.25 seconds, 1.30 minutes\n",
      "layer   1  Sparsity: 91.0904%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4090%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7270%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 95994   6.537%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.580975/ 37.376411, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.56 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1114%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4296%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5899%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 96278   6.513%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.577308/ 34.296520, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.37 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1864%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3696%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 96561   6.489%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.507719/ 33.481220, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.05 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0484%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.8875%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3848%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 96832   6.465%\n",
      "lif layer 2 self.abs_max_v: 3976.5\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.598493/ 39.394562, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.38 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.9195%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.0274%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 97149   6.444%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.547948/ 34.798653, val:  87.92%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.51 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0916%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.9013%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3950%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 97435   6.421%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.597308/ 30.813784, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1060%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.9299%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.2654%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 97725   6.399%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.565610/ 32.649963, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.57 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1897%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.1277%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 98007   6.376%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.538669/ 33.941910, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.44 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5256%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4659%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 98293   6.355%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.456288/ 31.291872, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1009%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5222%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4887%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 98572   6.332%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.645922/ 36.166798, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.32 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3307%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3333%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 98880   6.313%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.270756/ 30.451982, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.15 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1282%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4110%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3939%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 99132   6.289%\n",
      "lif layer 1 self.abs_max_v: 7359.0\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.518249/ 36.303215, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.67 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0793%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5355%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 99416   6.268%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.409738/ 37.870102, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0811%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2045%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3048%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 99669   6.246%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.363410/ 38.989655, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.31 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2321%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5639%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 99926   6.224%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.501212/ 36.322021, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1259%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1894%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4626%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 100183   6.202%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.447929/ 40.470745, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.55 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0997%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1866%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7187%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 100452   6.181%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.342550/ 39.805489, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4923%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6398%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 100705   6.160%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.437174/ 30.606150, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.56 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0310%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.5341%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5400%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 100983   6.140%\n",
      "fc layer 1 self.abs_max_out: 4408.0\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.198982/ 33.114033, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.56 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0995%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3506%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3504%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 101225   6.118%\n",
      "fc layer 1 self.abs_max_out: 4504.0\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.309747/ 36.265732, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.97 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2522%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3566%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 101458   6.096%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.185488/ 34.071453, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.45 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0552%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.0956%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 101696   6.075%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.491849/ 34.904819, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.66 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0546%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3610%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6529%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 101978   6.056%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.366759/ 35.553078, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.73 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1075%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2640%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7258%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 102231   6.036%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.041552/ 36.601250, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.92 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1132%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2582%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5414%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 102432   6.013%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.361653/ 30.892036, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.32 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0345%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.8132%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4000%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 102677   5.993%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.438531/ 32.082878, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.41 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0404%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.9707%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.0560%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 102927   5.974%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.148590/ 35.751083, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.97 seconds, 1.35 minutes\n",
      "layer   1  Sparsity: 91.0720%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0123%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.8345%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 103166   5.954%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.235070/ 31.816322, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 81.41 seconds, 1.36 minutes\n",
      "layer   1  Sparsity: 91.0528%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1611%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.2297%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 103405   5.934%\n",
      "fc layer 2 self.abs_max_out: 2272.0\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.278939/ 33.141773, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.84 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1127%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1357%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6543%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 103628   5.913%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.337855/ 36.972622, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.98 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.1207%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5578%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 103867   5.894%\n",
      "lif layer 2 self.abs_max_v: 4010.0\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.333115/ 36.145325, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.32 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0993%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1737%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7872%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 104134   5.877%\n",
      "fc layer 3 self.abs_max_out: 751.0\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.329298/ 33.811531, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 79.68 seconds, 1.33 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.0574%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9439%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 104376   5.858%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.298952/ 35.683041, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.37 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.1004%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2488%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6424%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 104600   5.838%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.384096/ 36.765308, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 80.64 seconds, 1.34 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7386%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 104856   5.821%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.653613/ 33.828278, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.45 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0523%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.9715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6322%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 105140   5.805%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.128727/ 34.424953, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0573%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.7531%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5361%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 105360   5.786%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.111418/ 33.507740, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.74 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0307%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 66.9443%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7240%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 105572   5.767%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.142223/ 36.114929, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.24 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1042%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9001%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 105776   5.747%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.249188/ 40.980103, val:  83.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.57 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2173%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0292%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 106022   5.730%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.114350/ 30.805105, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.63 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0474%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1966%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 106242   5.712%\n",
      "fc layer 3 self.abs_max_out: 761.0\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.155634/ 39.866791, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1101%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2324%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1677%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 106438   5.692%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.336966/ 34.670479, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.38 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.4647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9459%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 106689   5.676%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.063350/ 36.240639, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.52 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0999%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3906%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 106894   5.657%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.210587/ 41.644520, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.80 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2899%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 107119   5.640%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.416630/ 38.377693, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.20 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.3540%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.1110%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 107357   5.624%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.246448/ 39.896839, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.67 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0615%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2815%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.0854%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 107609   5.608%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  0.945882/ 33.931828, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.12 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0743%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1746%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 107803   5.590%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  0.999871/ 34.040638, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.61 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0348%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6467%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 107996   5.571%\n",
      "fc layer 3 self.abs_max_out: 762.0\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  0.997564/ 34.443787, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.23 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.1372%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.8997%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 108190   5.553%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.070743/ 34.145554, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.48 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0454%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 67.2507%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf8f529aebe420e8d706266f4cd74d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.07074</td></tr><tr><td>val_acc_best</td><td>0.91667</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>34.14555</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crisp-sweep-44</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/54a6ge5z' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/54a6ge5z</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251213_201521-54a6ge5z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mpqlk735 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 30760\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_003455-mpqlk735</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mpqlk735' target=\"_blank\">northern-sweep-51</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mpqlk735' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mpqlk735</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251214_003505_011', 'my_seed': 30760, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 324.0\n",
      "lif layer 1 self.abs_max_v: 324.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 318.0\n",
      "lif layer 2 self.abs_max_v: 318.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 71.0\n",
      "fc layer 1 self.abs_max_out: 339.0\n",
      "lif layer 1 self.abs_max_v: 444.0\n",
      "fc layer 3 self.abs_max_out: 114.0\n",
      "lif layer 1 self.abs_max_v: 478.0\n",
      "lif layer 2 self.abs_max_v: 351.5\n",
      "lif layer 1 self.abs_max_v: 508.0\n",
      "fc layer 2 self.abs_max_out: 349.0\n",
      "lif layer 2 self.abs_max_v: 469.5\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 3 self.abs_max_out: 145.0\n",
      "lif layer 1 self.abs_max_v: 519.5\n",
      "fc layer 1 self.abs_max_out: 427.0\n",
      "lif layer 1 self.abs_max_v: 548.5\n",
      "fc layer 2 self.abs_max_out: 408.0\n",
      "lif layer 2 self.abs_max_v: 540.0\n",
      "fc layer 3 self.abs_max_out: 172.0\n",
      "fc layer 1 self.abs_max_out: 552.0\n",
      "lif layer 1 self.abs_max_v: 552.0\n",
      "fc layer 2 self.abs_max_out: 412.0\n",
      "lif layer 2 self.abs_max_v: 550.0\n",
      "fc layer 2 self.abs_max_out: 424.0\n",
      "lif layer 2 self.abs_max_v: 555.5\n",
      "fc layer 1 self.abs_max_out: 662.0\n",
      "lif layer 1 self.abs_max_v: 662.0\n",
      "lif layer 2 self.abs_max_v: 605.0\n",
      "lif layer 2 self.abs_max_v: 610.5\n",
      "fc layer 2 self.abs_max_out: 441.0\n",
      "fc layer 1 self.abs_max_out: 789.0\n",
      "lif layer 1 self.abs_max_v: 789.0\n",
      "fc layer 2 self.abs_max_out: 467.0\n",
      "fc layer 3 self.abs_max_out: 188.0\n",
      "fc layer 2 self.abs_max_out: 509.0\n",
      "lif layer 2 self.abs_max_v: 615.5\n",
      "lif layer 2 self.abs_max_v: 688.5\n",
      "lif layer 2 self.abs_max_v: 724.5\n",
      "fc layer 3 self.abs_max_out: 207.0\n",
      "fc layer 3 self.abs_max_out: 225.0\n",
      "fc layer 2 self.abs_max_out: 550.0\n",
      "lif layer 2 self.abs_max_v: 882.5\n",
      "lif layer 1 self.abs_max_v: 791.0\n",
      "fc layer 2 self.abs_max_out: 670.0\n",
      "lif layer 2 self.abs_max_v: 956.5\n",
      "fc layer 3 self.abs_max_out: 241.0\n",
      "fc layer 1 self.abs_max_out: 800.0\n",
      "lif layer 1 self.abs_max_v: 800.0\n",
      "fc layer 1 self.abs_max_out: 858.0\n",
      "lif layer 1 self.abs_max_v: 858.0\n",
      "lif layer 2 self.abs_max_v: 959.5\n",
      "lif layer 2 self.abs_max_v: 1009.0\n",
      "fc layer 3 self.abs_max_out: 242.0\n",
      "lif layer 1 self.abs_max_v: 859.0\n",
      "lif layer 1 self.abs_max_v: 878.5\n",
      "fc layer 3 self.abs_max_out: 266.0\n",
      "fc layer 3 self.abs_max_out: 302.0\n",
      "fc layer 1 self.abs_max_out: 944.0\n",
      "lif layer 1 self.abs_max_v: 944.0\n",
      "lif layer 1 self.abs_max_v: 965.5\n",
      "lif layer 1 self.abs_max_v: 1030.0\n",
      "lif layer 1 self.abs_max_v: 1060.5\n",
      "lif layer 1 self.abs_max_v: 1308.5\n",
      "fc layer 1 self.abs_max_out: 969.0\n",
      "fc layer 1 self.abs_max_out: 974.0\n",
      "fc layer 1 self.abs_max_out: 1083.0\n",
      "fc layer 1 self.abs_max_out: 1107.0\n",
      "fc layer 2 self.abs_max_out: 724.0\n",
      "lif layer 2 self.abs_max_v: 1021.5\n",
      "fc layer 2 self.abs_max_out: 729.0\n",
      "lif layer 2 self.abs_max_v: 1057.5\n",
      "lif layer 2 self.abs_max_v: 1069.5\n",
      "lif layer 2 self.abs_max_v: 1075.0\n",
      "fc layer 1 self.abs_max_out: 1262.0\n",
      "lif layer 2 self.abs_max_v: 1086.0\n",
      "lif layer 1 self.abs_max_v: 1309.5\n",
      "fc layer 3 self.abs_max_out: 303.0\n",
      "lif layer 2 self.abs_max_v: 1093.5\n",
      "lif layer 2 self.abs_max_v: 1103.0\n",
      "lif layer 2 self.abs_max_v: 1148.5\n",
      "fc layer 2 self.abs_max_out: 755.0\n",
      "lif layer 1 self.abs_max_v: 1372.0\n",
      "lif layer 1 self.abs_max_v: 1486.5\n",
      "fc layer 1 self.abs_max_out: 1321.0\n",
      "lif layer 1 self.abs_max_v: 1586.5\n",
      "lif layer 1 self.abs_max_v: 1774.5\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "lif layer 1 self.abs_max_v: 1824.5\n",
      "fc layer 3 self.abs_max_out: 320.0\n",
      "fc layer 2 self.abs_max_out: 795.0\n",
      "fc layer 2 self.abs_max_out: 816.0\n",
      "fc layer 3 self.abs_max_out: 333.0\n",
      "fc layer 1 self.abs_max_out: 1329.0\n",
      "lif layer 1 self.abs_max_v: 1833.0\n",
      "fc layer 1 self.abs_max_out: 1357.0\n",
      "lif layer 1 self.abs_max_v: 1876.5\n",
      "fc layer 2 self.abs_max_out: 862.0\n",
      "lif layer 1 self.abs_max_v: 2159.5\n",
      "fc layer 3 self.abs_max_out: 336.0\n",
      "fc layer 2 self.abs_max_out: 941.0\n",
      "lif layer 1 self.abs_max_v: 2310.5\n",
      "fc layer 2 self.abs_max_out: 953.0\n",
      "fc layer 2 self.abs_max_out: 993.0\n",
      "fc layer 2 self.abs_max_out: 1079.0\n",
      "fc layer 3 self.abs_max_out: 368.0\n",
      "fc layer 3 self.abs_max_out: 383.0\n",
      "fc layer 1 self.abs_max_out: 1479.0\n",
      "fc layer 3 self.abs_max_out: 397.0\n",
      "fc layer 3 self.abs_max_out: 403.0\n",
      "lif layer 2 self.abs_max_v: 1189.5\n",
      "lif layer 2 self.abs_max_v: 1254.5\n",
      "lif layer 2 self.abs_max_v: 1280.5\n",
      "lif layer 1 self.abs_max_v: 2387.0\n",
      "fc layer 2 self.abs_max_out: 1111.0\n",
      "fc layer 2 self.abs_max_out: 1131.0\n",
      "fc layer 1 self.abs_max_out: 1548.0\n",
      "lif layer 2 self.abs_max_v: 1297.0\n",
      "lif layer 2 self.abs_max_v: 1330.5\n",
      "lif layer 2 self.abs_max_v: 1392.5\n",
      "lif layer 1 self.abs_max_v: 2410.5\n",
      "lif layer 1 self.abs_max_v: 2510.5\n",
      "lif layer 1 self.abs_max_v: 2645.0\n",
      "lif layer 1 self.abs_max_v: 2859.5\n",
      "fc layer 1 self.abs_max_out: 1555.0\n",
      "lif layer 1 self.abs_max_v: 2985.0\n",
      "lif layer 2 self.abs_max_v: 1417.5\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 14.311820/ 58.661308, val:  41.67%, val_best:  41.67%, tr:  95.71%, tr_best:  95.71%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0359%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.5146%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2979%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2375  24.259%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 1565.0\n",
      "fc layer 1 self.abs_max_out: 1568.0\n",
      "fc layer 1 self.abs_max_out: 1592.0\n",
      "lif layer 2 self.abs_max_v: 1440.5\n",
      "lif layer 2 self.abs_max_v: 1459.5\n",
      "fc layer 2 self.abs_max_out: 1150.0\n",
      "fc layer 2 self.abs_max_out: 1232.0\n",
      "fc layer 2 self.abs_max_out: 1318.0\n",
      "fc layer 1 self.abs_max_out: 1613.0\n",
      "fc layer 1 self.abs_max_out: 1668.0\n",
      "fc layer 1 self.abs_max_out: 1792.0\n",
      "fc layer 2 self.abs_max_out: 1380.0\n",
      "lif layer 2 self.abs_max_v: 1465.5\n",
      "lif layer 2 self.abs_max_v: 1469.0\n",
      "lif layer 2 self.abs_max_v: 1514.5\n",
      "lif layer 2 self.abs_max_v: 1518.5\n",
      "fc layer 2 self.abs_max_out: 1398.0\n",
      "fc layer 1 self.abs_max_out: 1925.0\n",
      "lif layer 1 self.abs_max_v: 3044.5\n",
      "lif layer 1 self.abs_max_v: 3317.5\n",
      "lif layer 1 self.abs_max_v: 3425.0\n",
      "lif layer 1 self.abs_max_v: 3460.5\n",
      "lif layer 1 self.abs_max_v: 3536.5\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  9.627994/ 53.730770, val:  41.67%, val_best:  41.67%, tr:  99.28%, tr_best:  99.28%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3469%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.4551%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 4070  20.787%\n",
      "fc layer 2 self.abs_max_out: 1425.0\n",
      "fc layer 2 self.abs_max_out: 1458.0\n",
      "lif layer 2 self.abs_max_v: 1519.0\n",
      "fc layer 1 self.abs_max_out: 2046.0\n",
      "lif layer 2 self.abs_max_v: 1532.0\n",
      "lif layer 2 self.abs_max_v: 1575.0\n",
      "lif layer 2 self.abs_max_v: 1607.5\n",
      "lif layer 2 self.abs_max_v: 1631.0\n",
      "lif layer 2 self.abs_max_v: 1635.5\n",
      "lif layer 1 self.abs_max_v: 3564.0\n",
      "lif layer 2 self.abs_max_v: 1643.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.065124/ 56.494778, val:  42.92%, val_best:  42.92%, tr:  99.49%, tr_best:  99.49%, epoch time: 74.10 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0732%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.6948%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.3411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5626  19.156%\n",
      "lif layer 2 self.abs_max_v: 1666.0\n",
      "fc layer 3 self.abs_max_out: 417.0\n",
      "fc layer 1 self.abs_max_out: 2146.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  8.474843/ 57.935066, val:  38.33%, val_best:  42.92%, tr:  99.39%, tr_best:  99.49%, epoch time: 74.78 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.6964%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.5472%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 7101  18.133%\n",
      "fc layer 3 self.abs_max_out: 431.0\n",
      "lif layer 2 self.abs_max_v: 1673.5\n",
      "lif layer 2 self.abs_max_v: 1725.5\n",
      "fc layer 1 self.abs_max_out: 2179.0\n",
      "lif layer 1 self.abs_max_v: 3674.5\n",
      "lif layer 1 self.abs_max_v: 3679.0\n",
      "lif layer 1 self.abs_max_v: 3750.5\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  7.788409/ 43.746929, val:  49.17%, val_best:  49.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0336%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.7989%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.3719%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 8487  17.338%\n",
      "lif layer 2 self.abs_max_v: 1739.0\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "lif layer 1 self.abs_max_v: 3890.5\n",
      "fc layer 1 self.abs_max_out: 2201.0\n",
      "fc layer 1 self.abs_max_out: 2373.0\n",
      "lif layer 1 self.abs_max_v: 3896.0\n",
      "lif layer 1 self.abs_max_v: 4008.5\n",
      "lif layer 1 self.abs_max_v: 4093.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  7.771209/ 65.684151, val:  42.08%, val_best:  49.17%, tr:  99.69%, tr_best:  99.69%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0660%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.8854%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.4522%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9885  16.828%\n",
      "lif layer 2 self.abs_max_v: 1806.5\n",
      "fc layer 3 self.abs_max_out: 438.0\n",
      "fc layer 1 self.abs_max_out: 2508.0\n",
      "lif layer 2 self.abs_max_v: 1948.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  7.472982/ 48.498531, val:  50.83%, val_best:  50.83%, tr:  99.80%, tr_best:  99.80%, epoch time: 74.38 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0842%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.5605%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.4620%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 11235  16.394%\n",
      "fc layer 2 self.abs_max_out: 1473.0\n",
      "lif layer 2 self.abs_max_v: 2041.0\n",
      "fc layer 2 self.abs_max_out: 1523.0\n",
      "fc layer 1 self.abs_max_out: 2522.0\n",
      "lif layer 1 self.abs_max_v: 4306.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  7.000667/ 60.584667, val:  45.42%, val_best:  50.83%, tr:  99.18%, tr_best:  99.80%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0467%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9189%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.4253%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 12511  15.974%\n",
      "fc layer 3 self.abs_max_out: 449.0\n",
      "lif layer 2 self.abs_max_v: 2054.0\n",
      "lif layer 2 self.abs_max_v: 2078.0\n",
      "fc layer 3 self.abs_max_out: 492.0\n",
      "fc layer 1 self.abs_max_out: 2591.0\n",
      "fc layer 2 self.abs_max_out: 1535.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  7.265028/ 38.569973, val:  52.50%, val_best:  52.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7676%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.7509%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13798  15.660%\n",
      "lif layer 2 self.abs_max_v: 2119.0\n",
      "lif layer 2 self.abs_max_v: 2230.5\n",
      "lif layer 2 self.abs_max_v: 2281.5\n",
      "fc layer 2 self.abs_max_out: 1604.0\n",
      "fc layer 1 self.abs_max_out: 2676.0\n",
      "lif layer 1 self.abs_max_v: 4581.0\n",
      "lif layer 1 self.abs_max_v: 4597.5\n",
      "lif layer 1 self.abs_max_v: 4642.0\n",
      "fc layer 2 self.abs_max_out: 1606.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  6.886423/ 56.009808, val:  44.58%, val_best:  52.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 74.36 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0897%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6842%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8574%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 15029  15.351%\n",
      "fc layer 2 self.abs_max_out: 1626.0\n",
      "fc layer 2 self.abs_max_out: 1648.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  6.588184/ 44.686504, val:  51.25%, val_best:  52.50%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1010%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5271%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1876%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 16241  15.081%\n",
      "lif layer 1 self.abs_max_v: 4717.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  6.741605/ 47.867474, val:  54.17%, val_best:  54.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1255%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2087%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.5110%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 17468  14.869%\n",
      "fc layer 1 self.abs_max_out: 2680.0\n",
      "fc layer 1 self.abs_max_out: 2749.0\n",
      "lif layer 1 self.abs_max_v: 4734.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  6.493193/ 38.649372, val:  57.08%, val_best:  57.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 74.03 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.2898%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 18657  14.659%\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  6.204644/ 49.346920, val:  45.42%, val_best:  57.08%, tr:  99.69%, tr_best:  99.90%, epoch time: 73.86 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3464%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.5781%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19755  14.413%\n",
      "fc layer 1 self.abs_max_out: 2820.0\n",
      "lif layer 1 self.abs_max_v: 4919.5\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  5.814471/ 48.114483, val:  56.25%, val_best:  57.08%, tr:  99.80%, tr_best:  99.90%, epoch time: 74.54 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0658%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0902%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.8214%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20838  14.190%\n",
      "fc layer 2 self.abs_max_out: 1660.0\n",
      "fc layer 2 self.abs_max_out: 1674.0\n",
      "fc layer 1 self.abs_max_out: 2901.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  6.197917/ 34.846977, val:  61.67%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.60 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3062%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.0084%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21977  14.030%\n",
      "fc layer 3 self.abs_max_out: 507.0\n",
      "fc layer 1 self.abs_max_out: 3040.0\n",
      "lif layer 1 self.abs_max_v: 4970.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  5.690055/ 43.290684, val:  55.42%, val_best:  61.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0689%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9768%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6629%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 23038  13.842%\n",
      "fc layer 3 self.abs_max_out: 532.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  6.001813/ 51.167107, val:  50.42%, val_best:  61.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.56 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5537%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.9985%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 24113  13.683%\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  6.061653/ 36.001064, val:  69.58%, val_best:  69.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.29 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5084%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6466%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 25205  13.550%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  5.809941/ 38.741852, val:  63.75%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.39 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0671%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8494%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.0954%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 26275  13.419%\n",
      "lif layer 2 self.abs_max_v: 2304.0\n",
      "lif layer 1 self.abs_max_v: 5055.5\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  5.685973/ 38.770542, val:  61.25%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.48 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4336%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.8841%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 27313  13.285%\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  5.853343/ 32.858616, val:  65.00%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.39 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1283%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9434%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6500%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 28357  13.166%\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  5.597080/ 39.261902, val:  64.58%, val_best:  69.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.1069%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 29368  13.043%\n",
      "lif layer 2 self.abs_max_v: 2401.0\n",
      "lif layer 2 self.abs_max_v: 2406.5\n",
      "fc layer 1 self.abs_max_out: 3055.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  5.442938/ 32.060638, val:  69.58%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0391%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.1179%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 30349  12.917%\n",
      "fc layer 2 self.abs_max_out: 1714.0\n",
      "lif layer 2 self.abs_max_v: 2468.0\n",
      "fc layer 1 self.abs_max_out: 3077.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  5.290851/ 40.155804, val:  64.58%, val_best:  69.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1253%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.0203%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 31297  12.787%\n",
      "fc layer 1 self.abs_max_out: 3207.0\n",
      "lif layer 1 self.abs_max_v: 5079.0\n",
      "lif layer 1 self.abs_max_v: 5135.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  4.957373/ 29.458576, val:  76.25%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0570%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9420%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.0789%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 32219  12.658%\n",
      "lif layer 2 self.abs_max_v: 2575.5\n",
      "fc layer 2 self.abs_max_out: 1723.0\n",
      "fc layer 1 self.abs_max_out: 3209.0\n",
      "lif layer 1 self.abs_max_v: 5225.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  5.156354/ 30.959141, val:  67.50%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.96 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7388%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.0637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 33178  12.552%\n",
      "fc layer 1 self.abs_max_out: 3216.0\n",
      "lif layer 1 self.abs_max_v: 5381.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  5.115951/ 36.792820, val:  70.42%, val_best:  76.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.90 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5049%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.1521%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 34118  12.446%\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  4.822738/ 30.136288, val:  73.75%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1672%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6926%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 35037  12.341%\n",
      "fc layer 2 self.abs_max_out: 1769.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  4.934321/ 34.336899, val:  75.42%, val_best:  76.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.65 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3607%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.1516%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35972  12.248%\n",
      "lif layer 2 self.abs_max_v: 2585.0\n",
      "fc layer 2 self.abs_max_out: 1776.0\n",
      "fc layer 3 self.abs_max_out: 533.0\n",
      "fc layer 3 self.abs_max_out: 539.0\n",
      "fc layer 2 self.abs_max_out: 1838.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  4.770184/ 33.817966, val:  73.75%, val_best:  76.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0829%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2913%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.8667%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 36837  12.138%\n",
      "fc layer 3 self.abs_max_out: 565.0\n",
      "lif layer 2 self.abs_max_v: 2595.5\n",
      "fc layer 3 self.abs_max_out: 568.0\n",
      "fc layer 1 self.abs_max_out: 3326.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  4.977027/ 26.182455, val:  82.92%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1746%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.8556%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 37729  12.043%\n",
      "fc layer 1 self.abs_max_out: 3346.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  4.772167/ 40.060646, val:  62.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.82 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0745%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4413%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1865%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 38599  11.948%\n",
      "fc layer 1 self.abs_max_out: 3360.0\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  5.141369/ 42.185345, val:  59.58%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0957%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2692%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0089%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 39485  11.862%\n",
      "fc layer 1 self.abs_max_out: 3374.0\n",
      "fc layer 1 self.abs_max_out: 3476.0\n",
      "lif layer 1 self.abs_max_v: 5431.0\n",
      "lif layer 1 self.abs_max_v: 5664.5\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  4.749737/ 30.449820, val:  76.25%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0823%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.0538%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 40317  11.766%\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  4.378009/ 37.904537, val:  68.75%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.60 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2712%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.3540%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 41099  11.661%\n",
      "fc layer 1 self.abs_max_out: 3486.0\n",
      "lif layer 1 self.abs_max_v: 5666.5\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  4.781568/ 28.958324, val:  75.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.22 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.1051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9868%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.5416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 41919  11.572%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  4.702683/ 35.208603, val:  75.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0944%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8431%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.6595%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 42797  11.504%\n",
      "lif layer 1 self.abs_max_v: 5672.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  4.351188/ 37.121819, val:  69.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0314%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.0406%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 43569  11.411%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  4.313200/ 33.054314, val:  78.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.58 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1035%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0800%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.2703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 44370  11.330%\n",
      "fc layer 2 self.abs_max_out: 1864.0\n",
      "fc layer 1 self.abs_max_out: 3514.0\n",
      "lif layer 1 self.abs_max_v: 5962.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  4.360582/ 35.064735, val:  73.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0832%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1682%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.8427%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 45155  11.250%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  4.524035/ 34.218254, val:  73.75%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0824%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2820%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3782%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 45975  11.181%\n",
      "fc layer 1 self.abs_max_out: 3543.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  4.226269/ 27.455683, val:  82.08%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0522%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2258%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7849%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 46740  11.103%\n",
      "fc layer 2 self.abs_max_out: 1921.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  3.742190/ 40.539890, val:  65.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0203%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.8130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 47442  11.014%\n",
      "fc layer 2 self.abs_max_out: 1987.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.240480/ 28.800093, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4918%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 48217  10.945%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  3.963951/ 28.183043, val:  81.67%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1049%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0749%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0976%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 48960  10.872%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  3.908765/ 29.074930, val:  78.33%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 73.42 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0706%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2022%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1606%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 49661  10.793%\n",
      "lif layer 1 self.abs_max_v: 6090.0\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.003201/ 30.442396, val:  82.08%, val_best:  82.92%, tr:  99.80%, tr_best: 100.00%, epoch time: 76.47 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2778%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9267%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 50393  10.724%\n",
      "lif layer 2 self.abs_max_v: 2647.0\n",
      "lif layer 1 self.abs_max_v: 6106.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  3.659050/ 35.753185, val:  80.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0964%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0520%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 51078  10.648%\n",
      "lif layer 1 self.abs_max_v: 6170.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  3.667202/ 38.859573, val:  71.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0800%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8454%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.8632%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 51768  10.576%\n",
      "lif layer 2 self.abs_max_v: 2724.0\n",
      "lif layer 2 self.abs_max_v: 2742.5\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  3.691201/ 31.747290, val:  82.50%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0737%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7106%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 52473  10.510%\n",
      "fc layer 1 self.abs_max_out: 3669.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  3.805356/ 24.772758, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1083%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0593%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5914%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 53162  10.443%\n",
      "fc layer 1 self.abs_max_out: 3705.0\n",
      "lif layer 2 self.abs_max_v: 2835.5\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  3.522959/ 28.046980, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0742%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2031%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9049%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 53818  10.372%\n",
      "fc layer 2 self.abs_max_out: 2032.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  3.384783/ 32.174019, val:  77.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.97 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0807%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0474%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0487%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 54455  10.301%\n",
      "lif layer 2 self.abs_max_v: 2856.5\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  3.714555/ 26.803089, val:  86.25%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0104%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7745%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9203%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 55138  10.240%\n",
      "lif layer 2 self.abs_max_v: 2905.0\n",
      "lif layer 2 self.abs_max_v: 2923.5\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  3.025145/ 26.377821, val:  84.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7121%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1128%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 55713  10.162%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  3.704930/ 30.907284, val:  77.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1852%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 56371  10.102%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  3.455342/ 34.264782, val:  77.08%, val_best:  86.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0571%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4737%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3797%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 57021  10.042%\n",
      "lif layer 2 self.abs_max_v: 2939.5\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  3.421832/ 27.310719, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.18 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6181%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.8240%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 57657   9.982%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  3.439695/ 33.498001, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.09 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0463%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6360%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.1472%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 58305   9.926%\n",
      "lif layer 2 self.abs_max_v: 2967.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  3.273875/ 26.811281, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0956%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 58929   9.868%\n",
      "fc layer 1 self.abs_max_out: 3729.0\n",
      "lif layer 2 self.abs_max_v: 2989.0\n",
      "lif layer 2 self.abs_max_v: 3133.0\n",
      "lif layer 2 self.abs_max_v: 3191.5\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  3.780182/ 46.133774, val:  69.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9855%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4402%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 59596   9.818%\n",
      "fc layer 1 self.abs_max_out: 3732.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  3.368973/ 33.852467, val:  81.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4867%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.1032%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 60179   9.757%\n",
      "fc layer 1 self.abs_max_out: 3783.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  3.042233/ 31.675159, val:  77.08%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0822%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4632%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.2130%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 60756   9.697%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.214899/ 28.827961, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1181%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3730%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 61379   9.645%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.301669/ 31.039339, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0460%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6285%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6192%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 61977   9.592%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  3.333209/ 39.007462, val:  80.42%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5392%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4793%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 62596   9.543%\n",
      "fc layer 3 self.abs_max_out: 573.0\n",
      "fc layer 3 self.abs_max_out: 577.0\n",
      "fc layer 3 self.abs_max_out: 586.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.272493/ 29.405970, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0768%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4732%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6783%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 63189   9.492%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  2.915015/ 32.897305, val:  83.75%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0702%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6733%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 63747   9.437%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.098324/ 26.354118, val:  85.83%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0427%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6910%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 64302   9.383%\n",
      "fc layer 3 self.abs_max_out: 593.0\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  2.747831/ 30.960581, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0780%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7807%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 64838   9.328%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  2.891579/ 31.473164, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.81 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2895%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7167%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 65393   9.277%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  2.940381/ 45.641415, val:  72.08%, val_best:  86.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1010%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5253%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0982%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 65928   9.225%\n",
      "fc layer 1 self.abs_max_out: 3791.0\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.482046/ 28.814442, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7359%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4437%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 66517   9.182%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.249756/ 36.968319, val:  74.58%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.4457%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 67107   9.140%\n",
      "fc layer 3 self.abs_max_out: 626.0\n",
      "fc layer 3 self.abs_max_out: 631.0\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  2.597487/ 26.786516, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1015%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 67602   9.086%\n",
      "fc layer 1 self.abs_max_out: 3806.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  2.729522/ 27.292376, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0720%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6342%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3432%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 68131   9.038%\n",
      "lif layer 1 self.abs_max_v: 6171.0\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  2.815161/ 28.539225, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0949%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5054%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3024%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 68661   8.992%\n",
      "fc layer 1 self.abs_max_out: 3814.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  2.518469/ 27.576612, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0321%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1465%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 69145   8.940%\n",
      "fc layer 2 self.abs_max_out: 2064.0\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  2.955325/ 26.976427, val:  85.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1081%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4591%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1734%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 69658   8.894%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  2.363138/ 30.541838, val:  85.42%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6623%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 70109   8.841%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  2.452260/ 31.790873, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0756%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6454%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7276%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 70574   8.791%\n",
      "fc layer 2 self.abs_max_out: 2109.0\n",
      "fc layer 2 self.abs_max_out: 2128.0\n",
      "fc layer 1 self.abs_max_out: 3815.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  2.617981/ 34.386097, val:  80.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0623%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7971%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 71051   8.744%\n",
      "fc layer 1 self.abs_max_out: 3866.0\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  2.794700/ 39.063221, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.46 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5248%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 71563   8.702%\n",
      "fc layer 1 self.abs_max_out: 3882.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  2.720414/ 35.995457, val:  81.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0652%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5674%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0051%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 72060   8.659%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  2.505350/ 31.127478, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.98 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0675%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4825%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1216%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 72539   8.616%\n",
      "fc layer 1 self.abs_max_out: 3912.0\n",
      "lif layer 1 self.abs_max_v: 6287.0\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  2.428606/ 28.250641, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3330%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6729%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 72989   8.569%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  2.572420/ 29.514893, val:  88.33%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.84 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0887%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0925%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0412%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 73473   8.528%\n",
      "fc layer 1 self.abs_max_out: 3922.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  2.704466/ 38.103683, val:  78.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1001%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.6185%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 73959   8.488%\n",
      "fc layer 1 self.abs_max_out: 4060.0\n",
      "lif layer 2 self.abs_max_v: 3197.5\n",
      "lif layer 2 self.abs_max_v: 3211.0\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  2.633055/ 32.156612, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0677%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.8788%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 74431   8.448%\n",
      "lif layer 2 self.abs_max_v: 3224.0\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  2.427052/ 37.659546, val:  81.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0830%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2508%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.3191%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 74880   8.405%\n",
      "lif layer 2 self.abs_max_v: 3226.0\n",
      "lif layer 2 self.abs_max_v: 3251.0\n",
      "lif layer 2 self.abs_max_v: 3345.5\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  2.500023/ 30.889021, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0457%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7217%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 75338   8.365%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  2.322674/ 29.356791, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0987%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1485%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 75779   8.323%\n",
      "fc layer 1 self.abs_max_out: 4097.0\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  2.210729/ 43.407070, val:  79.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0597%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3781%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.6349%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 76210   8.281%\n",
      "fc layer 3 self.abs_max_out: 646.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.208106/ 32.338169, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0623%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1297%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5606%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 76640   8.240%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.460781/ 30.090586, val:  88.75%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0480%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5594%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 77108   8.204%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.237290/ 30.178640, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0574%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4817%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.6425%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 77541   8.165%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  2.515037/ 32.901958, val:  84.17%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3194%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.8631%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 77980   8.128%\n",
      "fc layer 2 self.abs_max_out: 2132.0\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.524730/ 34.807564, val:  86.25%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.87 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0479%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0904%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9563%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 78446   8.094%\n",
      "lif layer 1 self.abs_max_v: 6362.0\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  1.924832/ 31.028671, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.41 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0894%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9233%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1946%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 78820   8.051%\n",
      "fc layer 2 self.abs_max_out: 2164.0\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.044837/ 27.286085, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.94 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7563%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3752%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 79211   8.011%\n",
      "fc layer 1 self.abs_max_out: 4105.0\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.056730/ 32.854862, val:  84.58%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.77 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1076%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8425%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 79601   7.971%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.204064/ 30.965851, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.68 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0932%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7753%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.8719%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 80022   7.936%\n",
      "lif layer 1 self.abs_max_v: 6487.5\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.424288/ 33.856014, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.02 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0891%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7612%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2822%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 80455   7.902%\n",
      "lif layer 2 self.abs_max_v: 3404.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.116549/ 28.736708, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.86 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8931%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2446%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 80858   7.866%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.017554/ 30.615219, val:  86.67%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0451%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8621%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5253%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 81246   7.829%\n",
      "fc layer 1 self.abs_max_out: 4117.0\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.142163/ 29.456326, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8716%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5927%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 81617   7.791%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.168187/ 28.441141, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.4191%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 82006   7.756%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.017777/ 30.433178, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0393%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2147%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.4846%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 82422   7.724%\n",
      "lif layer 2 self.abs_max_v: 3443.5\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  1.965230/ 35.926983, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0653%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0964%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.8573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 82800   7.689%\n",
      "lif layer 2 self.abs_max_v: 3456.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  1.881830/ 34.820599, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0710%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5847%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 83167   7.653%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  1.966734/ 33.121658, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0615%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.4943%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 83530   7.618%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  1.809931/ 34.193062, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.20 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1143%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3039%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2852%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 83884   7.583%\n",
      "lif layer 1 self.abs_max_v: 6527.0\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.060882/ 33.497730, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0870%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3240%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.4269%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 84274   7.551%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  1.965489/ 31.455631, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1189%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1320%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 84649   7.519%\n",
      "fc layer 2 self.abs_max_out: 2185.0\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.149258/ 36.459812, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.91 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1767%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0407%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 85035   7.488%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  1.974192/ 38.493252, val:  82.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.11 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3664%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 85421   7.458%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  1.919601/ 31.472174, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3328%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.9985%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 85769   7.424%\n",
      "fc layer 1 self.abs_max_out: 4146.0\n",
      "lif layer 2 self.abs_max_v: 3484.5\n",
      "fc layer 1 self.abs_max_out: 4260.0\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.043949/ 34.269619, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1370%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3809%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 86134   7.393%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  1.882333/ 33.394512, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.14 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1030%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 86464   7.360%\n",
      "lif layer 2 self.abs_max_v: 3489.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  1.939507/ 29.537054, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5029%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 86824   7.329%\n",
      "lif layer 1 self.abs_max_v: 6717.5\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  1.797096/ 31.619459, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.49 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.6836%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 87175   7.299%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  1.702401/ 31.217531, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.24 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0876%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8959%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.6244%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 87488   7.265%\n",
      "lif layer 2 self.abs_max_v: 3569.0\n",
      "lif layer 2 self.abs_max_v: 3645.5\n",
      "lif layer 1 self.abs_max_v: 6880.5\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  1.984540/ 29.339760, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0689%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8954%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1042%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 87852   7.237%\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  1.697310/ 30.414156, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.09 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0396%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9464%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.7778%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 88186   7.206%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.193475/ 32.772957, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.11 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0522%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0280%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.2955%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 88579   7.181%\n",
      "lif layer 1 self.abs_max_v: 6899.5\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  1.952774/ 27.880470, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1787%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.5477%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 88954   7.154%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  1.755124/ 32.215931, val:  86.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1093%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.8041%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 89280   7.125%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  1.802097/ 30.577745, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0881%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3848%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0618%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 89625   7.097%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  1.879696/ 33.627274, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.25 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0585%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2969%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 89977   7.070%\n",
      "fc layer 1 self.abs_max_out: 4276.0\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  1.855130/ 33.189537, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0834%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2911%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.8790%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 90323   7.043%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  1.747572/ 35.870613, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.90 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0260%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3406%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 90651   7.015%\n",
      "lif layer 1 self.abs_max_v: 6937.5\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  1.713451/ 33.598923, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 77.13 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0750%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0942%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.4875%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 90980   6.987%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  1.929458/ 30.884283, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0341%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2452%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 91326   6.962%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  1.730671/ 29.596811, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9657%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 91649   6.934%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  1.526550/ 30.033207, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9140%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.4025%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 91956   6.907%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  1.627668/ 32.691441, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.36 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0892%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1394%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.4710%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 92278   6.880%\n",
      "fc layer 2 self.abs_max_out: 2225.0\n",
      "fc layer 1 self.abs_max_out: 4282.0\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  1.531985/ 30.058540, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0333%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0561%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1210%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 92574   6.852%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  1.635984/ 35.734470, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.21 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0997%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0571%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0727%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 92897   6.827%\n",
      "lif layer 1 self.abs_max_v: 7079.5\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  1.677485/ 32.230068, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0783%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0354%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 93212   6.801%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  1.787640/ 32.458458, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0607%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0436%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0679%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 93537   6.776%\n",
      "fc layer 1 self.abs_max_out: 4335.0\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  1.654095/ 32.020138, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2597%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2646%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 93860   6.752%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  1.793017/ 33.426693, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1417%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3177%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 94181   6.727%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  1.590026/ 30.140179, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.13 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0800%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1704%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.6247%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 94480   6.702%\n",
      "fc layer 3 self.abs_max_out: 653.0\n",
      "fc layer 1 self.abs_max_out: 4338.0\n",
      "lif layer 1 self.abs_max_v: 7266.5\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  1.856012/ 29.379810, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0811%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1712%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5434%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 94824   6.680%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.226309/ 31.122646, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0588%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3548%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5158%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 95069   6.651%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.783816/ 29.791414, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.03 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1056%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 95387   6.628%\n",
      "lif layer 1 self.abs_max_v: 7320.0\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.532381/ 30.517689, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1070%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9742%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1423%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 95663   6.602%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.495688/ 28.784548, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.44 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0615%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0218%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3975%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 95953   6.578%\n",
      "lif layer 1 self.abs_max_v: 7419.5\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.426281/ 28.918413, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1585%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.6192%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 96240   6.554%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.339045/ 34.224361, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.9650%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 96517   6.529%\n",
      "fc layer 3 self.abs_max_out: 658.0\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.400128/ 31.196693, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0723%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.2539%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 96794   6.505%\n",
      "fc layer 1 self.abs_max_out: 4348.0\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.383346/ 37.722927, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1161%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2006%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0800%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 97059   6.480%\n",
      "fc layer 1 self.abs_max_out: 4369.0\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.267909/ 29.857838, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.23 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1765%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.7954%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 97321   6.455%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.754688/ 34.009804, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1305%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1344%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2813%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 97632   6.434%\n",
      "lif layer 1 self.abs_max_v: 7505.0\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.448913/ 30.107389, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.89 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1947%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3353%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 97891   6.410%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.407612/ 33.128742, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0436%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1481%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2374%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 98168   6.387%\n",
      "fc layer 3 self.abs_max_out: 683.0\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.240723/ 35.094891, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.99 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0590%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8457%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5909%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 98429   6.363%\n",
      "fc layer 3 self.abs_max_out: 710.0\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.252159/ 30.957972, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.41 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0993%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0280%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.9184%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 98691   6.340%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.343330/ 37.702236, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1882%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.8163%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 98959   6.318%\n",
      "fc layer 2 self.abs_max_out: 2296.0\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.488937/ 39.353474, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.22 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0811%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1274%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5668%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 99236   6.296%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.313618/ 35.465267, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9874%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.8378%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 99492   6.273%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.219385/ 38.459690, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0657%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.8011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 99735   6.250%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.499048/ 35.084053, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0629%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8568%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.8202%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 100009   6.229%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.318930/ 30.723555, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.88 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0894%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0569%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.7226%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 100264   6.207%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.232583/ 30.141441, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0366%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.7956%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 100497   6.184%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.305312/ 34.617733, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.45 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0752%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.1213%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 100749   6.162%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.222215/ 36.844051, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9996%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.1575%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 100985   6.140%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.397463/ 34.656200, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.70 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0719%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.7555%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 101245   6.119%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.307273/ 35.830910, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8255%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0379%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 101503   6.099%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.124836/ 29.977919, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0437%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.3154%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 101730   6.077%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.055537/ 28.922384, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0761%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0454%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.2038%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 101940   6.054%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.206336/ 25.691704, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1008%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7738%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0138%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 102164   6.032%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.378573/ 30.896616, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7944%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.9134%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 102411   6.012%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.099116/ 30.245283, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9716%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3895%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 102642   5.991%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  0.912008/ 31.963049, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.28 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0536%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1478%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.4972%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 102845   5.969%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.398646/ 34.778812, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.31 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0520%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0769%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5242%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 103077   5.948%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.233864/ 27.394560, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.82 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1107%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6945%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3905%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 103314   5.929%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.008933/ 29.016817, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.75 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1084%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9066%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2391%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 103525   5.908%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.104881/ 33.037750, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.65 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7993%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2633%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 103747   5.887%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.313807/ 29.011705, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.98 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0923%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7342%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3396%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 103975   5.868%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.021719/ 30.849302, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.31 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8247%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3721%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 104189   5.847%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.010638/ 27.956572, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.62 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0521%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8972%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.1685%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 104398   5.827%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.132603/ 29.486345, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.16 seconds, 1.10 minutes\n",
      "layer   1  Sparsity: 91.0928%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6926%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 104623   5.808%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  0.821591/ 30.172550, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.24 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0830%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8043%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.6073%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 104806   5.787%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.074128/ 33.202026, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.21 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0533%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7613%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 105021   5.767%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.066112/ 30.427017, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.03 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8942%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0111%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 105211   5.747%\n",
      "fc layer 1 self.abs_max_out: 4412.0\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.004011/ 32.297989, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.78 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0160%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.0062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 105394   5.726%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  0.910313/ 31.055996, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.30 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0897%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8613%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.8614%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 105566   5.705%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  0.954710/ 28.948975, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.72 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8457%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.5003%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 105771   5.686%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.132111/ 33.934265, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.45 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7699%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.3302%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 105992   5.668%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.252091/ 35.160545, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.22 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7936%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.2266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 106221   5.651%\n",
      "fc layer 2 self.abs_max_out: 2315.0\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.244261/ 32.362480, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.55 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1236%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.0598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 106444   5.634%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.179394/ 34.968197, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.39 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0507%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8670%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.4414%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 106679   5.617%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  0.951739/ 35.195805, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.16 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6743%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.9518%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 106878   5.598%\n",
      "lif layer 2 self.abs_max_v: 3705.0\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.328027/ 40.850597, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.87 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1206%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7437%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.7237%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 107111   5.582%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  0.963515/ 35.401196, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.16 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7536%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 74.9858%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 107299   5.563%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.078972/ 35.462334, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.55 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0960%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0172%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.1513%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 107504   5.546%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  0.982153/ 31.704592, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.72 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1230%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9070%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.1381%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 107698   5.528%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.129870/ 32.446590, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.32 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0409%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 75.1070%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c399d163904fea857bd5ea33016b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÑ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.12987</td></tr><tr><td>val_acc_best</td><td>0.9125</td></tr><tr><td>val_acc_now</td><td>0.89583</td></tr><tr><td>val_loss</td><td>32.44659</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">northern-sweep-51</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mpqlk735' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/mpqlk735</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_003455-mpqlk735/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qbt1b9z0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 12177\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_044347-qbt1b9z0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qbt1b9z0' target=\"_blank\">ethereal-sweep-58</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qbt1b9z0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qbt1b9z0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251214_044356_290', 'my_seed': 12177, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 165.0\n",
      "lif layer 1 self.abs_max_v: 165.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 164.0\n",
      "lif layer 2 self.abs_max_v: 164.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 28.0\n",
      "lif layer 1 self.abs_max_v: 239.5\n",
      "fc layer 1 self.abs_max_out: 166.0\n",
      "lif layer 1 self.abs_max_v: 280.0\n",
      "fc layer 2 self.abs_max_out: 180.0\n",
      "lif layer 2 self.abs_max_v: 235.5\n",
      "fc layer 3 self.abs_max_out: 31.0\n",
      "fc layer 1 self.abs_max_out: 197.0\n",
      "fc layer 2 self.abs_max_out: 195.0\n",
      "lif layer 2 self.abs_max_v: 255.5\n",
      "fc layer 3 self.abs_max_out: 81.0\n",
      "lif layer 1 self.abs_max_v: 281.0\n",
      "fc layer 2 self.abs_max_out: 225.0\n",
      "lif layer 2 self.abs_max_v: 273.0\n",
      "lif layer 1 self.abs_max_v: 293.5\n",
      "lif layer 1 self.abs_max_v: 318.0\n",
      "fc layer 2 self.abs_max_out: 266.0\n",
      "lif layer 2 self.abs_max_v: 346.0\n",
      "fc layer 3 self.abs_max_out: 130.0\n",
      "lif layer 1 self.abs_max_v: 341.0\n",
      "fc layer 1 self.abs_max_out: 229.0\n",
      "fc layer 2 self.abs_max_out: 268.0\n",
      "fc layer 1 self.abs_max_out: 253.0\n",
      "lif layer 1 self.abs_max_v: 354.0\n",
      "fc layer 1 self.abs_max_out: 268.0\n",
      "lif layer 1 self.abs_max_v: 356.5\n",
      "fc layer 2 self.abs_max_out: 271.0\n",
      "fc layer 1 self.abs_max_out: 332.0\n",
      "fc layer 2 self.abs_max_out: 318.0\n",
      "lif layer 2 self.abs_max_v: 431.5\n",
      "lif layer 2 self.abs_max_v: 436.0\n",
      "fc layer 1 self.abs_max_out: 382.0\n",
      "lif layer 1 self.abs_max_v: 439.5\n",
      "fc layer 3 self.abs_max_out: 159.0\n",
      "fc layer 2 self.abs_max_out: 352.0\n",
      "lif layer 2 self.abs_max_v: 517.5\n",
      "lif layer 2 self.abs_max_v: 563.5\n",
      "fc layer 2 self.abs_max_out: 418.0\n",
      "lif layer 2 self.abs_max_v: 639.0\n",
      "fc layer 1 self.abs_max_out: 403.0\n",
      "fc layer 1 self.abs_max_out: 472.0\n",
      "lif layer 1 self.abs_max_v: 673.5\n",
      "lif layer 1 self.abs_max_v: 750.0\n",
      "fc layer 1 self.abs_max_out: 510.0\n",
      "lif layer 1 self.abs_max_v: 852.0\n",
      "fc layer 1 self.abs_max_out: 572.0\n",
      "fc layer 2 self.abs_max_out: 452.0\n",
      "fc layer 2 self.abs_max_out: 482.0\n",
      "fc layer 2 self.abs_max_out: 541.0\n",
      "fc layer 3 self.abs_max_out: 196.0\n",
      "fc layer 1 self.abs_max_out: 575.0\n",
      "lif layer 2 self.abs_max_v: 677.5\n",
      "fc layer 2 self.abs_max_out: 591.0\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "fc layer 1 self.abs_max_out: 576.0\n",
      "fc layer 1 self.abs_max_out: 691.0\n",
      "fc layer 1 self.abs_max_out: 860.0\n",
      "lif layer 1 self.abs_max_v: 860.0\n",
      "fc layer 2 self.abs_max_out: 620.0\n",
      "lif layer 2 self.abs_max_v: 713.5\n",
      "fc layer 3 self.abs_max_out: 227.0\n",
      "lif layer 2 self.abs_max_v: 742.5\n",
      "lif layer 2 self.abs_max_v: 773.5\n",
      "lif layer 2 self.abs_max_v: 836.5\n",
      "lif layer 2 self.abs_max_v: 842.0\n",
      "lif layer 2 self.abs_max_v: 869.0\n",
      "lif layer 2 self.abs_max_v: 888.5\n",
      "lif layer 1 self.abs_max_v: 877.0\n",
      "lif layer 1 self.abs_max_v: 877.5\n",
      "lif layer 1 self.abs_max_v: 879.0\n",
      "fc layer 2 self.abs_max_out: 651.0\n",
      "fc layer 2 self.abs_max_out: 720.0\n",
      "fc layer 2 self.abs_max_out: 722.0\n",
      "lif layer 2 self.abs_max_v: 979.5\n",
      "fc layer 3 self.abs_max_out: 228.0\n",
      "lif layer 1 self.abs_max_v: 937.0\n",
      "fc layer 3 self.abs_max_out: 245.0\n",
      "fc layer 3 self.abs_max_out: 257.0\n",
      "fc layer 3 self.abs_max_out: 273.0\n",
      "fc layer 1 self.abs_max_out: 941.0\n",
      "lif layer 1 self.abs_max_v: 941.0\n",
      "fc layer 1 self.abs_max_out: 1036.0\n",
      "lif layer 1 self.abs_max_v: 1036.0\n",
      "fc layer 3 self.abs_max_out: 274.0\n",
      "fc layer 2 self.abs_max_out: 764.0\n",
      "lif layer 1 self.abs_max_v: 1049.5\n",
      "lif layer 1 self.abs_max_v: 1140.5\n",
      "lif layer 2 self.abs_max_v: 1071.0\n",
      "lif layer 1 self.abs_max_v: 1189.5\n",
      "fc layer 2 self.abs_max_out: 836.0\n",
      "fc layer 3 self.abs_max_out: 285.0\n",
      "fc layer 1 self.abs_max_out: 1065.0\n",
      "fc layer 3 self.abs_max_out: 289.0\n",
      "fc layer 2 self.abs_max_out: 926.0\n",
      "fc layer 1 self.abs_max_out: 1137.0\n",
      "fc layer 3 self.abs_max_out: 323.0\n",
      "lif layer 1 self.abs_max_v: 1218.5\n",
      "fc layer 3 self.abs_max_out: 336.0\n",
      "lif layer 2 self.abs_max_v: 1272.5\n",
      "fc layer 2 self.abs_max_out: 930.0\n",
      "fc layer 1 self.abs_max_out: 1153.0\n",
      "lif layer 1 self.abs_max_v: 1310.0\n",
      "fc layer 1 self.abs_max_out: 1193.0\n",
      "lif layer 1 self.abs_max_v: 1324.0\n",
      "lif layer 1 self.abs_max_v: 1371.5\n",
      "lif layer 1 self.abs_max_v: 1444.0\n",
      "fc layer 2 self.abs_max_out: 940.0\n",
      "fc layer 1 self.abs_max_out: 1297.0\n",
      "lif layer 1 self.abs_max_v: 1511.5\n",
      "lif layer 1 self.abs_max_v: 1572.5\n",
      "lif layer 1 self.abs_max_v: 1624.0\n",
      "lif layer 1 self.abs_max_v: 1713.0\n",
      "fc layer 1 self.abs_max_out: 1443.0\n",
      "fc layer 1 self.abs_max_out: 1465.0\n",
      "fc layer 2 self.abs_max_out: 941.0\n",
      "lif layer 1 self.abs_max_v: 1799.5\n",
      "fc layer 1 self.abs_max_out: 1661.0\n",
      "fc layer 2 self.abs_max_out: 1023.0\n",
      "fc layer 3 self.abs_max_out: 349.0\n",
      "fc layer 3 self.abs_max_out: 370.0\n",
      "lif layer 2 self.abs_max_v: 1321.0\n",
      "lif layer 1 self.abs_max_v: 1838.0\n",
      "lif layer 1 self.abs_max_v: 1864.0\n",
      "lif layer 2 self.abs_max_v: 1323.0\n",
      "fc layer 1 self.abs_max_out: 1675.0\n",
      "fc layer 2 self.abs_max_out: 1030.0\n",
      "fc layer 3 self.abs_max_out: 371.0\n",
      "fc layer 3 self.abs_max_out: 373.0\n",
      "lif layer 1 self.abs_max_v: 1933.5\n",
      "fc layer 1 self.abs_max_out: 1704.0\n",
      "lif layer 1 self.abs_max_v: 1971.0\n",
      "lif layer 1 self.abs_max_v: 2119.0\n",
      "lif layer 1 self.abs_max_v: 2136.0\n",
      "lif layer 2 self.abs_max_v: 1333.0\n",
      "lif layer 1 self.abs_max_v: 2243.0\n",
      "fc layer 2 self.abs_max_out: 1043.0\n",
      "fc layer 2 self.abs_max_out: 1079.0\n",
      "lif layer 2 self.abs_max_v: 1334.5\n",
      "lif layer 2 self.abs_max_v: 1354.5\n",
      "lif layer 2 self.abs_max_v: 1421.0\n",
      "lif layer 2 self.abs_max_v: 1485.5\n",
      "fc layer 1 self.abs_max_out: 1753.0\n",
      "fc layer 1 self.abs_max_out: 1834.0\n",
      "fc layer 2 self.abs_max_out: 1168.0\n",
      "fc layer 2 self.abs_max_out: 1230.0\n",
      "lif layer 1 self.abs_max_v: 2511.5\n",
      "lif layer 1 self.abs_max_v: 2539.5\n",
      "lif layer 1 self.abs_max_v: 2577.5\n",
      "fc layer 1 self.abs_max_out: 1838.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 13.743591/ 64.451202, val:  43.33%, val_best:  43.33%, tr:  96.53%, tr_best:  96.53%, epoch time: 67.09 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0944%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5359%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.9007%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2127  21.726%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 375.0\n",
      "fc layer 3 self.abs_max_out: 381.0\n",
      "fc layer 1 self.abs_max_out: 1906.0\n",
      "fc layer 1 self.abs_max_out: 2122.0\n",
      "fc layer 3 self.abs_max_out: 386.0\n",
      "fc layer 3 self.abs_max_out: 396.0\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "fc layer 3 self.abs_max_out: 428.0\n",
      "lif layer 2 self.abs_max_v: 1493.0\n",
      "fc layer 3 self.abs_max_out: 446.0\n",
      "lif layer 2 self.abs_max_v: 1524.0\n",
      "lif layer 2 self.abs_max_v: 1560.0\n",
      "fc layer 3 self.abs_max_out: 461.0\n",
      "lif layer 2 self.abs_max_v: 1589.0\n",
      "lif layer 2 self.abs_max_v: 1600.0\n",
      "lif layer 1 self.abs_max_v: 2625.5\n",
      "lif layer 2 self.abs_max_v: 1618.5\n",
      "lif layer 2 self.abs_max_v: 1635.0\n",
      "lif layer 1 self.abs_max_v: 2872.0\n",
      "lif layer 1 self.abs_max_v: 3041.5\n",
      "lif layer 1 self.abs_max_v: 3097.0\n",
      "lif layer 1 self.abs_max_v: 3234.5\n",
      "lif layer 2 self.abs_max_v: 1644.0\n",
      "lif layer 2 self.abs_max_v: 1693.5\n",
      "lif layer 2 self.abs_max_v: 1735.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.245502/ 66.372498, val:  35.83%, val_best:  43.33%, tr:  99.28%, tr_best:  99.28%, epoch time: 66.70 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1120%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4533%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5360%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3733  19.065%\n",
      "fc layer 1 self.abs_max_out: 2159.0\n",
      "lif layer 2 self.abs_max_v: 1812.5\n",
      "lif layer 2 self.abs_max_v: 1890.0\n",
      "fc layer 3 self.abs_max_out: 469.0\n",
      "fc layer 2 self.abs_max_out: 1234.0\n",
      "lif layer 1 self.abs_max_v: 3281.5\n",
      "lif layer 1 self.abs_max_v: 3481.0\n",
      "lif layer 1 self.abs_max_v: 3661.5\n",
      "lif layer 2 self.abs_max_v: 1957.0\n",
      "fc layer 1 self.abs_max_out: 2333.0\n",
      "lif layer 1 self.abs_max_v: 4047.5\n",
      "fc layer 3 self.abs_max_out: 510.0\n",
      "lif layer 1 self.abs_max_v: 4096.5\n",
      "lif layer 1 self.abs_max_v: 4335.5\n",
      "lif layer 1 self.abs_max_v: 4387.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.945502/ 73.414581, val:  38.75%, val_best:  43.33%, tr:  99.49%, tr_best:  99.49%, epoch time: 66.58 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0971%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5321%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5188  17.664%\n",
      "fc layer 1 self.abs_max_out: 2340.0\n",
      "lif layer 2 self.abs_max_v: 2002.5\n",
      "fc layer 1 self.abs_max_out: 2408.0\n",
      "fc layer 1 self.abs_max_out: 2532.0\n",
      "lif layer 1 self.abs_max_v: 4413.5\n",
      "fc layer 1 self.abs_max_out: 2633.0\n",
      "lif layer 1 self.abs_max_v: 4840.0\n",
      "fc layer 1 self.abs_max_out: 2682.0\n",
      "lif layer 1 self.abs_max_v: 5102.0\n",
      "lif layer 1 self.abs_max_v: 5152.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss: 10.050700/ 66.211517, val:  38.33%, val_best:  43.33%, tr:  99.59%, tr_best:  99.59%, epoch time: 66.36 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8440%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6612  16.885%\n",
      "fc layer 2 self.abs_max_out: 1266.0\n",
      "fc layer 2 self.abs_max_out: 1267.0\n",
      "fc layer 2 self.abs_max_out: 1276.0\n",
      "fc layer 2 self.abs_max_out: 1284.0\n",
      "fc layer 2 self.abs_max_out: 1340.0\n",
      "fc layer 1 self.abs_max_out: 2685.0\n",
      "fc layer 1 self.abs_max_out: 2760.0\n",
      "fc layer 1 self.abs_max_out: 2816.0\n",
      "lif layer 1 self.abs_max_v: 5377.5\n",
      "lif layer 1 self.abs_max_v: 5461.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  9.750998/ 57.675274, val:  46.67%, val_best:  46.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 67.33 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8799%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9184%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 7971  16.284%\n",
      "fc layer 2 self.abs_max_out: 1343.0\n",
      "lif layer 2 self.abs_max_v: 2045.0\n",
      "fc layer 2 self.abs_max_out: 1361.0\n",
      "lif layer 2 self.abs_max_v: 2116.5\n",
      "lif layer 2 self.abs_max_v: 2171.0\n",
      "fc layer 2 self.abs_max_out: 1419.0\n",
      "fc layer 1 self.abs_max_out: 2847.0\n",
      "fc layer 1 self.abs_max_out: 2892.0\n",
      "lif layer 1 self.abs_max_v: 5528.0\n",
      "lif layer 1 self.abs_max_v: 5573.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  8.494034/ 54.386467, val:  47.50%, val_best:  47.50%, tr:  99.69%, tr_best:  99.69%, epoch time: 67.97 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0615%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2786%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.0322%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9212  15.683%\n",
      "lif layer 2 self.abs_max_v: 2182.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss: 10.118606/ 43.701809, val:  48.33%, val_best:  48.33%, tr:  99.18%, tr_best:  99.69%, epoch time: 67.06 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.4440%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4441%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10552  15.398%\n",
      "fc layer 3 self.abs_max_out: 518.0\n",
      "fc layer 3 self.abs_max_out: 542.0\n",
      "lif layer 2 self.abs_max_v: 2294.0\n",
      "fc layer 1 self.abs_max_out: 2904.0\n",
      "fc layer 1 self.abs_max_out: 2943.0\n",
      "lif layer 1 self.abs_max_v: 5634.0\n",
      "lif layer 1 self.abs_max_v: 5674.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.820608/ 55.271572, val:  52.08%, val_best:  52.08%, tr:  99.69%, tr_best:  99.69%, epoch time: 67.55 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0243%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3899%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11799  15.065%\n",
      "fc layer 2 self.abs_max_out: 1425.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.872349/ 57.595238, val:  47.92%, val_best:  52.08%, tr:  99.90%, tr_best:  99.90%, epoch time: 66.98 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1330%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.9608%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13006  14.761%\n",
      "fc layer 2 self.abs_max_out: 1471.0\n",
      "lif layer 2 self.abs_max_v: 2307.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.460210/ 45.096928, val:  55.83%, val_best:  55.83%, tr:  99.80%, tr_best:  99.90%, epoch time: 67.27 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0053%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7095%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14205  14.510%\n",
      "fc layer 3 self.abs_max_out: 557.0\n",
      "lif layer 2 self.abs_max_v: 2413.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.222966/ 48.455856, val:  55.83%, val_best:  55.83%, tr:  99.69%, tr_best:  99.90%, epoch time: 67.06 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1117%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9927%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15363  14.266%\n",
      "fc layer 3 self.abs_max_out: 568.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  8.330995/ 49.839821, val:  56.67%, val_best:  56.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 66.96 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0597%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3548%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6433%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16526  14.067%\n",
      "fc layer 2 self.abs_max_out: 1564.0\n",
      "lif layer 2 self.abs_max_v: 2416.5\n",
      "lif layer 2 self.abs_max_v: 2447.5\n",
      "lif layer 2 self.abs_max_v: 2503.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  8.229495/ 52.391403, val:  57.50%, val_best:  57.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.08 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0980%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5210%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8757%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17676  13.889%\n",
      "fc layer 1 self.abs_max_out: 3116.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  8.087852/ 48.670040, val:  56.25%, val_best:  57.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.48 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1015%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6787%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2278%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18829  13.738%\n",
      "fc layer 3 self.abs_max_out: 584.0\n",
      "fc layer 3 self.abs_max_out: 589.0\n",
      "fc layer 3 self.abs_max_out: 590.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  8.084480/ 62.131584, val:  48.33%, val_best:  57.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0915%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4054%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5422%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 19997  13.617%\n",
      "fc layer 3 self.abs_max_out: 598.0\n",
      "fc layer 3 self.abs_max_out: 621.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.974510/ 54.217331, val:  64.58%, val_best:  64.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.40 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3139%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.9248%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21090  13.464%\n",
      "fc layer 1 self.abs_max_out: 3143.0\n",
      "lif layer 2 self.abs_max_v: 2520.5\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.739864/ 39.748322, val:  68.75%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.10 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1091%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8812%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22149  13.308%\n",
      "fc layer 1 self.abs_max_out: 3181.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  8.213752/ 38.055218, val:  62.92%, val_best:  68.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.39 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0868%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3568%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23271  13.206%\n",
      "lif layer 2 self.abs_max_v: 2538.5\n",
      "fc layer 2 self.abs_max_out: 1598.0\n",
      "fc layer 2 self.abs_max_out: 1608.0\n",
      "fc layer 2 self.abs_max_out: 1668.0\n",
      "fc layer 2 self.abs_max_out: 1684.0\n",
      "fc layer 2 self.abs_max_out: 1725.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.888866/ 61.068306, val:  53.75%, val_best:  68.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.42 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0968%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5859%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.7150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24395  13.115%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.305904/ 41.728573, val:  77.50%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 68.32 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8312%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0849%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25446  12.996%\n",
      "lif layer 2 self.abs_max_v: 2547.0\n",
      "lif layer 2 self.abs_max_v: 2636.5\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  7.098232/ 43.090950, val:  63.75%, val_best:  77.50%, tr:  99.59%, tr_best: 100.00%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6877%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0183%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26466  12.873%\n",
      "lif layer 2 self.abs_max_v: 2659.5\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  7.658313/ 60.430172, val:  55.00%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.17 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1052%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7428%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7904%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27567  12.799%\n",
      "lif layer 2 self.abs_max_v: 2660.5\n",
      "fc layer 1 self.abs_max_out: 3284.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.926244/ 47.784187, val:  68.33%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.34 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0611%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2154%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8067%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28595  12.699%\n",
      "lif layer 2 self.abs_max_v: 2669.5\n",
      "fc layer 1 self.abs_max_out: 3448.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  7.366230/ 61.799080, val:  55.00%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.35 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1033%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8304%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29630  12.611%\n",
      "fc layer 3 self.abs_max_out: 630.0\n",
      "fc layer 1 self.abs_max_out: 3591.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  7.625081/ 82.076515, val:  47.08%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.67 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0524%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7846%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0374%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30683  12.536%\n",
      "fc layer 2 self.abs_max_out: 1738.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.859641/ 35.366310, val:  69.17%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 66.98 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0792%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0856%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31640  12.430%\n",
      "fc layer 2 self.abs_max_out: 1768.0\n",
      "fc layer 2 self.abs_max_out: 1808.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.858180/ 42.221184, val:  66.67%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.51 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0689%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0955%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0321%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32614  12.338%\n",
      "fc layer 2 self.abs_max_out: 1820.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.543699/ 44.980618, val:  69.58%, val_best:  77.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.26 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0760%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3553%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7630%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33555  12.241%\n",
      "lif layer 2 self.abs_max_v: 2670.5\n",
      "lif layer 2 self.abs_max_v: 2794.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  7.014157/ 43.485607, val:  67.50%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.98 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0473%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3232%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2244%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 34533  12.163%\n",
      "fc layer 2 self.abs_max_out: 1839.0\n",
      "fc layer 2 self.abs_max_out: 1849.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  5.904025/ 55.547783, val:  67.08%, val_best:  77.50%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.76 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0761%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3885%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2192%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35432  12.064%\n",
      "fc layer 3 self.abs_max_out: 646.0\n",
      "fc layer 3 self.abs_max_out: 657.0\n",
      "lif layer 1 self.abs_max_v: 5722.5\n",
      "fc layer 2 self.abs_max_out: 1857.0\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  6.163903/ 43.146866, val:  68.75%, val_best:  77.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1025%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2566%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6242%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 36322  11.968%\n",
      "fc layer 2 self.abs_max_out: 1858.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.726563/ 31.544252, val:  78.33%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.08 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0780%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4769%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 37239  11.887%\n",
      "lif layer 1 self.abs_max_v: 5733.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.975268/ 39.454510, val:  75.42%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.00 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0852%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9581%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3271%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 38120  11.799%\n",
      "lif layer 1 self.abs_max_v: 5767.0\n",
      "lif layer 1 self.abs_max_v: 5894.5\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  6.317542/ 39.242504, val:  78.33%, val_best:  78.33%, tr:  99.80%, tr_best: 100.00%, epoch time: 66.79 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1035%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6945%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 39039  11.728%\n",
      "lif layer 2 self.abs_max_v: 2829.5\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.533653/ 45.780235, val:  69.58%, val_best:  78.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.30 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0783%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6750%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5636%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 39839  11.627%\n",
      "lif layer 2 self.abs_max_v: 2884.5\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  6.111243/ 59.887115, val:  62.08%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 66.81 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8027%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5192%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 40694  11.546%\n",
      "fc layer 2 self.abs_max_out: 1890.0\n",
      "lif layer 1 self.abs_max_v: 5962.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.866638/ 56.723541, val:  58.75%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.05 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9876%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2494%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 41558  11.473%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  6.028048/ 43.968922, val:  71.67%, val_best:  78.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.38 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0988%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5646%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 42438  11.407%\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  6.019270/ 35.048004, val:  79.17%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.32 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9183%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9443%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 43280  11.335%\n",
      "lif layer 2 self.abs_max_v: 2892.5\n",
      "fc layer 2 self.abs_max_out: 1928.0\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.873864/ 45.500614, val:  65.42%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.45 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9827%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8724%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 44115  11.265%\n",
      "fc layer 3 self.abs_max_out: 661.0\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.882421/ 37.261845, val:  74.58%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 63.85 seconds, 1.06 minutes\n",
      "layer   1  Sparsity: 91.0609%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9893%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 44950  11.199%\n",
      "lif layer 2 self.abs_max_v: 2924.5\n",
      "lif layer 2 self.abs_max_v: 2960.0\n",
      "fc layer 1 self.abs_max_out: 3762.0\n",
      "lif layer 1 self.abs_max_v: 5967.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.466735/ 59.654312, val:  60.42%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.78 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8235%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 45778  11.133%\n",
      "fc layer 3 self.abs_max_out: 681.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.469276/ 39.663593, val:  78.33%, val_best:  79.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.61 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0662%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7164%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3198%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 46552  11.058%\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.298577/ 42.407269, val:  76.67%, val_best:  79.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.86 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0818%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8160%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 47347  10.992%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  5.285617/ 35.563869, val:  82.92%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.26 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3126%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3322%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 48103  10.919%\n",
      "lif layer 1 self.abs_max_v: 6243.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  5.522286/ 45.169762, val:  75.00%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.26 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0954%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9951%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6174%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 48885  10.855%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  5.571105/ 36.593208, val:  78.33%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.20 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0915%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8195%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3015%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 49672  10.795%\n",
      "fc layer 3 self.abs_max_out: 689.0\n",
      "fc layer 3 self.abs_max_out: 691.0\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  5.063000/ 39.359856, val:  80.42%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7927%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 50380  10.721%\n",
      "lif layer 2 self.abs_max_v: 2977.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  5.191535/ 42.338680, val:  74.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.66 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0496%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7526%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2589%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 51139  10.660%\n",
      "fc layer 1 self.abs_max_out: 3940.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.587798/ 48.819981, val:  66.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.76 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0366%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3848%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9099%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 51829  10.588%\n",
      "fc layer 1 self.abs_max_out: 3967.0\n",
      "fc layer 3 self.abs_max_out: 707.0\n",
      "fc layer 2 self.abs_max_out: 1944.0\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.960031/ 42.211647, val:  75.83%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.20 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0590%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4155%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1879%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 52589  10.533%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.373207/ 36.910164, val:  80.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.03 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0923%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3758%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9132%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 53248  10.460%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.683657/ 38.663380, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.79 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0644%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3774%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9556%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 53945  10.397%\n",
      "fc layer 2 self.abs_max_out: 1983.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  5.005929/ 39.444744, val:  77.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.34 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0836%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1652%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7695%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 54662  10.340%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.918539/ 44.334446, val:  74.58%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.65 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1550%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 55378  10.285%\n",
      "fc layer 3 self.abs_max_out: 717.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.566323/ 47.326080, val:  74.17%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.58 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0909%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9091%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 56068  10.227%\n",
      "fc layer 2 self.abs_max_out: 1998.0\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.391407/ 42.863129, val:  75.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.35 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0984%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6259%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 56713  10.163%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.877417/ 58.354744, val:  73.33%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.13 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9749%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6151%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 57416  10.112%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.323257/ 47.536392, val:  78.33%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.80 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6172%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9765%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 58053  10.051%\n",
      "fc layer 2 self.abs_max_out: 2074.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.778685/ 35.463932, val:  81.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.08 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0568%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4842%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2496%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 58744  10.001%\n",
      "fc layer 1 self.abs_max_out: 3981.0\n",
      "lif layer 2 self.abs_max_v: 3019.5\n",
      "lif layer 1 self.abs_max_v: 6255.5\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.714395/ 54.674660, val:  79.17%, val_best:  82.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 66.59 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1074%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9656%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7414%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 59447   9.954%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.468952/ 43.117989, val:  75.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.74 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0756%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8502%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7100%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 60099   9.901%\n",
      "fc layer 3 self.abs_max_out: 730.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.206800/ 48.628387, val:  71.67%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.97 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0772%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9891%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0183%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 60711   9.843%\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.114241/ 47.514580, val:  75.42%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.89 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9607%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2523%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 61327   9.788%\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.147496/ 32.767006, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.27 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0969%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7438%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 61943   9.734%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  4.311354/ 41.390736, val:  82.50%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.99 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5974%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2218%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 62584   9.686%\n",
      "fc layer 1 self.abs_max_out: 4045.0\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.456276/ 37.737926, val:  80.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.25 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0525%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7599%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6986%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 63213   9.637%\n",
      "lif layer 2 self.abs_max_v: 3046.5\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  4.386482/ 43.464592, val:  76.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.07 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1316%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1010%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7498%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 63873   9.595%\n",
      "lif layer 2 self.abs_max_v: 3211.0\n",
      "fc layer 1 self.abs_max_out: 4187.0\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.016110/ 38.778980, val:  84.58%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.30 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9075%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 64508   9.550%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.868137/ 50.147095, val:  72.08%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.82 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0808%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8897%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 65093   9.498%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.872100/ 47.239098, val:  78.33%, val_best:  84.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.84 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1235%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0604%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0414%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 65677   9.449%\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  4.323188/ 35.871723, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.91 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8715%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7783%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 66319   9.409%\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.826113/ 45.614506, val:  75.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.07 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0859%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9758%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7835%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 66921   9.364%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.972335/ 49.291389, val:  72.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.26 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1058%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8107%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8977%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 67493   9.316%\n",
      "lif layer 1 self.abs_max_v: 6296.5\n",
      "lif layer 1 self.abs_max_v: 6315.5\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.536830/ 38.903492, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8749%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6112%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 68064   9.270%\n",
      "fc layer 3 self.abs_max_out: 732.0\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.392930/ 40.922089, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0525%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6302%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 68600   9.220%\n",
      "fc layer 2 self.abs_max_out: 2092.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.960209/ 38.479691, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.55 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0579%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6214%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3356%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 69196   9.179%\n",
      "lif layer 2 self.abs_max_v: 3212.0\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.596023/ 65.937599, val:  71.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.76 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0774%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8359%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6935%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 69737   9.132%\n",
      "fc layer 1 self.abs_max_out: 4218.0\n",
      "lif layer 2 self.abs_max_v: 3245.0\n",
      "lif layer 2 self.abs_max_v: 3269.0\n",
      "fc layer 3 self.abs_max_out: 743.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.787933/ 54.525299, val:  77.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.09 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0766%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6876%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7148%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 70310   9.091%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.527131/ 41.188583, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.99 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1233%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 70829   9.044%\n",
      "fc layer 2 self.abs_max_out: 2140.0\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.388839/ 35.929543, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.54 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5781%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 71336   8.996%\n",
      "fc layer 3 self.abs_max_out: 755.0\n",
      "fc layer 2 self.abs_max_out: 2227.0\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.537500/ 39.835575, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.41 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7144%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9534%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 71870   8.953%\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.686417/ 36.634666, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4414%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4460%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 72416   8.912%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.641356/ 39.999439, val:  78.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.99 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5040%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5773%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 72941   8.870%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.337202/ 53.326817, val:  68.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.59 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5852%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7905%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 73456   8.827%\n",
      "fc layer 2 self.abs_max_out: 2232.0\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.651457/ 47.849438, val:  78.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.15 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1154%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4970%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6225%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 73985   8.787%\n",
      "fc layer 1 self.abs_max_out: 4394.0\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.694571/ 36.958374, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.93 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0226%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0132%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 74525   8.750%\n",
      "lif layer 1 self.abs_max_v: 6405.5\n",
      "fc layer 2 self.abs_max_out: 2234.0\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.650917/ 42.059788, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9669%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5868%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 75088   8.716%\n",
      "fc layer 3 self.abs_max_out: 771.0\n",
      "lif layer 1 self.abs_max_v: 6621.5\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.421408/ 44.007236, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.69 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0436%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0393%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6193%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 75599   8.676%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.284199/ 33.433540, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.65 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0542%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0748%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0692%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 76106   8.638%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.516239/ 39.185398, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.19 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9916%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2972%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 76629   8.601%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.174197/ 45.516151, val:  77.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.25 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0486%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2007%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 77124   8.563%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.110734/ 40.960209, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.25 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4517%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1296%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 77600   8.523%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.476686/ 42.668827, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.01 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0909%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3671%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5553%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 78100   8.487%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  3.173014/ 38.975163, val:  84.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.72 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0782%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2819%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9048%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 78577   8.449%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  3.412469/ 43.994026, val:  80.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.58 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1210%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4082%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7735%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 79072   8.413%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  3.539860/ 51.929001, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.86 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6465%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1355%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 79593   8.381%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  3.005438/ 40.387074, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.13 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3770%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4795%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 80074   8.346%\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  3.456326/ 34.133625, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.31 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0489%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3090%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2862%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 80559   8.312%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  3.213151/ 41.918533, val:  81.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.43 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0548%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2987%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2578%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 81026   8.276%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.935543/ 41.152897, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.60 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0755%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0902%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 81495   8.242%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  3.252614/ 38.271671, val:  86.67%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.70 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8171%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9030%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 81991   8.211%\n",
      "lif layer 1 self.abs_max_v: 6728.5\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  3.198395/ 47.869061, val:  78.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.02 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9981%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 82472   8.179%\n",
      "fc layer 3 self.abs_max_out: 776.0\n",
      "fc layer 3 self.abs_max_out: 786.0\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  3.169503/ 44.035400, val:  82.92%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.90 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7141%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9936%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 82962   8.148%\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.828290/ 38.858528, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.49 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0705%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0159%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 83399   8.113%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.883839/ 38.815590, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.90 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6692%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4071%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 83868   8.082%\n",
      "lif layer 1 self.abs_max_v: 6845.0\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  3.012284/ 44.928890, val:  80.42%, val_best:  87.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7344%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4725%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 84318   8.049%\n",
      "fc layer 1 self.abs_max_out: 4437.0\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.597291/ 40.465359, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.21 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0473%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5660%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2673%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 84739   8.015%\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.963193/ 42.970257, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.61 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0869%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5190%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6298%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 85212   7.985%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.494391/ 41.721725, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.83 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0972%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4264%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 85605   7.949%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  3.457393/ 52.803299, val:  78.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.58 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1209%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5106%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7243%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 86086   7.922%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.852395/ 48.252823, val:  77.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.09 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0856%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5783%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0935%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 86509   7.890%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.924675/ 38.898315, val:  85.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.26 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0518%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6502%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9095%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 86938   7.859%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.912099/ 42.606239, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.19 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4662%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 87358   7.827%\n",
      "fc layer 3 self.abs_max_out: 787.0\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.663007/ 41.366879, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.39 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0983%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5417%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1024%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 87805   7.799%\n",
      "fc layer 3 self.abs_max_out: 799.0\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.840386/ 37.664963, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.72 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0657%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5534%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3832%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 88225   7.769%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.559831/ 49.755829, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.51 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6837%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3387%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 88626   7.737%\n",
      "fc layer 3 self.abs_max_out: 800.0\n",
      "lif layer 1 self.abs_max_v: 6895.5\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.699495/ 34.511745, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.50 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0922%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7516%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7942%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 89050   7.708%\n",
      "fc layer 1 self.abs_max_out: 4525.0\n",
      "fc layer 2 self.abs_max_out: 2378.0\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.548807/ 38.004780, val:  86.25%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.69 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0994%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7051%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4592%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 89427   7.676%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.697759/ 44.587482, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.63 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0872%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6051%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6034%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 89823   7.646%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  3.135056/ 38.630566, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.12 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0970%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6349%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 90277   7.621%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.461868/ 38.423420, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.06 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4894%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5099%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 90662   7.591%\n",
      "fc layer 1 self.abs_max_out: 4619.0\n",
      "fc layer 3 self.abs_max_out: 801.0\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.288213/ 39.869308, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.29 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0663%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6251%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1325%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 91027   7.559%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.684783/ 34.277424, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.89 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1010%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6847%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1959%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 91415   7.530%\n",
      "lif layer 2 self.abs_max_v: 3344.0\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.416464/ 42.026814, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.05 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6565%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7642%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 91817   7.503%\n",
      "lif layer 2 self.abs_max_v: 3374.5\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.318525/ 42.549522, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.92 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0799%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8660%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5382%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 92184   7.473%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.340848/ 40.427151, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.41 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0527%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8217%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 92525   7.442%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.257412/ 44.249561, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.74 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0973%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5232%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2681%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 92876   7.412%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.589298/ 51.140411, val:  77.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.47 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0631%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7011%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5094%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 93280   7.386%\n",
      "fc layer 3 self.abs_max_out: 803.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.523536/ 43.240517, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.61 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5347%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9872%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 93668   7.360%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.475402/ 41.608559, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.87 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0620%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8583%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6930%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 94034   7.332%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.687613/ 37.482201, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.35 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0886%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 94422   7.307%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.219258/ 48.269814, val:  80.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.27 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0490%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4519%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6227%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 94781   7.279%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.306392/ 41.201557, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.80 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0372%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5964%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3638%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 95124   7.251%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.369530/ 44.842968, val:  81.67%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.44 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7807%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7993%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 95491   7.225%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.686461/ 34.282879, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.87 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0676%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0079%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 95884   7.202%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  2.131966/ 45.413696, val:  82.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.74 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0706%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8189%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8778%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 96225   7.174%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.342751/ 39.610783, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.13 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1145%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6511%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9955%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 96559   7.147%\n",
      "fc layer 3 self.abs_max_out: 806.0\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  2.286141/ 44.645576, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.26 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0917%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6719%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1217%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 96900   7.121%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.252054/ 34.340435, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.45 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1067%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7225%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 97256   7.096%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  2.129763/ 40.097343, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.22 seconds, 1.10 minutes\n",
      "layer   1  Sparsity: 91.1142%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6827%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6354%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 97584   7.069%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.538162/ 42.904671, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.28 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0741%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4107%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4914%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 97940   7.045%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  2.150050/ 38.651779, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.65 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0608%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8642%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6414%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 98276   7.020%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  2.515533/ 41.476807, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.67 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8561%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7630%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 98650   6.998%\n",
      "fc layer 1 self.abs_max_out: 4647.0\n",
      "lif layer 2 self.abs_max_v: 3398.0\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  2.042073/ 47.248150, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.30 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1245%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8800%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5951%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 98976   6.972%\n",
      "lif layer 2 self.abs_max_v: 3472.0\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  2.300430/ 41.278419, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.49 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0251%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3389%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 99331   6.949%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  2.101342/ 38.390198, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.24 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0639%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9336%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6576%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 99662   6.925%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.940655/ 42.237514, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.56 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6040%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2274%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 99946   6.898%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  2.082333/ 44.812054, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.03 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4986%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1109%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 100277   6.874%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  2.025430/ 44.691017, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.44 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5725%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6235%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 100602   6.851%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  2.110369/ 37.370502, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.99 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0887%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6412%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3028%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 100930   6.827%\n",
      "fc layer 3 self.abs_max_out: 837.0\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.997784/ 38.361374, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.73 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0632%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6962%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0699%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 101267   6.805%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  2.330132/ 40.371841, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.13 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1073%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5569%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0616%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 101604   6.783%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.916629/ 37.833588, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.89 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1256%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4723%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0499%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 101904   6.759%\n",
      "lif layer 2 self.abs_max_v: 3584.0\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  2.007811/ 36.603367, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.74 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5375%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2442%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 102206   6.735%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.786614/ 43.266487, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.12 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0923%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4556%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7005%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 102480   6.710%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.986577/ 45.202785, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.74 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0929%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2772%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4584%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 102787   6.687%\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  2.006921/ 47.230865, val:  84.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.26 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.2584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6322%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 103109   6.666%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.787166/ 47.898899, val:  82.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.92 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1131%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4738%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2961%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 103392   6.642%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  2.122661/ 40.721085, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.35 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0892%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4442%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0475%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 103711   6.621%\n",
      "fc layer 1 self.abs_max_out: 4719.0\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.652116/ 39.463970, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.65 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0569%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5613%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3366%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 103985   6.597%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.652723/ 41.350857, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.85 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0816%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6113%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6279%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 104247   6.573%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.964669/ 44.248581, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.25 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0944%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6559%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1753%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 104537   6.551%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.835195/ 54.274921, val:  78.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.03 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1251%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6518%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0080%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 104807   6.528%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  2.016583/ 38.600967, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.51 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0468%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6332%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1819%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 105107   6.507%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.635985/ 41.976475, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.81 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6618%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5016%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 105378   6.484%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.842791/ 42.034187, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.16 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0512%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5014%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4910%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 105673   6.463%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.740501/ 41.418243, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.77 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0781%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5926%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 105955   6.442%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  2.094282/ 40.360207, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.11 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0937%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8166%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7369%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 106253   6.422%\n",
      "fc layer 3 self.abs_max_out: 850.0\n",
      "fc layer 3 self.abs_max_out: 853.0\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.838560/ 43.637127, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.34 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5346%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8882%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 106555   6.402%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.821383/ 44.756607, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.31 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0628%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6015%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8412%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 106816   6.381%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.909119/ 42.294800, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.24 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0540%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6822%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5589%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 107110   6.361%\n",
      "fc layer 1 self.abs_max_out: 4730.0\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.671960/ 44.888893, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.26 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6933%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7191%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 107364   6.339%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.910948/ 47.960159, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.45 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7788%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7929%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 107645   6.319%\n",
      "fc layer 1 self.abs_max_out: 4737.0\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.815545/ 41.625614, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.96 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0871%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7857%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1460%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 107901   6.298%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.669991/ 47.614307, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.14 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1069%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8628%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 108162   6.277%\n",
      "fc layer 1 self.abs_max_out: 4857.0\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.488810/ 45.427475, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.72 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0489%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3498%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3679%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 108411   6.256%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.559893/ 41.820110, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.99 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5198%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6510%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 108665   6.236%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.520339/ 42.860245, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.60 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1060%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5482%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 108929   6.216%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.937592/ 38.085056, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.17 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1342%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6993%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2539%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 109213   6.198%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.870807/ 43.101837, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.36 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6208%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6502%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 109481   6.178%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.959102/ 44.599236, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.51 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0988%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5817%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5673%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 109769   6.161%\n",
      "lif layer 2 self.abs_max_v: 3613.5\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.384168/ 43.053825, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.40 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0719%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7932%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2945%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 110013   6.141%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.745939/ 44.917065, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.90 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0415%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7079%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7736%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 110271   6.122%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.346018/ 48.897125, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.54 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7077%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7252%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 110499   6.101%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.742573/ 38.257545, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.91 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7949%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5044%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 110767   6.083%\n",
      "lif layer 2 self.abs_max_v: 3658.5\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.519174/ 46.959511, val:  80.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.80 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0578%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.4681%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 110990   6.063%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.698746/ 40.947792, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.23 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1196%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5349%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5545%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 111254   6.045%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.809170/ 43.248466, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.29 seconds, 1.10 minutes\n",
      "layer   1  Sparsity: 91.0343%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9357%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5892%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 111533   6.028%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.880131/ 39.354401, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.19 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9238%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6709%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 111799   6.010%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.544136/ 40.268051, val:  87.50%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.54 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0884%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3847%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 112050   5.992%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.715904/ 37.271450, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.55 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0884%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5474%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 112327   5.976%\n",
      "fc layer 3 self.abs_max_out: 854.0\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.517786/ 46.149078, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.44 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1030%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7317%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7644%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 112560   5.957%\n",
      "lif layer 1 self.abs_max_v: 7053.5\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.695102/ 38.701668, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.11 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0630%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.5944%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4595%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 112821   5.940%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.544009/ 39.427448, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.90 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0500%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7420%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9621%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 113079   5.923%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.544059/ 46.107193, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.45 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8695%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4234%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 113324   5.906%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.445653/ 35.721321, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.21 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0789%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6381%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9819%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 113562   5.888%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.193473/ 36.877899, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0854%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.3026%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1974%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 113782   5.870%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.352465/ 39.036514, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.26 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1897%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9790%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 114016   5.852%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.553686/ 38.914551, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.09 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0621%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.1798%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0251%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7316ba86dc4c13bd6ef278f773a264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÇ‚ñÅ‚ñá‚ñá‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.55369</td></tr><tr><td>val_acc_best</td><td>0.90417</td></tr><tr><td>val_acc_now</td><td>0.8875</td></tr><tr><td>val_loss</td><td>38.91455</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-sweep-58</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qbt1b9z0' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/qbt1b9z0</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_044347-qbt1b9z0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bu4w5jd6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 9967\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_082850-bu4w5jd6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bu4w5jd6' target=\"_blank\">sunny-sweep-64</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bu4w5jd6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bu4w5jd6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251214_082858_513', 'my_seed': 9967, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 100.0\n",
      "lif layer 1 self.abs_max_v: 100.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 152.0\n",
      "lif layer 1 self.abs_max_v: 200.5\n",
      "fc layer 2 self.abs_max_out: 119.0\n",
      "lif layer 2 self.abs_max_v: 119.0\n",
      "lif layer 1 self.abs_max_v: 240.5\n",
      "fc layer 1 self.abs_max_out: 166.0\n",
      "lif layer 1 self.abs_max_v: 242.0\n",
      "fc layer 2 self.abs_max_out: 149.0\n",
      "lif layer 2 self.abs_max_v: 175.0\n",
      "fc layer 3 self.abs_max_out: 22.0\n",
      "lif layer 1 self.abs_max_v: 285.0\n",
      "fc layer 2 self.abs_max_out: 183.0\n",
      "lif layer 2 self.abs_max_v: 212.0\n",
      "fc layer 3 self.abs_max_out: 34.0\n",
      "fc layer 1 self.abs_max_out: 252.0\n",
      "fc layer 2 self.abs_max_out: 202.0\n",
      "fc layer 3 self.abs_max_out: 52.0\n",
      "lif layer 1 self.abs_max_v: 298.5\n",
      "lif layer 1 self.abs_max_v: 320.0\n",
      "lif layer 2 self.abs_max_v: 227.5\n",
      "lif layer 1 self.abs_max_v: 371.0\n",
      "fc layer 2 self.abs_max_out: 259.0\n",
      "lif layer 2 self.abs_max_v: 320.5\n",
      "fc layer 3 self.abs_max_out: 102.0\n",
      "fc layer 2 self.abs_max_out: 288.0\n",
      "lif layer 2 self.abs_max_v: 379.5\n",
      "fc layer 1 self.abs_max_out: 267.0\n",
      "fc layer 2 self.abs_max_out: 296.0\n",
      "lif layer 2 self.abs_max_v: 404.5\n",
      "fc layer 1 self.abs_max_out: 354.0\n",
      "lif layer 1 self.abs_max_v: 435.0\n",
      "lif layer 1 self.abs_max_v: 507.5\n",
      "fc layer 2 self.abs_max_out: 389.0\n",
      "lif layer 2 self.abs_max_v: 431.0\n",
      "fc layer 2 self.abs_max_out: 391.0\n",
      "lif layer 2 self.abs_max_v: 579.0\n",
      "fc layer 3 self.abs_max_out: 134.0\n",
      "fc layer 1 self.abs_max_out: 393.0\n",
      "fc layer 3 self.abs_max_out: 138.0\n",
      "fc layer 1 self.abs_max_out: 497.0\n",
      "fc layer 3 self.abs_max_out: 149.0\n",
      "fc layer 2 self.abs_max_out: 520.0\n",
      "lif layer 2 self.abs_max_v: 611.5\n",
      "fc layer 2 self.abs_max_out: 546.0\n",
      "fc layer 1 self.abs_max_out: 554.0\n",
      "lif layer 1 self.abs_max_v: 554.0\n",
      "lif layer 2 self.abs_max_v: 646.5\n",
      "fc layer 3 self.abs_max_out: 238.0\n",
      "lif layer 2 self.abs_max_v: 712.5\n",
      "lif layer 1 self.abs_max_v: 683.5\n",
      "lif layer 1 self.abs_max_v: 702.0\n",
      "lif layer 1 self.abs_max_v: 843.0\n",
      "lif layer 1 self.abs_max_v: 873.5\n",
      "lif layer 1 self.abs_max_v: 933.0\n",
      "fc layer 1 self.abs_max_out: 572.0\n",
      "fc layer 1 self.abs_max_out: 586.0\n",
      "fc layer 1 self.abs_max_out: 618.0\n",
      "fc layer 1 self.abs_max_out: 638.0\n",
      "fc layer 1 self.abs_max_out: 678.0\n",
      "lif layer 2 self.abs_max_v: 724.0\n",
      "fc layer 1 self.abs_max_out: 698.0\n",
      "lif layer 2 self.abs_max_v: 731.0\n",
      "fc layer 1 self.abs_max_out: 728.0\n",
      "fc layer 1 self.abs_max_out: 787.0\n",
      "fc layer 1 self.abs_max_out: 861.0\n",
      "lif layer 2 self.abs_max_v: 738.0\n",
      "lif layer 2 self.abs_max_v: 802.0\n",
      "lif layer 2 self.abs_max_v: 827.0\n",
      "fc layer 2 self.abs_max_out: 553.0\n",
      "fc layer 2 self.abs_max_out: 601.0\n",
      "lif layer 1 self.abs_max_v: 943.5\n",
      "lif layer 2 self.abs_max_v: 827.5\n",
      "fc layer 2 self.abs_max_out: 611.0\n",
      "lif layer 2 self.abs_max_v: 899.0\n",
      "fc layer 2 self.abs_max_out: 650.0\n",
      "lif layer 2 self.abs_max_v: 916.5\n",
      "fc layer 3 self.abs_max_out: 298.0\n",
      "lif layer 1 self.abs_max_v: 949.5\n",
      "fc layer 1 self.abs_max_out: 885.0\n",
      "fc layer 2 self.abs_max_out: 739.0\n",
      "lif layer 1 self.abs_max_v: 1094.5\n",
      "lif layer 1 self.abs_max_v: 1202.5\n",
      "lif layer 2 self.abs_max_v: 996.5\n",
      "fc layer 1 self.abs_max_out: 920.0\n",
      "fc layer 1 self.abs_max_out: 1186.0\n",
      "lif layer 2 self.abs_max_v: 1039.5\n",
      "fc layer 2 self.abs_max_out: 758.0\n",
      "fc layer 2 self.abs_max_out: 764.0\n",
      "lif layer 1 self.abs_max_v: 1240.5\n",
      "fc layer 2 self.abs_max_out: 774.0\n",
      "lif layer 1 self.abs_max_v: 1296.0\n",
      "fc layer 2 self.abs_max_out: 809.0\n",
      "fc layer 2 self.abs_max_out: 817.0\n",
      "fc layer 2 self.abs_max_out: 858.0\n",
      "fc layer 3 self.abs_max_out: 305.0\n",
      "lif layer 2 self.abs_max_v: 1079.0\n",
      "lif layer 2 self.abs_max_v: 1125.5\n",
      "lif layer 2 self.abs_max_v: 1156.0\n",
      "lif layer 2 self.abs_max_v: 1156.5\n",
      "lif layer 2 self.abs_max_v: 1177.0\n",
      "fc layer 3 self.abs_max_out: 313.0\n",
      "lif layer 1 self.abs_max_v: 1303.0\n",
      "lif layer 1 self.abs_max_v: 1334.0\n",
      "lif layer 1 self.abs_max_v: 1522.0\n",
      "fc layer 3 self.abs_max_out: 345.0\n",
      "lif layer 2 self.abs_max_v: 1181.5\n",
      "lif layer 1 self.abs_max_v: 1576.5\n",
      "fc layer 2 self.abs_max_out: 875.0\n",
      "fc layer 2 self.abs_max_out: 917.0\n",
      "fc layer 2 self.abs_max_out: 932.0\n",
      "fc layer 1 self.abs_max_out: 1198.0\n",
      "fc layer 3 self.abs_max_out: 369.0\n",
      "lif layer 2 self.abs_max_v: 1202.5\n",
      "fc layer 2 self.abs_max_out: 943.0\n",
      "lif layer 1 self.abs_max_v: 1592.5\n",
      "lif layer 2 self.abs_max_v: 1265.5\n",
      "fc layer 2 self.abs_max_out: 979.0\n",
      "fc layer 2 self.abs_max_out: 988.0\n",
      "fc layer 2 self.abs_max_out: 1013.0\n",
      "lif layer 2 self.abs_max_v: 1304.5\n",
      "fc layer 1 self.abs_max_out: 1210.0\n",
      "lif layer 1 self.abs_max_v: 1816.5\n",
      "lif layer 2 self.abs_max_v: 1344.5\n",
      "fc layer 1 self.abs_max_out: 1243.0\n",
      "lif layer 1 self.abs_max_v: 2083.0\n",
      "lif layer 2 self.abs_max_v: 1385.5\n",
      "fc layer 1 self.abs_max_out: 1251.0\n",
      "lif layer 1 self.abs_max_v: 2284.0\n",
      "lif layer 2 self.abs_max_v: 1523.0\n",
      "fc layer 1 self.abs_max_out: 1258.0\n",
      "fc layer 1 self.abs_max_out: 1338.0\n",
      "fc layer 1 self.abs_max_out: 1352.0\n",
      "fc layer 1 self.abs_max_out: 1432.0\n",
      "fc layer 2 self.abs_max_out: 1035.0\n",
      "lif layer 2 self.abs_max_v: 1528.5\n",
      "lif layer 2 self.abs_max_v: 1553.5\n",
      "fc layer 2 self.abs_max_out: 1047.0\n",
      "fc layer 3 self.abs_max_out: 386.0\n",
      "fc layer 1 self.abs_max_out: 1439.0\n",
      "fc layer 2 self.abs_max_out: 1147.0\n",
      "fc layer 1 self.abs_max_out: 1542.0\n",
      "fc layer 1 self.abs_max_out: 1562.0\n",
      "lif layer 1 self.abs_max_v: 2441.5\n",
      "lif layer 1 self.abs_max_v: 2580.0\n",
      "fc layer 2 self.abs_max_out: 1148.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 14.300427/ 53.285854, val:  40.42%, val_best:  40.42%, tr:  95.71%, tr_best:  95.71%, epoch time: 68.45 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.8854%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 73.1655%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2377  24.280%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "fc layer 3 self.abs_max_out: 422.0\n",
      "lif layer 2 self.abs_max_v: 1681.5\n",
      "fc layer 1 self.abs_max_out: 1651.0\n",
      "fc layer 2 self.abs_max_out: 1177.0\n",
      "fc layer 2 self.abs_max_out: 1200.0\n",
      "fc layer 1 self.abs_max_out: 1674.0\n",
      "fc layer 1 self.abs_max_out: 1864.0\n",
      "lif layer 1 self.abs_max_v: 2813.5\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.568483/ 77.299950, val:  37.08%, val_best:  40.42%, tr:  99.49%, tr_best:  99.49%, epoch time: 67.19 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0383%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.4731%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1962%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 4062  20.746%\n",
      "fc layer 2 self.abs_max_out: 1206.0\n",
      "fc layer 2 self.abs_max_out: 1207.0\n",
      "fc layer 2 self.abs_max_out: 1222.0\n",
      "fc layer 3 self.abs_max_out: 494.0\n",
      "lif layer 1 self.abs_max_v: 3077.0\n",
      "fc layer 2 self.abs_max_out: 1238.0\n",
      "fc layer 2 self.abs_max_out: 1239.0\n",
      "fc layer 1 self.abs_max_out: 1935.0\n",
      "fc layer 1 self.abs_max_out: 1978.0\n",
      "lif layer 1 self.abs_max_v: 3365.5\n",
      "lif layer 1 self.abs_max_v: 3422.0\n",
      "lif layer 1 self.abs_max_v: 3541.0\n",
      "fc layer 2 self.abs_max_out: 1244.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss: 10.062619/ 59.394058, val:  43.75%, val_best:  43.75%, tr:  99.49%, tr_best:  99.49%, epoch time: 67.54 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9508%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5219%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5589  19.030%\n",
      "fc layer 3 self.abs_max_out: 498.0\n",
      "fc layer 2 self.abs_max_out: 1263.0\n",
      "fc layer 2 self.abs_max_out: 1278.0\n",
      "lif layer 2 self.abs_max_v: 1683.0\n",
      "fc layer 2 self.abs_max_out: 1317.0\n",
      "fc layer 1 self.abs_max_out: 2071.0\n",
      "fc layer 1 self.abs_max_out: 2093.0\n",
      "lif layer 1 self.abs_max_v: 3582.5\n",
      "lif layer 1 self.abs_max_v: 3629.5\n",
      "lif layer 1 self.abs_max_v: 3776.0\n",
      "fc layer 2 self.abs_max_out: 1328.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.120925/ 86.629066, val:  41.67%, val_best:  43.75%, tr:  99.69%, tr_best:  99.69%, epoch time: 67.86 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0605%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5423%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2070%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6959  17.771%\n",
      "fc layer 2 self.abs_max_out: 1425.0\n",
      "lif layer 1 self.abs_max_v: 3783.0\n",
      "lif layer 2 self.abs_max_v: 1710.0\n",
      "lif layer 2 self.abs_max_v: 1801.5\n",
      "fc layer 3 self.abs_max_out: 506.0\n",
      "fc layer 1 self.abs_max_out: 2250.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  9.344364/ 64.075356, val:  46.67%, val_best:  46.67%, tr:  99.59%, tr_best:  99.69%, epoch time: 67.70 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8888%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 8337  17.032%\n",
      "lif layer 1 self.abs_max_v: 3831.5\n",
      "fc layer 3 self.abs_max_out: 512.0\n",
      "fc layer 2 self.abs_max_out: 1465.0\n",
      "fc layer 3 self.abs_max_out: 525.0\n",
      "lif layer 2 self.abs_max_v: 1803.0\n",
      "lif layer 2 self.abs_max_v: 1858.0\n",
      "lif layer 2 self.abs_max_v: 1922.5\n",
      "lif layer 2 self.abs_max_v: 2055.5\n",
      "fc layer 1 self.abs_max_out: 2458.0\n",
      "lif layer 2 self.abs_max_v: 2160.0\n",
      "fc layer 1 self.abs_max_out: 2511.0\n",
      "lif layer 1 self.abs_max_v: 4146.5\n",
      "lif layer 1 self.abs_max_v: 4159.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  9.370988/ 64.488686, val:  44.58%, val_best:  46.67%, tr:  99.49%, tr_best:  99.69%, epoch time: 67.40 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0937%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8330%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8204%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9694  16.503%\n",
      "lif layer 2 self.abs_max_v: 2205.5\n",
      "lif layer 2 self.abs_max_v: 2311.5\n",
      "lif layer 2 self.abs_max_v: 2356.0\n",
      "fc layer 1 self.abs_max_out: 2562.0\n",
      "lif layer 1 self.abs_max_v: 4220.5\n",
      "lif layer 1 self.abs_max_v: 4296.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  9.501733/ 86.589546, val:  38.75%, val_best:  46.67%, tr:  99.69%, tr_best:  99.69%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1033%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4684%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.4357%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 11029  16.094%\n",
      "lif layer 2 self.abs_max_v: 2367.5\n",
      "lif layer 2 self.abs_max_v: 2555.0\n",
      "fc layer 1 self.abs_max_out: 2692.0\n",
      "lif layer 1 self.abs_max_v: 4442.5\n",
      "lif layer 1 self.abs_max_v: 4512.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  9.149707/ 54.087646, val:  54.17%, val_best:  54.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 67.12 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0813%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6801%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.8578%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 12308  15.715%\n",
      "fc layer 3 self.abs_max_out: 541.0\n",
      "fc layer 3 self.abs_max_out: 555.0\n",
      "fc layer 2 self.abs_max_out: 1488.0\n",
      "fc layer 1 self.abs_max_out: 2704.0\n",
      "lif layer 1 self.abs_max_v: 4531.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.720302/ 64.846718, val:  47.50%, val_best:  54.17%, tr:  99.59%, tr_best:  99.90%, epoch time: 67.28 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.0267%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.5804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13536  15.363%\n",
      "fc layer 3 self.abs_max_out: 566.0\n",
      "fc layer 1 self.abs_max_out: 2865.0\n",
      "lif layer 1 self.abs_max_v: 4705.0\n",
      "lif layer 1 self.abs_max_v: 4755.5\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.491394/ 45.636433, val:  59.17%, val_best:  59.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 66.82 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0991%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.4913%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14757  15.074%\n",
      "fc layer 3 self.abs_max_out: 567.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.865273/ 55.554771, val:  57.08%, val_best:  59.17%, tr:  99.69%, tr_best:  99.90%, epoch time: 67.27 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0467%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2356%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 16020  14.876%\n",
      "fc layer 2 self.abs_max_out: 1554.0\n",
      "lif layer 1 self.abs_max_v: 4879.0\n",
      "fc layer 3 self.abs_max_out: 583.0\n",
      "fc layer 1 self.abs_max_out: 2883.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  8.739761/ 68.504379, val:  53.33%, val_best:  59.17%, tr:  99.49%, tr_best:  99.90%, epoch time: 67.96 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2114%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.7971%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 17208  14.648%\n",
      "fc layer 3 self.abs_max_out: 585.0\n",
      "fc layer 3 self.abs_max_out: 590.0\n",
      "fc layer 3 self.abs_max_out: 607.0\n",
      "lif layer 2 self.abs_max_v: 2628.5\n",
      "lif layer 2 self.abs_max_v: 2660.5\n",
      "fc layer 2 self.abs_max_out: 1556.0\n",
      "fc layer 2 self.abs_max_out: 1576.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  8.126451/ 45.641613, val:  53.75%, val_best:  59.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 67.28 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1169%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7942%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 63.6193%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 18340  14.410%\n",
      "fc layer 3 self.abs_max_out: 623.0\n",
      "fc layer 3 self.abs_max_out: 639.0\n",
      "fc layer 3 self.abs_max_out: 665.0\n",
      "lif layer 2 self.abs_max_v: 2742.0\n",
      "fc layer 1 self.abs_max_out: 2898.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.881438/ 41.827820, val:  62.92%, val_best:  62.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 67.28 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0511%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6591%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.8598%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19503  14.230%\n",
      "fc layer 1 self.abs_max_out: 2916.0\n",
      "fc layer 2 self.abs_max_out: 1637.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  8.369546/102.613609, val:  40.00%, val_best:  62.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 67.82 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.7496%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.6676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20662  14.070%\n",
      "fc layer 1 self.abs_max_out: 2962.0\n",
      "fc layer 1 self.abs_max_out: 2963.0\n",
      "lif layer 1 self.abs_max_v: 4922.5\n",
      "lif layer 1 self.abs_max_v: 5004.5\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.846469/ 51.040955, val:  60.00%, val_best:  62.92%, tr:  99.90%, tr_best:  99.90%, epoch time: 66.21 seconds, 1.10 minutes\n",
      "layer   1  Sparsity: 91.0383%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3633%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.3779%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21769  13.897%\n",
      "fc layer 2 self.abs_max_out: 1758.0\n",
      "fc layer 1 self.abs_max_out: 3005.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  8.048366/ 37.318802, val:  65.00%, val_best:  65.00%, tr:  99.69%, tr_best:  99.90%, epoch time: 67.27 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9623%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 64.2019%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22904  13.762%\n",
      "fc layer 3 self.abs_max_out: 708.0\n",
      "fc layer 2 self.abs_max_out: 1766.0\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.604649/ 53.498657, val:  61.67%, val_best:  65.00%, tr:  99.80%, tr_best:  99.90%, epoch time: 67.35 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0801%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7237%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6629%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 24005  13.622%\n",
      "fc layer 1 self.abs_max_out: 3010.0\n",
      "fc layer 2 self.abs_max_out: 1776.0\n",
      "fc layer 1 self.abs_max_out: 3035.0\n",
      "fc layer 1 self.abs_max_out: 3083.0\n",
      "lif layer 1 self.abs_max_v: 5190.5\n",
      "lif layer 1 self.abs_max_v: 5349.5\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.733388/ 33.469376, val:  70.42%, val_best:  70.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 67.67 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1597%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5363%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 25100  13.494%\n",
      "fc layer 2 self.abs_max_out: 1821.0\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.021724/ 56.154423, val:  57.08%, val_best:  70.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 68.27 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.0671%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9020%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2674%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 26109  13.335%\n",
      "lif layer 2 self.abs_max_v: 2810.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  7.282040/ 76.424141, val:  47.50%, val_best:  70.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.52 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0169%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 27196  13.228%\n",
      "lif layer 2 self.abs_max_v: 2816.5\n",
      "fc layer 1 self.abs_max_out: 3105.0\n",
      "lif layer 1 self.abs_max_v: 5370.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  7.093026/ 46.739571, val:  71.25%, val_best:  71.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.23 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0682%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5957%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9804%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 28223  13.104%\n",
      "fc layer 1 self.abs_max_out: 3147.0\n",
      "lif layer 2 self.abs_max_v: 2840.5\n",
      "fc layer 1 self.abs_max_out: 3187.0\n",
      "lif layer 1 self.abs_max_v: 5528.5\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  7.307080/ 43.023350, val:  73.75%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0533%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1610%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9715%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 29269  12.999%\n",
      "lif layer 1 self.abs_max_v: 5553.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  7.020204/ 57.353287, val:  58.33%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.47 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0909%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0271%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4919%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 30301  12.896%\n",
      "fc layer 1 self.abs_max_out: 3288.0\n",
      "fc layer 2 self.abs_max_out: 1902.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.708024/ 41.131344, val:  72.08%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.29 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0291%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1219%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8270%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 31283  12.782%\n",
      "lif layer 1 self.abs_max_v: 5642.5\n",
      "lif layer 1 self.abs_max_v: 5815.5\n",
      "lif layer 1 self.abs_max_v: 5825.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.609620/ 58.067665, val:  62.50%, val_best:  73.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.27 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0715%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0212%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8001%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 32264  12.675%\n",
      "fc layer 1 self.abs_max_out: 3301.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.677402/ 49.412701, val:  64.17%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.21 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0990%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7708%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1296%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 33247  12.578%\n",
      "fc layer 1 self.abs_max_out: 3316.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.014293/ 54.211422, val:  62.08%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.27 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0924%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8227%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0844%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 34150  12.458%\n",
      "fc layer 1 self.abs_max_out: 3317.0\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  6.692409/ 46.468735, val:  65.00%, val_best:  73.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.13 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0475%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0629%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0341%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 35084  12.357%\n",
      "fc layer 2 self.abs_max_out: 2038.0\n",
      "fc layer 1 self.abs_max_out: 3467.0\n",
      "lif layer 1 self.abs_max_v: 5972.5\n",
      "lif layer 1 self.abs_max_v: 6179.5\n",
      "lif layer 1 self.abs_max_v: 6210.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  5.857687/ 53.396347, val:  72.08%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.95 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0992%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0818%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1414%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35961  12.244%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  5.523288/ 47.042698, val:  72.50%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.73 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1596%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8584%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 36805  12.127%\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.193689/ 48.082848, val:  71.25%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.61 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7267%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9814%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 37697  12.033%\n",
      "fc layer 1 self.abs_max_out: 3514.0\n",
      "fc layer 2 self.abs_max_out: 2046.0\n",
      "fc layer 2 self.abs_max_out: 2064.0\n",
      "fc layer 1 self.abs_max_out: 3527.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  6.039364/ 41.432575, val:  69.58%, val_best:  73.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.52 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8462%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7853%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 38591  11.945%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  6.257851/ 33.217270, val:  81.67%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1084%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0982%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 39486  11.863%\n",
      "fc layer 2 self.abs_max_out: 2196.0\n",
      "fc layer 3 self.abs_max_out: 721.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  6.029861/ 52.849831, val:  67.92%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.82 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0842%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1411%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 40332  11.771%\n",
      "fc layer 3 self.abs_max_out: 736.0\n",
      "fc layer 1 self.abs_max_out: 3534.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.923839/ 41.651947, val:  73.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.51 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0917%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1515%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 41179  11.684%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  6.052752/ 38.271622, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1329%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7657%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1776%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 42030  11.603%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.486551/ 46.979305, val:  68.33%, val_best:  81.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.55 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0213%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4491%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2669%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 42831  11.513%\n",
      "fc layer 1 self.abs_max_out: 3667.0\n",
      "lif layer 1 self.abs_max_v: 6250.5\n",
      "lif layer 1 self.abs_max_v: 6274.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.018163/ 64.289787, val:  54.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.00 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1094%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7291%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9507%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 43570  11.411%\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.048914/ 45.733761, val:  79.58%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.96 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0940%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.7832%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9503%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 44335  11.322%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.241606/ 55.004864, val:  63.33%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.68 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0728%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6025%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6625%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 45115  11.240%\n",
      "fc layer 1 self.abs_max_out: 3701.0\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.749582/ 36.310501, val:  78.75%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.24 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0358%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4961%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 45934  11.171%\n",
      "fc layer 1 self.abs_max_out: 3721.0\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.278119/ 57.167118, val:  69.17%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.34 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0777%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4279%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 46685  11.090%\n",
      "fc layer 1 self.abs_max_out: 3816.0\n",
      "lif layer 1 self.abs_max_v: 6372.5\n",
      "lif layer 1 self.abs_max_v: 6616.5\n",
      "lif layer 1 self.abs_max_v: 6649.0\n",
      "lif layer 1 self.abs_max_v: 6740.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.188867/ 32.506073, val:  81.67%, val_best:  81.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.52 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0914%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3044%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.3046%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 47432  11.011%\n",
      "fc layer 3 self.abs_max_out: 737.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  5.105814/ 38.731495, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.06 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0433%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3329%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.6397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 48162  10.932%\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  5.135210/ 52.402779, val:  70.00%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 62.55 seconds, 1.04 minutes\n",
      "layer   1  Sparsity: 91.1137%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2155%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5271%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 48930  10.865%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  5.078265/ 45.371212, val:  80.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.25 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1041%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2935%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7172%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 49685  10.798%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.839570/ 40.546780, val:  82.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.46 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0511%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4535%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8469%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 50386  10.722%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  5.023986/ 40.946884, val:  79.17%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.60 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0560%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.6480%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.7226%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 51075  10.647%\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  4.916075/ 50.315578, val:  70.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.28 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0968%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2033%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.4963%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 51780  10.578%\n",
      "fc layer 3 self.abs_max_out: 770.0\n",
      "fc layer 1 self.abs_max_out: 3849.0\n",
      "fc layer 1 self.abs_max_out: 3870.0\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.891127/ 45.052486, val:  71.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.39 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1823%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5814%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 52482  10.511%\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.546764/ 43.316364, val:  75.83%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.36 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1023%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3236%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5326%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 53137  10.438%\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.794554/ 41.183407, val:  82.92%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.37 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1112%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1320%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9361%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 53852  10.379%\n",
      "fc layer 1 self.abs_max_out: 3904.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.291548/ 39.282318, val:  80.83%, val_best:  82.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.30 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0573%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1701%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0876%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 54494  10.308%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.638908/ 35.057678, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.43 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0421%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0881%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0194%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 55125  10.238%\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  3.923335/ 61.248814, val:  61.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.54 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0850%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1970%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2214%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 55724  10.164%\n",
      "lif layer 2 self.abs_max_v: 3093.5\n",
      "lif layer 2 self.abs_max_v: 3168.0\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.324069/ 38.531548, val:  77.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.97 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0350%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3986%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 56369  10.101%\n",
      "fc layer 1 self.abs_max_out: 3909.0\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.211695/ 40.266312, val:  77.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.78 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0635%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8844%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5316%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 57001  10.039%\n",
      "fc layer 1 self.abs_max_out: 3922.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.111917/ 36.033875, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.27 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0491%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9237%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9966%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 57607   9.973%\n",
      "fc layer 1 self.abs_max_out: 4050.0\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  3.942647/ 42.958210, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.41 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5359%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9053%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 58219   9.911%\n",
      "fc layer 1 self.abs_max_out: 4111.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.214628/ 44.003922, val:  78.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.18 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1255%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5070%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1385%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 58814   9.848%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  3.970325/ 37.739010, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.78 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3087%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 59403   9.787%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.147646/ 39.596447, val:  84.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.41 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4174%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 59975   9.724%\n",
      "lif layer 1 self.abs_max_v: 6792.0\n",
      "lif layer 1 self.abs_max_v: 7017.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  3.835131/ 42.209347, val:  80.00%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.39 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0666%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0703%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5453%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 60575   9.668%\n",
      "lif layer 1 self.abs_max_v: 7023.5\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.934324/ 38.418201, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.90 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1031%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0582%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9868%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 61165   9.612%\n",
      "fc layer 1 self.abs_max_out: 4115.0\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  3.498662/ 42.470940, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.91 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8248%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1493%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 61691   9.548%\n",
      "fc layer 2 self.abs_max_out: 2206.0\n",
      "fc layer 1 self.abs_max_out: 4135.0\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.083944/ 38.448914, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.39 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0499%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9219%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1614%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 62285   9.496%\n",
      "fc layer 1 self.abs_max_out: 4170.0\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.978594/ 37.267292, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.71 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0722%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9897%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 62860   9.442%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.008677/ 42.082027, val:  78.33%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.68 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0309%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1162%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4981%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 63442   9.392%\n",
      "fc layer 1 self.abs_max_out: 4216.0\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.955428/ 38.080769, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.76 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0754%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2802%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3431%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 64016   9.341%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.548214/ 40.795544, val:  85.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.79 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1136%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0722%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4571%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 64585   9.292%\n",
      "lif layer 1 self.abs_max_v: 7054.0\n",
      "lif layer 1 self.abs_max_v: 7090.0\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.544757/ 42.007988, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.44 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0603%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7472%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 65090   9.234%\n",
      "fc layer 1 self.abs_max_out: 4257.0\n",
      "lif layer 1 self.abs_max_v: 7129.5\n",
      "lif layer 1 self.abs_max_v: 7132.0\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.815795/ 37.203690, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.21 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0895%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8765%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4071%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 65647   9.186%\n",
      "lif layer 1 self.abs_max_v: 7142.5\n",
      "lif layer 1 self.abs_max_v: 7147.5\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.924326/ 44.280899, val:  77.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.06 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1166%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5057%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 66205   9.139%\n",
      "fc layer 2 self.abs_max_out: 2280.0\n",
      "lif layer 1 self.abs_max_v: 7177.0\n",
      "lif layer 1 self.abs_max_v: 7220.5\n",
      "lif layer 1 self.abs_max_v: 7263.5\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.721051/ 49.047340, val:  76.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.19 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0992%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0529%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6997%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 66735   9.089%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.241784/ 44.777710, val:  82.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.46 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0536%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2027%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3691%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 67237   9.037%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.531146/ 44.663315, val:  80.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.10 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1033%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4064%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6685%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 67743   8.987%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  4.057455/ 35.953682, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.94 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0921%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8945%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4660%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 68282   8.942%\n",
      "fc layer 2 self.abs_max_out: 2287.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.107563/ 44.994705, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.74 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8187%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 68752   8.889%\n",
      "lif layer 1 self.abs_max_v: 7275.5\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.378060/ 38.169815, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.71 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1008%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8729%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4192%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 69255   8.843%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.683778/ 43.811493, val:  79.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.27 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1281%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8205%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3918%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 69776   8.799%\n",
      "fc layer 2 self.abs_max_out: 2326.0\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.104789/ 38.724319, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.19 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0900%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5824%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7475%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 70267   8.753%\n",
      "fc layer 1 self.abs_max_out: 4264.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.616619/ 47.654850, val:  76.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.46 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0625%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8069%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 70780   8.711%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  2.863083/ 33.806427, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.43 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0981%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7483%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7935%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 71235   8.662%\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.292540/ 39.205837, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1251%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5535%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9304%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 71708   8.617%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.267788/ 37.132446, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.29 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5876%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7751%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 72173   8.572%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.252527/ 43.728897, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.21 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0499%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1401%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 72651   8.530%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.426360/ 37.322823, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.21 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6205%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0366%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 73131   8.489%\n",
      "fc layer 3 self.abs_max_out: 787.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.226338/ 52.207905, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.93 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0434%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7299%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8934%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 73627   8.450%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.161196/ 36.240452, val:  85.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.67 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1058%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4634%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5056%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 74055   8.405%\n",
      "fc layer 1 self.abs_max_out: 4338.0\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.222799/ 38.861958, val:  87.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.45 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0883%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1051%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5806%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 74522   8.365%\n",
      "lif layer 1 self.abs_max_v: 7347.0\n",
      "lif layer 1 self.abs_max_v: 7397.0\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  2.832556/ 43.108017, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.23 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0713%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2295%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2542%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 74966   8.323%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  2.637978/ 41.338600, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.02 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1045%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3076%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6510%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 75387   8.280%\n",
      "fc layer 2 self.abs_max_out: 2334.0\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.179658/ 38.588146, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.78 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0509%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9393%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6913%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 75872   8.245%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.774946/ 36.499573, val:  86.67%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.65 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0790%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7066%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4286%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 76275   8.201%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.934874/ 42.596298, val:  84.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.80 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0547%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1877%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0469%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 76714   8.162%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.943077/ 36.661446, val:  85.42%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.46 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0729%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3412%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4803%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 77159   8.125%\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  3.139335/ 43.896927, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.37 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8067%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 77601   8.088%\n",
      "fc layer 2 self.abs_max_out: 2363.0\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.591245/ 37.372982, val:  87.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.68 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0911%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1537%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1238%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 78012   8.049%\n",
      "fc layer 2 self.abs_max_out: 2375.0\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.854235/ 36.809566, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.42 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1160%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2343%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 78435   8.012%\n",
      "fc layer 3 self.abs_max_out: 789.0\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.777934/ 41.330193, val:  82.50%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.84 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1858%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2998%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 78863   7.976%\n",
      "lif layer 1 self.abs_max_v: 7398.0\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.699925/ 35.955925, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.20 seconds, 1.10 minutes\n",
      "layer   1  Sparsity: 91.0143%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3215%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.7194%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 79264   7.938%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.404745/ 38.333511, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.25 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0701%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3225%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6439%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 79654   7.899%\n",
      "lif layer 1 self.abs_max_v: 7446.5\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.622528/ 45.031162, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.25 seconds, 1.10 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2828%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5682%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 80033   7.861%\n",
      "fc layer 2 self.abs_max_out: 2390.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.700640/ 37.589417, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.02 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4128%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3961%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 80431   7.824%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.954035/ 37.451576, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.68 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0524%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2764%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5525%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 80840   7.790%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.885133/ 43.138412, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.65 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0845%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3079%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1775%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 81276   7.759%\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.385807/ 38.820698, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.71 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0776%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2853%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5160%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 81634   7.721%\n",
      "fc layer 2 self.abs_max_out: 2427.0\n",
      "fc layer 3 self.abs_max_out: 791.0\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.692629/ 38.450882, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.88 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6197%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9208%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 82036   7.688%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.412109/ 35.840611, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.86 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0624%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1281%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1981%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 82386   7.650%\n",
      "fc layer 3 self.abs_max_out: 796.0\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.761338/ 40.856098, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.69 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0682%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0517%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0297%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 82802   7.620%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.629214/ 43.181511, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.00 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0677%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1067%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9501%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 83179   7.586%\n",
      "fc layer 3 self.abs_max_out: 807.0\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.547891/ 46.495995, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.79 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0785%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1804%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3507%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 83568   7.554%\n",
      "fc layer 3 self.abs_max_out: 817.0\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.671517/ 36.147743, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.94 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0875%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9547%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 83946   7.522%\n",
      "fc layer 1 self.abs_max_out: 4376.0\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.630222/ 39.886517, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.87 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9854%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6429%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 84317   7.489%\n",
      "fc layer 1 self.abs_max_out: 4427.0\n",
      "fc layer 3 self.abs_max_out: 823.0\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.480456/ 37.274700, val:  88.75%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.03 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0673%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9645%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9790%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 84703   7.459%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.410931/ 38.615417, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.29 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0512%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7244%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0886%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 85056   7.426%\n",
      "fc layer 3 self.abs_max_out: 827.0\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.420401/ 43.934433, val:  85.83%, val_best:  88.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.59 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0655%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2575%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 85427   7.395%\n",
      "fc layer 1 self.abs_max_out: 4440.0\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.381026/ 39.343163, val:  87.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.10 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0361%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1200%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1374%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 85790   7.364%\n",
      "fc layer 1 self.abs_max_out: 4452.0\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.545181/ 42.760563, val:  85.42%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.70 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9925%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6019%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 86166   7.335%\n",
      "fc layer 3 self.abs_max_out: 849.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.298053/ 46.961609, val:  82.50%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.54 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1332%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.6877%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4400%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 86511   7.303%\n",
      "fc layer 1 self.abs_max_out: 4500.0\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.488923/ 45.969429, val:  85.83%, val_best:  88.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.41 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0436%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9568%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8872%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 86897   7.275%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.334646/ 35.968956, val:  89.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.64 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0729%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0055%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 87236   7.244%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.457203/ 46.692928, val:  84.17%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.02 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0651%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9520%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9886%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 87587   7.215%\n",
      "fc layer 1 self.abs_max_out: 4534.0\n",
      "lif layer 1 self.abs_max_v: 7496.5\n",
      "lif layer 1 self.abs_max_v: 7513.5\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.388596/ 40.394821, val:  87.08%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.40 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1158%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8874%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2446%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 87935   7.186%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.491574/ 38.664948, val:  87.50%, val_best:  89.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.04 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0848%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1682%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5904%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 88290   7.157%\n",
      "fc layer 2 self.abs_max_out: 2476.0\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.179143/ 35.661747, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.03 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0423%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2400%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5226%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 88628   7.128%\n",
      "fc layer 2 self.abs_max_out: 2503.0\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.204551/ 36.927830, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.64 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0352%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9743%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3713%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 88969   7.100%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.391439/ 42.964996, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.89 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0835%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2631%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6318%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 89316   7.072%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  1.884795/ 38.040894, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.51 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0626%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4398%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9555%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 89613   7.041%\n",
      "fc layer 3 self.abs_max_out: 853.0\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.288099/ 40.285378, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.95 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1747%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8241%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 89933   7.012%\n",
      "fc layer 3 self.abs_max_out: 871.0\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  1.931619/ 39.241913, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.99 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0691%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0971%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9499%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 90205   6.980%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.037174/ 46.470577, val:  80.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.41 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3127%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8858%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 90526   6.952%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.315977/ 34.421814, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.60 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0725%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2672%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1583%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 90873   6.927%\n",
      "fc layer 3 self.abs_max_out: 889.0\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  1.889328/ 39.657944, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.60 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0854%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9414%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2374%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 91185   6.899%\n",
      "fc layer 1 self.abs_max_out: 4607.0\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.139922/ 48.019009, val:  81.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.71 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9318%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8593%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 91515   6.873%\n",
      "fc layer 3 self.abs_max_out: 910.0\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  2.236628/ 49.328850, val:  79.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.12 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1051%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9178%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0162%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 91842   6.848%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  1.973673/ 35.228027, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.27 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0588%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0793%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9624%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 92138   6.820%\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  1.846491/ 35.961479, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.87 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0584%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2321%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0160%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 92426   6.792%\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  1.816162/ 48.075535, val:  81.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.23 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0732%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1923%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6801%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 92716   6.765%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  2.158515/ 38.730877, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.16 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0877%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2133%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5197%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 93007   6.738%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.291696/ 39.796871, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.25 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0670%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2849%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 93332   6.714%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  1.978974/ 40.182182, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.57 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0769%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1852%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2143%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 93666   6.691%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  2.022496/ 39.476143, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.60 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0839%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7410%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9352%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 93967   6.665%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  1.597929/ 38.491898, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.50 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8355%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1245%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 94227   6.638%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  2.125488/ 43.743099, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.76 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9034%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9716%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 94504   6.612%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.801828/ 46.233433, val:  81.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.96 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8135%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9192%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 94771   6.585%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.879402/ 39.163689, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.00 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1109%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9552%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1891%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 95041   6.559%\n",
      "fc layer 2 self.abs_max_out: 2529.0\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.758457/ 46.755016, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.90 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1351%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9569%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5425%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 95309   6.534%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.939912/ 43.269901, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.37 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0984%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3079%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 95611   6.511%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.780002/ 38.351006, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.23 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0800%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0612%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3945%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 95882   6.486%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  1.885622/ 38.590519, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.01 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0417%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0578%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4316%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 96176   6.463%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.773949/ 39.923695, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.56 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0661%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0828%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1242%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 96440   6.438%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.992431/ 47.414566, val:  80.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.56 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0787%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9477%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0738%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 96738   6.416%\n",
      "fc layer 2 self.abs_max_out: 2546.0\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.831129/ 40.089146, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 68.19 seconds, 1.14 minutes\n",
      "layer   1  Sparsity: 91.1037%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5691%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 97028   6.394%\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.803640/ 43.192310, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.28 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9272%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2313%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 97304   6.371%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.823280/ 41.537411, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.75 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0889%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7249%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2313%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 97570   6.348%\n",
      "fc layer 2 self.abs_max_out: 2554.0\n",
      "fc layer 2 self.abs_max_out: 2567.0\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.693055/ 41.119778, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.49 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0347%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4691%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 97815   6.324%\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.736115/ 42.306755, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0925%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2229%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4834%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 98064   6.300%\n",
      "lif layer 1 self.abs_max_v: 7571.0\n",
      "lif layer 1 self.abs_max_v: 7609.5\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.648592/ 42.967331, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.28 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0829%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2616%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5906%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 98345   6.278%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.662300/ 41.178967, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.74 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0533%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9288%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9203%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 98610   6.256%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.517223/ 44.867760, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.03 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0687%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8858%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8788%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 98872   6.234%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.757981/ 45.960869, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1788%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9494%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 99125   6.212%\n",
      "fc layer 3 self.abs_max_out: 947.0\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.816176/ 45.545727, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.98 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0938%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7024%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 99393   6.191%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.616971/ 42.107670, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.84 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0864%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2699%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7260%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 99641   6.168%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.544455/ 43.109505, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.61 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0461%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2222%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7425%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 99899   6.147%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.731534/ 44.834007, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.82 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.1229%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0416%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7735%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 100161   6.126%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.734687/ 47.400002, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.81 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0763%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1117%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1585%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 100417   6.105%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.413725/ 51.192390, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.44 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0506%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2690%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3201%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 100643   6.083%\n",
      "fc layer 1 self.abs_max_out: 4625.0\n",
      "fc layer 2 self.abs_max_out: 2689.0\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.552637/ 46.418606, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.38 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1180%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1415%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1190%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 100880   6.061%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.841171/ 46.829903, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.16 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0878%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9253%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 101152   6.042%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.672424/ 40.811127, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.31 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2657%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6286%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 101394   6.021%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.411011/ 47.159172, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.18 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0516%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2345%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0011%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 101603   5.999%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.539274/ 47.554749, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.87 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1202%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0280%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4483%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 101830   5.978%\n",
      "fc layer 1 self.abs_max_out: 4650.0\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.539997/ 44.262508, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.78 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0509%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8406%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 102058   5.957%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.531650/ 42.543587, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.99 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8913%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7915%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 102299   5.937%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.542288/ 46.214710, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.44 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0637%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0901%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2788%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 102556   5.918%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.415113/ 50.669300, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.15 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0380%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1662%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3941%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 102791   5.899%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.507973/ 46.507362, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0689%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6384%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 103005   5.878%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.286802/ 43.271549, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.36 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0473%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4025%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 103210   5.857%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.336531/ 44.392162, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.25 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1031%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4891%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 103420   5.836%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.601514/ 48.595669, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.86 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0755%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5722%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 103675   5.819%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.554655/ 51.156277, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.77 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1226%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6760%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 103922   5.801%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.417938/ 42.362812, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.95 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0585%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3698%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 104143   5.781%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.445879/ 43.671787, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.93 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0956%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8451%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6589%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 104368   5.763%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.310506/ 46.864029, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.63 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0569%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 104577   5.743%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.445054/ 50.729256, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.12 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0186%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0754%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7972%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 104796   5.724%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.665263/ 49.411850, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.83 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0668%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9266%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3408%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 105026   5.706%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.615865/ 47.293159, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 70.23 seconds, 1.17 minutes\n",
      "layer   1  Sparsity: 91.0580%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7931%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2312%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 105280   5.690%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.368060/ 44.002239, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.43 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0995%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5750%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 105497   5.672%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.335125/ 44.458599, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.47 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0115%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 105702   5.653%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.218935/ 43.033909, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.35 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1021%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2275%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4632%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 105907   5.634%\n",
      "fc layer 1 self.abs_max_out: 4735.0\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.161049/ 45.934864, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.94 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0889%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2363%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6512%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 106106   5.616%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.371718/ 47.175869, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.49 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0825%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2001%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4963%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 106306   5.597%\n",
      "lif layer 1 self.abs_max_v: 7785.5\n",
      "lif layer 1 self.abs_max_v: 7843.0\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.494529/ 45.313469, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.46 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0559%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3422%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7113%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 106518   5.580%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.374088/ 50.071110, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.29 seconds, 1.10 minutes\n",
      "layer   1  Sparsity: 91.0904%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2402%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7960%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 106726   5.562%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.187664/ 45.272598, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.38 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0209%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8442%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0372%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 106921   5.544%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.276924/ 42.845345, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.73 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0645%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.7111%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1087%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 107129   5.527%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.304716/ 44.060631, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.98 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1023%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.8430%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9641%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 107325   5.509%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.278179/ 44.636814, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.13 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0733%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9333%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1991%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17bdd83b4d2f40729f924c252a70f746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñà‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.27818</td></tr><tr><td>val_acc_best</td><td>0.9</td></tr><tr><td>val_acc_now</td><td>0.89167</td></tr><tr><td>val_loss</td><td>44.63681</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunny-sweep-64</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bu4w5jd6' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/bu4w5jd6</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_082850-bu4w5jd6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: di93h6ms with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 25243\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_121347-di93h6ms</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/di93h6ms' target=\"_blank\">major-sweep-71</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/di93h6ms' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/di93h6ms</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251214_121356_237', 'my_seed': 25243, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 198.0\n",
      "lif layer 1 self.abs_max_v: 198.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 164.0\n",
      "lif layer 2 self.abs_max_v: 164.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 23.0\n",
      "lif layer 1 self.abs_max_v: 266.0\n",
      "fc layer 2 self.abs_max_out: 202.0\n",
      "lif layer 2 self.abs_max_v: 229.5\n",
      "fc layer 3 self.abs_max_out: 75.0\n",
      "fc layer 1 self.abs_max_out: 249.0\n",
      "lif layer 1 self.abs_max_v: 324.0\n",
      "fc layer 2 self.abs_max_out: 250.0\n",
      "lif layer 2 self.abs_max_v: 301.0\n",
      "fc layer 3 self.abs_max_out: 85.0\n",
      "fc layer 1 self.abs_max_out: 294.0\n",
      "lif layer 1 self.abs_max_v: 389.5\n",
      "fc layer 2 self.abs_max_out: 370.0\n",
      "lif layer 2 self.abs_max_v: 437.5\n",
      "fc layer 3 self.abs_max_out: 117.0\n",
      "fc layer 1 self.abs_max_out: 399.0\n",
      "lif layer 1 self.abs_max_v: 449.0\n",
      "lif layer 2 self.abs_max_v: 465.0\n",
      "lif layer 1 self.abs_max_v: 482.5\n",
      "lif layer 2 self.abs_max_v: 519.5\n",
      "fc layer 1 self.abs_max_out: 404.0\n",
      "lif layer 1 self.abs_max_v: 484.5\n",
      "fc layer 1 self.abs_max_out: 498.0\n",
      "lif layer 1 self.abs_max_v: 498.0\n",
      "fc layer 2 self.abs_max_out: 384.0\n",
      "fc layer 3 self.abs_max_out: 122.0\n",
      "lif layer 2 self.abs_max_v: 602.0\n",
      "fc layer 2 self.abs_max_out: 394.0\n",
      "fc layer 1 self.abs_max_out: 767.0\n",
      "lif layer 1 self.abs_max_v: 767.0\n",
      "fc layer 3 self.abs_max_out: 124.0\n",
      "fc layer 2 self.abs_max_out: 432.0\n",
      "lif layer 2 self.abs_max_v: 686.5\n",
      "fc layer 2 self.abs_max_out: 572.0\n",
      "lif layer 2 self.abs_max_v: 722.5\n",
      "lif layer 2 self.abs_max_v: 851.5\n",
      "fc layer 3 self.abs_max_out: 181.0\n",
      "fc layer 3 self.abs_max_out: 208.0\n",
      "fc layer 3 self.abs_max_out: 234.0\n",
      "lif layer 1 self.abs_max_v: 770.5\n",
      "fc layer 2 self.abs_max_out: 586.0\n",
      "lif layer 2 self.abs_max_v: 890.0\n",
      "fc layer 2 self.abs_max_out: 632.0\n",
      "lif layer 2 self.abs_max_v: 934.5\n",
      "lif layer 2 self.abs_max_v: 956.5\n",
      "lif layer 2 self.abs_max_v: 971.5\n",
      "fc layer 2 self.abs_max_out: 653.0\n",
      "fc layer 2 self.abs_max_out: 670.0\n",
      "lif layer 1 self.abs_max_v: 825.0\n",
      "lif layer 1 self.abs_max_v: 852.5\n",
      "fc layer 3 self.abs_max_out: 240.0\n",
      "lif layer 1 self.abs_max_v: 875.0\n",
      "lif layer 1 self.abs_max_v: 1004.0\n",
      "lif layer 1 self.abs_max_v: 1005.0\n",
      "fc layer 1 self.abs_max_out: 780.0\n",
      "lif layer 2 self.abs_max_v: 1025.0\n",
      "fc layer 1 self.abs_max_out: 803.0\n",
      "fc layer 1 self.abs_max_out: 811.0\n",
      "fc layer 2 self.abs_max_out: 674.0\n",
      "fc layer 2 self.abs_max_out: 704.0\n",
      "lif layer 2 self.abs_max_v: 1026.0\n",
      "lif layer 1 self.abs_max_v: 1041.0\n",
      "fc layer 1 self.abs_max_out: 839.0\n",
      "fc layer 1 self.abs_max_out: 847.0\n",
      "lif layer 1 self.abs_max_v: 1135.0\n",
      "lif layer 1 self.abs_max_v: 1344.5\n",
      "fc layer 2 self.abs_max_out: 718.0\n",
      "fc layer 1 self.abs_max_out: 917.0\n",
      "lif layer 1 self.abs_max_v: 1538.5\n",
      "lif layer 2 self.abs_max_v: 1059.5\n",
      "lif layer 2 self.abs_max_v: 1109.0\n",
      "fc layer 3 self.abs_max_out: 250.0\n",
      "lif layer 2 self.abs_max_v: 1118.0\n",
      "lif layer 2 self.abs_max_v: 1151.0\n",
      "fc layer 1 self.abs_max_out: 923.0\n",
      "fc layer 1 self.abs_max_out: 1067.0\n",
      "lif layer 2 self.abs_max_v: 1216.0\n",
      "fc layer 3 self.abs_max_out: 270.0\n",
      "fc layer 3 self.abs_max_out: 281.0\n",
      "fc layer 2 self.abs_max_out: 755.0\n",
      "fc layer 2 self.abs_max_out: 817.0\n",
      "fc layer 1 self.abs_max_out: 1163.0\n",
      "fc layer 3 self.abs_max_out: 302.0\n",
      "lif layer 2 self.abs_max_v: 1230.0\n",
      "lif layer 2 self.abs_max_v: 1303.0\n",
      "fc layer 2 self.abs_max_out: 829.0\n",
      "lif layer 2 self.abs_max_v: 1350.0\n",
      "fc layer 3 self.abs_max_out: 345.0\n",
      "fc layer 2 self.abs_max_out: 855.0\n",
      "fc layer 2 self.abs_max_out: 956.0\n",
      "lif layer 1 self.abs_max_v: 1576.5\n",
      "lif layer 1 self.abs_max_v: 1689.5\n",
      "fc layer 2 self.abs_max_out: 981.0\n",
      "fc layer 1 self.abs_max_out: 1173.0\n",
      "fc layer 1 self.abs_max_out: 1209.0\n",
      "fc layer 1 self.abs_max_out: 1298.0\n",
      "lif layer 1 self.abs_max_v: 1702.5\n",
      "fc layer 2 self.abs_max_out: 1026.0\n",
      "lif layer 1 self.abs_max_v: 1730.5\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "lif layer 1 self.abs_max_v: 1874.0\n",
      "lif layer 2 self.abs_max_v: 1381.0\n",
      "lif layer 2 self.abs_max_v: 1396.5\n",
      "fc layer 2 self.abs_max_out: 1050.0\n",
      "fc layer 1 self.abs_max_out: 1317.0\n",
      "fc layer 1 self.abs_max_out: 1391.0\n",
      "fc layer 2 self.abs_max_out: 1077.0\n",
      "lif layer 2 self.abs_max_v: 1440.5\n",
      "lif layer 2 self.abs_max_v: 1463.5\n",
      "lif layer 1 self.abs_max_v: 1965.0\n",
      "lif layer 2 self.abs_max_v: 1515.0\n",
      "lif layer 1 self.abs_max_v: 2091.5\n",
      "lif layer 2 self.abs_max_v: 1620.5\n",
      "lif layer 2 self.abs_max_v: 1697.5\n",
      "fc layer 1 self.abs_max_out: 1456.0\n",
      "fc layer 1 self.abs_max_out: 1695.0\n",
      "fc layer 2 self.abs_max_out: 1112.0\n",
      "fc layer 2 self.abs_max_out: 1145.0\n",
      "fc layer 2 self.abs_max_out: 1213.0\n",
      "fc layer 2 self.abs_max_out: 1299.0\n",
      "lif layer 1 self.abs_max_v: 2129.5\n",
      "lif layer 1 self.abs_max_v: 2284.0\n",
      "lif layer 1 self.abs_max_v: 2418.0\n",
      "fc layer 2 self.abs_max_out: 1337.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 14.389706/ 65.011177, val:  36.25%, val_best:  36.25%, tr:  97.04%, tr_best:  97.04%, epoch time: 67.63 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1300%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6961%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.7771%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2279  23.279%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 355.0\n",
      "fc layer 3 self.abs_max_out: 366.0\n",
      "fc layer 3 self.abs_max_out: 398.0\n",
      "lif layer 1 self.abs_max_v: 2520.5\n",
      "lif layer 1 self.abs_max_v: 2618.5\n",
      "lif layer 1 self.abs_max_v: 2859.5\n",
      "lif layer 1 self.abs_max_v: 3031.0\n",
      "fc layer 1 self.abs_max_out: 1712.0\n",
      "fc layer 1 self.abs_max_out: 1713.0\n",
      "fc layer 1 self.abs_max_out: 1746.0\n",
      "fc layer 3 self.abs_max_out: 401.0\n",
      "fc layer 3 self.abs_max_out: 408.0\n",
      "fc layer 1 self.abs_max_out: 1870.0\n",
      "lif layer 1 self.abs_max_v: 3210.5\n",
      "lif layer 1 self.abs_max_v: 3246.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss:  9.892789/ 78.281502, val:  35.00%, val_best:  36.25%, tr:  99.49%, tr_best:  99.49%, epoch time: 67.55 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0643%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.8585%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2911%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3874  19.785%\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "fc layer 3 self.abs_max_out: 422.0\n",
      "fc layer 3 self.abs_max_out: 440.0\n",
      "fc layer 3 self.abs_max_out: 465.0\n",
      "fc layer 3 self.abs_max_out: 491.0\n",
      "fc layer 2 self.abs_max_out: 1349.0\n",
      "fc layer 2 self.abs_max_out: 1395.0\n",
      "fc layer 1 self.abs_max_out: 1968.0\n",
      "lif layer 1 self.abs_max_v: 3251.0\n",
      "lif layer 1 self.abs_max_v: 3585.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.806575/ 54.230923, val:  49.17%, val_best:  49.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 67.35 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1088%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0780%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8997%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5345  18.199%\n",
      "fc layer 1 self.abs_max_out: 1974.0\n",
      "lif layer 2 self.abs_max_v: 1777.5\n",
      "lif layer 2 self.abs_max_v: 1819.5\n",
      "lif layer 2 self.abs_max_v: 1878.0\n",
      "fc layer 2 self.abs_max_out: 1442.0\n",
      "fc layer 1 self.abs_max_out: 2208.0\n",
      "lif layer 1 self.abs_max_v: 3749.0\n",
      "fc layer 1 self.abs_max_out: 2434.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  8.591239/ 84.212402, val:  35.42%, val_best:  49.17%, tr:  99.49%, tr_best:  99.59%, epoch time: 67.16 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0790%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.1051%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7436%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6678  17.053%\n",
      "lif layer 2 self.abs_max_v: 1901.5\n",
      "lif layer 2 self.abs_max_v: 1953.5\n",
      "lif layer 1 self.abs_max_v: 4026.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  9.043702/ 66.455856, val:  44.58%, val_best:  49.17%, tr:  99.59%, tr_best:  99.59%, epoch time: 66.61 seconds, 1.11 minutes\n",
      "layer   1  Sparsity: 91.0740%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.3540%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8243%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 8037  16.419%\n",
      "lif layer 2 self.abs_max_v: 1997.5\n",
      "lif layer 2 self.abs_max_v: 2112.0\n",
      "lif layer 2 self.abs_max_v: 2165.0\n",
      "lif layer 2 self.abs_max_v: 2309.5\n",
      "lif layer 1 self.abs_max_v: 4070.5\n",
      "fc layer 2 self.abs_max_out: 1470.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  8.663341/ 58.066647, val:  48.75%, val_best:  49.17%, tr:  99.80%, tr_best:  99.80%, epoch time: 67.26 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2319%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8924%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9326  15.877%\n",
      "fc layer 3 self.abs_max_out: 504.0\n",
      "lif layer 1 self.abs_max_v: 4242.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  8.116780/ 74.674568, val:  44.17%, val_best:  49.17%, tr:  99.69%, tr_best:  99.80%, epoch time: 67.15 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1078%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6091%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2188%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10570  15.424%\n",
      "lif layer 2 self.abs_max_v: 2349.0\n",
      "fc layer 3 self.abs_max_out: 518.0\n",
      "fc layer 3 self.abs_max_out: 520.0\n",
      "lif layer 2 self.abs_max_v: 2407.0\n",
      "fc layer 1 self.abs_max_out: 2488.0\n",
      "lif layer 1 self.abs_max_v: 4423.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.938731/ 72.766182, val:  44.17%, val_best:  49.17%, tr:  99.49%, tr_best:  99.80%, epoch time: 67.38 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0622%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5803%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4761%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 11881  15.170%\n",
      "fc layer 1 self.abs_max_out: 2591.0\n",
      "lif layer 1 self.abs_max_v: 4542.5\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.477475/ 80.897011, val:  44.17%, val_best:  49.17%, tr:  99.90%, tr_best:  99.90%, epoch time: 67.67 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0846%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3557%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6394%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13166  14.943%\n",
      "fc layer 3 self.abs_max_out: 524.0\n",
      "fc layer 3 self.abs_max_out: 528.0\n",
      "fc layer 3 self.abs_max_out: 566.0\n",
      "fc layer 2 self.abs_max_out: 1477.0\n",
      "fc layer 1 self.abs_max_out: 2670.0\n",
      "lif layer 1 self.abs_max_v: 4721.0\n",
      "fc layer 3 self.abs_max_out: 598.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.411102/ 47.664955, val:  60.42%, val_best:  60.42%, tr:  99.59%, tr_best:  99.90%, epoch time: 67.33 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1011%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.7003%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0347%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14419  14.728%\n",
      "fc layer 2 self.abs_max_out: 1498.0\n",
      "fc layer 1 self.abs_max_out: 2787.0\n",
      "lif layer 1 self.abs_max_v: 4894.5\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.652021/ 38.059086, val:  61.25%, val_best:  61.25%, tr:  99.28%, tr_best:  99.90%, epoch time: 67.21 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0884%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3010%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.9112%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15651  14.533%\n",
      "fc layer 2 self.abs_max_out: 1533.0\n",
      "fc layer 1 self.abs_max_out: 2890.0\n",
      "lif layer 1 self.abs_max_v: 5094.0\n",
      "lif layer 1 self.abs_max_v: 5112.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  7.397834/ 72.706383, val:  44.17%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.54 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1030%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.1856%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.5420%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16741  14.250%\n",
      "fc layer 2 self.abs_max_out: 1556.0\n",
      "fc layer 2 self.abs_max_out: 1584.0\n",
      "fc layer 1 self.abs_max_out: 2943.0\n",
      "lif layer 1 self.abs_max_v: 5199.5\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.775810/ 53.923103, val:  51.67%, val_best:  61.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.78 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0558%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9086%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.0766%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 17842  14.019%\n",
      "fc layer 2 self.abs_max_out: 1606.0\n",
      "fc layer 2 self.abs_max_out: 1672.0\n",
      "lif layer 2 self.abs_max_v: 2435.5\n",
      "fc layer 1 self.abs_max_out: 2961.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.528139/ 57.395988, val:  50.83%, val_best:  61.25%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.81 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1139%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8526%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.2549%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 18937  13.817%\n",
      "fc layer 3 self.abs_max_out: 604.0\n",
      "fc layer 1 self.abs_max_out: 3003.0\n",
      "lif layer 1 self.abs_max_v: 5236.0\n",
      "lif layer 1 self.abs_max_v: 5388.5\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.675757/ 40.286488, val:  60.00%, val_best:  61.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 66.90 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0730%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.6184%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 65.8895%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20049  13.653%\n",
      "fc layer 1 self.abs_max_out: 3009.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.884275/ 36.952305, val:  69.17%, val_best:  69.17%, tr:  99.69%, tr_best: 100.00%, epoch time: 67.65 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.4297%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.1882%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21196  13.532%\n",
      "fc layer 3 self.abs_max_out: 611.0\n",
      "fc layer 3 self.abs_max_out: 613.0\n",
      "fc layer 3 self.abs_max_out: 640.0\n",
      "lif layer 2 self.abs_max_v: 2542.0\n",
      "lif layer 2 self.abs_max_v: 2571.0\n",
      "fc layer 1 self.abs_max_out: 3025.0\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.384386/ 47.180462, val:  56.25%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.49 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0672%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8727%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4208%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 22256  13.373%\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.360717/ 54.458469, val:  53.33%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.05 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0682%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.8449%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3298%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 23352  13.252%\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  6.632538/ 52.361298, val:  63.75%, val_best:  69.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.34 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0872%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.1470%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0993%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 24363  13.098%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  6.670922/ 54.910778, val:  63.75%, val_best:  69.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.68 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0828%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6027%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3993%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 25418  12.982%\n",
      "fc layer 1 self.abs_max_out: 3056.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  7.325034/ 46.847645, val:  60.42%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.52 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0254%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1868%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1522%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 26512  12.896%\n",
      "fc layer 3 self.abs_max_out: 644.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.829042/ 81.643753, val:  52.08%, val_best:  69.17%, tr:  99.49%, tr_best: 100.00%, epoch time: 67.31 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9724%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0872%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 27545  12.789%\n",
      "fc layer 2 self.abs_max_out: 1675.0\n",
      "fc layer 2 self.abs_max_out: 1695.0\n",
      "fc layer 2 self.abs_max_out: 1709.0\n",
      "lif layer 2 self.abs_max_v: 2633.0\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.639951/ 45.700890, val:  55.83%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.67 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0690%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2426%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1887%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 28553  12.681%\n",
      "fc layer 2 self.abs_max_out: 1715.0\n",
      "fc layer 1 self.abs_max_out: 3059.0\n",
      "lif layer 1 self.abs_max_v: 5392.5\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.797263/ 45.916389, val:  65.42%, val_best:  69.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.50 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0310%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1772%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4152%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 29586  12.592%\n",
      "lif layer 2 self.abs_max_v: 2661.0\n",
      "lif layer 2 self.abs_max_v: 2668.5\n",
      "fc layer 1 self.abs_max_out: 3179.0\n",
      "lif layer 1 self.abs_max_v: 5576.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.179764/ 27.025036, val:  74.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.88 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0682%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2496%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 30537  12.477%\n",
      "fc layer 2 self.abs_max_out: 1733.0\n",
      "fc layer 2 self.abs_max_out: 1740.0\n",
      "fc layer 2 self.abs_max_out: 1785.0\n",
      "fc layer 1 self.abs_max_out: 3209.0\n",
      "lif layer 1 self.abs_max_v: 5610.5\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.427852/ 62.057076, val:  55.42%, val_best:  74.58%, tr:  99.80%, tr_best: 100.00%, epoch time: 67.34 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0932%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2231%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5085%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 31529  12.387%\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.607784/ 49.986080, val:  57.08%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.51 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1265%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9979%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 32528  12.306%\n",
      "lif layer 1 self.abs_max_v: 5614.0\n",
      "fc layer 1 self.abs_max_out: 3234.0\n",
      "lif layer 1 self.abs_max_v: 5637.5\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.713779/ 39.759289, val:  72.08%, val_best:  74.58%, tr:  99.69%, tr_best: 100.00%, epoch time: 67.35 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0568%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5218%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3241%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 33515  12.226%\n",
      "fc layer 2 self.abs_max_out: 1827.0\n",
      "fc layer 3 self.abs_max_out: 668.0\n",
      "fc layer 1 self.abs_max_out: 3279.0\n",
      "lif layer 1 self.abs_max_v: 5755.5\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  5.849254/ 53.771137, val:  61.25%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 66.93 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.1289%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0765%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2183%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 34445  12.132%\n",
      "fc layer 1 self.abs_max_out: 3363.0\n",
      "lif layer 1 self.abs_max_v: 5898.0\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  6.031188/ 45.179321, val:  69.58%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.71 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0829%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4516%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5557%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 35350  12.036%\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  6.395154/ 59.302971, val:  58.75%, val_best:  74.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.46 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0130%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3021%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 36294  11.959%\n",
      "fc layer 3 self.abs_max_out: 669.0\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  5.678727/ 34.814514, val:  76.67%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.30 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0881%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1608%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0148%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 37188  11.871%\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.898830/ 35.448135, val:  69.17%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.67 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0430%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8002%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8101%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 38047  11.777%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  6.152721/ 57.041859, val:  52.08%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.10 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0682%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5218%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4943%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 38970  11.708%\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.596534/ 36.630005, val:  70.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.24 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0746%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5243%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2293%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 39826  11.623%\n",
      "lif layer 2 self.abs_max_v: 2721.0\n",
      "fc layer 3 self.abs_max_out: 715.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  5.429596/ 50.511692, val:  68.75%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.49 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0810%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.5568%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9669%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 40633  11.529%\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.762911/ 43.281227, val:  72.08%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.82 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4906%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6896%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 41496  11.456%\n",
      "fc layer 2 self.abs_max_out: 1833.0\n",
      "fc layer 1 self.abs_max_out: 3422.0\n",
      "lif layer 1 self.abs_max_v: 5971.0\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.133668/ 55.312748, val:  65.42%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 67.28 seconds, 1.12 minutes\n",
      "layer   1  Sparsity: 91.0600%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4098%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 42273  11.363%\n",
      "fc layer 1 self.abs_max_out: 3465.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.916896/ 39.517059, val:  75.00%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 67.73 seconds, 1.13 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.4100%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6086%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 43154  11.302%\n",
      "fc layer 2 self.abs_max_out: 1839.0\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.665872/ 38.946728, val:  74.17%, val_best:  76.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 70.81 seconds, 1.18 minutes\n",
      "layer   1  Sparsity: 91.0408%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.2132%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0609%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 43966  11.227%\n",
      "fc layer 2 self.abs_max_out: 1845.0\n",
      "fc layer 2 self.abs_max_out: 1868.0\n",
      "fc layer 3 self.abs_max_out: 733.0\n",
      "fc layer 3 self.abs_max_out: 736.0\n",
      "fc layer 1 self.abs_max_out: 3506.0\n",
      "lif layer 2 self.abs_max_v: 2729.5\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.031070/ 58.926849, val:  62.50%, val_best:  76.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0894%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1945%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4045%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 44747  11.148%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  5.263006/ 33.405136, val:  78.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0525%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7825%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6788%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 45533  11.074%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.415737/ 31.636648, val:  82.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0935%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9273%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.6476%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 46316  11.002%\n",
      "fc layer 1 self.abs_max_out: 3535.0\n",
      "lif layer 1 self.abs_max_v: 5991.5\n",
      "lif layer 1 self.abs_max_v: 6015.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  4.958550/ 40.776260, val:  77.50%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0819%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0589%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 47071  10.927%\n",
      "fc layer 3 self.abs_max_out: 740.0\n",
      "fc layer 3 self.abs_max_out: 773.0\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  4.954809/ 39.008869, val:  80.42%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.74 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1066%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.3840%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9332%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 47820  10.855%\n",
      "fc layer 1 self.abs_max_out: 3586.0\n",
      "lif layer 1 self.abs_max_v: 6026.5\n",
      "fc layer 3 self.abs_max_out: 799.0\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  5.230276/ 47.306587, val:  67.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0894%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0090%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2854%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 48548  10.780%\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  4.874359/ 36.311218, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.47 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0656%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9630%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2649%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 49264  10.707%\n",
      "fc layer 1 self.abs_max_out: 3689.0\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.883227/ 43.496510, val:  75.83%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0905%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6902%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.0771%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 49978  10.635%\n",
      "lif layer 1 self.abs_max_v: 6053.0\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  4.723313/ 41.390530, val:  71.67%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0471%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8287%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.5468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 50707  10.570%\n",
      "fc layer 1 self.abs_max_out: 3696.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  5.014214/ 33.625992, val:  82.50%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1774%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8124%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 51427  10.506%\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  4.915155/ 38.573597, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.41 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0788%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6945%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9772%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 52173  10.449%\n",
      "lif layer 1 self.abs_max_v: 6113.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  4.638942/ 36.986824, val:  77.92%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.09 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0568%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6737%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 52886  10.389%\n",
      "fc layer 2 self.abs_max_out: 1898.0\n",
      "lif layer 1 self.abs_max_v: 6147.5\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.730399/ 37.201019, val:  74.58%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.57 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7136%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 53571  10.325%\n",
      "lif layer 1 self.abs_max_v: 6211.0\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.031453/ 47.727200, val:  62.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.34 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6773%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 54204  10.253%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.475929/ 38.594128, val:  77.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.51 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1230%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0085%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6736%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 54856  10.188%\n",
      "fc layer 1 self.abs_max_out: 3871.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.500269/ 37.299751, val:  81.67%, val_best:  82.50%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0694%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9683%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4317%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 55547  10.132%\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.495563/ 49.873665, val:  67.08%, val_best:  82.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.14 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6941%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2587%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 56221  10.075%\n",
      "lif layer 1 self.abs_max_v: 6380.5\n",
      "lif layer 1 self.abs_max_v: 6398.5\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.503906/ 35.330269, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.62 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0593%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6655%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9710%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 56895  10.020%\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.433883/ 36.579052, val:  82.50%, val_best:  84.17%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1275%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3826%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 57551   9.964%\n",
      "lif layer 1 self.abs_max_v: 6416.5\n",
      "lif layer 1 self.abs_max_v: 6530.5\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.129850/ 39.216000, val:  81.67%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0126%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3124%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 58194   9.907%\n",
      "fc layer 3 self.abs_max_out: 835.0\n",
      "fc layer 2 self.abs_max_out: 1934.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.432845/ 37.340637, val:  77.92%, val_best:  84.17%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0421%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9959%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 58821   9.850%\n",
      "fc layer 2 self.abs_max_out: 1967.0\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.016211/ 47.035767, val:  70.83%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.44 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1401%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 59431   9.791%\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.329673/ 34.581009, val:  84.17%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.43 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0554%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7625%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 60052   9.737%\n",
      "lif layer 1 self.abs_max_v: 6541.5\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  3.856098/ 37.765247, val:  81.25%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 69.48 seconds, 1.16 minutes\n",
      "layer   1  Sparsity: 91.0600%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9097%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0224%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 60641   9.678%\n",
      "fc layer 2 self.abs_max_out: 1976.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  4.118711/ 40.225147, val:  78.75%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 71.49 seconds, 1.19 minutes\n",
      "layer   1  Sparsity: 91.0355%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2388%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 61247   9.625%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  4.260904/ 38.062531, val:  82.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1009%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9308%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5624%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 61864   9.574%\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.057356/ 44.732216, val:  74.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.26 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1105%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.0240%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 62493   9.527%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.858820/ 43.902748, val:  74.58%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0861%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7932%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3886%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 63062   9.473%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  3.820769/ 44.044094, val:  77.92%, val_best:  84.17%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.79 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1113%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5994%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4439%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 63631   9.420%\n",
      "fc layer 1 self.abs_max_out: 3984.0\n",
      "fc layer 2 self.abs_max_out: 1993.0\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  3.436901/ 29.877403, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.52 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0784%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6614%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2494%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 64161   9.362%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  3.892188/ 38.390900, val:  82.08%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0536%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4224%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1781%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 64741   9.314%\n",
      "fc layer 2 self.abs_max_out: 1999.0\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.692058/ 52.878910, val:  71.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5805%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 65321   9.267%\n",
      "lif layer 2 self.abs_max_v: 2744.5\n",
      "fc layer 2 self.abs_max_out: 2003.0\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.633592/ 41.052395, val:  78.33%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 78.37 seconds, 1.31 minutes\n",
      "layer   1  Sparsity: 91.0550%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5354%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3896%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 65879   9.218%\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.290021/ 35.617489, val:  85.83%, val_best:  87.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 77.60 seconds, 1.29 minutes\n",
      "layer   1  Sparsity: 91.0492%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3575%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9078%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 66400   9.165%\n",
      "fc layer 1 self.abs_max_out: 4017.0\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.579618/ 46.656837, val:  69.58%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.21 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5085%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9664%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 66931   9.116%\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.616486/ 39.322498, val:  84.17%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1032%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5797%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6068%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 67468   9.068%\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.636461/ 38.312210, val:  80.00%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.23 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1566%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6010%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1866%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 68044   9.026%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.310714/ 37.018696, val:  82.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.02 seconds, 1.23 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2574%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2456%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 68564   8.979%\n",
      "lif layer 1 self.abs_max_v: 6692.0\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.286160/ 47.449322, val:  75.83%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0996%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3961%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2360%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 69065   8.930%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.311328/ 37.681732, val:  81.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.69 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6147%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1727%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 69595   8.886%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.245290/ 30.353645, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.31 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0886%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2116%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 70074   8.837%\n",
      "lif layer 2 self.abs_max_v: 2754.0\n",
      "lif layer 2 self.abs_max_v: 2777.0\n",
      "fc layer 2 self.abs_max_out: 2026.0\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.115521/ 38.056850, val:  80.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0916%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5102%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 70577   8.792%\n",
      "fc layer 1 self.abs_max_out: 4066.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.350288/ 50.225533, val:  74.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.55 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6689%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3731%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 71093   8.749%\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.241343/ 42.284973, val:  80.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7397%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4530%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 71597   8.706%\n",
      "lif layer 2 self.abs_max_v: 2832.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  2.888196/ 37.987915, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.12 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1055%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4847%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 72090   8.663%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  2.942469/ 37.283337, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.49 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0362%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1947%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3001%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 72572   8.620%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.116086/ 45.993904, val:  77.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.45 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.1223%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9340%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 73053   8.577%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.096022/ 38.861912, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.70 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8346%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 73549   8.537%\n",
      "fc layer 1 self.abs_max_out: 4170.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.324493/ 43.610035, val:  78.75%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.52 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0536%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5294%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5468%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 74073   8.501%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.349331/ 45.767315, val:  80.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.36 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0575%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5993%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 74573   8.464%\n",
      "fc layer 2 self.abs_max_out: 2064.0\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.132975/ 35.851746, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.27 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0513%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7267%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4636%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 75049   8.424%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  2.910037/ 37.270325, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.65 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0331%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7683%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5222%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 75489   8.381%\n",
      "lif layer 2 self.abs_max_v: 2895.0\n",
      "lif layer 2 self.abs_max_v: 3018.5\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  2.973882/ 34.854916, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0868%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7256%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9514%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 75953   8.342%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.212638/ 36.612473, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1233%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7521%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4244%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 76429   8.305%\n",
      "fc layer 2 self.abs_max_out: 2083.0\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  3.110258/ 42.428387, val:  82.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0615%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5269%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0109%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 76901   8.268%\n",
      "fc layer 1 self.abs_max_out: 4262.0\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  2.913311/ 36.085163, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.42 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7629%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6652%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 77331   8.228%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  2.813630/ 46.872597, val:  75.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.49 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8276%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1324%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 77732   8.186%\n",
      "fc layer 1 self.abs_max_out: 4267.0\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  3.150048/ 34.583321, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.80 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0437%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3337%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7817%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 78205   8.151%\n",
      "fc layer 1 self.abs_max_out: 4367.0\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.797762/ 41.218754, val:  79.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1556%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8952%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 78623   8.112%\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.607778/ 43.929665, val:  80.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.68 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0771%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3767%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3682%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 79037   8.073%\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.862859/ 35.212448, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.29 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.1036%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5911%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3151%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 79491   8.039%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.871105/ 41.956352, val:  82.08%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.32 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0615%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7443%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0266%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 79929   8.004%\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.946492/ 41.603645, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0684%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5693%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 80377   7.971%\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.609736/ 40.805023, val:  81.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.66 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0714%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.9522%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3001%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 80761   7.932%\n",
      "fc layer 2 self.abs_max_out: 2116.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  2.414448/ 32.456459, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0531%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4773%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4397%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 81164   7.896%\n",
      "fc layer 2 self.abs_max_out: 2127.0\n",
      "fc layer 2 self.abs_max_out: 2152.0\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.860811/ 36.257938, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0369%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2276%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3139%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 81610   7.864%\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  2.698479/ 38.706455, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.45 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0874%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4673%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2824%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 82038   7.832%\n",
      "lif layer 1 self.abs_max_v: 6710.0\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.667284/ 37.181259, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.93 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.1035%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4758%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5736%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 82462   7.799%\n",
      "fc layer 1 self.abs_max_out: 4368.0\n",
      "fc layer 1 self.abs_max_out: 4438.0\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.681042/ 39.215797, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0898%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4016%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7371%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 82888   7.768%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.547642/ 41.726410, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1014%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3115%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7367%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 83278   7.733%\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.678070/ 51.984177, val:  75.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0532%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3467%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4402%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 83687   7.701%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.710205/ 38.522213, val:  83.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0549%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2505%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4079%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 84115   7.671%\n",
      "fc layer 2 self.abs_max_out: 2163.0\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.588906/ 40.182045, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1012%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5918%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6842%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 84513   7.639%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.656587/ 38.604107, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2561%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3683%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 84927   7.610%\n",
      "fc layer 1 self.abs_max_out: 4464.0\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.193232/ 35.047840, val:  86.25%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.02 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0844%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3306%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2080%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 85284   7.575%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.542956/ 43.580536, val:  82.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.86 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0733%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8680%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 85678   7.544%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.621404/ 34.527069, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0830%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5845%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4331%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 86086   7.516%\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.398138/ 49.510582, val:  83.75%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.53 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0930%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5007%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9313%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 86437   7.482%\n",
      "fc layer 3 self.abs_max_out: 844.0\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.245344/ 38.440205, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4998%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4716%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 86820   7.452%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.318286/ 36.382584, val:  86.67%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0880%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6549%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5461%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 87163   7.419%\n",
      "fc layer 2 self.abs_max_out: 2204.0\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.480536/ 37.437992, val:  84.58%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 76.19 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0902%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5005%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0852%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 87523   7.388%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.477217/ 35.700077, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0752%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7405%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1577%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 87911   7.360%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.065479/ 43.872444, val:  83.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0935%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6828%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 88267   7.330%\n",
      "fc layer 2 self.abs_max_out: 2252.0\n",
      "lif layer 1 self.abs_max_v: 6713.5\n",
      "lif layer 1 self.abs_max_v: 6874.0\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.178801/ 51.299541, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0823%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.8610%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2207%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 88611   7.299%\n",
      "lif layer 1 self.abs_max_v: 6921.5\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  1.914705/ 42.857109, val:  82.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0941%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6484%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2766%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 88905   7.265%\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  1.901175/ 35.200478, val:  87.92%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0717%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4041%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6303%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 89229   7.234%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.076557/ 36.890709, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0687%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6983%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7429%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 89556   7.203%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.365940/ 39.131744, val:  85.42%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4842%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6552%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 89931   7.177%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.196676/ 43.166500, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.75 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1046%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3733%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4202%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 90293   7.150%\n",
      "fc layer 1 self.abs_max_out: 4529.0\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  2.157795/ 42.763565, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0949%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1796%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3919%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 90629   7.121%\n",
      "fc layer 1 self.abs_max_out: 4532.0\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.253153/ 39.843666, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0978%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3708%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9998%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 90976   7.094%\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  1.944554/ 37.348202, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.77 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0535%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4340%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2945%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 91290   7.064%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  2.103838/ 41.193813, val:  78.75%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5561%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2498%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 91627   7.037%\n",
      "fc layer 1 self.abs_max_out: 4627.0\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.014775/ 39.203426, val:  85.83%, val_best:  88.33%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0761%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4568%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0961%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 91957   7.010%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.051233/ 36.324562, val:  85.83%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1204%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6162%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3090%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 92283   6.982%\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.300252/ 34.499088, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0704%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5218%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1800%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 92618   6.956%\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  2.041602/ 35.046490, val:  85.00%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.70 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0823%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3028%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9218%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 92938   6.929%\n",
      "lif layer 1 self.abs_max_v: 6927.0\n",
      "fc layer 2 self.abs_max_out: 2258.0\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  2.319338/ 34.565559, val:  90.42%, val_best:  90.42%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4955%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9239%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 93291   6.905%\n",
      "fc layer 2 self.abs_max_out: 2288.0\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  1.808373/ 41.931202, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0837%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7197%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3711%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 93599   6.878%\n",
      "lif layer 1 self.abs_max_v: 6945.5\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.149479/ 36.533958, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0863%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6997%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3425%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 93928   6.853%\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  1.688389/ 46.069981, val:  85.00%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.49 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0200%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3406%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2434%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 94214   6.825%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.077917/ 38.402527, val:  85.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0761%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3063%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5373%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 94543   6.801%\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  1.996856/ 39.496929, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0331%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5403%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8931%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 94863   6.776%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  1.643535/ 41.431084, val:  83.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0887%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6658%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 95148   6.749%\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  1.916440/ 37.667236, val:  86.67%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0731%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.7950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8153%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 95470   6.725%\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  2.071119/ 44.569454, val:  83.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6061%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5951%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 95771   6.700%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  1.809163/ 33.865482, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0828%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3784%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4843%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 96063   6.675%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  1.695070/ 39.828766, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1020%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6497%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4838%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 96335   6.649%\n",
      "fc layer 1 self.abs_max_out: 4763.0\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.946577/ 33.970081, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0649%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6174%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3914%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 96631   6.624%\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.638434/ 40.551060, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.21 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6108%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7445%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 96910   6.599%\n",
      "fc layer 2 self.abs_max_out: 2314.0\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  1.818919/ 39.386024, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.01 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6080%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 97205   6.576%\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  2.271782/ 37.897263, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1116%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5920%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 97554   6.556%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.823437/ 39.315258, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4458%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5329%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 97870   6.534%\n",
      "fc layer 3 self.abs_max_out: 847.0\n",
      "fc layer 2 self.abs_max_out: 2316.0\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  1.847691/ 41.432125, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0550%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5009%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4062%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 98165   6.511%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.900639/ 36.410271, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1100%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3551%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6671%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 98481   6.490%\n",
      "fc layer 2 self.abs_max_out: 2342.0\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  1.880571/ 39.859276, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1090%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3204%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8109%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 98762   6.467%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  2.081505/ 40.201160, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2849%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6030%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 99063   6.445%\n",
      "fc layer 3 self.abs_max_out: 870.0\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  1.745799/ 41.141758, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0804%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0197%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5930%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 99349   6.423%\n",
      "lif layer 1 self.abs_max_v: 6953.0\n",
      "lif layer 1 self.abs_max_v: 7060.5\n",
      "lif layer 1 self.abs_max_v: 7204.5\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.638292/ 39.329201, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0703%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0246%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 99613   6.399%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.408377/ 42.174091, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0975%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0311%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6496%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 99846   6.374%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.884005/ 42.827702, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.70 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0239%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0411%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8358%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 100125   6.352%\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.658439/ 39.620846, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0738%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1851%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2880%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 100385   6.330%\n",
      "fc layer 3 self.abs_max_out: 887.0\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.890824/ 38.459827, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.94 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0474%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1931%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1909%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 100673   6.309%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.431460/ 35.015980, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0476%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1678%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8456%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 100933   6.286%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.729075/ 34.234886, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0840%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1759%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9884%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 101222   6.266%\n",
      "fc layer 1 self.abs_max_out: 4771.0\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.820879/ 38.606075, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.87 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0894%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2905%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0566%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 101486   6.245%\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.717076/ 43.340382, val:  82.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.86 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0977%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4265%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7801%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 101762   6.224%\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.887603/ 44.704491, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0877%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5292%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0723%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 102048   6.205%\n",
      "lif layer 1 self.abs_max_v: 7299.5\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.946423/ 42.735992, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0723%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3518%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7497%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 102343   6.186%\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  1.735974/ 43.695446, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0740%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3836%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9305%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 102617   6.166%\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.486879/ 45.398621, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.00 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0925%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.6977%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3239%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 102894   6.146%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.800750/ 49.742775, val:  80.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4517%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6706%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 103171   6.127%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.400781/ 42.636528, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.1862%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7077%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 103401   6.105%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.601367/ 35.666084, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1007%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9917%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9999%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 103646   6.084%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.662344/ 41.357475, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0542%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7946%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 103914   6.065%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.394190/ 44.715721, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 90.9950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4232%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5832%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 104158   6.045%\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.631266/ 38.643494, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.08 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0522%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2905%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0295%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 104399   6.025%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.352502/ 37.408302, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0631%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0309%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 104620   6.004%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.350215/ 41.115067, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0609%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0789%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1006%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 104833   5.982%\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.395867/ 45.194519, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3766%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3359%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 105043   5.961%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.387093/ 42.388943, val:  84.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5262%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3101%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 105285   5.942%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.589824/ 39.304157, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.32 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0950%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4973%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0396%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 105535   5.923%\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.724110/ 42.868263, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1065%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4502%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0262%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 105785   5.905%\n",
      "fc layer 3 self.abs_max_out: 895.0\n",
      "lif layer 2 self.abs_max_v: 3146.5\n",
      "lif layer 2 self.abs_max_v: 3297.5\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.378542/ 41.767937, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2668%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 106012   5.885%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.734216/ 42.638504, val:  86.25%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0693%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3563%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8345%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 106252   5.867%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.249108/ 40.352840, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1015%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3352%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1412%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 106464   5.847%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.590623/ 40.222591, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1203%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.3024%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 106682   5.827%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.436745/ 43.472969, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0896%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4656%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1787%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 106916   5.809%\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.528206/ 37.965992, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0857%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3523%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9442%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 107161   5.792%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.357006/ 42.089077, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0941%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.4466%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8123%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 107375   5.773%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.572122/ 50.892525, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.12 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3721%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7927%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 107627   5.756%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.434986/ 37.557594, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0928%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.5659%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0361%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 107865   5.738%\n",
      "fc layer 1 self.abs_max_out: 4793.0\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.403834/ 38.450802, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.02 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0726%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3610%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 108096   5.721%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.198716/ 42.283649, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1050%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.2575%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.4005%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 108287   5.702%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.298920/ 41.738831, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0587%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3303%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1169%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 108509   5.684%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.253982/ 43.796589, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0421%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3500%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1861%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 108732   5.667%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.260815/ 44.685200, val:  87.50%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0983%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.3584%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0438%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 108930   5.648%\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.359012/ 44.237595, val:  85.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.76 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0736%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0904%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9297%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 109132   5.630%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.329910/ 40.643486, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0513%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 71.0848%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 109338   5.612%\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.312411/ 54.317738, val:  82.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.92 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1004%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 70.9168%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5826%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7516806cc7b4461ea03b36efd3d94d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÖ‚ñà‚ñÑ‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÖ‚ñá‚ñÇ‚ñà‚ñÖ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.31241</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.82083</td></tr><tr><td>val_loss</td><td>54.31774</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">major-sweep-71</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/di93h6ms' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/di93h6ms</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_121347-di93h6ms/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yicr2j39 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 3003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_161936-yicr2j39</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yicr2j39' target=\"_blank\">radiant-sweep-75</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yicr2j39' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yicr2j39</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251214_161946_061', 'my_seed': 3003, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 103.0\n",
      "lif layer 1 self.abs_max_v: 103.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 133.0\n",
      "lif layer 1 self.abs_max_v: 164.5\n",
      "fc layer 2 self.abs_max_out: 113.0\n",
      "lif layer 2 self.abs_max_v: 113.0\n",
      "fc layer 1 self.abs_max_out: 139.0\n",
      "lif layer 1 self.abs_max_v: 215.0\n",
      "fc layer 2 self.abs_max_out: 251.0\n",
      "lif layer 2 self.abs_max_v: 252.0\n",
      "fc layer 3 self.abs_max_out: 59.0\n",
      "lif layer 1 self.abs_max_v: 241.5\n",
      "fc layer 1 self.abs_max_out: 150.0\n",
      "lif layer 1 self.abs_max_v: 271.0\n",
      "fc layer 1 self.abs_max_out: 176.0\n",
      "lif layer 2 self.abs_max_v: 257.5\n",
      "fc layer 1 self.abs_max_out: 190.0\n",
      "lif layer 1 self.abs_max_v: 280.5\n",
      "lif layer 2 self.abs_max_v: 289.0\n",
      "fc layer 3 self.abs_max_out: 87.0\n",
      "fc layer 1 self.abs_max_out: 210.0\n",
      "lif layer 1 self.abs_max_v: 297.5\n",
      "fc layer 2 self.abs_max_out: 277.0\n",
      "lif layer 2 self.abs_max_v: 293.5\n",
      "lif layer 2 self.abs_max_v: 381.5\n",
      "fc layer 1 self.abs_max_out: 237.0\n",
      "fc layer 1 self.abs_max_out: 277.0\n",
      "lif layer 1 self.abs_max_v: 371.5\n",
      "fc layer 2 self.abs_max_out: 282.0\n",
      "fc layer 3 self.abs_max_out: 112.0\n",
      "lif layer 1 self.abs_max_v: 410.0\n",
      "fc layer 2 self.abs_max_out: 295.0\n",
      "fc layer 2 self.abs_max_out: 299.0\n",
      "lif layer 2 self.abs_max_v: 432.5\n",
      "fc layer 3 self.abs_max_out: 175.0\n",
      "fc layer 1 self.abs_max_out: 296.0\n",
      "lif layer 1 self.abs_max_v: 420.0\n",
      "fc layer 2 self.abs_max_out: 307.0\n",
      "fc layer 1 self.abs_max_out: 375.0\n",
      "lif layer 1 self.abs_max_v: 475.0\n",
      "fc layer 2 self.abs_max_out: 399.0\n",
      "lif layer 2 self.abs_max_v: 588.0\n",
      "fc layer 1 self.abs_max_out: 497.0\n",
      "lif layer 1 self.abs_max_v: 552.5\n",
      "lif layer 2 self.abs_max_v: 621.0\n",
      "fc layer 1 self.abs_max_out: 567.0\n",
      "lif layer 1 self.abs_max_v: 567.0\n",
      "lif layer 2 self.abs_max_v: 640.5\n",
      "fc layer 2 self.abs_max_out: 412.0\n",
      "fc layer 2 self.abs_max_out: 419.0\n",
      "lif layer 1 self.abs_max_v: 640.5\n",
      "fc layer 2 self.abs_max_out: 483.0\n",
      "fc layer 1 self.abs_max_out: 615.0\n",
      "lif layer 2 self.abs_max_v: 653.0\n",
      "lif layer 2 self.abs_max_v: 701.5\n",
      "fc layer 3 self.abs_max_out: 177.0\n",
      "fc layer 2 self.abs_max_out: 551.0\n",
      "lif layer 2 self.abs_max_v: 747.5\n",
      "fc layer 3 self.abs_max_out: 185.0\n",
      "lif layer 2 self.abs_max_v: 749.0\n",
      "lif layer 2 self.abs_max_v: 783.5\n",
      "fc layer 3 self.abs_max_out: 199.0\n",
      "fc layer 1 self.abs_max_out: 616.0\n",
      "lif layer 1 self.abs_max_v: 676.5\n",
      "fc layer 3 self.abs_max_out: 204.0\n",
      "fc layer 3 self.abs_max_out: 212.0\n",
      "fc layer 1 self.abs_max_out: 618.0\n",
      "lif layer 1 self.abs_max_v: 685.5\n",
      "lif layer 1 self.abs_max_v: 694.0\n",
      "lif layer 1 self.abs_max_v: 732.0\n",
      "fc layer 2 self.abs_max_out: 599.0\n",
      "lif layer 1 self.abs_max_v: 764.0\n",
      "lif layer 2 self.abs_max_v: 909.5\n",
      "lif layer 2 self.abs_max_v: 1016.0\n",
      "fc layer 1 self.abs_max_out: 635.0\n",
      "fc layer 2 self.abs_max_out: 614.0\n",
      "fc layer 1 self.abs_max_out: 639.0\n",
      "lif layer 1 self.abs_max_v: 819.0\n",
      "lif layer 1 self.abs_max_v: 837.5\n",
      "fc layer 1 self.abs_max_out: 779.0\n",
      "fc layer 2 self.abs_max_out: 636.0\n",
      "fc layer 1 self.abs_max_out: 838.0\n",
      "lif layer 1 self.abs_max_v: 838.0\n",
      "lif layer 1 self.abs_max_v: 874.0\n",
      "lif layer 1 self.abs_max_v: 994.0\n",
      "fc layer 2 self.abs_max_out: 654.0\n",
      "fc layer 3 self.abs_max_out: 260.0\n",
      "fc layer 3 self.abs_max_out: 273.0\n",
      "fc layer 2 self.abs_max_out: 661.0\n",
      "fc layer 2 self.abs_max_out: 682.0\n",
      "fc layer 2 self.abs_max_out: 693.0\n",
      "fc layer 1 self.abs_max_out: 845.0\n",
      "fc layer 2 self.abs_max_out: 724.0\n",
      "fc layer 1 self.abs_max_out: 886.0\n",
      "fc layer 1 self.abs_max_out: 929.0\n",
      "fc layer 2 self.abs_max_out: 776.0\n",
      "lif layer 1 self.abs_max_v: 1002.0\n",
      "lif layer 1 self.abs_max_v: 1011.0\n",
      "fc layer 1 self.abs_max_out: 938.0\n",
      "lif layer 2 self.abs_max_v: 1028.5\n",
      "lif layer 2 self.abs_max_v: 1056.0\n",
      "fc layer 2 self.abs_max_out: 785.0\n",
      "lif layer 1 self.abs_max_v: 1075.0\n",
      "fc layer 1 self.abs_max_out: 1097.0\n",
      "lif layer 1 self.abs_max_v: 1097.0\n",
      "lif layer 1 self.abs_max_v: 1100.5\n",
      "lif layer 1 self.abs_max_v: 1120.5\n",
      "lif layer 1 self.abs_max_v: 1131.5\n",
      "lif layer 1 self.abs_max_v: 1151.0\n",
      "lif layer 1 self.abs_max_v: 1173.5\n",
      "lif layer 2 self.abs_max_v: 1070.0\n",
      "fc layer 2 self.abs_max_out: 802.0\n",
      "lif layer 1 self.abs_max_v: 1265.0\n",
      "lif layer 1 self.abs_max_v: 1311.0\n",
      "fc layer 1 self.abs_max_out: 1155.0\n",
      "lif layer 2 self.abs_max_v: 1112.5\n",
      "lif layer 2 self.abs_max_v: 1153.5\n",
      "lif layer 2 self.abs_max_v: 1163.5\n",
      "fc layer 2 self.abs_max_out: 828.0\n",
      "fc layer 3 self.abs_max_out: 274.0\n",
      "fc layer 2 self.abs_max_out: 872.0\n",
      "lif layer 1 self.abs_max_v: 1318.0\n",
      "fc layer 1 self.abs_max_out: 1169.0\n",
      "lif layer 2 self.abs_max_v: 1192.0\n",
      "fc layer 1 self.abs_max_out: 1217.0\n",
      "lif layer 1 self.abs_max_v: 1391.5\n",
      "fc layer 1 self.abs_max_out: 1319.0\n",
      "lif layer 1 self.abs_max_v: 1404.5\n",
      "lif layer 1 self.abs_max_v: 1498.5\n",
      "lif layer 1 self.abs_max_v: 1504.5\n",
      "lif layer 2 self.abs_max_v: 1199.5\n",
      "lif layer 2 self.abs_max_v: 1215.5\n",
      "fc layer 3 self.abs_max_out: 279.0\n",
      "lif layer 2 self.abs_max_v: 1232.0\n",
      "lif layer 2 self.abs_max_v: 1238.0\n",
      "lif layer 1 self.abs_max_v: 1558.5\n",
      "lif layer 1 self.abs_max_v: 1578.5\n",
      "fc layer 3 self.abs_max_out: 302.0\n",
      "lif layer 1 self.abs_max_v: 1617.5\n",
      "lif layer 1 self.abs_max_v: 1755.0\n",
      "lif layer 2 self.abs_max_v: 1269.0\n",
      "lif layer 2 self.abs_max_v: 1269.5\n",
      "fc layer 2 self.abs_max_out: 874.0\n",
      "lif layer 2 self.abs_max_v: 1322.0\n",
      "lif layer 2 self.abs_max_v: 1418.0\n",
      "fc layer 3 self.abs_max_out: 316.0\n",
      "fc layer 3 self.abs_max_out: 327.0\n",
      "fc layer 2 self.abs_max_out: 911.0\n",
      "fc layer 2 self.abs_max_out: 921.0\n",
      "fc layer 1 self.abs_max_out: 1364.0\n",
      "fc layer 3 self.abs_max_out: 337.0\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "fc layer 2 self.abs_max_out: 930.0\n",
      "fc layer 3 self.abs_max_out: 376.0\n",
      "fc layer 2 self.abs_max_out: 943.0\n",
      "fc layer 1 self.abs_max_out: 1456.0\n",
      "fc layer 2 self.abs_max_out: 956.0\n",
      "fc layer 2 self.abs_max_out: 974.0\n",
      "fc layer 2 self.abs_max_out: 986.0\n",
      "fc layer 2 self.abs_max_out: 1025.0\n",
      "lif layer 2 self.abs_max_v: 1442.0\n",
      "fc layer 2 self.abs_max_out: 1043.0\n",
      "lif layer 1 self.abs_max_v: 1761.5\n",
      "fc layer 3 self.abs_max_out: 380.0\n",
      "fc layer 1 self.abs_max_out: 1500.0\n",
      "lif layer 1 self.abs_max_v: 1842.5\n",
      "fc layer 1 self.abs_max_out: 1531.0\n",
      "fc layer 2 self.abs_max_out: 1082.0\n",
      "fc layer 1 self.abs_max_out: 1533.0\n",
      "lif layer 1 self.abs_max_v: 1879.5\n",
      "fc layer 1 self.abs_max_out: 1552.0\n",
      "lif layer 1 self.abs_max_v: 1904.0\n",
      "lif layer 1 self.abs_max_v: 1941.5\n",
      "fc layer 2 self.abs_max_out: 1165.0\n",
      "fc layer 3 self.abs_max_out: 385.0\n",
      "fc layer 3 self.abs_max_out: 393.0\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "fc layer 1 self.abs_max_out: 1587.0\n",
      "lif layer 1 self.abs_max_v: 2064.5\n",
      "lif layer 1 self.abs_max_v: 2138.0\n",
      "fc layer 1 self.abs_max_out: 1608.0\n",
      "lif layer 1 self.abs_max_v: 2478.5\n",
      "lif layer 1 self.abs_max_v: 2660.5\n",
      "lif layer 1 self.abs_max_v: 2665.0\n",
      "lif layer 1 self.abs_max_v: 2785.5\n",
      "lif layer 1 self.abs_max_v: 2868.0\n",
      "lif layer 1 self.abs_max_v: 2905.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 14.027080/ 55.744362, val:  39.17%, val_best:  39.17%, tr:  97.55%, tr_best:  97.55%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0394%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.9611%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 72.1986%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2270  23.187%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 1445.5\n",
      "lif layer 2 self.abs_max_v: 1483.5\n",
      "lif layer 2 self.abs_max_v: 1516.0\n",
      "fc layer 1 self.abs_max_out: 1617.0\n",
      "fc layer 1 self.abs_max_out: 1633.0\n",
      "fc layer 3 self.abs_max_out: 404.0\n",
      "fc layer 1 self.abs_max_out: 1677.0\n",
      "lif layer 2 self.abs_max_v: 1524.0\n",
      "fc layer 1 self.abs_max_out: 1709.0\n",
      "fc layer 2 self.abs_max_out: 1176.0\n",
      "fc layer 2 self.abs_max_out: 1181.0\n",
      "fc layer 2 self.abs_max_out: 1244.0\n",
      "fc layer 3 self.abs_max_out: 415.0\n",
      "fc layer 2 self.abs_max_out: 1274.0\n",
      "fc layer 1 self.abs_max_out: 1730.0\n",
      "fc layer 1 self.abs_max_out: 1856.0\n",
      "fc layer 2 self.abs_max_out: 1292.0\n",
      "fc layer 1 self.abs_max_out: 1997.0\n",
      "lif layer 1 self.abs_max_v: 3163.5\n",
      "lif layer 1 self.abs_max_v: 3310.0\n",
      "lif layer 1 self.abs_max_v: 3467.5\n",
      "lif layer 1 self.abs_max_v: 3583.0\n",
      "lif layer 2 self.abs_max_v: 1527.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.343159/ 81.175720, val:  33.33%, val_best:  39.17%, tr:  99.39%, tr_best:  99.39%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0464%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.3058%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 70.1728%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3954  20.194%\n",
      "lif layer 2 self.abs_max_v: 1532.5\n",
      "lif layer 2 self.abs_max_v: 1600.5\n",
      "lif layer 2 self.abs_max_v: 1604.0\n",
      "lif layer 2 self.abs_max_v: 1704.0\n",
      "lif layer 2 self.abs_max_v: 1735.0\n",
      "fc layer 1 self.abs_max_out: 2092.0\n",
      "lif layer 1 self.abs_max_v: 3651.5\n",
      "lif layer 1 self.abs_max_v: 3781.5\n",
      "fc layer 2 self.abs_max_out: 1332.0\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  9.625161/ 41.729050, val:  54.58%, val_best:  54.58%, tr:  99.28%, tr_best:  99.39%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1122%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0790%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4453%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5426  18.475%\n",
      "fc layer 1 self.abs_max_out: 2107.0\n",
      "fc layer 1 self.abs_max_out: 2136.0\n",
      "fc layer 1 self.abs_max_out: 2197.0\n",
      "fc layer 3 self.abs_max_out: 417.0\n",
      "fc layer 1 self.abs_max_out: 2199.0\n",
      "lif layer 1 self.abs_max_v: 3928.0\n",
      "lif layer 1 self.abs_max_v: 3981.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  8.992685/ 91.134308, val:  39.17%, val_best:  54.58%, tr:  99.28%, tr_best:  99.39%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0782%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.1835%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6227%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6835  17.454%\n",
      "fc layer 1 self.abs_max_out: 2219.0\n",
      "fc layer 3 self.abs_max_out: 423.0\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "fc layer 3 self.abs_max_out: 448.0\n",
      "fc layer 1 self.abs_max_out: 2275.0\n",
      "lif layer 2 self.abs_max_v: 1842.0\n",
      "fc layer 1 self.abs_max_out: 2537.0\n",
      "fc layer 2 self.abs_max_out: 1397.0\n",
      "fc layer 3 self.abs_max_out: 451.0\n",
      "lif layer 1 self.abs_max_v: 4014.5\n",
      "lif layer 1 self.abs_max_v: 4132.5\n",
      "lif layer 1 self.abs_max_v: 4151.0\n",
      "fc layer 2 self.abs_max_out: 1454.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  8.798025/ 69.083275, val:  35.83%, val_best:  54.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 75.38 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0821%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.9293%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2863%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 8220  16.793%\n",
      "lif layer 2 self.abs_max_v: 1878.5\n",
      "fc layer 1 self.abs_max_out: 2548.0\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  8.572103/ 65.980080, val:  46.25%, val_best:  54.58%, tr:  99.49%, tr_best:  99.49%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0609%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.6637%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8307%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9554  16.265%\n",
      "lif layer 1 self.abs_max_v: 4206.5\n",
      "lif layer 1 self.abs_max_v: 4422.5\n",
      "lif layer 1 self.abs_max_v: 4469.5\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  9.215484/ 38.523739, val:  55.00%, val_best:  55.00%, tr:  99.59%, tr_best:  99.59%, epoch time: 75.63 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0732%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6967%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.3346%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10900  15.905%\n",
      "fc layer 1 self.abs_max_out: 2582.0\n",
      "fc layer 3 self.abs_max_out: 472.0\n",
      "fc layer 2 self.abs_max_out: 1476.0\n",
      "lif layer 2 self.abs_max_v: 1889.0\n",
      "lif layer 2 self.abs_max_v: 2023.5\n",
      "lif layer 2 self.abs_max_v: 2052.0\n",
      "lif layer 2 self.abs_max_v: 2099.0\n",
      "lif layer 1 self.abs_max_v: 4517.5\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.316129/ 67.771675, val:  49.58%, val_best:  55.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0719%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2675%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1766%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 12173  15.543%\n",
      "fc layer 3 self.abs_max_out: 539.0\n",
      "fc layer 1 self.abs_max_out: 2645.0\n",
      "lif layer 1 self.abs_max_v: 4541.0\n",
      "lif layer 1 self.abs_max_v: 4755.5\n",
      "lif layer 1 self.abs_max_v: 4824.0\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  8.472667/ 76.208450, val:  43.75%, val_best:  55.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 75.85 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0841%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.0257%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2150%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13435  15.248%\n",
      "fc layer 1 self.abs_max_out: 2817.0\n",
      "fc layer 2 self.abs_max_out: 1497.0\n",
      "fc layer 2 self.abs_max_out: 1523.0\n",
      "lif layer 1 self.abs_max_v: 4943.5\n",
      "lif layer 1 self.abs_max_v: 5015.0\n",
      "fc layer 2 self.abs_max_out: 1536.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  7.728626/ 70.777565, val:  43.75%, val_best:  55.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0320%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6737%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9327%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14679  14.994%\n",
      "lif layer 2 self.abs_max_v: 2204.5\n",
      "fc layer 1 self.abs_max_out: 2972.0\n",
      "lif layer 2 self.abs_max_v: 2237.5\n",
      "lif layer 2 self.abs_max_v: 2247.5\n",
      "lif layer 1 self.abs_max_v: 5103.5\n",
      "lif layer 1 self.abs_max_v: 5176.0\n",
      "fc layer 2 self.abs_max_out: 1539.0\n",
      "fc layer 2 self.abs_max_out: 1546.0\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  8.253566/ 71.545502, val:  46.25%, val_best:  55.00%, tr:  99.69%, tr_best:  99.80%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0450%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6420%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5096%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15954  14.815%\n",
      "lif layer 2 self.abs_max_v: 2285.0\n",
      "fc layer 2 self.abs_max_out: 1607.0\n",
      "fc layer 1 self.abs_max_out: 3036.0\n",
      "fc layer 2 self.abs_max_out: 1690.0\n",
      "fc layer 2 self.abs_max_out: 1716.0\n",
      "fc layer 2 self.abs_max_out: 1730.0\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  8.616688/ 35.033535, val:  60.42%, val_best:  60.42%, tr:  99.69%, tr_best:  99.80%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0755%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.1076%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1657%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 17248  14.682%\n",
      "fc layer 1 self.abs_max_out: 3056.0\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.960204/ 53.689678, val:  53.33%, val_best:  60.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.19 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0692%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.5055%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4593%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 18469  14.512%\n",
      "fc layer 3 self.abs_max_out: 571.0\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  8.028807/ 67.676422, val:  52.92%, val_best:  60.42%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1245%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2604%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5865%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19662  14.346%\n",
      "lif layer 1 self.abs_max_v: 5319.5\n",
      "lif layer 1 self.abs_max_v: 5410.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.583337/ 44.601383, val:  55.00%, val_best:  60.42%, tr:  99.80%, tr_best:  99.90%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0431%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6046%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.2505%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20812  14.172%\n",
      "lif layer 2 self.abs_max_v: 2294.5\n",
      "lif layer 2 self.abs_max_v: 2303.5\n",
      "lif layer 2 self.abs_max_v: 2358.0\n",
      "epoch-15  lr=['1.0000000'], tr/val_loss:  7.530409/ 34.432060, val:  71.67%, val_best:  71.67%, tr:  99.69%, tr_best:  99.90%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0933%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9186%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 156640 real_backward_count 21947  14.011%\n",
      "epoch-16  lr=['1.0000000'], tr/val_loss:  7.429360/ 41.071766, val:  64.17%, val_best:  71.67%, tr:  99.59%, tr_best:  99.90%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8968%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9580%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 166430 real_backward_count 23092  13.875%\n",
      "epoch-17  lr=['1.0000000'], tr/val_loss:  7.214334/ 37.446304, val:  63.33%, val_best:  71.67%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0402%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.9508%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8552%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 176220 real_backward_count 24179  13.721%\n",
      "lif layer 2 self.abs_max_v: 2363.0\n",
      "lif layer 2 self.abs_max_v: 2442.5\n",
      "lif layer 1 self.abs_max_v: 5462.0\n",
      "lif layer 1 self.abs_max_v: 5566.0\n",
      "epoch-18  lr=['1.0000000'], tr/val_loss:  7.121007/ 32.879864, val:  69.17%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.82 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1023%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.8157%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4530%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 186010 real_backward_count 25268  13.584%\n",
      "epoch-19  lr=['1.0000000'], tr/val_loss:  7.150494/ 51.280045, val:  55.83%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.57 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0704%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5401%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1361%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 195800 real_backward_count 26378  13.472%\n",
      "lif layer 1 self.abs_max_v: 5609.5\n",
      "fc layer 3 self.abs_max_out: 577.0\n",
      "epoch-20  lr=['1.0000000'], tr/val_loss:  7.113916/ 48.251564, val:  66.25%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 74.73 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0960%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.3381%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0020%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 205590 real_backward_count 27437  13.345%\n",
      "fc layer 3 self.abs_max_out: 601.0\n",
      "epoch-21  lr=['1.0000000'], tr/val_loss:  6.826063/ 75.771294, val:  48.33%, val_best:  71.67%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0324%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4815%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1935%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 215380 real_backward_count 28508  13.236%\n",
      "fc layer 3 self.abs_max_out: 618.0\n",
      "fc layer 3 self.abs_max_out: 679.0\n",
      "lif layer 1 self.abs_max_v: 5653.5\n",
      "epoch-22  lr=['1.0000000'], tr/val_loss:  6.899690/ 61.579254, val:  58.33%, val_best:  71.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.54 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0924%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.2647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2588%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 225170 real_backward_count 29572  13.133%\n",
      "lif layer 2 self.abs_max_v: 2444.5\n",
      "fc layer 1 self.abs_max_out: 3059.0\n",
      "lif layer 2 self.abs_max_v: 2525.5\n",
      "fc layer 2 self.abs_max_out: 1786.0\n",
      "fc layer 1 self.abs_max_out: 3070.0\n",
      "epoch-23  lr=['1.0000000'], tr/val_loss:  6.907214/ 43.150867, val:  62.08%, val_best:  71.67%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0403%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.9834%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9588%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 234960 real_backward_count 30648  13.044%\n",
      "lif layer 2 self.abs_max_v: 2571.5\n",
      "fc layer 1 self.abs_max_out: 3104.0\n",
      "lif layer 1 self.abs_max_v: 5774.0\n",
      "epoch-24  lr=['1.0000000'], tr/val_loss:  6.342689/ 30.162930, val:  77.92%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0697%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.0950%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5653%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 244750 real_backward_count 31664  12.937%\n",
      "fc layer 1 self.abs_max_out: 3105.0\n",
      "lif layer 2 self.abs_max_v: 2586.5\n",
      "fc layer 2 self.abs_max_out: 1789.0\n",
      "fc layer 1 self.abs_max_out: 3270.0\n",
      "epoch-25  lr=['1.0000000'], tr/val_loss:  6.209414/ 48.728130, val:  63.75%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5431%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4398%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 254540 real_backward_count 32631  12.820%\n",
      "fc layer 1 self.abs_max_out: 3304.0\n",
      "epoch-26  lr=['1.0000000'], tr/val_loss:  6.780384/ 50.860359, val:  56.25%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0817%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5340%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6413%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 264330 real_backward_count 33668  12.737%\n",
      "fc layer 2 self.abs_max_out: 1793.0\n",
      "fc layer 2 self.abs_max_out: 1874.0\n",
      "fc layer 1 self.abs_max_out: 3445.0\n",
      "epoch-27  lr=['1.0000000'], tr/val_loss:  6.279706/ 36.092850, val:  73.75%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.06 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0592%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5432%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9633%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 274120 real_backward_count 34647  12.639%\n",
      "lif layer 2 self.abs_max_v: 2624.5\n",
      "epoch-28  lr=['1.0000000'], tr/val_loss:  5.860384/ 58.097534, val:  59.58%, val_best:  77.92%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0931%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5420%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1301%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 283910 real_backward_count 35582  12.533%\n",
      "lif layer 1 self.abs_max_v: 5964.0\n",
      "lif layer 1 self.abs_max_v: 6078.0\n",
      "lif layer 2 self.abs_max_v: 2681.5\n",
      "lif layer 2 self.abs_max_v: 2697.5\n",
      "epoch-29  lr=['1.0000000'], tr/val_loss:  5.938626/ 51.024712, val:  66.25%, val_best:  77.92%, tr:  99.69%, tr_best: 100.00%, epoch time: 74.61 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.5862%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7269%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 293700 real_backward_count 36522  12.435%\n",
      "lif layer 2 self.abs_max_v: 2723.5\n",
      "epoch-30  lr=['1.0000000'], tr/val_loss:  6.114646/ 50.785515, val:  57.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0748%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.2938%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3637%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 303490 real_backward_count 37454  12.341%\n",
      "fc layer 2 self.abs_max_out: 1876.0\n",
      "lif layer 2 self.abs_max_v: 2808.5\n",
      "epoch-31  lr=['1.0000000'], tr/val_loss:  6.353311/ 56.275368, val:  55.83%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0718%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.3205%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5379%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 313280 real_backward_count 38424  12.265%\n",
      "lif layer 2 self.abs_max_v: 2865.0\n",
      "lif layer 2 self.abs_max_v: 2871.0\n",
      "epoch-32  lr=['1.0000000'], tr/val_loss:  5.936639/ 39.985611, val:  67.08%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.64 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0544%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0013%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8485%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 323070 real_backward_count 39339  12.177%\n",
      "epoch-33  lr=['1.0000000'], tr/val_loss:  6.070379/ 39.873562, val:  71.67%, val_best:  77.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0287%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9286%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0829%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 332860 real_backward_count 40260  12.095%\n",
      "fc layer 1 self.abs_max_out: 3449.0\n",
      "epoch-34  lr=['1.0000000'], tr/val_loss:  5.721805/ 34.289833, val:  78.75%, val_best:  78.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0552%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 74.0096%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2873%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 342650 real_backward_count 41162  12.013%\n",
      "fc layer 2 self.abs_max_out: 1903.0\n",
      "fc layer 2 self.abs_max_out: 1921.0\n",
      "fc layer 1 self.abs_max_out: 3489.0\n",
      "lif layer 1 self.abs_max_v: 6102.0\n",
      "lif layer 1 self.abs_max_v: 6467.0\n",
      "epoch-35  lr=['1.0000000'], tr/val_loss:  6.017585/ 35.629452, val:  73.75%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0408%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8224%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6964%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 352440 real_backward_count 42075  11.938%\n",
      "fc layer 2 self.abs_max_out: 1993.0\n",
      "epoch-36  lr=['1.0000000'], tr/val_loss:  5.349579/ 41.513374, val:  75.83%, val_best:  78.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.93 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0932%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6546%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7729%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 362230 real_backward_count 42918  11.848%\n",
      "epoch-37  lr=['1.0000000'], tr/val_loss:  5.580053/ 36.277851, val:  82.08%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0592%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7450%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9336%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 372020 real_backward_count 43783  11.769%\n",
      "fc layer 1 self.abs_max_out: 3512.0\n",
      "epoch-38  lr=['1.0000000'], tr/val_loss:  5.903451/ 43.910648, val:  67.92%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.45 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0961%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6405%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6541%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 381810 real_backward_count 44633  11.690%\n",
      "fc layer 1 self.abs_max_out: 3533.0\n",
      "fc layer 1 self.abs_max_out: 3541.0\n",
      "epoch-39  lr=['1.0000000'], tr/val_loss:  5.034497/ 52.442860, val:  75.00%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 73.47 seconds, 1.22 minutes\n",
      "layer   1  Sparsity: 91.0377%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6223%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 391600 real_backward_count 45425  11.600%\n",
      "epoch-40  lr=['1.0000000'], tr/val_loss:  5.340315/ 56.100441, val:  65.83%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0982%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9730%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3864%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 401390 real_backward_count 46241  11.520%\n",
      "epoch-41  lr=['1.0000000'], tr/val_loss:  6.080091/ 43.935730, val:  71.25%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.83 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0721%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7445%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4562%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 411180 real_backward_count 47128  11.462%\n",
      "epoch-42  lr=['1.0000000'], tr/val_loss:  5.637911/ 43.297707, val:  73.75%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6772%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4445%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 420970 real_backward_count 47978  11.397%\n",
      "fc layer 1 self.abs_max_out: 3609.0\n",
      "epoch-43  lr=['1.0000000'], tr/val_loss:  5.213444/ 43.947247, val:  70.83%, val_best:  82.08%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0897%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3626%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4722%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 430760 real_backward_count 48771  11.322%\n",
      "epoch-44  lr=['1.0000000'], tr/val_loss:  5.213984/ 47.880474, val:  71.25%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.58 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0412%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6429%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8350%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 440550 real_backward_count 49572  11.252%\n",
      "fc layer 2 self.abs_max_out: 2022.0\n",
      "lif layer 1 self.abs_max_v: 6477.5\n",
      "epoch-45  lr=['1.0000000'], tr/val_loss:  4.914565/ 44.438560, val:  72.92%, val_best:  82.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0395%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6530%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.9070%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 450340 real_backward_count 50317  11.173%\n",
      "lif layer 1 self.abs_max_v: 6482.0\n",
      "lif layer 1 self.abs_max_v: 6841.0\n",
      "epoch-46  lr=['1.0000000'], tr/val_loss:  5.637551/ 34.456402, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0477%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7027%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0917%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 460130 real_backward_count 51140  11.114%\n",
      "epoch-47  lr=['1.0000000'], tr/val_loss:  4.850745/ 31.347736, val:  84.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.21 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5836%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4471%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 469920 real_backward_count 51867  11.037%\n",
      "epoch-48  lr=['1.0000000'], tr/val_loss:  5.088007/ 53.901226, val:  70.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0859%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4279%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4822%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 479710 real_backward_count 52635  10.972%\n",
      "fc layer 2 self.abs_max_out: 2051.0\n",
      "epoch-49  lr=['1.0000000'], tr/val_loss:  5.098583/ 42.537930, val:  74.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0703%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7146%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5534%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 489500 real_backward_count 53419  10.913%\n",
      "fc layer 1 self.abs_max_out: 3686.0\n",
      "epoch-50  lr=['1.0000000'], tr/val_loss:  5.185651/ 38.071709, val:  80.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0853%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8073%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5938%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 499290 real_backward_count 54194  10.854%\n",
      "fc layer 1 self.abs_max_out: 3706.0\n",
      "epoch-51  lr=['1.0000000'], tr/val_loss:  5.184548/ 75.284355, val:  54.58%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.59 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.1072%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8217%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2144%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 509080 real_backward_count 54966  10.797%\n",
      "lif layer 2 self.abs_max_v: 2936.0\n",
      "epoch-52  lr=['1.0000000'], tr/val_loss:  4.696995/ 39.526146, val:  81.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.80 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0537%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8588%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.4294%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 518870 real_backward_count 55672  10.729%\n",
      "epoch-53  lr=['1.0000000'], tr/val_loss:  4.958987/ 38.763454, val:  76.67%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.61 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0408%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7190%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 66.8421%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 528660 real_backward_count 56395  10.668%\n",
      "epoch-54  lr=['1.0000000'], tr/val_loss:  4.894557/ 34.709923, val:  85.42%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.68 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1077%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5068%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 538450 real_backward_count 57125  10.609%\n",
      "fc layer 1 self.abs_max_out: 3719.0\n",
      "epoch-55  lr=['1.0000000'], tr/val_loss:  4.673946/ 42.807861, val:  76.25%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.62 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0598%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4023%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7246%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 548240 real_backward_count 57811  10.545%\n",
      "lif layer 2 self.abs_max_v: 2976.5\n",
      "epoch-56  lr=['1.0000000'], tr/val_loss:  4.306401/ 33.617367, val:  82.08%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0824%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4514%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0439%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 558030 real_backward_count 58488  10.481%\n",
      "epoch-57  lr=['1.0000000'], tr/val_loss:  4.434703/ 38.286110, val:  79.17%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.58 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0849%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7129%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5845%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 567820 real_backward_count 59159  10.419%\n",
      "fc layer 1 self.abs_max_out: 3751.0\n",
      "fc layer 1 self.abs_max_out: 3857.0\n",
      "epoch-58  lr=['1.0000000'], tr/val_loss:  4.650467/ 29.136503, val:  86.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0835%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5113%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3086%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 577610 real_backward_count 59863  10.364%\n",
      "epoch-59  lr=['1.0000000'], tr/val_loss:  4.634780/ 45.110779, val:  71.25%, val_best:  86.25%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0683%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7663%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0648%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 587400 real_backward_count 60540  10.306%\n",
      "fc layer 1 self.abs_max_out: 3978.0\n",
      "epoch-60  lr=['1.0000000'], tr/val_loss:  4.454583/ 43.578743, val:  74.17%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8744%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0692%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 597190 real_backward_count 61218  10.251%\n",
      "epoch-61  lr=['1.0000000'], tr/val_loss:  4.589308/ 64.645798, val:  60.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0876%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9446%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.0994%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 606980 real_backward_count 61889  10.196%\n",
      "fc layer 1 self.abs_max_out: 3996.0\n",
      "epoch-62  lr=['1.0000000'], tr/val_loss:  4.341447/ 43.211311, val:  81.67%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0634%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6218%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3019%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 616770 real_backward_count 62529  10.138%\n",
      "fc layer 1 self.abs_max_out: 4042.0\n",
      "epoch-63  lr=['1.0000000'], tr/val_loss:  4.165924/ 45.080921, val:  75.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0780%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6029%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2047%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 626560 real_backward_count 63172  10.082%\n",
      "fc layer 3 self.abs_max_out: 681.0\n",
      "fc layer 3 self.abs_max_out: 682.0\n",
      "epoch-64  lr=['1.0000000'], tr/val_loss:  3.930480/ 35.665447, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0906%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4908%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1253%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 636350 real_backward_count 63768  10.021%\n",
      "epoch-65  lr=['1.0000000'], tr/val_loss:  4.126215/ 39.099037, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0619%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7704%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3545%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 646140 real_backward_count 64382   9.964%\n",
      "lif layer 2 self.abs_max_v: 3037.0\n",
      "fc layer 1 self.abs_max_out: 4054.0\n",
      "epoch-66  lr=['1.0000000'], tr/val_loss:  4.527293/ 35.380894, val:  82.50%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0650%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5819%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6590%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 655930 real_backward_count 65040   9.916%\n",
      "epoch-67  lr=['1.0000000'], tr/val_loss:  3.756678/ 46.113476, val:  78.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4460%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9511%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 665720 real_backward_count 65656   9.862%\n",
      "epoch-68  lr=['1.0000000'], tr/val_loss:  4.166654/ 44.182846, val:  75.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.51 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0569%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7536%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8676%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 675510 real_backward_count 66281   9.812%\n",
      "epoch-69  lr=['1.0000000'], tr/val_loss:  4.331888/ 45.034237, val:  76.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.30 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0912%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7832%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4578%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 685300 real_backward_count 66930   9.767%\n",
      "epoch-70  lr=['1.0000000'], tr/val_loss:  4.012011/ 42.673874, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 72.67 seconds, 1.21 minutes\n",
      "layer   1  Sparsity: 91.0646%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4066%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2394%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 695090 real_backward_count 67539   9.717%\n",
      "fc layer 3 self.abs_max_out: 697.0\n",
      "fc layer 3 self.abs_max_out: 717.0\n",
      "epoch-71  lr=['1.0000000'], tr/val_loss:  3.957721/ 35.330860, val:  82.92%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0798%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5455%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5536%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 704880 real_backward_count 68154   9.669%\n",
      "lif layer 2 self.abs_max_v: 3062.5\n",
      "lif layer 2 self.abs_max_v: 3093.5\n",
      "epoch-72  lr=['1.0000000'], tr/val_loss:  3.754192/ 41.334232, val:  80.83%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0604%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7123%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9781%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 714670 real_backward_count 68729   9.617%\n",
      "fc layer 3 self.abs_max_out: 720.0\n",
      "epoch-73  lr=['1.0000000'], tr/val_loss:  3.914024/ 34.529907, val:  85.42%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.50 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0900%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7601%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8314%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 724460 real_backward_count 69334   9.570%\n",
      "epoch-74  lr=['1.0000000'], tr/val_loss:  3.507625/ 38.004135, val:  78.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.83 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0982%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4018%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7177%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 734250 real_backward_count 69876   9.517%\n",
      "fc layer 1 self.abs_max_out: 4063.0\n",
      "epoch-75  lr=['1.0000000'], tr/val_loss:  3.775883/ 41.243404, val:  81.25%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0847%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4794%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5957%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 744040 real_backward_count 70460   9.470%\n",
      "lif layer 2 self.abs_max_v: 3148.0\n",
      "fc layer 1 self.abs_max_out: 4078.0\n",
      "epoch-76  lr=['1.0000000'], tr/val_loss:  3.449817/ 46.523788, val:  78.75%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1053%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4870%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0324%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 753830 real_backward_count 71007   9.419%\n",
      "epoch-77  lr=['1.0000000'], tr/val_loss:  3.893866/ 53.771439, val:  72.08%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0680%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2459%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6451%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 763620 real_backward_count 71578   9.374%\n",
      "epoch-78  lr=['1.0000000'], tr/val_loss:  3.250438/ 32.632690, val:  87.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1502%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2503%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 773410 real_backward_count 72087   9.321%\n",
      "epoch-79  lr=['1.0000000'], tr/val_loss:  3.597630/ 44.409782, val:  77.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1286%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2047%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8697%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 783200 real_backward_count 72650   9.276%\n",
      "epoch-80  lr=['1.0000000'], tr/val_loss:  3.774373/ 40.981331, val:  79.58%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0664%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0597%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7158%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 792990 real_backward_count 73206   9.232%\n",
      "epoch-81  lr=['1.0000000'], tr/val_loss:  3.522584/ 33.988419, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.39 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0531%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3517%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.3924%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 802780 real_backward_count 73741   9.186%\n",
      "fc layer 1 self.abs_max_out: 4118.0\n",
      "epoch-82  lr=['1.0000000'], tr/val_loss:  3.288040/ 37.982140, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.89 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0993%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1648%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.4275%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 812570 real_backward_count 74271   9.140%\n",
      "fc layer 1 self.abs_max_out: 4122.0\n",
      "epoch-83  lr=['1.0000000'], tr/val_loss:  3.673037/ 32.674671, val:  83.75%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1906%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5840%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 822360 real_backward_count 74816   9.098%\n",
      "fc layer 1 self.abs_max_out: 4177.0\n",
      "epoch-84  lr=['1.0000000'], tr/val_loss:  3.445040/ 45.880356, val:  77.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.10 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1146%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9852%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2019%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 832150 real_backward_count 75361   9.056%\n",
      "epoch-85  lr=['1.0000000'], tr/val_loss:  3.574806/ 37.844093, val:  82.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0971%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5186%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.2535%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 841940 real_backward_count 75903   9.015%\n",
      "epoch-86  lr=['1.0000000'], tr/val_loss:  3.912070/ 41.172913, val:  80.83%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0688%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3258%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.1368%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 851730 real_backward_count 76451   8.976%\n",
      "epoch-87  lr=['1.0000000'], tr/val_loss:  3.254541/ 37.236065, val:  83.33%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0763%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3185%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0382%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 861520 real_backward_count 76960   8.933%\n",
      "fc layer 1 self.abs_max_out: 4193.0\n",
      "epoch-88  lr=['1.0000000'], tr/val_loss:  3.267022/ 40.004177, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0843%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3553%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9901%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 871310 real_backward_count 77463   8.890%\n",
      "epoch-89  lr=['1.0000000'], tr/val_loss:  3.520010/ 33.809986, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0734%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2178%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2228%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 881100 real_backward_count 77982   8.851%\n",
      "epoch-90  lr=['1.0000000'], tr/val_loss:  3.335232/ 46.463814, val:  80.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0881%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2730%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8701%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 890890 real_backward_count 78504   8.812%\n",
      "epoch-91  lr=['1.0000000'], tr/val_loss:  3.437687/ 39.504902, val:  81.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0809%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2376%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9197%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 900680 real_backward_count 79017   8.773%\n",
      "epoch-92  lr=['1.0000000'], tr/val_loss:  3.078634/ 48.372108, val:  75.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0805%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4971%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6010%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 910470 real_backward_count 79521   8.734%\n",
      "epoch-93  lr=['1.0000000'], tr/val_loss:  3.470748/ 36.002785, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1097%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7516%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6722%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 920260 real_backward_count 80030   8.696%\n",
      "epoch-94  lr=['1.0000000'], tr/val_loss:  2.941732/ 48.775215, val:  75.00%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0579%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8538%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1473%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 930050 real_backward_count 80492   8.655%\n",
      "epoch-95  lr=['1.0000000'], tr/val_loss:  3.008725/ 45.118000, val:  82.50%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0255%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.9544%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8700%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 939840 real_backward_count 80973   8.616%\n",
      "epoch-96  lr=['1.0000000'], tr/val_loss:  3.037461/ 30.720707, val:  86.25%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0422%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8091%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0199%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 949630 real_backward_count 81419   8.574%\n",
      "fc layer 1 self.abs_max_out: 4224.0\n",
      "epoch-97  lr=['1.0000000'], tr/val_loss:  3.208549/ 37.749966, val:  85.42%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0890%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8641%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2990%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 959420 real_backward_count 81901   8.537%\n",
      "fc layer 2 self.abs_max_out: 2066.0\n",
      "epoch-98  lr=['1.0000000'], tr/val_loss:  2.829462/ 30.688219, val:  89.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0827%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8441%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2024%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 969210 real_backward_count 82349   8.497%\n",
      "lif layer 2 self.abs_max_v: 3225.5\n",
      "fc layer 2 self.abs_max_out: 2125.0\n",
      "lif layer 2 self.abs_max_v: 3256.0\n",
      "epoch-99  lr=['1.0000000'], tr/val_loss:  2.605572/ 32.104412, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0667%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7386%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3858%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 979000 real_backward_count 82764   8.454%\n",
      "fc layer 3 self.abs_max_out: 745.0\n",
      "fc layer 1 self.abs_max_out: 4283.0\n",
      "epoch-100 lr=['1.0000000'], tr/val_loss:  2.948065/ 38.821308, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0376%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8338%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7591%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 988790 real_backward_count 83230   8.417%\n",
      "epoch-101 lr=['1.0000000'], tr/val_loss:  2.688879/ 36.880672, val:  81.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1032%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5877%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5374%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 998580 real_backward_count 83682   8.380%\n",
      "fc layer 2 self.abs_max_out: 2136.0\n",
      "epoch-102 lr=['1.0000000'], tr/val_loss:  2.905075/ 41.235317, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0943%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4820%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9413%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1008370 real_backward_count 84155   8.346%\n",
      "fc layer 3 self.abs_max_out: 746.0\n",
      "fc layer 3 self.abs_max_out: 750.0\n",
      "epoch-103 lr=['1.0000000'], tr/val_loss:  2.941581/ 33.035614, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0546%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5000%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4166%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1018160 real_backward_count 84620   8.311%\n",
      "fc layer 1 self.abs_max_out: 4293.0\n",
      "epoch-104 lr=['1.0000000'], tr/val_loss:  3.064575/ 38.347794, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0659%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5945%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8758%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1027950 real_backward_count 85090   8.278%\n",
      "epoch-105 lr=['1.0000000'], tr/val_loss:  2.935568/ 41.832672, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.27 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0791%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3423%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1965%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1037740 real_backward_count 85547   8.244%\n",
      "fc layer 2 self.abs_max_out: 2162.0\n",
      "epoch-106 lr=['1.0000000'], tr/val_loss:  3.304368/ 42.101135, val:  76.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0744%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6328%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8893%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1047530 real_backward_count 86023   8.212%\n",
      "lif layer 2 self.abs_max_v: 3325.0\n",
      "fc layer 1 self.abs_max_out: 4321.0\n",
      "epoch-107 lr=['1.0000000'], tr/val_loss:  2.875284/ 37.287907, val:  84.58%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0803%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4544%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9027%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1057320 real_backward_count 86503   8.181%\n",
      "lif layer 2 self.abs_max_v: 3356.0\n",
      "lif layer 2 self.abs_max_v: 3411.0\n",
      "lif layer 2 self.abs_max_v: 3442.5\n",
      "lif layer 1 self.abs_max_v: 6845.0\n",
      "lif layer 1 self.abs_max_v: 6965.5\n",
      "epoch-108 lr=['1.0000000'], tr/val_loss:  2.933387/ 32.130524, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.90 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0682%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5112%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0561%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1067110 real_backward_count 86968   8.150%\n",
      "epoch-109 lr=['1.0000000'], tr/val_loss:  2.749607/ 37.643803, val:  84.17%, val_best:  89.58%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0708%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3503%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9202%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1076900 real_backward_count 87405   8.116%\n",
      "lif layer 1 self.abs_max_v: 7005.5\n",
      "lif layer 1 self.abs_max_v: 7194.5\n",
      "epoch-110 lr=['1.0000000'], tr/val_loss:  2.470704/ 41.435810, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0987%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3258%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3603%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1086690 real_backward_count 87793   8.079%\n",
      "epoch-111 lr=['1.0000000'], tr/val_loss:  2.813902/ 38.915466, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.29 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0947%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5425%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1212%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1096480 real_backward_count 88219   8.046%\n",
      "epoch-112 lr=['1.0000000'], tr/val_loss:  2.847651/ 37.804996, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.83 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0921%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1546%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9997%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1106270 real_backward_count 88663   8.015%\n",
      "epoch-113 lr=['1.0000000'], tr/val_loss:  2.820298/ 43.414085, val:  76.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.91 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0687%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3598%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9287%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1116060 real_backward_count 89099   7.983%\n",
      "epoch-114 lr=['1.0000000'], tr/val_loss:  2.935754/ 33.651947, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.15 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0700%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4471%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1352%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1125850 real_backward_count 89539   7.953%\n",
      "epoch-115 lr=['1.0000000'], tr/val_loss:  2.518320/ 47.827156, val:  77.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.64 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0989%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3808%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2884%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1135640 real_backward_count 89920   7.918%\n",
      "epoch-116 lr=['1.0000000'], tr/val_loss:  2.369383/ 41.116768, val:  85.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.72 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0969%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3017%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8744%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1145430 real_backward_count 90324   7.886%\n",
      "lif layer 2 self.abs_max_v: 3453.0\n",
      "lif layer 2 self.abs_max_v: 3530.0\n",
      "epoch-117 lr=['1.0000000'], tr/val_loss:  2.960725/ 35.131931, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0503%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2775%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9720%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1155220 real_backward_count 90752   7.856%\n",
      "fc layer 1 self.abs_max_out: 4325.0\n",
      "epoch-118 lr=['1.0000000'], tr/val_loss:  2.820820/ 41.521942, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.95 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1138%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4361%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6275%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1165010 real_backward_count 91184   7.827%\n",
      "epoch-119 lr=['1.0000000'], tr/val_loss:  2.473849/ 36.593899, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.52 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0774%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2394%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2941%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1174800 real_backward_count 91595   7.797%\n",
      "epoch-120 lr=['1.0000000'], tr/val_loss:  2.479363/ 32.529243, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0831%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2301%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6956%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1184590 real_backward_count 91972   7.764%\n",
      "epoch-121 lr=['1.0000000'], tr/val_loss:  2.493321/ 40.873695, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.96 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0776%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6576%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1382%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1194380 real_backward_count 92360   7.733%\n",
      "epoch-122 lr=['1.0000000'], tr/val_loss:  2.852396/ 33.682747, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0867%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6156%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7016%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1204170 real_backward_count 92799   7.706%\n",
      "epoch-123 lr=['1.0000000'], tr/val_loss:  2.406484/ 34.884644, val:  87.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0727%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4734%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1443%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1213960 real_backward_count 93172   7.675%\n",
      "lif layer 2 self.abs_max_v: 3584.5\n",
      "epoch-124 lr=['1.0000000'], tr/val_loss:  2.853765/ 35.458447, val:  88.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.23 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0627%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3858%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1971%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1223750 real_backward_count 93584   7.647%\n",
      "lif layer 2 self.abs_max_v: 3593.5\n",
      "lif layer 2 self.abs_max_v: 3596.0\n",
      "lif layer 2 self.abs_max_v: 3608.5\n",
      "lif layer 2 self.abs_max_v: 3661.0\n",
      "epoch-125 lr=['1.0000000'], tr/val_loss:  2.947723/ 36.199890, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0716%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0273%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8922%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1233540 real_backward_count 94020   7.622%\n",
      "epoch-126 lr=['1.0000000'], tr/val_loss:  2.628893/ 40.905525, val:  82.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0586%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1032%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.6283%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1243330 real_backward_count 94423   7.594%\n",
      "epoch-127 lr=['1.0000000'], tr/val_loss:  2.409722/ 37.834763, val:  80.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0617%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1647%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.5506%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1253120 real_backward_count 94802   7.565%\n",
      "epoch-128 lr=['1.0000000'], tr/val_loss:  2.130293/ 41.301003, val:  82.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.34 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0814%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4431%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0021%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1262910 real_backward_count 95132   7.533%\n",
      "epoch-129 lr=['1.0000000'], tr/val_loss:  1.982898/ 38.020641, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0971%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4686%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.7869%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1272700 real_backward_count 95458   7.500%\n",
      "epoch-130 lr=['1.0000000'], tr/val_loss:  2.575027/ 34.525654, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0900%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1644%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8703%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1282490 real_backward_count 95845   7.473%\n",
      "lif layer 2 self.abs_max_v: 3867.5\n",
      "epoch-131 lr=['1.0000000'], tr/val_loss:  2.726989/ 34.235119, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0455%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1600%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0416%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1292280 real_backward_count 96242   7.447%\n",
      "epoch-132 lr=['1.0000000'], tr/val_loss:  1.997357/ 39.429016, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0582%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1115%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4921%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1302070 real_backward_count 96578   7.417%\n",
      "epoch-133 lr=['1.0000000'], tr/val_loss:  2.116046/ 44.758106, val:  81.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.67 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0529%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1059%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3865%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1311860 real_backward_count 96912   7.387%\n",
      "epoch-134 lr=['1.0000000'], tr/val_loss:  2.398062/ 35.880775, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.97 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0640%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9724%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3382%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1321650 real_backward_count 97302   7.362%\n",
      "lif layer 2 self.abs_max_v: 3964.5\n",
      "epoch-135 lr=['1.0000000'], tr/val_loss:  2.212160/ 39.386612, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.55 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0815%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2364%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9635%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1331440 real_backward_count 97643   7.334%\n",
      "fc layer 3 self.abs_max_out: 764.0\n",
      "epoch-136 lr=['1.0000000'], tr/val_loss:  2.177459/ 36.337929, val:  88.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.70 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0897%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4980%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9955%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1341230 real_backward_count 97985   7.306%\n",
      "epoch-137 lr=['1.0000000'], tr/val_loss:  1.905192/ 37.213482, val:  82.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0550%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0406%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2374%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1351020 real_backward_count 98325   7.278%\n",
      "lif layer 2 self.abs_max_v: 3967.5\n",
      "epoch-138 lr=['1.0000000'], tr/val_loss:  2.097116/ 51.973717, val:  79.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.43 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0647%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0810%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.8379%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1360810 real_backward_count 98648   7.249%\n",
      "fc layer 3 self.abs_max_out: 776.0\n",
      "epoch-139 lr=['1.0000000'], tr/val_loss:  2.167886/ 42.880421, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.25 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1033%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1917%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2952%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1370600 real_backward_count 98983   7.222%\n",
      "fc layer 2 self.abs_max_out: 2203.0\n",
      "lif layer 2 self.abs_max_v: 4186.0\n",
      "fc layer 1 self.abs_max_out: 4404.0\n",
      "epoch-140 lr=['1.0000000'], tr/val_loss:  2.203083/ 39.429176, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0753%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3632%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4656%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1380390 real_backward_count 99324   7.195%\n",
      "epoch-141 lr=['1.0000000'], tr/val_loss:  2.201100/ 37.694843, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.88 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0641%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4812%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8307%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1390180 real_backward_count 99660   7.169%\n",
      "fc layer 1 self.abs_max_out: 4445.0\n",
      "epoch-142 lr=['1.0000000'], tr/val_loss:  1.915872/ 41.782532, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.13 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0679%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2376%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1125%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1399970 real_backward_count 99974   7.141%\n",
      "epoch-143 lr=['1.0000000'], tr/val_loss:  2.470920/ 42.716236, val:  83.75%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.81 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0833%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3282%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0168%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1409760 real_backward_count 100343   7.118%\n",
      "fc layer 3 self.abs_max_out: 777.0\n",
      "epoch-144 lr=['1.0000000'], tr/val_loss:  2.340561/ 37.449051, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0691%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3228%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5608%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1419550 real_backward_count 100717   7.095%\n",
      "lif layer 1 self.abs_max_v: 7244.0\n",
      "epoch-145 lr=['1.0000000'], tr/val_loss:  1.943491/ 32.818169, val:  87.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.77 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0794%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4131%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5188%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1429340 real_backward_count 101039   7.069%\n",
      "epoch-146 lr=['1.0000000'], tr/val_loss:  2.148936/ 45.449249, val:  82.08%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0891%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3073%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6824%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1439130 real_backward_count 101370   7.044%\n",
      "epoch-147 lr=['1.0000000'], tr/val_loss:  2.203074/ 40.658329, val:  82.50%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.22 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0922%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1570%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7127%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1448920 real_backward_count 101703   7.019%\n",
      "epoch-148 lr=['1.0000000'], tr/val_loss:  1.800437/ 39.516247, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0761%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9714%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7735%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1458710 real_backward_count 102025   6.994%\n",
      "fc layer 3 self.abs_max_out: 798.0\n",
      "epoch-149 lr=['1.0000000'], tr/val_loss:  1.933867/ 37.615955, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1095%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0133%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1459%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1468500 real_backward_count 102333   6.969%\n",
      "epoch-150 lr=['1.0000000'], tr/val_loss:  2.115763/ 39.503883, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.31 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0749%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0516%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0304%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1478290 real_backward_count 102684   6.946%\n",
      "fc layer 1 self.abs_max_out: 4448.0\n",
      "epoch-151 lr=['1.0000000'], tr/val_loss:  2.069460/ 41.280201, val:  86.25%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.20 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0711%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2079%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8658%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1488080 real_backward_count 103030   6.924%\n",
      "epoch-152 lr=['1.0000000'], tr/val_loss:  1.938920/ 40.147728, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.80 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0677%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2105%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6721%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1497870 real_backward_count 103359   6.900%\n",
      "epoch-153 lr=['1.0000000'], tr/val_loss:  2.049842/ 36.339016, val:  89.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.59 seconds, 1.24 minutes\n",
      "layer   1  Sparsity: 91.0412%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3049%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6384%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1507660 real_backward_count 103686   6.877%\n",
      "epoch-154 lr=['1.0000000'], tr/val_loss:  1.904268/ 36.291233, val:  87.92%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.28 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0577%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.8623%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7394%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1517450 real_backward_count 103989   6.853%\n",
      "fc layer 3 self.abs_max_out: 809.0\n",
      "epoch-155 lr=['1.0000000'], tr/val_loss:  2.065783/ 44.545609, val:  84.58%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.41 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0762%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 72.9712%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5054%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1527240 real_backward_count 104299   6.829%\n",
      "epoch-156 lr=['1.0000000'], tr/val_loss:  1.906258/ 45.765781, val:  83.33%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.84 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0678%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1102%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5334%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1537030 real_backward_count 104625   6.807%\n",
      "fc layer 2 self.abs_max_out: 2212.0\n",
      "epoch-157 lr=['1.0000000'], tr/val_loss:  2.131884/ 42.528336, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.04 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1010%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1639%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5753%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1546820 real_backward_count 104949   6.785%\n",
      "fc layer 2 self.abs_max_out: 2228.0\n",
      "epoch-158 lr=['1.0000000'], tr/val_loss:  1.997759/ 34.583923, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.44 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1176%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3177%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7005%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1556610 real_backward_count 105266   6.763%\n",
      "epoch-159 lr=['1.0000000'], tr/val_loss:  1.837395/ 39.330063, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.03 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0971%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3191%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.9600%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1566400 real_backward_count 105564   6.739%\n",
      "epoch-160 lr=['1.0000000'], tr/val_loss:  1.708095/ 40.192039, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0923%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4744%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6967%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1576190 real_backward_count 105848   6.715%\n",
      "fc layer 1 self.abs_max_out: 4545.0\n",
      "epoch-161 lr=['1.0000000'], tr/val_loss:  1.649395/ 39.726608, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0698%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3485%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8012%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1585980 real_backward_count 106125   6.691%\n",
      "epoch-162 lr=['1.0000000'], tr/val_loss:  1.907266/ 39.340519, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.07 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1002%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.8453%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8228%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1595770 real_backward_count 106410   6.668%\n",
      "epoch-163 lr=['1.0000000'], tr/val_loss:  1.884380/ 44.129120, val:  82.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.53 seconds, 1.28 minutes\n",
      "layer   1  Sparsity: 91.0759%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5816%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4695%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1605560 real_backward_count 106700   6.646%\n",
      "epoch-164 lr=['1.0000000'], tr/val_loss:  1.566769/ 42.700119, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.37 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0740%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1813%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0620%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1615350 real_backward_count 106957   6.621%\n",
      "epoch-165 lr=['1.0000000'], tr/val_loss:  1.822769/ 40.475719, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.65 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0592%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2436%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 67.9540%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1625140 real_backward_count 107272   6.601%\n",
      "fc layer 2 self.abs_max_out: 2234.0\n",
      "epoch-166 lr=['1.0000000'], tr/val_loss:  1.495246/ 39.608124, val:  87.08%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.42 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0800%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2426%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1899%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1634930 real_backward_count 107523   6.577%\n",
      "fc layer 2 self.abs_max_out: 2291.0\n",
      "epoch-167 lr=['1.0000000'], tr/val_loss:  1.683289/ 42.825134, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.33 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0336%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3783%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0818%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1644720 real_backward_count 107811   6.555%\n",
      "epoch-168 lr=['1.0000000'], tr/val_loss:  1.450769/ 38.774818, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.82 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0880%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3756%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4215%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1654510 real_backward_count 108071   6.532%\n",
      "fc layer 1 self.abs_max_out: 4554.0\n",
      "epoch-169 lr=['1.0000000'], tr/val_loss:  2.022424/ 42.203247, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.32 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0648%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3667%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0966%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1664300 real_backward_count 108365   6.511%\n",
      "fc layer 2 self.abs_max_out: 2297.0\n",
      "fc layer 2 self.abs_max_out: 2333.0\n",
      "lif layer 2 self.abs_max_v: 4298.5\n",
      "lif layer 2 self.abs_max_v: 4310.5\n",
      "fc layer 2 self.abs_max_out: 2343.0\n",
      "fc layer 2 self.abs_max_out: 2483.0\n",
      "lif layer 2 self.abs_max_v: 4357.5\n",
      "epoch-170 lr=['1.0000000'], tr/val_loss:  1.637062/ 44.819794, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.26 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1263%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3595%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.1583%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1674090 real_backward_count 108634   6.489%\n",
      "epoch-171 lr=['1.0000000'], tr/val_loss:  1.454970/ 39.189960, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.99 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1089%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1863%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4705%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1683880 real_backward_count 108876   6.466%\n",
      "epoch-172 lr=['1.0000000'], tr/val_loss:  1.473644/ 42.030628, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0936%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3680%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2831%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1693670 real_backward_count 109119   6.443%\n",
      "epoch-173 lr=['1.0000000'], tr/val_loss:  1.583278/ 41.229702, val:  89.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0474%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4479%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5223%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1703460 real_backward_count 109376   6.421%\n",
      "epoch-174 lr=['1.0000000'], tr/val_loss:  1.692884/ 40.565918, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.94 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0718%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4809%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4597%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1713250 real_backward_count 109649   6.400%\n",
      "epoch-175 lr=['1.0000000'], tr/val_loss:  1.648260/ 40.901936, val:  88.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.35 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1080%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.7073%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4927%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1723040 real_backward_count 109932   6.380%\n",
      "fc layer 1 self.abs_max_out: 4689.0\n",
      "fc layer 3 self.abs_max_out: 823.0\n",
      "epoch-176 lr=['1.0000000'], tr/val_loss:  1.706572/ 49.812851, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.48 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0758%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5622%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5284%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1732830 real_backward_count 110190   6.359%\n",
      "epoch-177 lr=['1.0000000'], tr/val_loss:  1.596195/ 41.461140, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.09 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0778%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4347%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5883%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1742620 real_backward_count 110452   6.338%\n",
      "epoch-178 lr=['1.0000000'], tr/val_loss:  1.813692/ 47.034725, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1242%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4824%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1728%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1752410 real_backward_count 110730   6.319%\n",
      "fc layer 3 self.abs_max_out: 826.0\n",
      "epoch-179 lr=['1.0000000'], tr/val_loss:  1.419551/ 44.803288, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.60 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1272%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5982%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.1548%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1762200 real_backward_count 110965   6.297%\n",
      "epoch-180 lr=['1.0000000'], tr/val_loss:  1.426420/ 42.150753, val:  83.75%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.73 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0865%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5709%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6052%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1771990 real_backward_count 111212   6.276%\n",
      "epoch-181 lr=['1.0000000'], tr/val_loss:  1.576968/ 35.184765, val:  90.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.24 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0735%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2741%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5306%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1781780 real_backward_count 111461   6.256%\n",
      "fc layer 3 self.abs_max_out: 840.0\n",
      "epoch-182 lr=['1.0000000'], tr/val_loss:  1.670514/ 41.372730, val:  87.08%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.17 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0746%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4239%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8597%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1791570 real_backward_count 111725   6.236%\n",
      "epoch-183 lr=['1.0000000'], tr/val_loss:  1.644737/ 37.258240, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.05 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0797%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.1828%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6602%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1801360 real_backward_count 111979   6.216%\n",
      "epoch-184 lr=['1.0000000'], tr/val_loss:  1.497152/ 38.177101, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 76.04 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0620%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4404%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4283%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1811150 real_backward_count 112228   6.197%\n",
      "epoch-185 lr=['1.0000000'], tr/val_loss:  1.635144/ 40.032951, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.72 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0745%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5601%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2098%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1820940 real_backward_count 112488   6.177%\n",
      "epoch-186 lr=['1.0000000'], tr/val_loss:  1.807009/ 42.390415, val:  88.75%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0344%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4509%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3491%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1830730 real_backward_count 112748   6.159%\n",
      "epoch-187 lr=['1.0000000'], tr/val_loss:  1.470960/ 40.339878, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.08 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0923%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5192%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2208%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1840520 real_backward_count 112981   6.139%\n",
      "fc layer 3 self.abs_max_out: 864.0\n",
      "epoch-188 lr=['1.0000000'], tr/val_loss:  1.613177/ 38.854004, val:  90.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.47 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0873%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4475%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5835%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1850310 real_backward_count 113242   6.120%\n",
      "epoch-189 lr=['1.0000000'], tr/val_loss:  1.377812/ 38.392166, val:  89.58%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.12 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0521%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3604%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4179%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1860100 real_backward_count 113482   6.101%\n",
      "epoch-190 lr=['1.0000000'], tr/val_loss:  1.564309/ 44.057877, val:  87.08%, val_best:  90.83%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0889%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3969%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7070%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1869890 real_backward_count 113734   6.082%\n",
      "epoch-191 lr=['1.0000000'], tr/val_loss:  1.539472/ 41.270481, val:  87.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.40 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1241%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4468%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3913%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1879680 real_backward_count 113974   6.063%\n",
      "epoch-192 lr=['1.0000000'], tr/val_loss:  1.357264/ 44.910435, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0879%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4369%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2965%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1889470 real_backward_count 114202   6.044%\n",
      "epoch-193 lr=['1.0000000'], tr/val_loss:  1.507627/ 39.624935, val:  89.17%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.18 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0602%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.4289%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3854%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1899260 real_backward_count 114440   6.026%\n",
      "epoch-194 lr=['1.0000000'], tr/val_loss:  1.474930/ 51.020966, val:  82.92%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0753%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.6283%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5356%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1909050 real_backward_count 114686   6.007%\n",
      "epoch-195 lr=['1.0000000'], tr/val_loss:  1.719376/ 37.864174, val:  90.00%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 74.71 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0707%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3919%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5064%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1918840 real_backward_count 114941   5.990%\n",
      "epoch-196 lr=['1.0000000'], tr/val_loss:  1.365403/ 47.667999, val:  85.42%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.71 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0795%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.5205%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6171%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1928630 real_backward_count 115177   5.972%\n",
      "fc layer 1 self.abs_max_out: 4726.0\n",
      "epoch-197 lr=['1.0000000'], tr/val_loss:  1.352793/ 42.338169, val:  88.33%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.11 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1171%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.3559%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6196%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1938420 real_backward_count 115398   5.953%\n",
      "epoch-198 lr=['1.0000000'], tr/val_loss:  1.452998/ 48.065376, val:  86.67%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.53 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0775%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.2898%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4208%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 1948210 real_backward_count 115639   5.936%\n",
      "lif layer 1 self.abs_max_v: 7297.0\n",
      "epoch-199 lr=['1.0000000'], tr/val_loss:  1.448209/ 46.271992, val:  85.83%, val_best:  90.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.78 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0888%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 73.0326%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8708%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ac6dd6f4784bed8f74762fc128a38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñá‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñá‚ñÇ‚ñÜ‚ñÉ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.44821</td></tr><tr><td>val_acc_best</td><td>0.90833</td></tr><tr><td>val_acc_now</td><td>0.85833</td></tr><tr><td>val_loss</td><td>46.27199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">radiant-sweep-75</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yicr2j39' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yicr2j39</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251214_161936-yicr2j39/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z7r4gl49 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: -1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_0: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_1: 10009\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_scaling_2: 10008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate2: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 11403\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251214_203119-z7r4gl49</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z7r4gl49' target=\"_blank\">glorious-sweep-77</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/gsnqjucp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z7r4gl49' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z7r4gl49</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate2' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '5', 'single_step': True, 'unique_name': '20251214_203130_168', 'my_seed': 11403, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 128, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 8, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': -1, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[0, 0], [0, 0], [0, 0]], 'lif_layer_sg_width2': 8, 'lif_layer_v_threshold2': 128, 'init_scaling': [10009, 10009, 10008], 'learning_rate': 1, 'learning_rate2': 1} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0e8a8f2d81b4fe037308b5d792c4a037\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 979 BATCH: 1 train_data_count: 979\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4 self.sg_width 8, self.v_threshold 128\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: 0\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp 0 0\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (3): Feedback_Receiver(connect_features=10, count=0, single_step=True, ANPI_MODE=False)\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=128, v_reset=10000, sg_width=8, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False)\n",
      "      (6): Feedback_Receiver(connect_features=10, count=1, single_step=True, ANPI_MODE=False)\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[0, 0], [0, 0], [0, 0]], ANPI_MODE=False, init_scaling=[10009, 10009, 10008])\n",
      "      (DFA_top): Top_Gradient(single_step=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 1\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 262.0\n",
      "lif layer 1 self.abs_max_v: 262.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 251.0\n",
      "lif layer 2 self.abs_max_v: 251.0\n",
      "inFeed spike.shape torch.Size([1, 200]) self.weight_fb.shape torch.Size([10, 200])\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 3 self.abs_max_out: 65.0\n",
      "lif layer 1 self.abs_max_v: 293.0\n",
      "lif layer 2 self.abs_max_v: 318.5\n",
      "fc layer 3 self.abs_max_out: 74.0\n",
      "fc layer 1 self.abs_max_out: 267.0\n",
      "lif layer 1 self.abs_max_v: 413.5\n",
      "fc layer 3 self.abs_max_out: 78.0\n",
      "lif layer 2 self.abs_max_v: 330.5\n",
      "fc layer 2 self.abs_max_out: 279.0\n",
      "lif layer 2 self.abs_max_v: 373.5\n",
      "fc layer 3 self.abs_max_out: 136.0\n",
      "fc layer 1 self.abs_max_out: 308.0\n",
      "fc layer 2 self.abs_max_out: 373.0\n",
      "lif layer 2 self.abs_max_v: 525.5\n",
      "fc layer 1 self.abs_max_out: 366.0\n",
      "fc layer 1 self.abs_max_out: 385.0\n",
      "fc layer 1 self.abs_max_out: 441.0\n",
      "lif layer 1 self.abs_max_v: 441.0\n",
      "fc layer 1 self.abs_max_out: 552.0\n",
      "lif layer 1 self.abs_max_v: 552.0\n",
      "fc layer 2 self.abs_max_out: 418.0\n",
      "lif layer 2 self.abs_max_v: 607.5\n",
      "lif layer 2 self.abs_max_v: 624.0\n",
      "fc layer 1 self.abs_max_out: 601.0\n",
      "lif layer 1 self.abs_max_v: 646.5\n",
      "fc layer 2 self.abs_max_out: 474.0\n",
      "lif layer 2 self.abs_max_v: 775.5\n",
      "lif layer 1 self.abs_max_v: 679.5\n",
      "fc layer 2 self.abs_max_out: 491.0\n",
      "lif layer 2 self.abs_max_v: 776.5\n",
      "fc layer 1 self.abs_max_out: 696.0\n",
      "lif layer 1 self.abs_max_v: 696.0\n",
      "fc layer 1 self.abs_max_out: 946.0\n",
      "lif layer 1 self.abs_max_v: 946.0\n",
      "lif layer 2 self.abs_max_v: 782.5\n",
      "fc layer 3 self.abs_max_out: 138.0\n",
      "fc layer 2 self.abs_max_out: 558.0\n",
      "fc layer 3 self.abs_max_out: 209.0\n",
      "lif layer 2 self.abs_max_v: 812.5\n",
      "lif layer 2 self.abs_max_v: 882.5\n",
      "lif layer 2 self.abs_max_v: 950.5\n",
      "lif layer 2 self.abs_max_v: 978.5\n",
      "fc layer 2 self.abs_max_out: 613.0\n",
      "lif layer 2 self.abs_max_v: 998.0\n",
      "fc layer 3 self.abs_max_out: 210.0\n",
      "fc layer 3 self.abs_max_out: 216.0\n",
      "fc layer 2 self.abs_max_out: 712.0\n",
      "lif layer 2 self.abs_max_v: 1169.5\n",
      "fc layer 3 self.abs_max_out: 217.0\n",
      "fc layer 3 self.abs_max_out: 227.0\n",
      "fc layer 3 self.abs_max_out: 247.0\n",
      "fc layer 3 self.abs_max_out: 265.0\n",
      "lif layer 1 self.abs_max_v: 991.0\n",
      "lif layer 1 self.abs_max_v: 1038.0\n",
      "fc layer 2 self.abs_max_out: 738.0\n",
      "fc layer 3 self.abs_max_out: 270.0\n",
      "lif layer 1 self.abs_max_v: 1068.5\n",
      "fc layer 2 self.abs_max_out: 804.0\n",
      "fc layer 3 self.abs_max_out: 278.0\n",
      "fc layer 2 self.abs_max_out: 805.0\n",
      "fc layer 3 self.abs_max_out: 282.0\n",
      "fc layer 1 self.abs_max_out: 947.0\n",
      "fc layer 1 self.abs_max_out: 1088.0\n",
      "lif layer 1 self.abs_max_v: 1117.0\n",
      "lif layer 1 self.abs_max_v: 1160.0\n",
      "lif layer 1 self.abs_max_v: 1289.0\n",
      "fc layer 2 self.abs_max_out: 898.0\n",
      "fc layer 3 self.abs_max_out: 285.0\n",
      "fc layer 3 self.abs_max_out: 300.0\n",
      "lif layer 2 self.abs_max_v: 1198.5\n",
      "lif layer 1 self.abs_max_v: 1456.5\n",
      "lif layer 1 self.abs_max_v: 1541.0\n",
      "fc layer 1 self.abs_max_out: 1094.0\n",
      "lif layer 1 self.abs_max_v: 1579.5\n",
      "lif layer 1 self.abs_max_v: 1604.0\n",
      "lif layer 2 self.abs_max_v: 1242.0\n",
      "fc layer 3 self.abs_max_out: 301.0\n",
      "fc layer 1 self.abs_max_out: 1228.0\n",
      "fc layer 1 self.abs_max_out: 1235.0\n",
      "fc layer 1 self.abs_max_out: 1295.0\n",
      "fc layer 1 self.abs_max_out: 1345.0\n",
      "fc layer 3 self.abs_max_out: 310.0\n",
      "fc layer 3 self.abs_max_out: 324.0\n",
      "lif layer 1 self.abs_max_v: 1634.5\n",
      "lif layer 1 self.abs_max_v: 1707.5\n",
      "lif layer 1 self.abs_max_v: 1738.0\n",
      "lif layer 2 self.abs_max_v: 1270.0\n",
      "lif layer 2 self.abs_max_v: 1298.0\n",
      "fc layer 2 self.abs_max_out: 936.0\n",
      "lif layer 2 self.abs_max_v: 1317.0\n",
      "lif layer 2 self.abs_max_v: 1329.5\n",
      "lif layer 2 self.abs_max_v: 1343.0\n",
      "lif layer 1 self.abs_max_v: 1791.5\n",
      "lif layer 1 self.abs_max_v: 1818.5\n",
      "lif layer 1 self.abs_max_v: 1823.5\n",
      "fc layer 1 self.abs_max_out: 1364.0\n",
      "lif layer 1 self.abs_max_v: 1877.5\n",
      "fc layer 2 self.abs_max_out: 1044.0\n",
      "fc layer 1 self.abs_max_out: 1423.0\n",
      "fc layer 1 self.abs_max_out: 1433.0\n",
      "fc layer 1 self.abs_max_out: 1527.0\n",
      "fc layer 1 self.abs_max_out: 1531.0\n",
      "lif layer 2 self.abs_max_v: 1406.0\n",
      "lif layer 2 self.abs_max_v: 1420.0\n",
      "fc layer 3 self.abs_max_out: 333.0\n",
      "fc layer 3 self.abs_max_out: 341.0\n",
      "fc layer 3 self.abs_max_out: 367.0\n",
      "fc layer 2 self.abs_max_out: 1095.0\n",
      "fc layer 3 self.abs_max_out: 379.0\n",
      "fc layer 1 self.abs_max_out: 1669.0\n",
      "fc layer 1 self.abs_max_out: 1858.0\n",
      "lif layer 1 self.abs_max_v: 1914.0\n",
      "lif layer 1 self.abs_max_v: 2026.0\n",
      "lif layer 2 self.abs_max_v: 1430.0\n",
      "lif layer 2 self.abs_max_v: 1455.5\n",
      "fc layer 2 self.abs_max_out: 1115.0\n",
      "fc layer 2 self.abs_max_out: 1157.0\n",
      "fc layer 2 self.abs_max_out: 1234.0\n",
      "lif layer 1 self.abs_max_v: 2173.5\n",
      "lif layer 1 self.abs_max_v: 2197.0\n",
      "fc layer 3 self.abs_max_out: 406.0\n",
      "fc layer 2 self.abs_max_out: 1268.0\n",
      "lif layer 1 self.abs_max_v: 2328.0\n",
      "fc layer 1 self.abs_max_out: 1923.0\n",
      "lif layer 1 self.abs_max_v: 2344.0\n",
      "lif layer 1 self.abs_max_v: 2503.0\n",
      "lif layer 1 self.abs_max_v: 2551.5\n",
      "lif layer 1 self.abs_max_v: 2717.0\n",
      "epoch-0   lr=['1.0000000'], tr/val_loss: 14.387704/ 79.074272, val:  35.00%, val_best:  35.00%, tr:  96.83%, tr_best:  96.83%, epoch time: 75.56 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0764%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.7650%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 71.0294%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 9790 real_backward_count 2241  22.891%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "lif layer 2 self.abs_max_v: 1477.0\n",
      "lif layer 2 self.abs_max_v: 1485.5\n",
      "lif layer 2 self.abs_max_v: 1551.0\n",
      "lif layer 2 self.abs_max_v: 1584.0\n",
      "lif layer 2 self.abs_max_v: 1596.0\n",
      "lif layer 2 self.abs_max_v: 1604.0\n",
      "lif layer 2 self.abs_max_v: 1729.0\n",
      "lif layer 1 self.abs_max_v: 2795.0\n",
      "epoch-1   lr=['1.0000000'], tr/val_loss: 10.402102/ 60.481354, val:  32.08%, val_best:  35.00%, tr:  99.39%, tr_best:  99.39%, epoch time: 76.10 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0802%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.0650%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.6568%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 19580 real_backward_count 3868  19.755%\n",
      "fc layer 3 self.abs_max_out: 453.0\n",
      "fc layer 2 self.abs_max_out: 1273.0\n",
      "fc layer 1 self.abs_max_out: 1968.0\n",
      "fc layer 3 self.abs_max_out: 455.0\n",
      "lif layer 2 self.abs_max_v: 1756.0\n",
      "lif layer 2 self.abs_max_v: 1786.0\n",
      "lif layer 2 self.abs_max_v: 1841.0\n",
      "lif layer 1 self.abs_max_v: 2851.0\n",
      "lif layer 1 self.abs_max_v: 2951.5\n",
      "lif layer 1 self.abs_max_v: 3102.5\n",
      "lif layer 1 self.abs_max_v: 3221.5\n",
      "epoch-2   lr=['1.0000000'], tr/val_loss:  8.970085/ 62.845661, val:  45.00%, val_best:  45.00%, tr:  99.49%, tr_best:  99.49%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.1140%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 78.5178%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.2714%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 29370 real_backward_count 5347  18.206%\n",
      "lif layer 1 self.abs_max_v: 3262.5\n",
      "fc layer 2 self.abs_max_out: 1313.0\n",
      "fc layer 2 self.abs_max_out: 1327.0\n",
      "lif layer 1 self.abs_max_v: 3315.5\n",
      "lif layer 1 self.abs_max_v: 3329.0\n",
      "fc layer 2 self.abs_max_out: 1340.0\n",
      "epoch-3   lr=['1.0000000'], tr/val_loss:  9.028207/ 52.743004, val:  50.00%, val_best:  50.00%, tr:  99.39%, tr_best:  99.49%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0894%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.9489%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.7180%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 39160 real_backward_count 6775  17.301%\n",
      "fc layer 2 self.abs_max_out: 1342.0\n",
      "fc layer 1 self.abs_max_out: 1990.0\n",
      "fc layer 2 self.abs_max_out: 1354.0\n",
      "fc layer 2 self.abs_max_out: 1359.0\n",
      "fc layer 1 self.abs_max_out: 2075.0\n",
      "lif layer 1 self.abs_max_v: 3492.5\n",
      "fc layer 2 self.abs_max_out: 1382.0\n",
      "fc layer 3 self.abs_max_out: 456.0\n",
      "fc layer 1 self.abs_max_out: 2084.0\n",
      "fc layer 1 self.abs_max_out: 2167.0\n",
      "epoch-4   lr=['1.0000000'], tr/val_loss:  8.662106/ 69.764137, val:  43.33%, val_best:  50.00%, tr:  99.18%, tr_best:  99.49%, epoch time: 75.69 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0908%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.9963%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5672%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 48950 real_backward_count 8148  16.646%\n",
      "fc layer 3 self.abs_max_out: 464.0\n",
      "fc layer 1 self.abs_max_out: 2174.0\n",
      "fc layer 2 self.abs_max_out: 1401.0\n",
      "lif layer 2 self.abs_max_v: 1843.5\n",
      "lif layer 2 self.abs_max_v: 1856.5\n",
      "fc layer 1 self.abs_max_out: 2241.0\n",
      "lif layer 1 self.abs_max_v: 3543.0\n",
      "lif layer 1 self.abs_max_v: 3883.5\n",
      "epoch-5   lr=['1.0000000'], tr/val_loss:  8.616900/ 55.304764, val:  45.83%, val_best:  50.00%, tr:  99.69%, tr_best:  99.69%, epoch time: 75.79 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0866%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.9681%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6799%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 58740 real_backward_count 9491  16.158%\n",
      "fc layer 2 self.abs_max_out: 1474.0\n",
      "lif layer 2 self.abs_max_v: 1875.5\n",
      "lif layer 2 self.abs_max_v: 1912.5\n",
      "fc layer 3 self.abs_max_out: 474.0\n",
      "lif layer 2 self.abs_max_v: 1964.5\n",
      "lif layer 2 self.abs_max_v: 2018.5\n",
      "lif layer 2 self.abs_max_v: 2163.5\n",
      "fc layer 1 self.abs_max_out: 2281.0\n",
      "fc layer 1 self.abs_max_out: 2284.0\n",
      "fc layer 1 self.abs_max_out: 2340.0\n",
      "lif layer 1 self.abs_max_v: 4046.0\n",
      "fc layer 1 self.abs_max_out: 2359.0\n",
      "fc layer 2 self.abs_max_out: 1507.0\n",
      "fc layer 3 self.abs_max_out: 482.0\n",
      "fc layer 1 self.abs_max_out: 2604.0\n",
      "lif layer 1 self.abs_max_v: 4229.5\n",
      "lif layer 1 self.abs_max_v: 4329.0\n",
      "epoch-6   lr=['1.0000000'], tr/val_loss:  8.129740/ 79.415726, val:  37.50%, val_best:  50.00%, tr:  99.80%, tr_best:  99.80%, epoch time: 75.83 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0086%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7610%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.0176%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 68530 real_backward_count 10772  15.719%\n",
      "fc layer 3 self.abs_max_out: 486.0\n",
      "lif layer 1 self.abs_max_v: 4352.5\n",
      "fc layer 3 self.abs_max_out: 493.0\n",
      "fc layer 3 self.abs_max_out: 494.0\n",
      "fc layer 3 self.abs_max_out: 516.0\n",
      "fc layer 3 self.abs_max_out: 532.0\n",
      "fc layer 3 self.abs_max_out: 590.0\n",
      "fc layer 3 self.abs_max_out: 600.0\n",
      "fc layer 3 self.abs_max_out: 608.0\n",
      "epoch-7   lr=['1.0000000'], tr/val_loss:  8.124951/ 49.163155, val:  41.25%, val_best:  50.00%, tr:  99.59%, tr_best:  99.80%, epoch time: 75.36 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0862%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 77.5419%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.4485%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 78320 real_backward_count 12048  15.383%\n",
      "epoch-8   lr=['1.0000000'], tr/val_loss:  7.499783/ 44.101505, val:  57.50%, val_best:  57.50%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.45 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.1073%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.7894%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8043%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 88110 real_backward_count 13271  15.062%\n",
      "lif layer 1 self.abs_max_v: 4444.5\n",
      "lif layer 1 self.abs_max_v: 4626.5\n",
      "fc layer 2 self.abs_max_out: 1580.0\n",
      "fc layer 2 self.abs_max_out: 1610.0\n",
      "epoch-9   lr=['1.0000000'], tr/val_loss:  8.120468/ 64.093834, val:  43.33%, val_best:  57.50%, tr:  99.69%, tr_best:  99.90%, epoch time: 76.15 seconds, 1.27 minutes\n",
      "layer   1  Sparsity: 91.0774%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.6443%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.2981%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 97900 real_backward_count 14520  14.831%\n",
      "epoch-10  lr=['1.0000000'], tr/val_loss:  7.503781/ 44.905346, val:  58.33%, val_best:  58.33%, tr:  99.90%, tr_best:  99.90%, epoch time: 75.75 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0706%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 76.2858%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.8728%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 107690 real_backward_count 15718  14.596%\n",
      "lif layer 2 self.abs_max_v: 2167.0\n",
      "lif layer 2 self.abs_max_v: 2261.0\n",
      "lif layer 2 self.abs_max_v: 2305.5\n",
      "epoch-11  lr=['1.0000000'], tr/val_loss:  8.060863/ 56.085194, val:  50.00%, val_best:  58.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 75.14 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0294%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5458%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.3573%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 117480 real_backward_count 16967  14.442%\n",
      "fc layer 1 self.abs_max_out: 2709.0\n",
      "lif layer 2 self.abs_max_v: 2341.0\n",
      "lif layer 2 self.abs_max_v: 2364.5\n",
      "epoch-12  lr=['1.0000000'], tr/val_loss:  7.673308/ 39.611320, val:  58.75%, val_best:  58.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 74.98 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0925%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.6832%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.5431%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 127270 real_backward_count 18164  14.272%\n",
      "lif layer 1 self.abs_max_v: 4688.5\n",
      "lif layer 1 self.abs_max_v: 4745.5\n",
      "lif layer 1 self.abs_max_v: 4770.0\n",
      "lif layer 1 self.abs_max_v: 4853.5\n",
      "epoch-13  lr=['1.0000000'], tr/val_loss:  7.479167/ 53.851540, val:  52.50%, val_best:  58.75%, tr:  99.90%, tr_best: 100.00%, epoch time: 75.59 seconds, 1.26 minutes\n",
      "layer   1  Sparsity: 91.0507%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.5886%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 68.6894%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 137060 real_backward_count 19340  14.111%\n",
      "fc layer 1 self.abs_max_out: 2737.0\n",
      "lif layer 1 self.abs_max_v: 4955.0\n",
      "epoch-14  lr=['1.0000000'], tr/val_loss:  7.318323/ 44.824467, val:  57.50%, val_best:  58.75%, tr:  99.80%, tr_best: 100.00%, epoch time: 75.16 seconds, 1.25 minutes\n",
      "layer   1  Sparsity: 91.0606%\n",
      "layer   1  average_sram_cycle_util: 0.0000%\n",
      "layer   2  Sparsity: 75.4745%\n",
      "layer   2  average_sram_cycle_util: 0.0000%\n",
      "layer   3  Sparsity: 69.0072%\n",
      "layer   3  average_sram_cycle_util: 0.0000%\n",
      "total_backward_count 146850 real_backward_count 20482  13.948%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'random', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        # \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [128.0]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [4.0*2]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "\n",
    "        \"learning_rate\": {\"values\": [1.0]}, \n",
    "        # \"lr_factor\": {\"values\": [-6, -7, -8, -9]}, \n",
    "        \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [-1]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [0]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [0]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [0]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "        \"lif_layer_sg_width2\": {\"values\": [4.0*2]},\n",
    "        \"lif_layer_v_threshold2\": {\"values\": [128.0]},\n",
    "        \"learning_rate2\": {\"values\": [1.0]},\n",
    "        \"init_scaling_0\": {\"values\": [10000+ 9]},\n",
    "        \"init_scaling_1\": {\"values\": [10000+ 9]},\n",
    "        \"init_scaling_2\": {\"values\": [10000+ 8]},\n",
    "        \n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"5\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "        lif_layer_sg_width2  =  wandb.config.lif_layer_sg_width2,\n",
    "        lif_layer_v_threshold2  =  wandb.config.lif_layer_v_threshold2,\n",
    "        learning_rate2  =  wandb.config.learning_rate2,\n",
    "        init_scaling = [wandb.config.init_scaling_0,wandb.config.init_scaling_1,wandb.config.init_scaling_2],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 'gsnqjucp'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
