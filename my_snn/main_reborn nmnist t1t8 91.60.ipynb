{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5739/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "\n",
    "                    test_timesteps = -1, # -1Ïù¥Î©¥ trainÏù¥Îûë ÎòëÍ∞ôÏù¥\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,\n",
    "            test_timesteps*temporal_filter) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "\n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    " \n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "\n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "                                    \n",
    "            ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "            if test_timesteps != -1:\n",
    "                assert real_batch == 1\n",
    "                this_data_timesteps = inputs.shape[0]\n",
    "                TIME = this_data_timesteps//temporal_filter\n",
    "                net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "            ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "            \n",
    "\n",
    "            \n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        \n",
    "                        ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "                        if test_timesteps != -1:\n",
    "                            assert real_batch == 1\n",
    "                            this_data_timesteps = inputs_val.shape[0]\n",
    "                            TIME = this_data_timesteps//temporal_filter\n",
    "                            net.module.change_timesteps(TIME) # netÏóê TIME ÏÑ§Ï†ï\n",
    "                            # print(net)    \n",
    "                        ## Îç∞Ïù¥ÌÑ∞ÎßàÎã§ TIMESTEPSÎã§Î•¥Îã§ ########################################################\n",
    "            \n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251024_182007-r03xvtcr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/r03xvtcr' target=\"_blank\">flowing-forest-15514</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/r03xvtcr' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/r03xvtcr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '1', 'single_step': True, 'unique_name': '20251024_182005_235', 'my_seed': 777, 'TIME': 1, 'BATCH': 1, 'IMAGE_SIZE': 17, 'which_data': 'NMNIST_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0.0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000.0, 'lif_layer_sg_width': 6.0, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_20250906_001313_333.pth', 'learning_rate': 0.00390625, 'epoch_num': 300, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 1, 'dvs_duration': 5000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': False, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1.0, 'bias': False, 'last_lif': False, 'temporal_filter': 1, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-9, -9], [-9, -9], [-8, -8]], 'test_timesteps': 8} \n",
      "\n",
      "dataset_hash = 4de1f410502ad52b6eca143e72129e2b\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 60000 BATCH: 1 train_data_count: 60000\n",
      "len(test_loader): 10000 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -9\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -8 -8\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=578, out_features=200, TIME=1, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (2): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.5, v_reset=10000.0, sg_width=6.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=1, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=1, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (5): LIF_layer(v_init=0.0, v_decay=0.5, v_threshold=0.5, v_reset=10000.0, sg_width=6.0, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=1, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=1, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-9, -9], [-9, -9], [-8, -8]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 157,600\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.00390625\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 133.0\n",
      "lif layer 1 self.abs_max_v: 133.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 147.0\n",
      "lif layer 1 self.abs_max_v: 147.0\n",
      "fc layer 1 self.abs_max_out: 192.0\n",
      "lif layer 1 self.abs_max_v: 192.0\n",
      "fc layer 1 self.abs_max_out: 266.0\n",
      "lif layer 1 self.abs_max_v: 266.0\n",
      "fc layer 2 self.abs_max_out: 36.0\n",
      "lif layer 2 self.abs_max_v: 36.0\n",
      "fc layer 1 self.abs_max_out: 275.0\n",
      "lif layer 1 self.abs_max_v: 275.0\n",
      "fc layer 1 self.abs_max_out: 320.0\n",
      "lif layer 1 self.abs_max_v: 320.0\n",
      "fc layer 2 self.abs_max_out: 63.0\n",
      "lif layer 2 self.abs_max_v: 63.0\n",
      "fc layer 1 self.abs_max_out: 324.0\n",
      "lif layer 1 self.abs_max_v: 324.0\n",
      "fc layer 2 self.abs_max_out: 70.0\n",
      "lif layer 2 self.abs_max_v: 70.0\n",
      "fc layer 1 self.abs_max_out: 337.0\n",
      "lif layer 1 self.abs_max_v: 337.0\n",
      "fc layer 2 self.abs_max_out: 71.0\n",
      "lif layer 2 self.abs_max_v: 71.0\n",
      "fc layer 1 self.abs_max_out: 424.0\n",
      "lif layer 1 self.abs_max_v: 424.0\n",
      "fc layer 2 self.abs_max_out: 131.0\n",
      "lif layer 2 self.abs_max_v: 131.0\n",
      "fc layer 2 self.abs_max_out: 147.0\n",
      "lif layer 2 self.abs_max_v: 147.0\n",
      "fc layer 1 self.abs_max_out: 425.0\n",
      "lif layer 1 self.abs_max_v: 425.0\n",
      "fc layer 2 self.abs_max_out: 163.0\n",
      "lif layer 2 self.abs_max_v: 163.0\n",
      "fc layer 2 self.abs_max_out: 165.0\n",
      "lif layer 2 self.abs_max_v: 165.0\n",
      "fc layer 1 self.abs_max_out: 678.0\n",
      "lif layer 1 self.abs_max_v: 678.0\n",
      "fc layer 2 self.abs_max_out: 202.0\n",
      "lif layer 2 self.abs_max_v: 202.0\n",
      "fc layer 2 self.abs_max_out: 243.0\n",
      "lif layer 2 self.abs_max_v: 243.0\n",
      "fc layer 2 self.abs_max_out: 254.0\n",
      "lif layer 2 self.abs_max_v: 254.0\n",
      "fc layer 2 self.abs_max_out: 359.0\n",
      "lif layer 2 self.abs_max_v: 359.0\n",
      "fc layer 3 self.abs_max_out: 22.0\n",
      "fc layer 2 self.abs_max_out: 377.0\n",
      "lif layer 2 self.abs_max_v: 377.0\n",
      "fc layer 3 self.abs_max_out: 37.0\n",
      "fc layer 2 self.abs_max_out: 421.0\n",
      "lif layer 2 self.abs_max_v: 421.0\n",
      "fc layer 3 self.abs_max_out: 51.0\n",
      "fc layer 3 self.abs_max_out: 52.0\n",
      "fc layer 2 self.abs_max_out: 530.0\n",
      "lif layer 2 self.abs_max_v: 530.0\n",
      "fc layer 2 self.abs_max_out: 567.0\n",
      "lif layer 2 self.abs_max_v: 567.0\n",
      "fc layer 2 self.abs_max_out: 568.0\n",
      "lif layer 2 self.abs_max_v: 568.0\n",
      "fc layer 3 self.abs_max_out: 57.0\n",
      "fc layer 3 self.abs_max_out: 65.0\n",
      "fc layer 2 self.abs_max_out: 620.0\n",
      "lif layer 2 self.abs_max_v: 620.0\n",
      "fc layer 1 self.abs_max_out: 739.0\n",
      "lif layer 1 self.abs_max_v: 739.0\n",
      "fc layer 2 self.abs_max_out: 672.0\n",
      "lif layer 2 self.abs_max_v: 672.0\n",
      "fc layer 3 self.abs_max_out: 66.0\n",
      "fc layer 2 self.abs_max_out: 680.0\n",
      "lif layer 2 self.abs_max_v: 680.0\n",
      "fc layer 3 self.abs_max_out: 69.0\n",
      "fc layer 3 self.abs_max_out: 92.0\n",
      "fc layer 2 self.abs_max_out: 707.0\n",
      "lif layer 2 self.abs_max_v: 707.0\n",
      "fc layer 3 self.abs_max_out: 100.0\n",
      "fc layer 2 self.abs_max_out: 724.0\n",
      "lif layer 2 self.abs_max_v: 724.0\n",
      "fc layer 1 self.abs_max_out: 789.0\n",
      "lif layer 1 self.abs_max_v: 789.0\n",
      "fc layer 3 self.abs_max_out: 101.0\n",
      "fc layer 3 self.abs_max_out: 119.0\n",
      "fc layer 1 self.abs_max_out: 925.0\n",
      "lif layer 1 self.abs_max_v: 925.0\n",
      "fc layer 2 self.abs_max_out: 760.0\n",
      "lif layer 2 self.abs_max_v: 760.0\n",
      "fc layer 3 self.abs_max_out: 141.0\n",
      "fc layer 3 self.abs_max_out: 142.0\n",
      "fc layer 3 self.abs_max_out: 146.0\n",
      "fc layer 2 self.abs_max_out: 817.0\n",
      "lif layer 2 self.abs_max_v: 817.0\n",
      "fc layer 3 self.abs_max_out: 176.0\n",
      "fc layer 2 self.abs_max_out: 866.0\n",
      "lif layer 2 self.abs_max_v: 866.0\n",
      "fc layer 2 self.abs_max_out: 918.0\n",
      "lif layer 2 self.abs_max_v: 918.0\n",
      "fc layer 2 self.abs_max_out: 920.0\n",
      "lif layer 2 self.abs_max_v: 920.0\n",
      "fc layer 2 self.abs_max_out: 1033.0\n",
      "lif layer 2 self.abs_max_v: 1033.0\n",
      "fc layer 2 self.abs_max_out: 1058.0\n",
      "lif layer 2 self.abs_max_v: 1058.0\n",
      "fc layer 2 self.abs_max_out: 1073.0\n",
      "lif layer 2 self.abs_max_v: 1073.0\n",
      "fc layer 3 self.abs_max_out: 185.0\n",
      "fc layer 2 self.abs_max_out: 1112.0\n",
      "lif layer 2 self.abs_max_v: 1112.0\n",
      "fc layer 2 self.abs_max_out: 1118.0\n",
      "lif layer 2 self.abs_max_v: 1118.0\n",
      "fc layer 3 self.abs_max_out: 226.0\n",
      "fc layer 2 self.abs_max_out: 1252.0\n",
      "lif layer 2 self.abs_max_v: 1252.0\n",
      "fc layer 1 self.abs_max_out: 950.0\n",
      "lif layer 1 self.abs_max_v: 950.0\n",
      "fc layer 1 self.abs_max_out: 1004.0\n",
      "lif layer 1 self.abs_max_v: 1004.0\n",
      "fc layer 1 self.abs_max_out: 1057.0\n",
      "lif layer 1 self.abs_max_v: 1057.0\n",
      "fc layer 1 self.abs_max_out: 1062.0\n",
      "lif layer 1 self.abs_max_v: 1062.0\n",
      "fc layer 1 self.abs_max_out: 1165.0\n",
      "lif layer 1 self.abs_max_v: 1165.0\n",
      "fc layer 2 self.abs_max_out: 1318.0\n",
      "lif layer 2 self.abs_max_v: 1318.0\n",
      "fc layer 1 self.abs_max_out: 1245.0\n",
      "lif layer 1 self.abs_max_v: 1245.0\n",
      "fc layer 1 self.abs_max_out: 1269.0\n",
      "lif layer 1 self.abs_max_v: 1269.0\n",
      "fc layer 2 self.abs_max_out: 1321.0\n",
      "lif layer 2 self.abs_max_v: 1321.0\n",
      "fc layer 2 self.abs_max_out: 1507.0\n",
      "lif layer 2 self.abs_max_v: 1507.0\n",
      "fc layer 1 self.abs_max_out: 1515.0\n",
      "lif layer 1 self.abs_max_v: 1515.0\n",
      "fc layer 1 self.abs_max_out: 1565.0\n",
      "lif layer 1 self.abs_max_v: 1565.0\n",
      "fc layer 2 self.abs_max_out: 1523.0\n",
      "lif layer 2 self.abs_max_v: 1523.0\n",
      "fc layer 1 self.abs_max_out: 1575.0\n",
      "lif layer 1 self.abs_max_v: 1575.0\n",
      "fc layer 1 self.abs_max_out: 1615.0\n",
      "lif layer 1 self.abs_max_v: 1615.0\n",
      "fc layer 2 self.abs_max_out: 1675.0\n",
      "lif layer 2 self.abs_max_v: 1675.0\n",
      "fc layer 1 self.abs_max_out: 1627.0\n",
      "lif layer 1 self.abs_max_v: 1627.0\n",
      "fc layer 1 self.abs_max_out: 1665.0\n",
      "lif layer 1 self.abs_max_v: 1665.0\n",
      "fc layer 1 self.abs_max_out: 1741.0\n",
      "lif layer 1 self.abs_max_v: 1741.0\n",
      "fc layer 1 self.abs_max_out: 1887.0\n",
      "lif layer 1 self.abs_max_v: 1887.0\n",
      "fc layer 2 self.abs_max_out: 1755.0\n",
      "lif layer 2 self.abs_max_v: 1755.0\n",
      "fc layer 1 self.abs_max_out: 1970.0\n",
      "lif layer 1 self.abs_max_v: 1970.0\n",
      "fc layer 1 self.abs_max_out: 1976.0\n",
      "lif layer 1 self.abs_max_v: 1976.0\n",
      "fc layer 1 self.abs_max_out: 2007.0\n",
      "lif layer 1 self.abs_max_v: 2007.0\n",
      "fc layer 1 self.abs_max_out: 2067.0\n",
      "lif layer 1 self.abs_max_v: 2067.0\n",
      "fc layer 1 self.abs_max_out: 2125.0\n",
      "lif layer 1 self.abs_max_v: 2125.0\n",
      "fc layer 2 self.abs_max_out: 1775.0\n",
      "lif layer 2 self.abs_max_v: 1775.0\n",
      "fc layer 1 self.abs_max_out: 2328.0\n",
      "lif layer 1 self.abs_max_v: 2328.0\n",
      "fc layer 2 self.abs_max_out: 1820.0\n",
      "lif layer 2 self.abs_max_v: 1820.0\n",
      "fc layer 2 self.abs_max_out: 1822.0\n",
      "lif layer 2 self.abs_max_v: 1822.0\n",
      "fc layer 2 self.abs_max_out: 1851.0\n",
      "lif layer 2 self.abs_max_v: 1851.0\n",
      "fc layer 1 self.abs_max_out: 2533.0\n",
      "lif layer 1 self.abs_max_v: 2533.0\n",
      "fc layer 2 self.abs_max_out: 1875.0\n",
      "lif layer 2 self.abs_max_v: 1875.0\n",
      "fc layer 2 self.abs_max_out: 1969.0\n",
      "lif layer 2 self.abs_max_v: 1969.0\n",
      "lif layer 2 self.abs_max_v: 2109.0\n",
      "lif layer 2 self.abs_max_v: 2273.5\n",
      "lif layer 2 self.abs_max_v: 2304.0\n",
      "lif layer 2 self.abs_max_v: 2476.5\n",
      "lif layer 2 self.abs_max_v: 2515.0\n",
      "lif layer 2 self.abs_max_v: 2550.5\n",
      "lif layer 2 self.abs_max_v: 2841.0\n",
      "lif layer 2 self.abs_max_v: 3140.0\n",
      "fc layer 1 self.abs_max_out: 2646.0\n",
      "lif layer 1 self.abs_max_v: 2646.0\n",
      "lif layer 1 self.abs_max_v: 2879.5\n",
      "lif layer 1 self.abs_max_v: 2981.0\n",
      "fc layer 1 self.abs_max_out: 2780.0\n",
      "fc layer 1 self.abs_max_out: 2908.0\n",
      "lif layer 2 self.abs_max_v: 3260.5\n",
      "fc layer 1 self.abs_max_out: 2942.0\n",
      "fc layer 1 self.abs_max_out: 3032.0\n",
      "lif layer 1 self.abs_max_v: 3032.0\n",
      "fc layer 2 self.abs_max_out: 2010.0\n",
      "lif layer 2 self.abs_max_v: 3290.5\n",
      "fc layer 1 self.abs_max_out: 3206.0\n",
      "lif layer 1 self.abs_max_v: 3206.0\n",
      "fc layer 1 self.abs_max_out: 3498.0\n",
      "lif layer 1 self.abs_max_v: 3498.0\n",
      "fc layer 1 self.abs_max_out: 3562.0\n",
      "lif layer 1 self.abs_max_v: 3562.0\n",
      "fc layer 1 self.abs_max_out: 3886.0\n",
      "lif layer 1 self.abs_max_v: 3886.0\n",
      "fc layer 1 self.abs_max_out: 3972.0\n",
      "lif layer 1 self.abs_max_v: 3972.0\n",
      "fc layer 2 self.abs_max_out: 2175.0\n",
      "fc layer 1 self.abs_max_out: 3996.0\n",
      "lif layer 1 self.abs_max_v: 3996.0\n",
      "fc layer 2 self.abs_max_out: 2267.0\n",
      "epoch-0   lr=['0.0039062'], tr/val_loss:  2.146759/  2.179491, val:  47.26%, val_best:  47.26%, tr:  63.76%, tr_best:  63.76%, epoch time: 710.38 seconds, 11.84 minutes\n",
      "total_backward_count 60000 real_backward_count 21744  36.240%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 2 self.abs_max_out: 2268.0\n",
      "lif layer 2 self.abs_max_v: 3293.5\n",
      "lif layer 2 self.abs_max_v: 3769.0\n",
      "fc layer 2 self.abs_max_out: 2286.0\n",
      "lif layer 2 self.abs_max_v: 4046.0\n",
      "lif layer 2 self.abs_max_v: 4191.0\n",
      "fc layer 2 self.abs_max_out: 2310.0\n",
      "fc layer 2 self.abs_max_out: 2618.0\n",
      "lif layer 2 self.abs_max_v: 4296.5\n",
      "fc layer 2 self.abs_max_out: 2820.0\n",
      "lif layer 2 self.abs_max_v: 4675.5\n",
      "lif layer 1 self.abs_max_v: 4056.5\n",
      "fc layer 1 self.abs_max_out: 4166.0\n",
      "lif layer 1 self.abs_max_v: 4166.0\n",
      "fc layer 1 self.abs_max_out: 4327.0\n",
      "lif layer 1 self.abs_max_v: 4327.0\n",
      "fc layer 1 self.abs_max_out: 4564.0\n",
      "lif layer 1 self.abs_max_v: 4564.0\n",
      "fc layer 1 self.abs_max_out: 4746.0\n",
      "lif layer 1 self.abs_max_v: 4746.0\n",
      "fc layer 1 self.abs_max_out: 4802.0\n",
      "lif layer 1 self.abs_max_v: 4802.0\n",
      "epoch-1   lr=['0.0039062'], tr/val_loss:  2.141900/  2.184731, val:  60.27%, val_best:  60.27%, tr:  73.64%, tr_best:  73.64%, epoch time: 703.08 seconds, 11.72 minutes\n",
      "total_backward_count 120000 real_backward_count 37560  31.300%\n",
      "fc layer 2 self.abs_max_out: 2950.0\n",
      "lif layer 2 self.abs_max_v: 4919.5\n",
      "lif layer 2 self.abs_max_v: 4997.0\n",
      "fc layer 2 self.abs_max_out: 3047.0\n",
      "fc layer 1 self.abs_max_out: 4903.0\n",
      "lif layer 1 self.abs_max_v: 4903.0\n",
      "fc layer 1 self.abs_max_out: 5153.0\n",
      "lif layer 1 self.abs_max_v: 5153.0\n",
      "lif layer 1 self.abs_max_v: 5203.5\n",
      "fc layer 1 self.abs_max_out: 5253.0\n",
      "lif layer 1 self.abs_max_v: 5253.0\n",
      "epoch-2   lr=['0.0039062'], tr/val_loss:  2.138541/  2.178695, val:  68.82%, val_best:  68.82%, tr:  76.83%, tr_best:  76.83%, epoch time: 700.85 seconds, 11.68 minutes\n",
      "total_backward_count 180000 real_backward_count 51465  28.592%\n",
      "lif layer 2 self.abs_max_v: 5087.5\n",
      "fc layer 1 self.abs_max_out: 5413.0\n",
      "lif layer 1 self.abs_max_v: 5413.0\n",
      "lif layer 1 self.abs_max_v: 5627.0\n",
      "fc layer 1 self.abs_max_out: 5578.0\n",
      "epoch-3   lr=['0.0039062'], tr/val_loss:  2.128649/  2.164798, val:  76.77%, val_best:  76.77%, tr:  78.60%, tr_best:  78.60%, epoch time: 698.05 seconds, 11.63 minutes\n",
      "total_backward_count 240000 real_backward_count 64306  26.794%\n",
      "fc layer 2 self.abs_max_out: 3221.0\n",
      "lif layer 2 self.abs_max_v: 5208.0\n",
      "fc layer 1 self.abs_max_out: 5593.0\n",
      "lif layer 1 self.abs_max_v: 5920.0\n",
      "fc layer 1 self.abs_max_out: 5805.0\n",
      "epoch-4   lr=['0.0039062'], tr/val_loss:  2.130059/  2.166533, val:  78.57%, val_best:  78.57%, tr:  80.27%, tr_best:  80.27%, epoch time: 707.63 seconds, 11.79 minutes\n",
      "total_backward_count 300000 real_backward_count 76143  25.381%\n",
      "lif layer 1 self.abs_max_v: 6113.0\n",
      "fc layer 1 self.abs_max_out: 6011.0\n",
      "epoch-5   lr=['0.0039062'], tr/val_loss:  2.118857/  2.167439, val:  69.90%, val_best:  78.57%, tr:  81.54%, tr_best:  81.54%, epoch time: 757.32 seconds, 12.62 minutes\n",
      "total_backward_count 360000 real_backward_count 87218  24.227%\n",
      "fc layer 2 self.abs_max_out: 3257.0\n",
      "lif layer 2 self.abs_max_v: 5325.0\n",
      "fc layer 2 self.abs_max_out: 3286.0\n",
      "lif layer 1 self.abs_max_v: 6174.0\n",
      "lif layer 1 self.abs_max_v: 6232.0\n",
      "lif layer 1 self.abs_max_v: 6263.0\n",
      "fc layer 1 self.abs_max_out: 6150.0\n",
      "epoch-6   lr=['0.0039062'], tr/val_loss:  2.103728/  2.153270, val:  66.89%, val_best:  78.57%, tr:  82.83%, tr_best:  82.83%, epoch time: 790.39 seconds, 13.17 minutes\n",
      "total_backward_count 420000 real_backward_count 97519  23.219%\n",
      "lif layer 2 self.abs_max_v: 5424.5\n",
      "lif layer 2 self.abs_max_v: 5458.0\n",
      "lif layer 1 self.abs_max_v: 6275.0\n",
      "lif layer 1 self.abs_max_v: 6359.5\n",
      "lif layer 1 self.abs_max_v: 6613.5\n",
      "fc layer 1 self.abs_max_out: 6262.0\n",
      "epoch-7   lr=['0.0039062'], tr/val_loss:  2.103419/  2.146209, val:  80.83%, val_best:  80.83%, tr:  83.58%, tr_best:  83.58%, epoch time: 788.43 seconds, 13.14 minutes\n",
      "total_backward_count 480000 real_backward_count 107370  22.369%\n",
      "fc layer 2 self.abs_max_out: 3500.0\n",
      "lif layer 2 self.abs_max_v: 5497.0\n",
      "lif layer 2 self.abs_max_v: 5755.0\n",
      "lif layer 1 self.abs_max_v: 6934.0\n",
      "fc layer 1 self.abs_max_out: 6367.0\n",
      "epoch-8   lr=['0.0039062'], tr/val_loss:  2.098999/  2.144199, val:  82.70%, val_best:  82.70%, tr:  84.57%, tr_best:  84.57%, epoch time: 800.13 seconds, 13.34 minutes\n",
      "total_backward_count 540000 real_backward_count 116626  21.597%\n",
      "lif layer 2 self.abs_max_v: 6076.5\n",
      "fc layer 1 self.abs_max_out: 6473.0\n",
      "epoch-9   lr=['0.0039062'], tr/val_loss:  2.092061/  2.146119, val:  86.31%, val_best:  86.31%, tr:  85.26%, tr_best:  85.26%, epoch time: 771.05 seconds, 12.85 minutes\n",
      "total_backward_count 600000 real_backward_count 125469  20.911%\n",
      "fc layer 1 self.abs_max_out: 6536.0\n",
      "epoch-10  lr=['0.0039062'], tr/val_loss:  2.068903/  2.129271, val:  82.00%, val_best:  86.31%, tr:  85.66%, tr_best:  85.66%, epoch time: 791.08 seconds, 13.18 minutes\n",
      "total_backward_count 660000 real_backward_count 134075  20.314%\n",
      "lif layer 1 self.abs_max_v: 7002.5\n",
      "fc layer 1 self.abs_max_out: 6593.0\n",
      "epoch-11  lr=['0.0039062'], tr/val_loss:  2.055023/  2.129268, val:  70.23%, val_best:  86.31%, tr:  86.17%, tr_best:  86.17%, epoch time: 802.85 seconds, 13.38 minutes\n",
      "total_backward_count 720000 real_backward_count 142374  19.774%\n",
      "lif layer 1 self.abs_max_v: 7042.5\n",
      "fc layer 1 self.abs_max_out: 6635.0\n",
      "epoch-12  lr=['0.0039062'], tr/val_loss:  2.065756/  2.109753, val:  82.94%, val_best:  86.31%, tr:  86.88%, tr_best:  86.88%, epoch time: 802.84 seconds, 13.38 minutes\n",
      "total_backward_count 780000 real_backward_count 150249  19.263%\n",
      "fc layer 2 self.abs_max_out: 3587.0\n",
      "lif layer 2 self.abs_max_v: 6125.0\n",
      "lif layer 1 self.abs_max_v: 7293.0\n",
      "fc layer 1 self.abs_max_out: 6678.0\n",
      "epoch-13  lr=['0.0039062'], tr/val_loss:  2.048677/  2.118328, val:  82.60%, val_best:  86.31%, tr:  87.49%, tr_best:  87.49%, epoch time: 777.88 seconds, 12.96 minutes\n",
      "total_backward_count 840000 real_backward_count 157756  18.780%\n",
      "fc layer 1 self.abs_max_out: 6681.0\n",
      "epoch-14  lr=['0.0039062'], tr/val_loss:  2.058682/  2.114772, val:  81.99%, val_best:  86.31%, tr:  87.92%, tr_best:  87.92%, epoch time: 779.54 seconds, 12.99 minutes\n",
      "total_backward_count 900000 real_backward_count 165005  18.334%\n",
      "fc layer 2 self.abs_max_out: 3675.0\n",
      "fc layer 2 self.abs_max_out: 3782.0\n",
      "lif layer 2 self.abs_max_v: 6552.0\n",
      "lif layer 1 self.abs_max_v: 7327.0\n",
      "fc layer 1 self.abs_max_out: 6722.0\n",
      "epoch-15  lr=['0.0039062'], tr/val_loss:  2.037033/  2.108158, val:  83.50%, val_best:  86.31%, tr:  88.45%, tr_best:  88.45%, epoch time: 784.65 seconds, 13.08 minutes\n",
      "total_backward_count 960000 real_backward_count 171933  17.910%\n",
      "lif layer 1 self.abs_max_v: 7391.5\n",
      "fc layer 1 self.abs_max_out: 6731.0\n",
      "epoch-16  lr=['0.0039062'], tr/val_loss:  2.037155/  2.141643, val:  81.52%, val_best:  86.31%, tr:  88.58%, tr_best:  88.58%, epoch time: 777.39 seconds, 12.96 minutes\n",
      "total_backward_count 1020000 real_backward_count 178787  17.528%\n",
      "fc layer 1 self.abs_max_out: 6745.0\n",
      "epoch-17  lr=['0.0039062'], tr/val_loss:  2.035516/  2.116982, val:  83.74%, val_best:  86.31%, tr:  89.06%, tr_best:  89.06%, epoch time: 782.20 seconds, 13.04 minutes\n",
      "total_backward_count 1080000 real_backward_count 185352  17.162%\n",
      "lif layer 1 self.abs_max_v: 7449.0\n",
      "fc layer 1 self.abs_max_out: 6764.0\n",
      "epoch-18  lr=['0.0039062'], tr/val_loss:  2.038988/  2.122222, val:  81.65%, val_best:  86.31%, tr:  89.62%, tr_best:  89.62%, epoch time: 784.76 seconds, 13.08 minutes\n",
      "total_backward_count 1140000 real_backward_count 191580  16.805%\n",
      "lif layer 1 self.abs_max_v: 7478.5\n",
      "fc layer 1 self.abs_max_out: 6792.0\n",
      "epoch-19  lr=['0.0039062'], tr/val_loss:  2.031792/  2.098482, val:  82.62%, val_best:  86.31%, tr:  89.66%, tr_best:  89.66%, epoch time: 776.94 seconds, 12.95 minutes\n",
      "total_backward_count 1200000 real_backward_count 197786  16.482%\n",
      "fc layer 1 self.abs_max_out: 6809.0\n",
      "epoch-20  lr=['0.0039062'], tr/val_loss:  1.997146/  2.093733, val:  77.89%, val_best:  86.31%, tr:  90.27%, tr_best:  90.27%, epoch time: 788.05 seconds, 13.13 minutes\n",
      "total_backward_count 1260000 real_backward_count 203626  16.161%\n",
      "lif layer 2 self.abs_max_v: 6571.0\n",
      "lif layer 1 self.abs_max_v: 7590.0\n",
      "epoch-21  lr=['0.0039062'], tr/val_loss:  2.003890/  2.106167, val:  84.38%, val_best:  86.31%, tr:  90.55%, tr_best:  90.55%, epoch time: 782.43 seconds, 13.04 minutes\n",
      "total_backward_count 1320000 real_backward_count 209293  15.856%\n",
      "fc layer 1 self.abs_max_out: 6844.0\n",
      "epoch-22  lr=['0.0039062'], tr/val_loss:  2.009089/  2.093275, val:  85.78%, val_best:  86.31%, tr:  90.70%, tr_best:  90.70%, epoch time: 779.77 seconds, 13.00 minutes\n",
      "total_backward_count 1380000 real_backward_count 214871  15.570%\n",
      "fc layer 1 self.abs_max_out: 6861.0\n",
      "epoch-23  lr=['0.0039062'], tr/val_loss:  1.989777/  2.100609, val:  79.67%, val_best:  86.31%, tr:  91.17%, tr_best:  91.17%, epoch time: 776.06 seconds, 12.93 minutes\n",
      "total_backward_count 1440000 real_backward_count 220167  15.289%\n",
      "fc layer 3 self.abs_max_out: 236.0\n",
      "fc layer 1 self.abs_max_out: 6873.0\n",
      "epoch-24  lr=['0.0039062'], tr/val_loss:  1.987668/  2.080004, val:  83.78%, val_best:  86.31%, tr:  91.36%, tr_best:  91.36%, epoch time: 814.54 seconds, 13.58 minutes\n",
      "total_backward_count 1500000 real_backward_count 225350  15.023%\n",
      "fc layer 1 self.abs_max_out: 6883.0\n",
      "epoch-25  lr=['0.0039062'], tr/val_loss:  1.982858/  2.096221, val:  87.70%, val_best:  87.70%, tr:  91.60%, tr_best:  91.60%, epoch time: 806.38 seconds, 13.44 minutes\n",
      "total_backward_count 1560000 real_backward_count 230391  14.769%\n",
      "lif layer 1 self.abs_max_v: 7604.5\n",
      "epoch-26  lr=['0.0039062'], tr/val_loss:  1.988465/  2.075813, val:  83.70%, val_best:  87.70%, tr:  91.96%, tr_best:  91.96%, epoch time: 795.31 seconds, 13.26 minutes\n",
      "total_backward_count 1620000 real_backward_count 235217  14.520%\n",
      "epoch-27  lr=['0.0039062'], tr/val_loss:  1.957844/  2.062383, val:  86.95%, val_best:  87.70%, tr:  92.16%, tr_best:  92.16%, epoch time: 767.51 seconds, 12.79 minutes\n",
      "total_backward_count 1680000 real_backward_count 239918  14.281%\n",
      "lif layer 1 self.abs_max_v: 7653.0\n",
      "epoch-28  lr=['0.0039062'], tr/val_loss:  1.951174/  2.045688, val:  84.11%, val_best:  87.70%, tr:  92.54%, tr_best:  92.54%, epoch time: 772.90 seconds, 12.88 minutes\n",
      "total_backward_count 1740000 real_backward_count 244394  14.046%\n",
      "fc layer 3 self.abs_max_out: 241.0\n",
      "fc layer 2 self.abs_max_out: 3843.0\n",
      "epoch-29  lr=['0.0039062'], tr/val_loss:  1.931236/  2.042099, val:  85.87%, val_best:  87.70%, tr:  92.62%, tr_best:  92.62%, epoch time: 777.24 seconds, 12.95 minutes\n",
      "total_backward_count 1800000 real_backward_count 248821  13.823%\n",
      "lif layer 1 self.abs_max_v: 7740.0\n",
      "epoch-30  lr=['0.0039062'], tr/val_loss:  1.950806/  2.071693, val:  89.89%, val_best:  89.89%, tr:  92.88%, tr_best:  92.88%, epoch time: 777.93 seconds, 12.97 minutes\n",
      "total_backward_count 1860000 real_backward_count 253094  13.607%\n",
      "lif layer 1 self.abs_max_v: 7758.0\n",
      "fc layer 1 self.abs_max_out: 6885.0\n",
      "epoch-31  lr=['0.0039062'], tr/val_loss:  1.951121/  2.068335, val:  83.58%, val_best:  89.89%, tr:  93.00%, tr_best:  93.00%, epoch time: 777.96 seconds, 12.97 minutes\n",
      "total_backward_count 1920000 real_backward_count 257297  13.401%\n",
      "lif layer 1 self.abs_max_v: 7801.0\n",
      "fc layer 1 self.abs_max_out: 6890.0\n",
      "fc layer 1 self.abs_max_out: 6898.0\n",
      "epoch-32  lr=['0.0039062'], tr/val_loss:  1.953693/  2.049339, val:  86.45%, val_best:  89.89%, tr:  93.27%, tr_best:  93.27%, epoch time: 775.84 seconds, 12.93 minutes\n",
      "total_backward_count 1980000 real_backward_count 261335  13.199%\n",
      "fc layer 3 self.abs_max_out: 242.0\n",
      "fc layer 2 self.abs_max_out: 3845.0\n",
      "lif layer 1 self.abs_max_v: 7850.5\n",
      "fc layer 1 self.abs_max_out: 6906.0\n",
      "fc layer 1 self.abs_max_out: 6911.0\n",
      "epoch-33  lr=['0.0039062'], tr/val_loss:  1.930320/  2.044924, val:  87.31%, val_best:  89.89%, tr:  93.58%, tr_best:  93.58%, epoch time: 777.50 seconds, 12.96 minutes\n",
      "total_backward_count 2040000 real_backward_count 265186  12.999%\n",
      "fc layer 3 self.abs_max_out: 247.0\n",
      "lif layer 1 self.abs_max_v: 7978.0\n",
      "fc layer 1 self.abs_max_out: 6913.0\n",
      "fc layer 1 self.abs_max_out: 6931.0\n",
      "epoch-34  lr=['0.0039062'], tr/val_loss:  1.929649/  2.043828, val:  90.07%, val_best:  90.07%, tr:  93.50%, tr_best:  93.58%, epoch time: 775.49 seconds, 12.92 minutes\n",
      "total_backward_count 2100000 real_backward_count 269084  12.814%\n",
      "epoch-35  lr=['0.0039062'], tr/val_loss:  1.922106/  2.044308, val:  87.59%, val_best:  90.07%, tr:  93.75%, tr_best:  93.75%, epoch time: 774.84 seconds, 12.91 minutes\n",
      "total_backward_count 2160000 real_backward_count 272832  12.631%\n",
      "fc layer 3 self.abs_max_out: 257.0\n",
      "epoch-36  lr=['0.0039062'], tr/val_loss:  1.923914/  2.039309, val:  87.12%, val_best:  90.07%, tr:  93.88%, tr_best:  93.88%, epoch time: 771.75 seconds, 12.86 minutes\n",
      "total_backward_count 2220000 real_backward_count 276501  12.455%\n",
      "fc layer 2 self.abs_max_out: 3889.0\n",
      "fc layer 1 self.abs_max_out: 6951.0\n",
      "epoch-37  lr=['0.0039062'], tr/val_loss:  1.913806/  2.034728, val:  90.34%, val_best:  90.34%, tr:  94.22%, tr_best:  94.22%, epoch time: 776.93 seconds, 12.95 minutes\n",
      "total_backward_count 2280000 real_backward_count 279969  12.279%\n",
      "lif layer 2 self.abs_max_v: 6749.0\n",
      "fc layer 2 self.abs_max_out: 4070.0\n",
      "fc layer 2 self.abs_max_out: 4182.0\n",
      "fc layer 2 self.abs_max_out: 4267.0\n",
      "lif layer 2 self.abs_max_v: 6783.5\n",
      "lif layer 2 self.abs_max_v: 7184.0\n",
      "epoch-38  lr=['0.0039062'], tr/val_loss:  1.913444/  2.030511, val:  88.66%, val_best:  90.34%, tr:  94.24%, tr_best:  94.24%, epoch time: 774.05 seconds, 12.90 minutes\n",
      "total_backward_count 2340000 real_backward_count 283427  12.112%\n",
      "fc layer 3 self.abs_max_out: 270.0\n",
      "fc layer 1 self.abs_max_out: 6966.0\n",
      "epoch-39  lr=['0.0039062'], tr/val_loss:  1.898988/  2.027810, val:  86.54%, val_best:  90.34%, tr:  94.34%, tr_best:  94.34%, epoch time: 771.81 seconds, 12.86 minutes\n",
      "total_backward_count 2400000 real_backward_count 286825  11.951%\n",
      "lif layer 1 self.abs_max_v: 8026.0\n",
      "fc layer 1 self.abs_max_out: 6975.0\n",
      "epoch-40  lr=['0.0039062'], tr/val_loss:  1.900635/  2.022167, val:  85.97%, val_best:  90.34%, tr:  94.57%, tr_best:  94.57%, epoch time: 771.76 seconds, 12.86 minutes\n",
      "total_backward_count 2460000 real_backward_count 290083  11.792%\n",
      "lif layer 1 self.abs_max_v: 8027.5\n",
      "fc layer 1 self.abs_max_out: 6989.0\n",
      "epoch-41  lr=['0.0039062'], tr/val_loss:  1.889861/  2.001127, val:  88.89%, val_best:  90.34%, tr:  94.61%, tr_best:  94.61%, epoch time: 775.51 seconds, 12.93 minutes\n",
      "total_backward_count 2520000 real_backward_count 293315  11.639%\n",
      "epoch-42  lr=['0.0039062'], tr/val_loss:  1.888729/  2.020118, val:  88.97%, val_best:  90.34%, tr:  94.80%, tr_best:  94.80%, epoch time: 774.89 seconds, 12.91 minutes\n",
      "total_backward_count 2580000 real_backward_count 296432  11.490%\n",
      "fc layer 1 self.abs_max_out: 7000.0\n",
      "epoch-43  lr=['0.0039062'], tr/val_loss:  1.882448/  1.985403, val:  90.68%, val_best:  90.68%, tr:  95.05%, tr_best:  95.05%, epoch time: 775.73 seconds, 12.93 minutes\n",
      "total_backward_count 2640000 real_backward_count 299400  11.341%\n",
      "fc layer 3 self.abs_max_out: 277.0\n",
      "fc layer 3 self.abs_max_out: 280.0\n",
      "epoch-44  lr=['0.0039062'], tr/val_loss:  1.857187/  1.987494, val:  89.10%, val_best:  90.68%, tr:  95.13%, tr_best:  95.13%, epoch time: 771.97 seconds, 12.87 minutes\n",
      "total_backward_count 2700000 real_backward_count 302324  11.197%\n",
      "fc layer 1 self.abs_max_out: 7002.0\n",
      "epoch-45  lr=['0.0039062'], tr/val_loss:  1.872513/  2.012594, val:  89.97%, val_best:  90.68%, tr:  95.25%, tr_best:  95.25%, epoch time: 772.51 seconds, 12.88 minutes\n",
      "total_backward_count 2760000 real_backward_count 305175  11.057%\n",
      "fc layer 1 self.abs_max_out: 7013.0\n",
      "epoch-46  lr=['0.0039062'], tr/val_loss:  1.864777/  1.986606, val:  88.24%, val_best:  90.68%, tr:  95.16%, tr_best:  95.25%, epoch time: 776.59 seconds, 12.94 minutes\n",
      "total_backward_count 2820000 real_backward_count 308082  10.925%\n",
      "epoch-47  lr=['0.0039062'], tr/val_loss:  1.842074/  1.982411, val:  87.33%, val_best:  90.68%, tr:  95.19%, tr_best:  95.25%, epoch time: 776.24 seconds, 12.94 minutes\n",
      "total_backward_count 2880000 real_backward_count 310967  10.797%\n",
      "fc layer 1 self.abs_max_out: 7024.0\n",
      "epoch-48  lr=['0.0039062'], tr/val_loss:  1.840871/  1.962001, val:  89.76%, val_best:  90.68%, tr:  95.25%, tr_best:  95.25%, epoch time: 770.40 seconds, 12.84 minutes\n",
      "total_backward_count 2940000 real_backward_count 313815  10.674%\n",
      "fc layer 3 self.abs_max_out: 283.0\n",
      "fc layer 3 self.abs_max_out: 307.0\n",
      "lif layer 1 self.abs_max_v: 8044.0\n",
      "fc layer 1 self.abs_max_out: 7040.0\n",
      "epoch-49  lr=['0.0039062'], tr/val_loss:  1.820853/  1.974406, val:  86.22%, val_best:  90.68%, tr:  95.39%, tr_best:  95.39%, epoch time: 771.98 seconds, 12.87 minutes\n",
      "total_backward_count 3000000 real_backward_count 316582  10.553%\n",
      "fc layer 3 self.abs_max_out: 331.0\n",
      "epoch-50  lr=['0.0039062'], tr/val_loss:  1.824172/  1.966437, val:  88.19%, val_best:  90.68%, tr:  95.53%, tr_best:  95.53%, epoch time: 774.03 seconds, 12.90 minutes\n",
      "total_backward_count 3060000 real_backward_count 319261  10.433%\n",
      "fc layer 1 self.abs_max_out: 7048.0\n",
      "epoch-51  lr=['0.0039062'], tr/val_loss:  1.826355/  1.971893, val:  88.35%, val_best:  90.68%, tr:  95.68%, tr_best:  95.68%, epoch time: 773.41 seconds, 12.89 minutes\n",
      "total_backward_count 3120000 real_backward_count 321854  10.316%\n",
      "epoch-52  lr=['0.0039062'], tr/val_loss:  1.825627/  1.957349, val:  88.84%, val_best:  90.68%, tr:  95.83%, tr_best:  95.83%, epoch time: 776.44 seconds, 12.94 minutes\n",
      "total_backward_count 3180000 real_backward_count 324358  10.200%\n",
      "lif layer 1 self.abs_max_v: 8146.0\n",
      "epoch-53  lr=['0.0039062'], tr/val_loss:  1.810806/  1.959759, val:  87.73%, val_best:  90.68%, tr:  95.80%, tr_best:  95.83%, epoch time: 774.25 seconds, 12.90 minutes\n",
      "total_backward_count 3240000 real_backward_count 326881  10.089%\n",
      "epoch-54  lr=['0.0039062'], tr/val_loss:  1.802531/  1.962487, val:  87.72%, val_best:  90.68%, tr:  96.02%, tr_best:  96.02%, epoch time: 772.63 seconds, 12.88 minutes\n",
      "total_backward_count 3300000 real_backward_count 329268   9.978%\n",
      "fc layer 1 self.abs_max_out: 7056.0\n",
      "epoch-55  lr=['0.0039062'], tr/val_loss:  1.813798/  1.964283, val:  89.00%, val_best:  90.68%, tr:  96.12%, tr_best:  96.12%, epoch time: 779.02 seconds, 12.98 minutes\n",
      "total_backward_count 3360000 real_backward_count 331598   9.869%\n",
      "fc layer 1 self.abs_max_out: 7065.0\n",
      "epoch-56  lr=['0.0039062'], tr/val_loss:  1.813408/  1.969812, val:  89.60%, val_best:  90.68%, tr:  96.04%, tr_best:  96.12%, epoch time: 776.27 seconds, 12.94 minutes\n",
      "total_backward_count 3420000 real_backward_count 333974   9.765%\n",
      "fc layer 1 self.abs_max_out: 7073.0\n",
      "epoch-57  lr=['0.0039062'], tr/val_loss:  1.821175/  1.958047, val:  89.12%, val_best:  90.68%, tr:  95.94%, tr_best:  96.12%, epoch time: 771.30 seconds, 12.86 minutes\n",
      "total_backward_count 3480000 real_backward_count 336412   9.667%\n",
      "fc layer 3 self.abs_max_out: 348.0\n",
      "fc layer 1 self.abs_max_out: 7077.0\n",
      "epoch-58  lr=['0.0039062'], tr/val_loss:  1.795033/  1.952335, val:  87.57%, val_best:  90.68%, tr:  96.22%, tr_best:  96.22%, epoch time: 773.30 seconds, 12.89 minutes\n",
      "total_backward_count 3540000 real_backward_count 338679   9.567%\n",
      "epoch-59  lr=['0.0039062'], tr/val_loss:  1.794469/  1.952231, val:  90.62%, val_best:  90.68%, tr:  96.31%, tr_best:  96.31%, epoch time: 774.04 seconds, 12.90 minutes\n",
      "total_backward_count 3600000 real_backward_count 340893   9.469%\n",
      "fc layer 1 self.abs_max_out: 7083.0\n",
      "fc layer 1 self.abs_max_out: 7084.0\n",
      "epoch-60  lr=['0.0039062'], tr/val_loss:  1.791180/  1.947260, val:  88.44%, val_best:  90.68%, tr:  96.27%, tr_best:  96.31%, epoch time: 775.05 seconds, 12.92 minutes\n",
      "total_backward_count 3660000 real_backward_count 343134   9.375%\n",
      "fc layer 1 self.abs_max_out: 7090.0\n",
      "epoch-61  lr=['0.0039062'], tr/val_loss:  1.794032/  1.943857, val:  86.01%, val_best:  90.68%, tr:  96.44%, tr_best:  96.44%, epoch time: 777.49 seconds, 12.96 minutes\n",
      "total_backward_count 3720000 real_backward_count 345271   9.281%\n",
      "epoch-62  lr=['0.0039062'], tr/val_loss:  1.783211/  1.948508, val:  87.41%, val_best:  90.68%, tr:  96.28%, tr_best:  96.44%, epoch time: 775.28 seconds, 12.92 minutes\n",
      "total_backward_count 3780000 real_backward_count 347502   9.193%\n",
      "fc layer 1 self.abs_max_out: 7099.0\n",
      "fc layer 1 self.abs_max_out: 7101.0\n",
      "epoch-63  lr=['0.0039062'], tr/val_loss:  1.780764/  1.944251, val:  89.67%, val_best:  90.68%, tr:  96.44%, tr_best:  96.44%, epoch time: 775.09 seconds, 12.92 minutes\n",
      "total_backward_count 3840000 real_backward_count 349638   9.105%\n",
      "fc layer 1 self.abs_max_out: 7102.0\n",
      "epoch-64  lr=['0.0039062'], tr/val_loss:  1.776822/  1.939330, val:  87.27%, val_best:  90.68%, tr:  96.42%, tr_best:  96.44%, epoch time: 776.95 seconds, 12.95 minutes\n",
      "total_backward_count 3900000 real_backward_count 351784   9.020%\n",
      "fc layer 1 self.abs_max_out: 7105.0\n",
      "fc layer 1 self.abs_max_out: 7110.0\n",
      "epoch-65  lr=['0.0039062'], tr/val_loss:  1.763959/  1.938668, val:  88.79%, val_best:  90.68%, tr:  96.68%, tr_best:  96.68%, epoch time: 775.41 seconds, 12.92 minutes\n",
      "total_backward_count 3960000 real_backward_count 353778   8.934%\n",
      "fc layer 1 self.abs_max_out: 7114.0\n",
      "fc layer 1 self.abs_max_out: 7118.0\n",
      "epoch-66  lr=['0.0039062'], tr/val_loss:  1.766627/  1.913955, val:  89.79%, val_best:  90.68%, tr:  96.71%, tr_best:  96.71%, epoch time: 771.07 seconds, 12.85 minutes\n",
      "total_backward_count 4020000 real_backward_count 355750   8.850%\n",
      "epoch-67  lr=['0.0039062'], tr/val_loss:  1.753752/  1.927382, val:  89.34%, val_best:  90.68%, tr:  96.86%, tr_best:  96.86%, epoch time: 775.17 seconds, 12.92 minutes\n",
      "total_backward_count 4080000 real_backward_count 357636   8.766%\n",
      "fc layer 1 self.abs_max_out: 7131.0\n",
      "epoch-68  lr=['0.0039062'], tr/val_loss:  1.755868/  1.922007, val:  91.07%, val_best:  91.07%, tr:  96.98%, tr_best:  96.98%, epoch time: 775.89 seconds, 12.93 minutes\n",
      "total_backward_count 4140000 real_backward_count 359446   8.682%\n",
      "fc layer 1 self.abs_max_out: 7141.0\n",
      "epoch-69  lr=['0.0039062'], tr/val_loss:  1.765853/  1.933557, val:  84.43%, val_best:  91.07%, tr:  96.95%, tr_best:  96.98%, epoch time: 773.02 seconds, 12.88 minutes\n",
      "total_backward_count 4200000 real_backward_count 361276   8.602%\n",
      "fc layer 1 self.abs_max_out: 7146.0\n",
      "epoch-70  lr=['0.0039062'], tr/val_loss:  1.753902/  1.934646, val:  87.07%, val_best:  91.07%, tr:  97.01%, tr_best:  97.01%, epoch time: 768.85 seconds, 12.81 minutes\n",
      "total_backward_count 4260000 real_backward_count 363069   8.523%\n",
      "fc layer 1 self.abs_max_out: 7160.0\n",
      "epoch-71  lr=['0.0039062'], tr/val_loss:  1.743898/  1.913933, val:  89.93%, val_best:  91.07%, tr:  96.80%, tr_best:  97.01%, epoch time: 772.82 seconds, 12.88 minutes\n",
      "total_backward_count 4320000 real_backward_count 364987   8.449%\n",
      "fc layer 1 self.abs_max_out: 7166.0\n",
      "epoch-72  lr=['0.0039062'], tr/val_loss:  1.742623/  1.914818, val:  89.28%, val_best:  91.07%, tr:  96.96%, tr_best:  97.01%, epoch time: 774.21 seconds, 12.90 minutes\n",
      "total_backward_count 4380000 real_backward_count 366809   8.375%\n",
      "fc layer 1 self.abs_max_out: 7167.0\n",
      "epoch-73  lr=['0.0039062'], tr/val_loss:  1.743712/  1.911330, val:  90.68%, val_best:  91.07%, tr:  96.89%, tr_best:  97.01%, epoch time: 775.92 seconds, 12.93 minutes\n",
      "total_backward_count 4440000 real_backward_count 368677   8.304%\n",
      "fc layer 1 self.abs_max_out: 7176.0\n",
      "epoch-74  lr=['0.0039062'], tr/val_loss:  1.744779/  1.906224, val:  90.04%, val_best:  91.07%, tr:  96.95%, tr_best:  97.01%, epoch time: 775.71 seconds, 12.93 minutes\n",
      "total_backward_count 4500000 real_backward_count 370510   8.234%\n",
      "fc layer 1 self.abs_max_out: 7189.0\n",
      "epoch-75  lr=['0.0039062'], tr/val_loss:  1.738302/  1.907749, val:  89.03%, val_best:  91.07%, tr:  97.14%, tr_best:  97.14%, epoch time: 772.32 seconds, 12.87 minutes\n",
      "total_backward_count 4560000 real_backward_count 372226   8.163%\n",
      "fc layer 3 self.abs_max_out: 351.0\n",
      "epoch-76  lr=['0.0039062'], tr/val_loss:  1.735931/  1.913450, val:  90.50%, val_best:  91.07%, tr:  97.08%, tr_best:  97.14%, epoch time: 768.83 seconds, 12.81 minutes\n",
      "total_backward_count 4620000 real_backward_count 373976   8.095%\n",
      "epoch-77  lr=['0.0039062'], tr/val_loss:  1.745924/  1.918745, val:  89.73%, val_best:  91.07%, tr:  97.20%, tr_best:  97.20%, epoch time: 772.66 seconds, 12.88 minutes\n",
      "total_backward_count 4680000 real_backward_count 375653   8.027%\n",
      "epoch-78  lr=['0.0039062'], tr/val_loss:  1.738698/  1.904881, val:  89.19%, val_best:  91.07%, tr:  97.16%, tr_best:  97.20%, epoch time: 772.05 seconds, 12.87 minutes\n",
      "total_backward_count 4740000 real_backward_count 377357   7.961%\n",
      "epoch-79  lr=['0.0039062'], tr/val_loss:  1.729592/  1.904090, val:  88.74%, val_best:  91.07%, tr:  97.26%, tr_best:  97.26%, epoch time: 771.15 seconds, 12.85 minutes\n",
      "total_backward_count 4800000 real_backward_count 379001   7.896%\n",
      "epoch-80  lr=['0.0039062'], tr/val_loss:  1.722197/  1.898280, val:  89.80%, val_best:  91.07%, tr:  97.31%, tr_best:  97.31%, epoch time: 776.19 seconds, 12.94 minutes\n",
      "total_backward_count 4860000 real_backward_count 380616   7.832%\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "fc layer 3 self.abs_max_out: 356.0\n",
      "fc layer 1 self.abs_max_out: 7190.0\n",
      "epoch-81  lr=['0.0039062'], tr/val_loss:  1.713444/  1.899063, val:  88.96%, val_best:  91.07%, tr:  97.32%, tr_best:  97.32%, epoch time: 775.06 seconds, 12.92 minutes\n",
      "total_backward_count 4920000 real_backward_count 382223   7.769%\n",
      "fc layer 3 self.abs_max_out: 358.0\n",
      "fc layer 3 self.abs_max_out: 362.0\n",
      "fc layer 1 self.abs_max_out: 7199.0\n",
      "epoch-82  lr=['0.0039062'], tr/val_loss:  1.713157/  1.877633, val:  88.87%, val_best:  91.07%, tr:  97.44%, tr_best:  97.44%, epoch time: 770.84 seconds, 12.85 minutes\n",
      "total_backward_count 4980000 real_backward_count 383759   7.706%\n",
      "fc layer 1 self.abs_max_out: 7201.0\n",
      "fc layer 1 self.abs_max_out: 7206.0\n",
      "epoch-83  lr=['0.0039062'], tr/val_loss:  1.704748/  1.888089, val:  90.57%, val_best:  91.07%, tr:  97.41%, tr_best:  97.44%, epoch time: 776.28 seconds, 12.94 minutes\n",
      "total_backward_count 5040000 real_backward_count 385314   7.645%\n",
      "fc layer 1 self.abs_max_out: 7212.0\n",
      "epoch-84  lr=['0.0039062'], tr/val_loss:  1.702134/  1.875133, val:  88.67%, val_best:  91.07%, tr:  97.30%, tr_best:  97.44%, epoch time: 774.16 seconds, 12.90 minutes\n",
      "total_backward_count 5100000 real_backward_count 386931   7.587%\n",
      "fc layer 3 self.abs_max_out: 366.0\n",
      "fc layer 3 self.abs_max_out: 368.0\n",
      "fc layer 3 self.abs_max_out: 376.0\n",
      "lif layer 1 self.abs_max_v: 8150.0\n",
      "fc layer 1 self.abs_max_out: 7217.0\n",
      "epoch-85  lr=['0.0039062'], tr/val_loss:  1.701492/  1.885872, val:  89.39%, val_best:  91.07%, tr:  97.40%, tr_best:  97.44%, epoch time: 773.89 seconds, 12.90 minutes\n",
      "total_backward_count 5160000 real_backward_count 388491   7.529%\n",
      "fc layer 3 self.abs_max_out: 389.0\n",
      "fc layer 1 self.abs_max_out: 7222.0\n",
      "epoch-86  lr=['0.0039062'], tr/val_loss:  1.702485/  1.887984, val:  89.54%, val_best:  91.07%, tr:  97.42%, tr_best:  97.44%, epoch time: 773.39 seconds, 12.89 minutes\n",
      "total_backward_count 5220000 real_backward_count 390039   7.472%\n",
      "epoch-87  lr=['0.0039062'], tr/val_loss:  1.706475/  1.874313, val:  91.10%, val_best:  91.10%, tr:  97.57%, tr_best:  97.57%, epoch time: 775.69 seconds, 12.93 minutes\n",
      "total_backward_count 5280000 real_backward_count 391495   7.415%\n",
      "fc layer 3 self.abs_max_out: 395.0\n",
      "fc layer 2 self.abs_max_out: 4277.0\n",
      "fc layer 1 self.abs_max_out: 7227.0\n",
      "epoch-88  lr=['0.0039062'], tr/val_loss:  1.688879/  1.873893, val:  89.72%, val_best:  91.10%, tr:  97.59%, tr_best:  97.59%, epoch time: 773.62 seconds, 12.89 minutes\n",
      "total_backward_count 5340000 real_backward_count 392943   7.358%\n",
      "epoch-89  lr=['0.0039062'], tr/val_loss:  1.681634/  1.866449, val:  90.72%, val_best:  91.10%, tr:  97.64%, tr_best:  97.64%, epoch time: 776.90 seconds, 12.95 minutes\n",
      "total_backward_count 5400000 real_backward_count 394362   7.303%\n",
      "fc layer 3 self.abs_max_out: 397.0\n",
      "epoch-90  lr=['0.0039062'], tr/val_loss:  1.673523/  1.859297, val:  88.92%, val_best:  91.10%, tr:  97.62%, tr_best:  97.64%, epoch time: 774.46 seconds, 12.91 minutes\n",
      "total_backward_count 5460000 real_backward_count 395790   7.249%\n",
      "fc layer 3 self.abs_max_out: 400.0\n",
      "fc layer 1 self.abs_max_out: 7229.0\n",
      "epoch-91  lr=['0.0039062'], tr/val_loss:  1.687623/  1.881156, val:  89.91%, val_best:  91.10%, tr:  97.48%, tr_best:  97.64%, epoch time: 774.31 seconds, 12.91 minutes\n",
      "total_backward_count 5520000 real_backward_count 397302   7.197%\n",
      "fc layer 3 self.abs_max_out: 403.0\n",
      "fc layer 1 self.abs_max_out: 7230.0\n",
      "fc layer 1 self.abs_max_out: 7247.0\n",
      "epoch-92  lr=['0.0039062'], tr/val_loss:  1.680098/  1.880785, val:  90.04%, val_best:  91.10%, tr:  97.68%, tr_best:  97.68%, epoch time: 772.68 seconds, 12.88 minutes\n",
      "total_backward_count 5580000 real_backward_count 398694   7.145%\n",
      "lif layer 1 self.abs_max_v: 8160.0\n",
      "fc layer 1 self.abs_max_out: 7249.0\n",
      "epoch-93  lr=['0.0039062'], tr/val_loss:  1.671899/  1.876345, val:  88.87%, val_best:  91.10%, tr:  97.65%, tr_best:  97.68%, epoch time: 773.86 seconds, 12.90 minutes\n",
      "total_backward_count 5640000 real_backward_count 400106   7.094%\n",
      "fc layer 1 self.abs_max_out: 7253.0\n",
      "epoch-94  lr=['0.0039062'], tr/val_loss:  1.668797/  1.861048, val:  91.01%, val_best:  91.10%, tr:  97.64%, tr_best:  97.68%, epoch time: 781.04 seconds, 13.02 minutes\n",
      "total_backward_count 5700000 real_backward_count 401522   7.044%\n",
      "fc layer 3 self.abs_max_out: 407.0\n",
      "fc layer 3 self.abs_max_out: 412.0\n",
      "fc layer 1 self.abs_max_out: 7254.0\n",
      "epoch-95  lr=['0.0039062'], tr/val_loss:  1.680694/  1.898924, val:  89.25%, val_best:  91.10%, tr:  97.69%, tr_best:  97.69%, epoch time: 770.11 seconds, 12.84 minutes\n",
      "total_backward_count 5760000 real_backward_count 402905   6.995%\n",
      "fc layer 1 self.abs_max_out: 7262.0\n",
      "epoch-96  lr=['0.0039062'], tr/val_loss:  1.690430/  1.869304, val:  88.36%, val_best:  91.10%, tr:  97.67%, tr_best:  97.69%, epoch time: 766.19 seconds, 12.77 minutes\n",
      "total_backward_count 5820000 real_backward_count 404305   6.947%\n",
      "fc layer 1 self.abs_max_out: 7271.0\n",
      "epoch-97  lr=['0.0039062'], tr/val_loss:  1.671348/  1.859779, val:  90.12%, val_best:  91.10%, tr:  97.54%, tr_best:  97.69%, epoch time: 765.76 seconds, 12.76 minutes\n",
      "total_backward_count 5880000 real_backward_count 405782   6.901%\n",
      "fc layer 1 self.abs_max_out: 7278.0\n",
      "epoch-98  lr=['0.0039062'], tr/val_loss:  1.681678/  1.871812, val:  90.09%, val_best:  91.10%, tr:  97.69%, tr_best:  97.69%, epoch time: 768.77 seconds, 12.81 minutes\n",
      "total_backward_count 5940000 real_backward_count 407168   6.855%\n",
      "epoch-99  lr=['0.0039062'], tr/val_loss:  1.682915/  1.861016, val:  88.50%, val_best:  91.10%, tr:  97.71%, tr_best:  97.71%, epoch time: 769.83 seconds, 12.83 minutes\n",
      "total_backward_count 6000000 real_backward_count 408543   6.809%\n",
      "fc layer 3 self.abs_max_out: 416.0\n",
      "fc layer 1 self.abs_max_out: 7279.0\n",
      "epoch-100 lr=['0.0039062'], tr/val_loss:  1.668672/  1.860245, val:  89.96%, val_best:  91.10%, tr:  97.86%, tr_best:  97.86%, epoch time: 765.39 seconds, 12.76 minutes\n",
      "total_backward_count 6060000 real_backward_count 409825   6.763%\n",
      "epoch-101 lr=['0.0039062'], tr/val_loss:  1.664460/  1.841844, val:  90.80%, val_best:  91.10%, tr:  97.81%, tr_best:  97.86%, epoch time: 769.74 seconds, 12.83 minutes\n",
      "total_backward_count 6120000 real_backward_count 411138   6.718%\n",
      "fc layer 1 self.abs_max_out: 7285.0\n",
      "epoch-102 lr=['0.0039062'], tr/val_loss:  1.661623/  1.878315, val:  88.92%, val_best:  91.10%, tr:  97.93%, tr_best:  97.93%, epoch time: 769.78 seconds, 12.83 minutes\n",
      "total_backward_count 6180000 real_backward_count 412382   6.673%\n",
      "fc layer 1 self.abs_max_out: 7290.0\n",
      "epoch-103 lr=['0.0039062'], tr/val_loss:  1.661438/  1.849074, val:  90.92%, val_best:  91.10%, tr:  97.90%, tr_best:  97.93%, epoch time: 774.31 seconds, 12.91 minutes\n",
      "total_backward_count 6240000 real_backward_count 413644   6.629%\n",
      "epoch-104 lr=['0.0039062'], tr/val_loss:  1.663107/  1.850743, val:  90.12%, val_best:  91.10%, tr:  97.92%, tr_best:  97.93%, epoch time: 773.64 seconds, 12.89 minutes\n",
      "total_backward_count 6300000 real_backward_count 414889   6.586%\n",
      "fc layer 1 self.abs_max_out: 7293.0\n",
      "epoch-105 lr=['0.0039062'], tr/val_loss:  1.656359/  1.856751, val:  90.38%, val_best:  91.10%, tr:  98.07%, tr_best:  98.07%, epoch time: 767.88 seconds, 12.80 minutes\n",
      "total_backward_count 6360000 real_backward_count 416046   6.542%\n",
      "epoch-106 lr=['0.0039062'], tr/val_loss:  1.652783/  1.851066, val:  89.64%, val_best:  91.10%, tr:  97.88%, tr_best:  98.07%, epoch time: 766.87 seconds, 12.78 minutes\n",
      "total_backward_count 6420000 real_backward_count 417320   6.500%\n",
      "epoch-107 lr=['0.0039062'], tr/val_loss:  1.654781/  1.852044, val:  91.11%, val_best:  91.11%, tr:  97.97%, tr_best:  98.07%, epoch time: 768.39 seconds, 12.81 minutes\n",
      "total_backward_count 6480000 real_backward_count 418539   6.459%\n",
      "fc layer 1 self.abs_max_out: 7295.0\n",
      "epoch-108 lr=['0.0039062'], tr/val_loss:  1.644535/  1.864548, val:  90.30%, val_best:  91.11%, tr:  97.98%, tr_best:  98.07%, epoch time: 771.96 seconds, 12.87 minutes\n",
      "total_backward_count 6540000 real_backward_count 419750   6.418%\n",
      "fc layer 1 self.abs_max_out: 7296.0\n",
      "epoch-109 lr=['0.0039062'], tr/val_loss:  1.652406/  1.852301, val:  89.47%, val_best:  91.11%, tr:  98.06%, tr_best:  98.07%, epoch time: 767.26 seconds, 12.79 minutes\n",
      "total_backward_count 6600000 real_backward_count 420916   6.378%\n",
      "epoch-110 lr=['0.0039062'], tr/val_loss:  1.633879/  1.834125, val:  89.79%, val_best:  91.11%, tr:  98.00%, tr_best:  98.07%, epoch time: 766.08 seconds, 12.77 minutes\n",
      "total_backward_count 6660000 real_backward_count 422117   6.338%\n",
      "fc layer 1 self.abs_max_out: 7304.0\n",
      "epoch-111 lr=['0.0039062'], tr/val_loss:  1.641186/  1.865277, val:  88.92%, val_best:  91.11%, tr:  97.95%, tr_best:  98.07%, epoch time: 766.65 seconds, 12.78 minutes\n",
      "total_backward_count 6720000 real_backward_count 423350   6.300%\n",
      "fc layer 2 self.abs_max_out: 4304.0\n",
      "fc layer 1 self.abs_max_out: 7309.0\n",
      "epoch-112 lr=['0.0039062'], tr/val_loss:  1.654677/  1.841973, val:  90.65%, val_best:  91.11%, tr:  98.01%, tr_best:  98.07%, epoch time: 764.12 seconds, 12.74 minutes\n",
      "total_backward_count 6780000 real_backward_count 424543   6.262%\n",
      "fc layer 1 self.abs_max_out: 7313.0\n",
      "epoch-113 lr=['0.0039062'], tr/val_loss:  1.658247/  1.853159, val:  90.16%, val_best:  91.11%, tr:  98.05%, tr_best:  98.07%, epoch time: 760.21 seconds, 12.67 minutes\n",
      "total_backward_count 6840000 real_backward_count 425716   6.224%\n",
      "lif layer 2 self.abs_max_v: 7238.5\n",
      "epoch-114 lr=['0.0039062'], tr/val_loss:  1.641921/  1.840768, val:  90.85%, val_best:  91.11%, tr:  98.02%, tr_best:  98.07%, epoch time: 768.26 seconds, 12.80 minutes\n",
      "total_backward_count 6900000 real_backward_count 426903   6.187%\n",
      "fc layer 1 self.abs_max_out: 7315.0\n",
      "epoch-115 lr=['0.0039062'], tr/val_loss:  1.649354/  1.859830, val:  91.60%, val_best:  91.60%, tr:  97.98%, tr_best:  98.07%, epoch time: 764.23 seconds, 12.74 minutes\n",
      "total_backward_count 6960000 real_backward_count 428116   6.151%\n",
      "fc layer 1 self.abs_max_out: 7321.0\n",
      "epoch-116 lr=['0.0039062'], tr/val_loss:  1.664586/  1.871466, val:  87.52%, val_best:  91.60%, tr:  98.08%, tr_best:  98.08%, epoch time: 767.87 seconds, 12.80 minutes\n",
      "total_backward_count 7020000 real_backward_count 429271   6.115%\n",
      "lif layer 2 self.abs_max_v: 7269.0\n",
      "epoch-117 lr=['0.0039062'], tr/val_loss:  1.660764/  1.865397, val:  90.15%, val_best:  91.60%, tr:  98.06%, tr_best:  98.08%, epoch time: 765.12 seconds, 12.75 minutes\n",
      "total_backward_count 7080000 real_backward_count 430434   6.080%\n",
      "fc layer 1 self.abs_max_out: 7325.0\n",
      "epoch-118 lr=['0.0039062'], tr/val_loss:  1.657774/  1.844354, val:  90.13%, val_best:  91.60%, tr:  98.15%, tr_best:  98.15%, epoch time: 763.78 seconds, 12.73 minutes\n",
      "total_backward_count 7140000 real_backward_count 431542   6.044%\n",
      "lif layer 1 self.abs_max_v: 8175.0\n",
      "fc layer 1 self.abs_max_out: 7328.0\n",
      "epoch-119 lr=['0.0039062'], tr/val_loss:  1.651140/  1.851189, val:  88.41%, val_best:  91.60%, tr:  98.06%, tr_best:  98.15%, epoch time: 767.33 seconds, 12.79 minutes\n",
      "total_backward_count 7200000 real_backward_count 432708   6.010%\n",
      "epoch-120 lr=['0.0039062'], tr/val_loss:  1.641856/  1.831296, val:  89.64%, val_best:  91.60%, tr:  98.13%, tr_best:  98.15%, epoch time: 768.24 seconds, 12.80 minutes\n",
      "total_backward_count 7260000 real_backward_count 433831   5.976%\n",
      "fc layer 2 self.abs_max_out: 4344.0\n",
      "fc layer 1 self.abs_max_out: 7332.0\n",
      "epoch-121 lr=['0.0039062'], tr/val_loss:  1.643798/  1.843369, val:  90.56%, val_best:  91.60%, tr:  98.09%, tr_best:  98.15%, epoch time: 765.64 seconds, 12.76 minutes\n",
      "total_backward_count 7320000 real_backward_count 434979   5.942%\n",
      "fc layer 1 self.abs_max_out: 7334.0\n",
      "epoch-122 lr=['0.0039062'], tr/val_loss:  1.649558/  1.847532, val:  90.51%, val_best:  91.60%, tr:  98.06%, tr_best:  98.15%, epoch time: 765.10 seconds, 12.75 minutes\n",
      "total_backward_count 7380000 real_backward_count 436141   5.910%\n",
      "fc layer 1 self.abs_max_out: 7339.0\n",
      "epoch-123 lr=['0.0039062'], tr/val_loss:  1.650128/  1.840755, val:  90.82%, val_best:  91.60%, tr:  98.20%, tr_best:  98.20%, epoch time: 768.07 seconds, 12.80 minutes\n",
      "total_backward_count 7440000 real_backward_count 437221   5.877%\n",
      "epoch-124 lr=['0.0039062'], tr/val_loss:  1.636949/  1.840703, val:  90.82%, val_best:  91.60%, tr:  98.16%, tr_best:  98.20%, epoch time: 759.47 seconds, 12.66 minutes\n",
      "total_backward_count 7500000 real_backward_count 438326   5.844%\n",
      "fc layer 1 self.abs_max_out: 7344.0\n",
      "epoch-125 lr=['0.0039062'], tr/val_loss:  1.639597/  1.853201, val:  89.69%, val_best:  91.60%, tr:  98.19%, tr_best:  98.20%, epoch time: 760.52 seconds, 12.68 minutes\n",
      "total_backward_count 7560000 real_backward_count 439411   5.812%\n",
      "fc layer 1 self.abs_max_out: 7346.0\n",
      "epoch-126 lr=['0.0039062'], tr/val_loss:  1.649794/  1.849470, val:  89.23%, val_best:  91.60%, tr:  98.30%, tr_best:  98.30%, epoch time: 764.03 seconds, 12.73 minutes\n",
      "total_backward_count 7620000 real_backward_count 440429   5.780%\n",
      "fc layer 1 self.abs_max_out: 7347.0\n",
      "epoch-127 lr=['0.0039062'], tr/val_loss:  1.650159/  1.847984, val:  90.82%, val_best:  91.60%, tr:  98.16%, tr_best:  98.30%, epoch time: 759.18 seconds, 12.65 minutes\n",
      "total_backward_count 7680000 real_backward_count 441534   5.749%\n",
      "epoch-128 lr=['0.0039062'], tr/val_loss:  1.640604/  1.843193, val:  90.56%, val_best:  91.60%, tr:  98.13%, tr_best:  98.30%, epoch time: 763.20 seconds, 12.72 minutes\n",
      "total_backward_count 7740000 real_backward_count 442654   5.719%\n",
      "epoch-129 lr=['0.0039062'], tr/val_loss:  1.641335/  1.846918, val:  90.93%, val_best:  91.60%, tr:  98.27%, tr_best:  98.30%, epoch time: 762.58 seconds, 12.71 minutes\n",
      "total_backward_count 7800000 real_backward_count 443691   5.688%\n",
      "epoch-130 lr=['0.0039062'], tr/val_loss:  1.636682/  1.827744, val:  90.37%, val_best:  91.60%, tr:  98.18%, tr_best:  98.30%, epoch time: 761.18 seconds, 12.69 minutes\n",
      "total_backward_count 7860000 real_backward_count 444784   5.659%\n",
      "fc layer 2 self.abs_max_out: 4378.0\n",
      "epoch-131 lr=['0.0039062'], tr/val_loss:  1.629665/  1.834407, val:  90.30%, val_best:  91.60%, tr:  98.33%, tr_best:  98.33%, epoch time: 758.78 seconds, 12.65 minutes\n",
      "total_backward_count 7920000 real_backward_count 445787   5.629%\n",
      "epoch-132 lr=['0.0039062'], tr/val_loss:  1.632216/  1.831767, val:  90.74%, val_best:  91.60%, tr:  98.37%, tr_best:  98.37%, epoch time: 760.68 seconds, 12.68 minutes\n",
      "total_backward_count 7980000 real_backward_count 446766   5.599%\n",
      "epoch-133 lr=['0.0039062'], tr/val_loss:  1.631485/  1.848037, val:  90.38%, val_best:  91.60%, tr:  98.31%, tr_best:  98.37%, epoch time: 759.59 seconds, 12.66 minutes\n",
      "total_backward_count 8040000 real_backward_count 447779   5.569%\n",
      "epoch-134 lr=['0.0039062'], tr/val_loss:  1.641944/  1.851099, val:  90.12%, val_best:  91.60%, tr:  98.29%, tr_best:  98.37%, epoch time: 759.39 seconds, 12.66 minutes\n",
      "total_backward_count 8100000 real_backward_count 448805   5.541%\n",
      "epoch-135 lr=['0.0039062'], tr/val_loss:  1.646067/  1.841146, val:  91.08%, val_best:  91.60%, tr:  98.30%, tr_best:  98.37%, epoch time: 763.70 seconds, 12.73 minutes\n",
      "total_backward_count 8160000 real_backward_count 449828   5.513%\n",
      "lif layer 1 self.abs_max_v: 8194.0\n",
      "epoch-136 lr=['0.0039062'], tr/val_loss:  1.620724/  1.813574, val:  89.87%, val_best:  91.60%, tr:  98.39%, tr_best:  98.39%, epoch time: 761.03 seconds, 12.68 minutes\n",
      "total_backward_count 8220000 real_backward_count 450797   5.484%\n",
      "epoch-137 lr=['0.0039062'], tr/val_loss:  1.614536/  1.822705, val:  90.68%, val_best:  91.60%, tr:  98.38%, tr_best:  98.39%, epoch time: 762.87 seconds, 12.71 minutes\n",
      "total_backward_count 8280000 real_backward_count 451772   5.456%\n",
      "epoch-138 lr=['0.0039062'], tr/val_loss:  1.622766/  1.821527, val:  90.31%, val_best:  91.60%, tr:  98.38%, tr_best:  98.39%, epoch time: 763.80 seconds, 12.73 minutes\n",
      "total_backward_count 8340000 real_backward_count 452746   5.429%\n",
      "fc layer 1 self.abs_max_out: 7348.0\n",
      "epoch-139 lr=['0.0039062'], tr/val_loss:  1.614500/  1.828458, val:  89.88%, val_best:  91.60%, tr:  98.28%, tr_best:  98.39%, epoch time: 760.17 seconds, 12.67 minutes\n",
      "total_backward_count 8400000 real_backward_count 453781   5.402%\n",
      "fc layer 1 self.abs_max_out: 7350.0\n",
      "epoch-140 lr=['0.0039062'], tr/val_loss:  1.624880/  1.834768, val:  88.51%, val_best:  91.60%, tr:  98.41%, tr_best:  98.41%, epoch time: 762.06 seconds, 12.70 minutes\n",
      "total_backward_count 8460000 real_backward_count 454737   5.375%\n",
      "epoch-141 lr=['0.0039062'], tr/val_loss:  1.611401/  1.816576, val:  90.25%, val_best:  91.60%, tr:  98.36%, tr_best:  98.41%, epoch time: 765.53 seconds, 12.76 minutes\n",
      "total_backward_count 8520000 real_backward_count 455721   5.349%\n",
      "epoch-142 lr=['0.0039062'], tr/val_loss:  1.602361/  1.815509, val:  89.81%, val_best:  91.60%, tr:  98.50%, tr_best:  98.50%, epoch time: 760.96 seconds, 12.68 minutes\n",
      "total_backward_count 8580000 real_backward_count 456620   5.322%\n",
      "fc layer 1 self.abs_max_out: 7352.0\n",
      "epoch-143 lr=['0.0039062'], tr/val_loss:  1.599307/  1.817914, val:  90.55%, val_best:  91.60%, tr:  98.39%, tr_best:  98.50%, epoch time: 760.58 seconds, 12.68 minutes\n",
      "total_backward_count 8640000 real_backward_count 457588   5.296%\n",
      "epoch-144 lr=['0.0039062'], tr/val_loss:  1.609487/  1.822951, val:  90.28%, val_best:  91.60%, tr:  98.43%, tr_best:  98.50%, epoch time: 767.03 seconds, 12.78 minutes\n",
      "total_backward_count 8700000 real_backward_count 458527   5.270%\n",
      "fc layer 3 self.abs_max_out: 423.0\n",
      "fc layer 1 self.abs_max_out: 7355.0\n",
      "epoch-145 lr=['0.0039062'], tr/val_loss:  1.593349/  1.803201, val:  90.78%, val_best:  91.60%, tr:  98.53%, tr_best:  98.53%, epoch time: 763.65 seconds, 12.73 minutes\n",
      "total_backward_count 8760000 real_backward_count 459409   5.244%\n",
      "fc layer 1 self.abs_max_out: 7357.0\n",
      "epoch-146 lr=['0.0039062'], tr/val_loss:  1.589340/  1.805744, val:  90.55%, val_best:  91.60%, tr:  98.54%, tr_best:  98.54%, epoch time: 759.60 seconds, 12.66 minutes\n",
      "total_backward_count 8820000 real_backward_count 460286   5.219%\n",
      "fc layer 1 self.abs_max_out: 7359.0\n",
      "epoch-147 lr=['0.0039062'], tr/val_loss:  1.608567/  1.819493, val:  89.76%, val_best:  91.60%, tr:  98.37%, tr_best:  98.54%, epoch time: 764.18 seconds, 12.74 minutes\n",
      "total_backward_count 8880000 real_backward_count 461264   5.194%\n",
      "epoch-148 lr=['0.0039062'], tr/val_loss:  1.604831/  1.822634, val:  90.27%, val_best:  91.60%, tr:  98.42%, tr_best:  98.54%, epoch time: 762.73 seconds, 12.71 minutes\n",
      "total_backward_count 8940000 real_backward_count 462213   5.170%\n",
      "epoch-149 lr=['0.0039062'], tr/val_loss:  1.607925/  1.814810, val:  88.53%, val_best:  91.60%, tr:  98.52%, tr_best:  98.54%, epoch time: 758.32 seconds, 12.64 minutes\n",
      "total_backward_count 9000000 real_backward_count 463098   5.146%\n",
      "epoch-150 lr=['0.0039062'], tr/val_loss:  1.597337/  1.808244, val:  90.11%, val_best:  91.60%, tr:  98.56%, tr_best:  98.56%, epoch time: 760.20 seconds, 12.67 minutes\n",
      "total_backward_count 9060000 real_backward_count 463959   5.121%\n",
      "fc layer 3 self.abs_max_out: 430.0\n",
      "epoch-151 lr=['0.0039062'], tr/val_loss:  1.588264/  1.815196, val:  90.42%, val_best:  91.60%, tr:  98.55%, tr_best:  98.56%, epoch time: 769.25 seconds, 12.82 minutes\n",
      "total_backward_count 9120000 real_backward_count 464830   5.097%\n",
      "epoch-152 lr=['0.0039062'], tr/val_loss:  1.594501/  1.813108, val:  90.43%, val_best:  91.60%, tr:  98.62%, tr_best:  98.62%, epoch time: 767.41 seconds, 12.79 minutes\n",
      "total_backward_count 9180000 real_backward_count 465659   5.073%\n",
      "epoch-153 lr=['0.0039062'], tr/val_loss:  1.594000/  1.810413, val:  90.08%, val_best:  91.60%, tr:  98.58%, tr_best:  98.62%, epoch time: 771.12 seconds, 12.85 minutes\n",
      "total_backward_count 9240000 real_backward_count 466510   5.049%\n",
      "fc layer 3 self.abs_max_out: 436.0\n",
      "fc layer 1 self.abs_max_out: 7361.0\n",
      "epoch-154 lr=['0.0039062'], tr/val_loss:  1.598506/  1.812804, val:  90.06%, val_best:  91.60%, tr:  98.47%, tr_best:  98.62%, epoch time: 764.38 seconds, 12.74 minutes\n",
      "total_backward_count 9300000 real_backward_count 467431   5.026%\n",
      "fc layer 1 self.abs_max_out: 7362.0\n",
      "epoch-155 lr=['0.0039062'], tr/val_loss:  1.599088/  1.822531, val:  90.47%, val_best:  91.60%, tr:  98.46%, tr_best:  98.62%, epoch time: 766.86 seconds, 12.78 minutes\n",
      "total_backward_count 9360000 real_backward_count 468353   5.004%\n",
      "fc layer 1 self.abs_max_out: 7363.0\n",
      "epoch-156 lr=['0.0039062'], tr/val_loss:  1.611355/  1.821320, val:  90.38%, val_best:  91.60%, tr:  98.49%, tr_best:  98.62%, epoch time: 769.40 seconds, 12.82 minutes\n",
      "total_backward_count 9420000 real_backward_count 469259   4.982%\n",
      "fc layer 1 self.abs_max_out: 7366.0\n",
      "epoch-157 lr=['0.0039062'], tr/val_loss:  1.619173/  1.831602, val:  88.59%, val_best:  91.60%, tr:  98.36%, tr_best:  98.62%, epoch time: 768.23 seconds, 12.80 minutes\n",
      "total_backward_count 9480000 real_backward_count 470244   4.960%\n",
      "epoch-158 lr=['0.0039062'], tr/val_loss:  1.623235/  1.820893, val:  90.91%, val_best:  91.60%, tr:  98.49%, tr_best:  98.62%, epoch time: 766.51 seconds, 12.78 minutes\n",
      "total_backward_count 9540000 real_backward_count 471148   4.939%\n",
      "epoch-159 lr=['0.0039062'], tr/val_loss:  1.612156/  1.827998, val:  90.15%, val_best:  91.60%, tr:  98.50%, tr_best:  98.62%, epoch time: 770.98 seconds, 12.85 minutes\n",
      "total_backward_count 9600000 real_backward_count 472046   4.917%\n",
      "fc layer 1 self.abs_max_out: 7369.0\n",
      "epoch-160 lr=['0.0039062'], tr/val_loss:  1.601496/  1.809281, val:  89.52%, val_best:  91.60%, tr:  98.53%, tr_best:  98.62%, epoch time: 770.04 seconds, 12.83 minutes\n",
      "total_backward_count 9660000 real_backward_count 472928   4.896%\n",
      "epoch-161 lr=['0.0039062'], tr/val_loss:  1.603868/  1.806642, val:  91.06%, val_best:  91.60%, tr:  98.52%, tr_best:  98.62%, epoch time: 766.53 seconds, 12.78 minutes\n",
      "total_backward_count 9720000 real_backward_count 473813   4.875%\n",
      "epoch-162 lr=['0.0039062'], tr/val_loss:  1.598200/  1.806146, val:  89.99%, val_best:  91.60%, tr:  98.54%, tr_best:  98.62%, epoch time: 771.79 seconds, 12.86 minutes\n",
      "total_backward_count 9780000 real_backward_count 474690   4.854%\n",
      "epoch-163 lr=['0.0039062'], tr/val_loss:  1.593076/  1.812013, val:  89.24%, val_best:  91.60%, tr:  98.55%, tr_best:  98.62%, epoch time: 770.03 seconds, 12.83 minutes\n",
      "total_backward_count 9840000 real_backward_count 475558   4.833%\n",
      "epoch-164 lr=['0.0039062'], tr/val_loss:  1.600426/  1.811157, val:  89.53%, val_best:  91.60%, tr:  98.52%, tr_best:  98.62%, epoch time: 770.52 seconds, 12.84 minutes\n",
      "total_backward_count 9900000 real_backward_count 476449   4.813%\n",
      "epoch-165 lr=['0.0039062'], tr/val_loss:  1.595110/  1.801474, val:  90.63%, val_best:  91.60%, tr:  98.55%, tr_best:  98.62%, epoch time: 769.52 seconds, 12.83 minutes\n",
      "total_backward_count 9960000 real_backward_count 477319   4.792%\n",
      "lif layer 1 self.abs_max_v: 8213.5\n",
      "epoch-166 lr=['0.0039062'], tr/val_loss:  1.587132/  1.804265, val:  90.49%, val_best:  91.60%, tr:  98.62%, tr_best:  98.62%, epoch time: 768.13 seconds, 12.80 minutes\n",
      "total_backward_count 10020000 real_backward_count 478146   4.772%\n",
      "lif layer 1 self.abs_max_v: 8239.5\n",
      "epoch-167 lr=['0.0039062'], tr/val_loss:  1.585712/  1.799419, val:  89.04%, val_best:  91.60%, tr:  98.61%, tr_best:  98.62%, epoch time: 766.35 seconds, 12.77 minutes\n",
      "total_backward_count 10080000 real_backward_count 478978   4.752%\n",
      "lif layer 1 self.abs_max_v: 8240.5\n",
      "epoch-168 lr=['0.0039062'], tr/val_loss:  1.577076/  1.793301, val:  91.04%, val_best:  91.60%, tr:  98.58%, tr_best:  98.62%, epoch time: 768.96 seconds, 12.82 minutes\n",
      "total_backward_count 10140000 real_backward_count 479832   4.732%\n",
      "epoch-169 lr=['0.0039062'], tr/val_loss:  1.580841/  1.806147, val:  89.00%, val_best:  91.60%, tr:  98.54%, tr_best:  98.62%, epoch time: 771.75 seconds, 12.86 minutes\n",
      "total_backward_count 10200000 real_backward_count 480706   4.713%\n",
      "epoch-170 lr=['0.0039062'], tr/val_loss:  1.580111/  1.798935, val:  90.45%, val_best:  91.60%, tr:  98.67%, tr_best:  98.67%, epoch time: 764.36 seconds, 12.74 minutes\n",
      "total_backward_count 10260000 real_backward_count 481506   4.693%\n",
      "epoch-171 lr=['0.0039062'], tr/val_loss:  1.580433/  1.802156, val:  89.49%, val_best:  91.60%, tr:  98.57%, tr_best:  98.67%, epoch time: 762.56 seconds, 12.71 minutes\n",
      "total_backward_count 10320000 real_backward_count 482362   4.674%\n",
      "epoch-172 lr=['0.0039062'], tr/val_loss:  1.577211/  1.798501, val:  90.27%, val_best:  91.60%, tr:  98.59%, tr_best:  98.67%, epoch time: 760.74 seconds, 12.68 minutes\n",
      "total_backward_count 10380000 real_backward_count 483208   4.655%\n",
      "fc layer 3 self.abs_max_out: 437.0\n",
      "epoch-173 lr=['0.0039062'], tr/val_loss:  1.572712/  1.796997, val:  89.95%, val_best:  91.60%, tr:  98.66%, tr_best:  98.67%, epoch time: 758.27 seconds, 12.64 minutes\n",
      "total_backward_count 10440000 real_backward_count 484011   4.636%\n",
      "lif layer 1 self.abs_max_v: 8264.5\n",
      "epoch-174 lr=['0.0039062'], tr/val_loss:  1.581849/  1.797341, val:  90.65%, val_best:  91.60%, tr:  98.60%, tr_best:  98.67%, epoch time: 761.94 seconds, 12.70 minutes\n",
      "total_backward_count 10500000 real_backward_count 484853   4.618%\n",
      "epoch-175 lr=['0.0039062'], tr/val_loss:  1.577951/  1.787745, val:  91.54%, val_best:  91.60%, tr:  98.55%, tr_best:  98.67%, epoch time: 763.03 seconds, 12.72 minutes\n",
      "total_backward_count 10560000 real_backward_count 485723   4.600%\n",
      "epoch-176 lr=['0.0039062'], tr/val_loss:  1.574974/  1.800471, val:  91.35%, val_best:  91.60%, tr:  98.59%, tr_best:  98.67%, epoch time: 760.17 seconds, 12.67 minutes\n",
      "total_backward_count 10620000 real_backward_count 486569   4.582%\n",
      "epoch-177 lr=['0.0039062'], tr/val_loss:  1.582144/  1.799317, val:  89.16%, val_best:  91.60%, tr:  98.61%, tr_best:  98.67%, epoch time: 762.39 seconds, 12.71 minutes\n",
      "total_backward_count 10680000 real_backward_count 487400   4.564%\n",
      "epoch-178 lr=['0.0039062'], tr/val_loss:  1.565411/  1.787697, val:  90.00%, val_best:  91.60%, tr:  98.70%, tr_best:  98.70%, epoch time: 765.92 seconds, 12.77 minutes\n",
      "total_backward_count 10740000 real_backward_count 488182   4.545%\n",
      "epoch-179 lr=['0.0039062'], tr/val_loss:  1.560680/  1.786356, val:  90.60%, val_best:  91.60%, tr:  98.63%, tr_best:  98.70%, epoch time: 760.99 seconds, 12.68 minutes\n",
      "total_backward_count 10800000 real_backward_count 489006   4.528%\n",
      "epoch-180 lr=['0.0039062'], tr/val_loss:  1.569519/  1.790998, val:  89.44%, val_best:  91.60%, tr:  98.69%, tr_best:  98.70%, epoch time: 763.10 seconds, 12.72 minutes\n",
      "total_backward_count 10860000 real_backward_count 489789   4.510%\n",
      "fc layer 3 self.abs_max_out: 443.0\n",
      "epoch-181 lr=['0.0039062'], tr/val_loss:  1.570265/  1.796129, val:  89.45%, val_best:  91.60%, tr:  98.72%, tr_best:  98.72%, epoch time: 763.50 seconds, 12.72 minutes\n",
      "total_backward_count 10920000 real_backward_count 490560   4.492%\n",
      "epoch-182 lr=['0.0039062'], tr/val_loss:  1.561402/  1.788189, val:  90.00%, val_best:  91.60%, tr:  98.74%, tr_best:  98.74%, epoch time: 761.78 seconds, 12.70 minutes\n",
      "total_backward_count 10980000 real_backward_count 491315   4.475%\n",
      "epoch-183 lr=['0.0039062'], tr/val_loss:  1.565843/  1.792194, val:  89.90%, val_best:  91.60%, tr:  98.65%, tr_best:  98.74%, epoch time: 762.17 seconds, 12.70 minutes\n",
      "total_backward_count 11040000 real_backward_count 492124   4.458%\n",
      "epoch-184 lr=['0.0039062'], tr/val_loss:  1.575607/  1.791767, val:  89.22%, val_best:  91.60%, tr:  98.61%, tr_best:  98.74%, epoch time: 765.93 seconds, 12.77 minutes\n",
      "total_backward_count 11100000 real_backward_count 492960   4.441%\n",
      "epoch-185 lr=['0.0039062'], tr/val_loss:  1.571370/  1.789229, val:  90.71%, val_best:  91.60%, tr:  98.59%, tr_best:  98.74%, epoch time: 763.47 seconds, 12.72 minutes\n",
      "total_backward_count 11160000 real_backward_count 493807   4.425%\n",
      "fc layer 3 self.abs_max_out: 446.0\n",
      "fc layer 3 self.abs_max_out: 454.0\n",
      "epoch-186 lr=['0.0039062'], tr/val_loss:  1.559946/  1.777801, val:  91.30%, val_best:  91.60%, tr:  98.66%, tr_best:  98.74%, epoch time: 760.99 seconds, 12.68 minutes\n",
      "total_backward_count 11220000 real_backward_count 494610   4.408%\n",
      "fc layer 3 self.abs_max_out: 461.0\n",
      "epoch-187 lr=['0.0039062'], tr/val_loss:  1.551254/  1.788319, val:  90.83%, val_best:  91.60%, tr:  98.69%, tr_best:  98.74%, epoch time: 768.40 seconds, 12.81 minutes\n",
      "total_backward_count 11280000 real_backward_count 495397   4.392%\n",
      "epoch-188 lr=['0.0039062'], tr/val_loss:  1.546203/  1.765858, val:  91.29%, val_best:  91.60%, tr:  98.66%, tr_best:  98.74%, epoch time: 763.08 seconds, 12.72 minutes\n",
      "total_backward_count 11340000 real_backward_count 496202   4.376%\n",
      "epoch-189 lr=['0.0039062'], tr/val_loss:  1.550790/  1.786811, val:  89.98%, val_best:  91.60%, tr:  98.66%, tr_best:  98.74%, epoch time: 768.78 seconds, 12.81 minutes\n",
      "total_backward_count 11400000 real_backward_count 497004   4.360%\n",
      "epoch-190 lr=['0.0039062'], tr/val_loss:  1.551017/  1.773411, val:  90.05%, val_best:  91.60%, tr:  98.63%, tr_best:  98.74%, epoch time: 772.38 seconds, 12.87 minutes\n",
      "total_backward_count 11460000 real_backward_count 497826   4.344%\n",
      "epoch-191 lr=['0.0039062'], tr/val_loss:  1.549967/  1.782673, val:  90.02%, val_best:  91.60%, tr:  98.78%, tr_best:  98.78%, epoch time: 766.96 seconds, 12.78 minutes\n",
      "total_backward_count 11520000 real_backward_count 498561   4.328%\n",
      "epoch-192 lr=['0.0039062'], tr/val_loss:  1.557404/  1.797323, val:  90.04%, val_best:  91.60%, tr:  98.81%, tr_best:  98.81%, epoch time: 767.26 seconds, 12.79 minutes\n",
      "total_backward_count 11580000 real_backward_count 499275   4.312%\n",
      "epoch-193 lr=['0.0039062'], tr/val_loss:  1.563093/  1.784067, val:  89.49%, val_best:  91.60%, tr:  98.73%, tr_best:  98.81%, epoch time: 773.46 seconds, 12.89 minutes\n",
      "total_backward_count 11640000 real_backward_count 500039   4.296%\n",
      "epoch-194 lr=['0.0039062'], tr/val_loss:  1.556441/  1.777964, val:  87.38%, val_best:  91.60%, tr:  98.67%, tr_best:  98.81%, epoch time: 764.52 seconds, 12.74 minutes\n",
      "total_backward_count 11700000 real_backward_count 500839   4.281%\n",
      "epoch-195 lr=['0.0039062'], tr/val_loss:  1.528395/  1.755695, val:  90.13%, val_best:  91.60%, tr:  98.80%, tr_best:  98.81%, epoch time: 763.34 seconds, 12.72 minutes\n",
      "total_backward_count 11760000 real_backward_count 501560   4.265%\n",
      "lif layer 1 self.abs_max_v: 8298.5\n",
      "epoch-196 lr=['0.0039062'], tr/val_loss:  1.535081/  1.770660, val:  90.32%, val_best:  91.60%, tr:  98.84%, tr_best:  98.84%, epoch time: 769.94 seconds, 12.83 minutes\n",
      "total_backward_count 11820000 real_backward_count 502254   4.249%\n",
      "epoch-197 lr=['0.0039062'], tr/val_loss:  1.544620/  1.773793, val:  89.97%, val_best:  91.60%, tr:  98.83%, tr_best:  98.84%, epoch time: 766.22 seconds, 12.77 minutes\n",
      "total_backward_count 11880000 real_backward_count 502956   4.234%\n",
      "epoch-198 lr=['0.0039062'], tr/val_loss:  1.540511/  1.768481, val:  89.72%, val_best:  91.60%, tr:  98.77%, tr_best:  98.84%, epoch time: 763.43 seconds, 12.72 minutes\n",
      "total_backward_count 11940000 real_backward_count 503693   4.219%\n",
      "epoch-199 lr=['0.0039062'], tr/val_loss:  1.537393/  1.769513, val:  89.82%, val_best:  91.60%, tr:  98.79%, tr_best:  98.84%, epoch time: 768.88 seconds, 12.81 minutes\n",
      "total_backward_count 12000000 real_backward_count 504418   4.203%\n",
      "epoch-200 lr=['0.0039062'], tr/val_loss:  1.534764/  1.781084, val:  86.53%, val_best:  91.60%, tr:  98.85%, tr_best:  98.85%, epoch time: 767.49 seconds, 12.79 minutes\n",
      "total_backward_count 12060000 real_backward_count 505110   4.188%\n",
      "epoch-201 lr=['0.0039062'], tr/val_loss:  1.541097/  1.774682, val:  90.51%, val_best:  91.60%, tr:  98.83%, tr_best:  98.85%, epoch time: 764.94 seconds, 12.75 minutes\n",
      "total_backward_count 12120000 real_backward_count 505812   4.173%\n",
      "epoch-202 lr=['0.0039062'], tr/val_loss:  1.545268/  1.760738, val:  90.52%, val_best:  91.60%, tr:  98.84%, tr_best:  98.85%, epoch time: 768.56 seconds, 12.81 minutes\n",
      "total_backward_count 12180000 real_backward_count 506505   4.158%\n",
      "epoch-203 lr=['0.0039062'], tr/val_loss:  1.539854/  1.781771, val:  89.77%, val_best:  91.60%, tr:  98.77%, tr_best:  98.85%, epoch time: 764.49 seconds, 12.74 minutes\n",
      "total_backward_count 12240000 real_backward_count 507241   4.144%\n",
      "epoch-204 lr=['0.0039062'], tr/val_loss:  1.535610/  1.776145, val:  90.19%, val_best:  91.60%, tr:  98.74%, tr_best:  98.85%, epoch time: 765.23 seconds, 12.75 minutes\n",
      "total_backward_count 12300000 real_backward_count 507995   4.130%\n",
      "lif layer 1 self.abs_max_v: 8315.5\n",
      "epoch-205 lr=['0.0039062'], tr/val_loss:  1.544723/  1.767292, val:  90.71%, val_best:  91.60%, tr:  98.87%, tr_best:  98.87%, epoch time: 770.55 seconds, 12.84 minutes\n",
      "total_backward_count 12360000 real_backward_count 508674   4.115%\n",
      "lif layer 1 self.abs_max_v: 8318.0\n",
      "fc layer 1 self.abs_max_out: 7373.0\n",
      "epoch-206 lr=['0.0039062'], tr/val_loss:  1.543426/  1.783219, val:  89.21%, val_best:  91.60%, tr:  98.90%, tr_best:  98.90%, epoch time: 763.55 seconds, 12.73 minutes\n",
      "total_backward_count 12420000 real_backward_count 509336   4.101%\n",
      "lif layer 1 self.abs_max_v: 8329.0\n",
      "fc layer 1 self.abs_max_out: 7374.0\n",
      "epoch-207 lr=['0.0039062'], tr/val_loss:  1.550039/  1.769330, val:  91.00%, val_best:  91.60%, tr:  98.89%, tr_best:  98.90%, epoch time: 770.36 seconds, 12.84 minutes\n",
      "total_backward_count 12480000 real_backward_count 510003   4.087%\n",
      "fc layer 1 self.abs_max_out: 7378.0\n",
      "epoch-208 lr=['0.0039062'], tr/val_loss:  1.546641/  1.786616, val:  90.46%, val_best:  91.60%, tr:  98.85%, tr_best:  98.90%, epoch time: 769.98 seconds, 12.83 minutes\n",
      "total_backward_count 12540000 real_backward_count 510692   4.073%\n",
      "fc layer 1 self.abs_max_out: 7381.0\n",
      "epoch-209 lr=['0.0039062'], tr/val_loss:  1.553593/  1.773999, val:  90.94%, val_best:  91.60%, tr:  98.81%, tr_best:  98.90%, epoch time: 768.69 seconds, 12.81 minutes\n",
      "total_backward_count 12600000 real_backward_count 511404   4.059%\n",
      "lif layer 1 self.abs_max_v: 8345.0\n",
      "epoch-210 lr=['0.0039062'], tr/val_loss:  1.549736/  1.782725, val:  90.43%, val_best:  91.60%, tr:  98.83%, tr_best:  98.90%, epoch time: 766.15 seconds, 12.77 minutes\n",
      "total_backward_count 12660000 real_backward_count 512104   4.045%\n",
      "epoch-211 lr=['0.0039062'], tr/val_loss:  1.549325/  1.780161, val:  89.80%, val_best:  91.60%, tr:  98.83%, tr_best:  98.90%, epoch time: 765.55 seconds, 12.76 minutes\n",
      "total_backward_count 12720000 real_backward_count 512808   4.032%\n",
      "lif layer 1 self.abs_max_v: 8387.5\n",
      "epoch-212 lr=['0.0039062'], tr/val_loss:  1.554758/  1.785831, val:  91.16%, val_best:  91.60%, tr:  98.89%, tr_best:  98.90%, epoch time: 768.07 seconds, 12.80 minutes\n",
      "total_backward_count 12780000 real_backward_count 513475   4.018%\n",
      "epoch-213 lr=['0.0039062'], tr/val_loss:  1.558525/  1.784479, val:  89.33%, val_best:  91.60%, tr:  98.78%, tr_best:  98.90%, epoch time: 768.41 seconds, 12.81 minutes\n",
      "total_backward_count 12840000 real_backward_count 514205   4.005%\n",
      "fc layer 3 self.abs_max_out: 470.0\n",
      "epoch-214 lr=['0.0039062'], tr/val_loss:  1.545641/  1.762952, val:  90.90%, val_best:  91.60%, tr:  98.90%, tr_best:  98.90%, epoch time: 770.05 seconds, 12.83 minutes\n",
      "total_backward_count 12900000 real_backward_count 514865   3.991%\n",
      "epoch-215 lr=['0.0039062'], tr/val_loss:  1.537441/  1.778564, val:  90.18%, val_best:  91.60%, tr:  98.85%, tr_best:  98.90%, epoch time: 771.28 seconds, 12.85 minutes\n",
      "total_backward_count 12960000 real_backward_count 515556   3.978%\n",
      "epoch-216 lr=['0.0039062'], tr/val_loss:  1.542444/  1.764274, val:  90.16%, val_best:  91.60%, tr:  98.83%, tr_best:  98.90%, epoch time: 767.92 seconds, 12.80 minutes\n",
      "total_backward_count 13020000 real_backward_count 516257   3.965%\n",
      "fc layer 1 self.abs_max_out: 7383.0\n",
      "epoch-217 lr=['0.0039062'], tr/val_loss:  1.539889/  1.763132, val:  90.93%, val_best:  91.60%, tr:  98.87%, tr_best:  98.90%, epoch time: 767.91 seconds, 12.80 minutes\n",
      "total_backward_count 13080000 real_backward_count 516935   3.952%\n",
      "epoch-218 lr=['0.0039062'], tr/val_loss:  1.547345/  1.772447, val:  90.06%, val_best:  91.60%, tr:  98.87%, tr_best:  98.90%, epoch time: 767.25 seconds, 12.79 minutes\n",
      "total_backward_count 13140000 real_backward_count 517614   3.939%\n",
      "epoch-219 lr=['0.0039062'], tr/val_loss:  1.545875/  1.766539, val:  90.60%, val_best:  91.60%, tr:  98.90%, tr_best:  98.90%, epoch time: 765.17 seconds, 12.75 minutes\n",
      "total_backward_count 13200000 real_backward_count 518276   3.926%\n",
      "epoch-220 lr=['0.0039062'], tr/val_loss:  1.540214/  1.777213, val:  90.78%, val_best:  91.60%, tr:  98.90%, tr_best:  98.90%, epoch time: 768.38 seconds, 12.81 minutes\n",
      "total_backward_count 13260000 real_backward_count 518935   3.914%\n",
      "epoch-221 lr=['0.0039062'], tr/val_loss:  1.548086/  1.770455, val:  91.00%, val_best:  91.60%, tr:  98.89%, tr_best:  98.90%, epoch time: 770.52 seconds, 12.84 minutes\n",
      "total_backward_count 13320000 real_backward_count 519601   3.901%\n",
      "fc layer 1 self.abs_max_out: 7387.0\n",
      "epoch-222 lr=['0.0039062'], tr/val_loss:  1.536103/  1.748076, val:  90.32%, val_best:  91.60%, tr:  98.85%, tr_best:  98.90%, epoch time: 769.28 seconds, 12.82 minutes\n",
      "total_backward_count 13380000 real_backward_count 520291   3.889%\n",
      "fc layer 1 self.abs_max_out: 7390.0\n",
      "epoch-223 lr=['0.0039062'], tr/val_loss:  1.524389/  1.749784, val:  90.31%, val_best:  91.60%, tr:  98.91%, tr_best:  98.91%, epoch time: 769.01 seconds, 12.82 minutes\n",
      "total_backward_count 13440000 real_backward_count 520943   3.876%\n",
      "epoch-224 lr=['0.0039062'], tr/val_loss:  1.517226/  1.746599, val:  90.37%, val_best:  91.60%, tr:  98.83%, tr_best:  98.91%, epoch time: 772.99 seconds, 12.88 minutes\n",
      "total_backward_count 13500000 real_backward_count 521643   3.864%\n",
      "epoch-225 lr=['0.0039062'], tr/val_loss:  1.518702/  1.748938, val:  89.75%, val_best:  91.60%, tr:  98.89%, tr_best:  98.91%, epoch time: 766.16 seconds, 12.77 minutes\n",
      "total_backward_count 13560000 real_backward_count 522310   3.852%\n",
      "epoch-226 lr=['0.0039062'], tr/val_loss:  1.514882/  1.744324, val:  90.80%, val_best:  91.60%, tr:  98.88%, tr_best:  98.91%, epoch time: 762.72 seconds, 12.71 minutes\n",
      "total_backward_count 13620000 real_backward_count 522984   3.840%\n",
      "epoch-227 lr=['0.0039062'], tr/val_loss:  1.524750/  1.747273, val:  90.19%, val_best:  91.60%, tr:  98.94%, tr_best:  98.94%, epoch time: 766.44 seconds, 12.77 minutes\n",
      "total_backward_count 13680000 real_backward_count 523618   3.828%\n",
      "epoch-228 lr=['0.0039062'], tr/val_loss:  1.519263/  1.755273, val:  90.67%, val_best:  91.60%, tr:  98.98%, tr_best:  98.98%, epoch time: 762.38 seconds, 12.71 minutes\n",
      "total_backward_count 13740000 real_backward_count 524227   3.815%\n",
      "epoch-229 lr=['0.0039062'], tr/val_loss:  1.527668/  1.762627, val:  90.30%, val_best:  91.60%, tr:  99.05%, tr_best:  99.05%, epoch time: 762.55 seconds, 12.71 minutes\n",
      "total_backward_count 13800000 real_backward_count 524800   3.803%\n",
      "epoch-230 lr=['0.0039062'], tr/val_loss:  1.528171/  1.769750, val:  90.08%, val_best:  91.60%, tr:  98.94%, tr_best:  99.05%, epoch time: 762.63 seconds, 12.71 minutes\n",
      "total_backward_count 13860000 real_backward_count 525435   3.791%\n",
      "epoch-231 lr=['0.0039062'], tr/val_loss:  1.526370/  1.748735, val:  90.56%, val_best:  91.60%, tr:  98.95%, tr_best:  99.05%, epoch time: 764.14 seconds, 12.74 minutes\n",
      "total_backward_count 13920000 real_backward_count 526066   3.779%\n",
      "epoch-232 lr=['0.0039062'], tr/val_loss:  1.519562/  1.759980, val:  90.65%, val_best:  91.60%, tr:  99.01%, tr_best:  99.05%, epoch time: 758.45 seconds, 12.64 minutes\n",
      "total_backward_count 13980000 real_backward_count 526662   3.767%\n",
      "epoch-233 lr=['0.0039062'], tr/val_loss:  1.536148/  1.767838, val:  89.84%, val_best:  91.60%, tr:  98.96%, tr_best:  99.05%, epoch time: 764.92 seconds, 12.75 minutes\n",
      "total_backward_count 14040000 real_backward_count 527285   3.756%\n",
      "epoch-234 lr=['0.0039062'], tr/val_loss:  1.548865/  1.775105, val:  90.83%, val_best:  91.60%, tr:  98.88%, tr_best:  99.05%, epoch time: 761.11 seconds, 12.69 minutes\n",
      "total_backward_count 14100000 real_backward_count 527959   3.744%\n",
      "lif layer 1 self.abs_max_v: 8400.5\n",
      "epoch-235 lr=['0.0039062'], tr/val_loss:  1.542612/  1.760135, val:  90.94%, val_best:  91.60%, tr:  98.88%, tr_best:  99.05%, epoch time: 761.20 seconds, 12.69 minutes\n",
      "total_backward_count 14160000 real_backward_count 528634   3.733%\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "epoch-236 lr=['0.0039062'], tr/val_loss:  1.524505/  1.749008, val:  91.44%, val_best:  91.60%, tr:  98.92%, tr_best:  99.05%, epoch time: 764.25 seconds, 12.74 minutes\n",
      "total_backward_count 14220000 real_backward_count 529280   3.722%\n",
      "epoch-237 lr=['0.0039062'], tr/val_loss:  1.516801/  1.738975, val:  89.58%, val_best:  91.60%, tr:  98.87%, tr_best:  99.05%, epoch time: 761.51 seconds, 12.69 minutes\n",
      "total_backward_count 14280000 real_backward_count 529957   3.711%\n",
      "epoch-238 lr=['0.0039062'], tr/val_loss:  1.521406/  1.764632, val:  90.38%, val_best:  91.60%, tr:  98.92%, tr_best:  99.05%, epoch time: 762.71 seconds, 12.71 minutes\n",
      "total_backward_count 14340000 real_backward_count 530604   3.700%\n",
      "epoch-239 lr=['0.0039062'], tr/val_loss:  1.519745/  1.743875, val:  89.64%, val_best:  91.60%, tr:  98.95%, tr_best:  99.05%, epoch time: 766.33 seconds, 12.77 minutes\n",
      "total_backward_count 14400000 real_backward_count 531235   3.689%\n",
      "fc layer 1 self.abs_max_out: 7391.0\n",
      "epoch-240 lr=['0.0039062'], tr/val_loss:  1.517587/  1.761178, val:  89.86%, val_best:  91.60%, tr:  98.95%, tr_best:  99.05%, epoch time: 763.97 seconds, 12.73 minutes\n",
      "total_backward_count 14460000 real_backward_count 531863   3.678%\n",
      "fc layer 1 self.abs_max_out: 7393.0\n",
      "epoch-241 lr=['0.0039062'], tr/val_loss:  1.529248/  1.764303, val:  90.46%, val_best:  91.60%, tr:  98.99%, tr_best:  99.05%, epoch time: 761.05 seconds, 12.68 minutes\n",
      "total_backward_count 14520000 real_backward_count 532469   3.667%\n",
      "epoch-242 lr=['0.0039062'], tr/val_loss:  1.534670/  1.769818, val:  90.07%, val_best:  91.60%, tr:  98.92%, tr_best:  99.05%, epoch time: 769.56 seconds, 12.83 minutes\n",
      "total_backward_count 14580000 real_backward_count 533114   3.656%\n",
      "epoch-243 lr=['0.0039062'], tr/val_loss:  1.528483/  1.764456, val:  90.61%, val_best:  91.60%, tr:  98.93%, tr_best:  99.05%, epoch time: 757.77 seconds, 12.63 minutes\n",
      "total_backward_count 14640000 real_backward_count 533757   3.646%\n",
      "lif layer 1 self.abs_max_v: 8442.0\n",
      "epoch-244 lr=['0.0039062'], tr/val_loss:  1.536588/  1.781726, val:  86.96%, val_best:  91.60%, tr:  98.89%, tr_best:  99.05%, epoch time: 761.99 seconds, 12.70 minutes\n",
      "total_backward_count 14700000 real_backward_count 534422   3.636%\n",
      "epoch-245 lr=['0.0039062'], tr/val_loss:  1.539286/  1.772626, val:  89.65%, val_best:  91.60%, tr:  98.95%, tr_best:  99.05%, epoch time: 767.57 seconds, 12.79 minutes\n",
      "total_backward_count 14760000 real_backward_count 535055   3.625%\n",
      "epoch-246 lr=['0.0039062'], tr/val_loss:  1.536723/  1.768674, val:  90.26%, val_best:  91.60%, tr:  98.94%, tr_best:  99.05%, epoch time: 762.22 seconds, 12.70 minutes\n",
      "total_backward_count 14820000 real_backward_count 535694   3.615%\n",
      "lif layer 1 self.abs_max_v: 8468.5\n",
      "epoch-247 lr=['0.0039062'], tr/val_loss:  1.539404/  1.781322, val:  90.67%, val_best:  91.60%, tr:  98.95%, tr_best:  99.05%, epoch time: 761.69 seconds, 12.69 minutes\n",
      "total_backward_count 14880000 real_backward_count 536326   3.604%\n",
      "epoch-248 lr=['0.0039062'], tr/val_loss:  1.539407/  1.764227, val:  90.96%, val_best:  91.60%, tr:  98.91%, tr_best:  99.05%, epoch time: 766.16 seconds, 12.77 minutes\n",
      "total_backward_count 14940000 real_backward_count 536983   3.594%\n",
      "fc layer 3 self.abs_max_out: 482.0\n",
      "fc layer 1 self.abs_max_out: 7394.0\n",
      "epoch-249 lr=['0.0039062'], tr/val_loss:  1.524430/  1.751850, val:  91.27%, val_best:  91.60%, tr:  98.99%, tr_best:  99.05%, epoch time: 767.46 seconds, 12.79 minutes\n",
      "total_backward_count 15000000 real_backward_count 537588   3.584%\n",
      "epoch-250 lr=['0.0039062'], tr/val_loss:  1.516753/  1.741684, val:  90.78%, val_best:  91.60%, tr:  99.00%, tr_best:  99.05%, epoch time: 764.66 seconds, 12.74 minutes\n",
      "total_backward_count 15060000 real_backward_count 538190   3.574%\n",
      "epoch-251 lr=['0.0039062'], tr/val_loss:  1.517355/  1.765439, val:  90.96%, val_best:  91.60%, tr:  99.02%, tr_best:  99.05%, epoch time: 769.67 seconds, 12.83 minutes\n",
      "total_backward_count 15120000 real_backward_count 538778   3.563%\n",
      "epoch-252 lr=['0.0039062'], tr/val_loss:  1.525706/  1.757099, val:  90.67%, val_best:  91.60%, tr:  99.02%, tr_best:  99.05%, epoch time: 763.92 seconds, 12.73 minutes\n",
      "total_backward_count 15180000 real_backward_count 539367   3.553%\n",
      "epoch-253 lr=['0.0039062'], tr/val_loss:  1.516967/  1.746160, val:  90.52%, val_best:  91.60%, tr:  99.00%, tr_best:  99.05%, epoch time: 763.36 seconds, 12.72 minutes\n",
      "total_backward_count 15240000 real_backward_count 539970   3.543%\n",
      "epoch-254 lr=['0.0039062'], tr/val_loss:  1.499952/  1.742671, val:  90.12%, val_best:  91.60%, tr:  98.91%, tr_best:  99.05%, epoch time: 769.18 seconds, 12.82 minutes\n",
      "total_backward_count 15300000 real_backward_count 540627   3.534%\n",
      "fc layer 3 self.abs_max_out: 489.0\n",
      "epoch-255 lr=['0.0039062'], tr/val_loss:  1.505848/  1.744291, val:  90.48%, val_best:  91.60%, tr:  99.01%, tr_best:  99.05%, epoch time: 767.75 seconds, 12.80 minutes\n",
      "total_backward_count 15360000 real_backward_count 541219   3.524%\n"
     ]
    }
   ],
   "source": [
    "### my_snn control board (Gesture) ########################\n",
    "decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# nda 0.25 # ottt 0.5\n",
    "\n",
    "unique_name = 'main'\n",
    "run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "my_snn_system(  devices = \"1\",\n",
    "                single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "                unique_name = run_name,\n",
    "                my_seed = 777,\n",
    "                TIME = 1, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "                BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "                IMAGE_SIZE = 17, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "                # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "                # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "                which_data = 'NMNIST_TONIC',\n",
    "# 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "                # CLASS_NUM = 10,\n",
    "                data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "                rate_coding = False, # True # False\n",
    "\n",
    "                lif_layer_v_init = 0.0,\n",
    "                lif_layer_v_decay = decay,\n",
    "                lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "                lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "                lif_layer_sg_width = 6.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "                # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                synapse_conv_kernel_size = 3,\n",
    "                synapse_conv_stride = 1,\n",
    "                synapse_conv_padding = 1,\n",
    "\n",
    "                synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "                synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "                # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                pre_trained = False, # True # False\n",
    "                convTrue_fcFalse = False, # True # False\n",
    "\n",
    "                # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "                # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "                # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "                cfg = [200, 200], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96], \n",
    "                # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "                # cfg = ['M', 'M', 64], \n",
    "                # cfg = [64, 124, 64, 124],\n",
    "                # cfg = ['M','M',512], \n",
    "                # cfg = [512], \n",
    "                # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "                # cfg = ['M','M',512],\n",
    "                # cfg = ['M',200],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = ['M','M',200,200],\n",
    "                # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "                # cfg = ['M',200,200],\n",
    "                # cfg = ['M','M',1024,512,256,128,64],\n",
    "                # cfg = [200,200],\n",
    "                # cfg = [12], #fc\n",
    "                # cfg = [12, 'M', 48, 'M', 12], \n",
    "                # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "                # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "                # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "                # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "                # cfg = [20001,10001], # depthwise, separable\n",
    "                # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "                # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "                # cfg = [],        \n",
    "                \n",
    "                net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "                pre_trained_path = f\"net_save/save_now_net_weights_20250906_001313_333.pth\",\n",
    "                # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                learning_rate = 1/256, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "                epoch_num = 300,\n",
    "                tdBN_on = False,  # True # False\n",
    "                BN_on = False,  # True # False\n",
    "                \n",
    "                surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "                BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "                optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "                ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                dvs_clipping = 1, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "                # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "                dvs_duration = 5_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "                # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "                # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "                # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "                # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "                DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "                trace_on = False,   # True # False\n",
    "                OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "                exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "                extra_train_dataset = 0, \n",
    "\n",
    "                num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "                chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "                pin_memory = True, # True # False \n",
    "\n",
    "                UDA_on = False,  # DECREPATED # uda\n",
    "                alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                bias = False, # True # False \n",
    "\n",
    "                last_lif = False, # True # False \n",
    "\n",
    "                temporal_filter = 1, \n",
    "                initial_pooling = 1,\n",
    "\n",
    "                temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "                quantize_bit_list=[8,8,8],\n",
    "                scale_exp=[[-9,-9],[-9,-9],[-8,-8]], \n",
    "\n",
    "                test_timesteps = 8, # -1Ïù¥Î©¥ trainÏù¥Îûë ÎòëÍ∞ôÏù¥\n",
    "# 1w -11~-9\n",
    "# 1b -11~ -7\n",
    "# 2w -10~-8\n",
    "# 2b -10~-8\n",
    "# 3w -10\n",
    "# 3b -10\n",
    "                ) \n",
    "\n",
    "# num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# num_workers = batch_size / num_GPU\n",
    "# num_workers = batch_size / num_CPU\n",
    "\n",
    "# sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# average pooling  \n",
    "# Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "\n",
    "# 46ÍπåÏßÄÌñàÏóàÍ∏¥ÌñàÏùå\n",
    "# och-42  lr=['0.0039062'], tr/val_loss:  0.791181/  0.859875, val:  94.82%, val_best:  96.05%, tr:  99.85%, tr_best:  99.88%, epoch time: 2653.91 seconds, 44.23 minutes\n",
    "# epoch-43  lr=['0.0039062'], tr/val_loss:  0.797550/  0.848301, val:  95.38%, val_best:  96.05%, tr:  99.87%, tr_best:  99.88%, epoch time: 2648.23 seconds, 44.14 minutes\n",
    "# epoch-44  lr=['0.0039062'], tr/val_loss:  0.801503/  0.850083, val:  95.41%, val_best:  96.05%, tr:  99.87%, tr_best:  99.88%, epoch time: 2644.07 seconds, 44.07 minutes\n",
    "# epoch-45  lr=['0.0039062'], tr/val_loss:  0.802839/  0.866149, val:  94.36%, val_best:  96.05%, tr:  99.87%, tr_best:  99.88%, epoch time: 2633.83 seconds, 43.90 minutes\n",
    "# Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# # Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# # wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "# unique_name_hyper = 'main'\n",
    "# sweep_configuration = {\n",
    "#     'method': 'bayes', # 'random', 'bayes', 'grid'\n",
    "#     'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "#     'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "#     'parameters': \n",
    "#     {\n",
    "#         # \"devices\": {\"values\": [\"1\"]},\n",
    "#         \"single_step\": {\"values\": [True]},\n",
    "#         # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "#         \"my_seed\": {\"values\": [42]},\n",
    "#         \"TIME\": {\"values\": [8]},\n",
    "#         \"BATCH\": {\"values\": [1]},\n",
    "#         \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "#         \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "#         \"data_path\": {\"values\": ['/data2']},\n",
    "#         \"rate_coding\": {\"values\": [False]},\n",
    "#         \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "#         \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "#         \"lif_layer_v_threshold\": {\"values\": [0.5]},\n",
    "#         \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "#         \"lif_layer_sg_width\": {\"values\": [4.0]},\n",
    "\n",
    "#         \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "#         \"synapse_conv_stride\": {\"values\": [1]},\n",
    "#         \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "#         \"synapse_trace_const1\": {\"values\": [1]},\n",
    "#         \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "#         \"pre_trained\": {\"values\": [False]},\n",
    "#         \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "#         \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "#         \"net_print\": {\"values\": [True]},\n",
    "\n",
    "#         \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "#         \"learning_rate\": {\"values\": [0.1,0.01,0.001,0.0001,0.00001]}, \n",
    "#         \"epoch_num\": {\"values\": [1]}, \n",
    "#         \"tdBN_on\": {\"values\": [False]},\n",
    "#         \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "#         \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"optimizer_what\": {\"values\": ['SGD']},\n",
    "#         \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "#         \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "#         \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "#         \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "#         \"trace_on\": {\"values\": [False]},\n",
    "#         \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "#         \"merge_polarities\": {\"values\": [True]},\n",
    "#         \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "#         \"extra_train_dataset\": {\"values\": [9]},\n",
    "\n",
    "#         \"num_workers\": {\"values\": [2]},\n",
    "#         \"chaching_on\": {\"values\": [True]},\n",
    "#         \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "#         \"UDA_on\": {\"values\": [False]},\n",
    "#         \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "#         \"bias\": {\"values\": [True]},\n",
    "\n",
    "#         \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "#         \"temporal_filter\": {\"values\": [5]},\n",
    "#         \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "#         \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "#         \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "#         \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "#         \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "#         \"scale_exp_1w\": {\"values\": [-11,-10,-9]},\n",
    "#         # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "#         \"scale_exp_2w\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "#         # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "#         \"scale_exp_3w\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "#         # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "#      }\n",
    "# }\n",
    "\n",
    "# def hyper_iter():\n",
    "#     ### my_snn control board ########################\n",
    "#     wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "#     my_snn_system(  \n",
    "#         devices  =  \"5\",\n",
    "#         single_step  =  wandb.config.single_step,\n",
    "#         unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "#         my_seed  =  wandb.config.my_seed,\n",
    "#         TIME  =  wandb.config.TIME,\n",
    "#         BATCH  =  wandb.config.BATCH,\n",
    "#         IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "#         which_data  =  wandb.config.which_data,\n",
    "#         data_path  =  wandb.config.data_path,\n",
    "#         rate_coding  =  wandb.config.rate_coding,\n",
    "#         lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "#         lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "#         lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "#         lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "#         lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "#         synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "#         synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "#         synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "#         synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "#         synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "#         pre_trained  =  wandb.config.pre_trained,\n",
    "#         convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "#         cfg  =  wandb.config.cfg,\n",
    "#         net_print  =  wandb.config.net_print,\n",
    "#         pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "#         learning_rate  =  wandb.config.learning_rate,\n",
    "#         epoch_num  =  wandb.config.epoch_num,\n",
    "#         tdBN_on  =  wandb.config.tdBN_on,\n",
    "#         BN_on  =  wandb.config.BN_on,\n",
    "#         surrogate  =  wandb.config.surrogate,\n",
    "#         BPTT_on  =  wandb.config.BPTT_on,\n",
    "#         optimizer_what  =  wandb.config.optimizer_what,\n",
    "#         scheduler_name  =  wandb.config.scheduler_name,\n",
    "#         ddp_on  =  wandb.config.ddp_on,\n",
    "#         dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "#         dvs_duration  =  wandb.config.dvs_duration,\n",
    "#         DFA_on  =  wandb.config.DFA_on,\n",
    "#         trace_on  =  wandb.config.trace_on,\n",
    "#         OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "#         exclude_class  =  wandb.config.exclude_class,\n",
    "#         merge_polarities  =  wandb.config.merge_polarities,\n",
    "#         denoise_on  =  wandb.config.denoise_on,\n",
    "#         extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "#         num_workers  =  wandb.config.num_workers,\n",
    "#         chaching_on  =  wandb.config.chaching_on,\n",
    "#         pin_memory  =  wandb.config.pin_memory,\n",
    "#         UDA_on  =  wandb.config.UDA_on,\n",
    "#         alpha_uda  =  wandb.config.alpha_uda,\n",
    "#         bias  =  wandb.config.bias,\n",
    "#         last_lif  =  wandb.config.last_lif,\n",
    "#         temporal_filter  =  wandb.config.temporal_filter,\n",
    "#         initial_pooling  =  wandb.config.initial_pooling,\n",
    "#         temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "#         quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "#         scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "#                         ) \n",
    "#     # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "#     # average pooling\n",
    "#     # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "#     # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "#     ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# # sweep_id = 'v89awhtt'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "# wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
