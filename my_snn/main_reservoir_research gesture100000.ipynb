{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    " \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAIhCAYAAACfVbSSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA8D0lEQVR4nO3deXxU1f3/8fckkAlLEtYEkCTEpTWCGgyobP5wIZUCYl2giCwCFgyLLEVItS6gRNAirRgU2UQWIwUElaKpVEGFEiOCihYVJEHBCCIBhITM3N8flHw7JGBmnDmXmXk9H4/7eDQnd+79zFTk4/ucOddhWZYlAAAABFyE3QUAAACECxovAAAAQ2i8AAAADKHxAgAAMITGCwAAwBAaLwAAAENovAAAAAyh8QIAADCExgsAAMAQGi/ABwsWLJDD4ag4atSooaZNm+r3v/+9vvjiC9vqevjhh+VwOGy7/+kKCgo0fPhwXXrppYqJiVFCQoJuuOEGrVu3rtK5AwcO9PhM69SpoxYtWuimm27S/PnzVVpa6vX9x44dK4fDoe7du/vj7QDAL0bjBfwC8+fP18aNG/XPf/5TI0aM0OrVq9WxY0cdPHjQ7tLOCUuXLtXmzZs1aNAgrVq1SnPmzJHT6dT111+vhQsXVjq/Vq1a2rhxozZu3KjXXntNkyZNUp06dXT33XcrPT1de/bsqfa9T5w4oUWLFkmS1q5dq2+++cZv7wsAfGYB8Nr8+fMtSVZ+fr7H+COPPGJJsubNm2dLXQ899JB1Lv2x/u677yqNlZeXW5dddpl1wQUXeIwPGDDAqlOnTpXXeeONN6yaNWtaV111VbXvvWzZMkuS1a1bN0uS9dhjj1XrdWVlZdaJEyeq/N3Ro0erfX8AqAqJF+BHbdq0kSR99913FWPHjx/XuHHjlJaWpri4ODVo0EDt2rXTqlWrKr3e4XBoxIgRevHFF5WamqratWvr8ssv12uvvVbp3Ndff11paWlyOp1KSUnRk08+WWVNx48fV1ZWllJSUhQVFaXzzjtPw4cP148//uhxXosWLdS9e3e99tprat26tWrVqqXU1NSKey9YsECpqamqU6eOrrzySn3wwQc/+3nEx8dXGouMjFR6erqKiop+9vWnZGRk6O6779a///1vrV+/vlqvmTt3rqKiojR//nwlJiZq/vz5sizL45y3335bDodDL774osaNG6fzzjtPTqdTX375pQYOHKi6devq448/VkZGhmJiYnT99ddLkvLy8tSzZ081b95c0dHRuvDCCzV06FDt37+/4tobNmyQw+HQ0qVLK9W2cOFCORwO5efnV/szABAaaLwAP9q1a5ck6Ve/+lXFWGlpqX744Qf98Y9/1CuvvKKlS5eqY8eOuuWWW6qcbnv99dc1c+ZMTZo0ScuXL1eDBg30u9/9Tjt37qw456233lLPnj0VExOjl156SU888YRefvllzZ8/3+NalmXp5ptv1pNPPql+/frp9ddf19ixY/XCCy/ouuuuq7RuauvWrcrKytKECRO0YsUKxcXF6ZZbbtFDDz2kOXPmaMqUKVq8eLEOHTqk7t2769ixY15/RuXl5dqwYYNatmzp1etuuukmSapW47Vnzx69+eab6tmzpxo3bqwBAwboyy+/PONrs7KyVFhYqGeffVavvvpqRcNYVlamm266Sdddd51WrVqlRx55RJL01VdfqV27dpo1a5befPNNPfjgg/r3v/+tjh076sSJE5KkTp06qXXr1nrmmWcq3W/mzJlq27at2rZt69VnACAE2B25AcHo1FTjpk2brBMnTliHDx+21q5dazVp0sS65pprzjhVZVknp9pOnDhhDR482GrdurXH7yRZCQkJVklJScXYvn37rIiICCs7O7ti7KqrrrKaNWtmHTt2rGKspKTEatCggcdU49q1ay1J1rRp0zzuk5uba0myZs+eXTGWnJxs1apVy9qzZ0/F2EcffWRJspo2beoxzfbKK69YkqzVq1dX5+PycP/991uSrFdeecVj/GxTjZZlWZ999pklybrnnnt+9h6TJk2yJFlr1661LMuydu7caTkcDqtfv34e5/3rX/+yJFnXXHNNpWsMGDCgWtPGbrfbOnHihLV7925LkrVq1aqK353652TLli0VY5s3b7YkWS+88MLPvg8AoYfEC/gFrr76atWsWVMxMTG68cYbVb9+fa1atUo1atTwOG/ZsmXq0KGD6tatqxo1aqhmzZqaO3euPvvss0rXvPbaaxUTE1Pxc0JCguLj47V7925J0tGjR5Wfn69bbrlF0dHRFefFxMSoR48eHtc69e3BgQMHeozffvvtqlOnjt566y2P8bS0NJ133nkVP6empkqSOnfurNq1a1caP1VTdc2ZM0ePPfaYxo0bp549e3r1Wuu0acKznXdqerFLly6SpJSUFHXu3FnLly9XSUlJpdfceuutZ7xeVb8rLi7WsGHDlJiYWPH/Z3JysiR5/H/ap08fxcfHe6ReTz/9tBo3bqzevXtX6/0ACC00XsAvsHDhQuXn52vdunUaOnSoPvvsM/Xp08fjnBUrVqhXr14677zztGjRIm3cuFH5+fkaNGiQjh8/XumaDRs2rDTmdDorpvUOHjwot9utJk2aVDrv9LEDBw6oRo0aaty4sce4w+FQkyZNdODAAY/xBg0aePwcFRV11vGq6j+T+fPna+jQofrDH/6gJ554otqvO+VUk9esWbOznrdu3Trt2rVLt99+u0pKSvTjjz/qxx9/VK9evfTTTz9VueaqadOmVV6rdu3aio2N9Rhzu93KyMjQihUrdN999+mtt97S5s2btWnTJknymH51Op0aOnSolixZoh9//FHff/+9Xn75ZQ0ZMkROp9Or9w8gNNT4+VMAnElqamrFgvprr71WLpdLc+bM0d///nfddtttkqRFixYpJSVFubm5Hnts+bIvlSTVr19fDodD+/btq/S708caNmyo8vJyff/99x7Nl2VZ2rdvn7E1RvPnz9eQIUM0YMAAPfvssz7tNbZ69WpJJ9O3s5k7d64kafr06Zo+fXqVvx86dKjH2JnqqWr8k08+0datW7VgwQINGDCgYvzLL7+s8hr33HOPHn/8cc2bN0/Hjx9XeXm5hg0bdtb3ACB0kXgBfjRt2jTVr19fDz74oNxut6STf3lHRUV5/CW+b9++Kr/VWB2nvlW4YsUKj8Tp8OHDevXVVz3OPfUtvFP7WZ2yfPlyHT16tOL3gbRgwQINGTJEd955p+bMmeNT05WXl6c5c+aoffv26tix4xnPO3jwoFauXKkOHTroX//6V6Wjb9++ys/P1yeffOLz+zlV/+mJ1XPPPVfl+U2bNtXtt9+unJwcPfvss+rRo4eSkpJ8vj+A4EbiBfhR/fr1lZWVpfvuu09LlizRnXfeqe7du2vFihXKzMzUbbfdpqKiIk2ePFlNmzb1eZf7yZMn68Ybb1SXLl00btw4uVwuTZ06VXXq1NEPP/xQcV6XLl30m9/8RhMmTFBJSYk6dOigbdu26aGHHlLr1q3Vr18/f731Ki1btkyDBw9WWlqahg4dqs2bN3v8vnXr1h4NjNvtrpiyKy0tVWFhof7xj3/o5ZdfVmpqql5++eWz3m/x4sU6fvy4Ro0aVWUy1rBhQy1evFhz587VU0895dN7uvjii3XBBRdo4sSJsixLDRo00Kuvvqq8vLwzvubee+/VVVddJUmVvnkKIMzYu7YfCE5n2kDVsizr2LFjVlJSknXRRRdZ5eXllmVZ1uOPP261aNHCcjqdVmpqqvX8889XudmpJGv48OGVrpmcnGwNGDDAY2z16tXWZZddZkVFRVlJSUnW448/XuU1jx07Zk2YMMFKTk62atasaTVt2tS65557rIMHD1a6R7du3Srdu6qadu3aZUmynnjiiTN+Rpb1f98MPNOxa9euM55bq1YtKykpyerRo4c1b948q7S09Kz3sizLSktLs+Lj48967tVXX201atTIKi0trfhW47Jly6qs/Uzfsty+fbvVpUsXKyYmxqpfv751++23W4WFhZYk66GHHqryNS1atLBSU1N/9j0ACG0Oy6rmV4UAAD7Ztm2bLr/8cj3zzDPKzMy0uxwANqLxAoAA+eqrr7R792796U9/UmFhob788kuPbTkAhB8W1wNAgEyePFldunTRkSNHtGzZMpouACReAAAAppB4AQAAGELjBQAAYAiNFwAAgCFBvYGq2+3Wt99+q5iYGJ92wwYAIJxYlqXDhw+rWbNmiogwn70cP35cZWVlAbl2VFSUoqOjA3Jtfwrqxuvbb79VYmKi3WUAABBUioqK1Lx5c6P3PH78uFKS62pfsSsg12/SpIl27dp1zjdfQd14xcTESJKu+fUo1Yh0/szZ55b4p7+1uwSfzErcaHcJPrts9V12l+CT5nluu0vwyXUPv2d3CT57b/8Fdpfgk2GJb9tdgk9mDbvV7hJ89nX3WnaX4BX38eMqenRyxd+fJpWVlWlfsUu7C1ooNsa/aVvJYbeS079WWVkZjVcgnZperBHpDLrGK6pulN0l+MTff1hMiqh1bv9hPJMaNYOz8YquW9PuEnxW41hw/fvklNoxkXaX4JMaNYLzz6YkRZzjf8mfiZ3Lc+rGOFQ3xr/3dyt4lhsFdeMFAACCi8tyy+XnHURdVvD8B2rwxhcAAABBhsQLAAAY45Ylt/wbefn7eoFE4gUAAGAIiRcAADDGLbf8vSLL/1cMHBIvAAAAQ0i8AACAMS7Lksvy75osf18vkEi8AAAADCHxAgAAxoT7txppvAAAgDFuWXKFcePFVCMAAIAhJF4AAMCYcJ9qJPECAAAwhMQLAAAYw3YSAAAAMILECwAAGOP+7+HvawYL2xOvnJwcpaSkKDo6Wunp6dqwYYPdJQEAAASErY1Xbm6uRo8erfvvv19btmxRp06d1LVrVxUWFtpZFgAACBDXf/fx8vcRLGxtvKZPn67BgwdryJAhSk1N1YwZM5SYmKhZs2bZWRYAAAgQlxWYI1jY1niVlZWpoKBAGRkZHuMZGRl6//33q3xNaWmpSkpKPA4AAIBgYVvjtX//frlcLiUkJHiMJyQkaN++fVW+Jjs7W3FxcRVHYmKiiVIBAICfuAN0BAvbF9c7HA6Pny3LqjR2SlZWlg4dOlRxFBUVmSgRAADAL2zbTqJRo0aKjIyslG4VFxdXSsFOcTqdcjqdJsoDAAAB4JZDLlUdsPySawYL2xKvqKgopaenKy8vz2M8Ly9P7du3t6kqAACAwLF1A9WxY8eqX79+atOmjdq1a6fZs2ersLBQw4YNs7MsAAAQIG7r5OHvawYLWxuv3r1768CBA5o0aZL27t2rVq1aac2aNUpOTrazLAAAgICw/ZFBmZmZyszMtLsMAABggCsAa7z8fb1Asr3xAgAA4SPcGy/bt5MAAAAIFyReAADAGLflkNvy83YSfr5eIJF4AQAAGELiBQAAjGGNFwAAAIwg8QIAAMa4FCGXn3Mfl1+vFlgkXgAAAIaQeAEAAGOsAHyr0QqibzXSeAEAAGNYXA8AAAAjSLwAAIAxLitCLsvPi+stv14uoEi8AAAADCHxAgAAxrjlkNvPuY9bwRN5kXgBAAAYEhKJ165e9RQRHW13GV55IH6J3SX45NKnRthdgs+i6tpdgW/qbN9ndwk+WbDmOrtL8Fny68ftLsEnfy3vbXcJPvmqd3D9+/t/xewMnm/TSZKr1P56+VYjAAAAjAiJxAsAAASHwHyrMXjWeNF4AQAAY04urvfv1KC/rxdITDUCAAAYQuIFAACMcStCLraTAAAAQKCReAEAAGPCfXE9iRcAAIAhJF4AAMAYtyJ4ZBAAAAACj8QLAAAY47Iccll+fmSQn68XSDReAADAGFcAtpNwMdUIAACA05F4AQAAY9xWhNx+3k7CzXYSAAAAOB2JFwAAMIY1XgAAADCCxAsAABjjlv+3f3D79WqBReIFAABgCIkXAAAwJjCPDAqeHInGCwAAGOOyIuTy83YS/r5eIAVPpQAAAEGOxAsAABjjlkNu+XtxffA8q5HECwAAwBASLwAAYAxrvAAAAGAEiRcAADAmMI8MCp4cKXgqBQAACHIkXgAAwBi35ZDb348M8vP1AonECwAAwBASLwAAYIw7AGu8eGQQAABAFdxWhNx+3v7B39cLpOCpFAAAIMiReAEAAGNccsjl50f8+Pt6gUTiBQAAYAiJFwAAMIY1XgAAADCCxgsAABjj0v+t8/Lf4ZucnBylpKQoOjpa6enp2rBhw1nPX7x4sS6//HLVrl1bTZs21V133aUDBw54dU8aLwAAEHZyc3M1evRo3X///dqyZYs6deqkrl27qrCwsMrz3333XfXv31+DBw/Wp59+qmXLlik/P19Dhgzx6r40XgAAwJhTa7z8fXhr+vTpGjx4sIYMGaLU1FTNmDFDiYmJmjVrVpXnb9q0SS1atNCoUaOUkpKijh07aujQofrggw+8ui+NFwAAMMZlRQTkkKSSkhKPo7S0tMoaysrKVFBQoIyMDI/xjIwMvf/++1W+pn379tqzZ4/WrFkjy7L03Xff6e9//7u6devm1fun8QIAACEhMTFRcXFxFUd2dnaV5+3fv18ul0sJCQke4wkJCdq3b1+Vr2nfvr0WL16s3r17KyoqSk2aNFG9evX09NNPe1Uj20kAAABjLDnk9vOGp9Z/r1dUVKTY2NiKcafTedbXORyedViWVWnslO3bt2vUqFF68MEH9Zvf/EZ79+7V+PHjNWzYMM2dO7fatdJ4AQCAkBAbG+vReJ1Jo0aNFBkZWSndKi4urpSCnZKdna0OHTpo/PjxkqTLLrtMderUUadOnfToo4+qadOm1aqRqUYAAGBMINd4VVdUVJTS09OVl5fnMZ6Xl6f27dtX+ZqffvpJERGe94mMjJR0MimrLhovAAAQdsaOHas5c+Zo3rx5+uyzzzRmzBgVFhZq2LBhkqSsrCz179+/4vwePXpoxYoVmjVrlnbu3Kn33ntPo0aN0pVXXqlmzZpV+74hMdV4w7VbFFW3pt1leGXpgXZ2l+CT4XetsrsEn03d1NXuEnxy42sf2V2CT779spbdJfis5uN77S7BJ0s/WWt3CT7p3aKT3SX4bO/fL7K7BK9YP5VKz9pbg9tyyG35d42XL9fr3bu3Dhw4oEmTJmnv3r1q1aqV1qxZo+TkZEnS3r17Pfb0GjhwoA4fPqyZM2dq3Lhxqlevnq677jpNnTrVq/uGROMFAADgrczMTGVmZlb5uwULFlQaGzlypEaOHPmL7knjBQAAjHEpQi4/r3Ty9/UCicYLAAAYc65MNdoleFpEAACAIEfiBQAAjHErQm4/5z7+vl4gBU+lAAAAQY7ECwAAGOOyHHL5eU2Wv68XSCReAAAAhpB4AQAAY/hWIwAAAIwg8QIAAMZYVoTcXj7UujrXDBY0XgAAwBiXHHLJz4vr/Xy9QAqeFhEAACDIkXgBAABj3Jb/F8O7Lb9eLqBIvAAAAAwh8QIAAMa4A7C43t/XC6TgqRQAACDIkXgBAABj3HLI7edvIfr7eoFka+KVnZ2ttm3bKiYmRvHx8br55pv1n//8x86SAAAAAsbWxuudd97R8OHDtWnTJuXl5am8vFwZGRk6evSonWUBAIAAOfWQbH8fwcLWqca1a9d6/Dx//nzFx8eroKBA11xzjU1VAQCAQAn3xfXn1BqvQ4cOSZIaNGhQ5e9LS0tVWlpa8XNJSYmRugAAAPzhnGkRLcvS2LFj1bFjR7Vq1arKc7KzsxUXF1dxJCYmGq4SAAD8Em455Lb8fLC43nsjRozQtm3btHTp0jOek5WVpUOHDlUcRUVFBisEAAD4Zc6JqcaRI0dq9erVWr9+vZo3b37G85xOp5xOp8HKAACAP1kB2E7CCqLEy9bGy7IsjRw5UitXrtTbb7+tlJQUO8sBAAAIKFsbr+HDh2vJkiVatWqVYmJitG/fPklSXFycatWqZWdpAAAgAE6ty/L3NYOFrWu8Zs2apUOHDqlz585q2rRpxZGbm2tnWQAAAAFh+1QjAAAIH+zjBQAAYAhTjQAAADCCxAsAABjjDsB2EmygCgAAgEpIvAAAgDGs8QIAAIARJF4AAMAYEi8AAAAYQeIFAACMCffEi8YLAAAYE+6NF1ONAAAAhpB4AQAAYyz5f8PTYHryM4kXAACAISReAADAGNZ4AQAAwAgSLwAAYEy4J14h0Xj98+3WioiOtrsMr0QdDJ5/SP7Xf95LtbsEn3380ky7S/DJ965yu0vwyVNFN9pdgs8Sjnxpdwk+6fz4OLtL8EnpRLsr8J3zn3ZX4B2r7LjdJYS9kGi8AABAcCDxAgAAMCTcGy8W1wMAABhC4gUAAIyxLIcsPydU/r5eIJF4AQAAGELiBQAAjHHL4fdHBvn7eoFE4gUAAGAIiRcAADCGbzUCAADACBIvAABgDN9qBAAAgBEkXgAAwJhwX+NF4wUAAIxhqhEAAABGkHgBAABjrABMNZJ4AQAAoBISLwAAYIwlybL8f81gQeIFAABgCIkXAAAwxi2HHDwkGwAAAIFG4gUAAIwJ9328aLwAAIAxbsshRxjvXM9UIwAAgCEkXgAAwBjLCsB2EkG0nwSJFwAAgCEkXgAAwJhwX1xP4gUAAGAIiRcAADCGxAsAAABGkHgBAABjwn0fLxovAABgDNtJAAAAwAgSLwAAYMzJxMvfi+v9ermAIvECAAAwhMQLAAAYw3YSAAAAMILECwAAGGP99/D3NYMFiRcAAIAhJF4AAMCYcF/jReMFAADMCfO5RqYaAQAADKHxAgAA5vx3qtGfh3ycaszJyVFKSoqio6OVnp6uDRs2nPX80tJS3X///UpOTpbT6dQFF1ygefPmeXVPphoBAEDYyc3N1ejRo5WTk6MOHTroueeeU9euXbV9+3YlJSVV+ZpevXrpu+++09y5c3XhhRequLhY5eXlXt2XxgsAABgTyIdkl5SUeIw7nU45nc4qXzN9+nQNHjxYQ4YMkSTNmDFDb7zxhmbNmqXs7OxK569du1bvvPOOdu7cqQYNGkiSWrRo4XWtTDUCAICQkJiYqLi4uIqjqgZKksrKylRQUKCMjAyP8YyMDL3//vtVvmb16tVq06aNpk2bpvPOO0+/+tWv9Mc//lHHjh3zqsaQSLySH9qsGo6adpfhlVrvJNhdgk/Kboy0uwSfPXPwUrtL8MnrD1xrdwk+SSpz212Cz6w3mthdgk8aTCm1uwSffN0tuP79/b/K4oJnGwNJch+3/+t/gdxOoqioSLGxsRXjZ0q79u/fL5fLpYQEz7+LExIStG/fvipfs3PnTr377ruKjo7WypUrtX//fmVmZuqHH37wap1XSDReAAAAsbGxHo3Xz3E4PBtAy7IqjZ3idrvlcDi0ePFixcXFSTo5XXnbbbfpmWeeUa1atap1T6YaAQCAOae+hejvwwuNGjVSZGRkpXSruLi4Ugp2StOmTXXeeedVNF2SlJqaKsuytGfPnmrfm8YLAAAYc2pxvb8Pb0RFRSk9PV15eXke43l5eWrfvn2Vr+nQoYO+/fZbHTlypGJsx44dioiIUPPmzat9bxovAAAQdsaOHas5c+Zo3rx5+uyzzzRmzBgVFhZq2LBhkqSsrCz179+/4vw77rhDDRs21F133aXt27dr/fr1Gj9+vAYNGlTtaUaJNV4AAMCkc+SRQb1799aBAwc0adIk7d27V61atdKaNWuUnJwsSdq7d68KCwsrzq9bt67y8vI0cuRItWnTRg0bNlSvXr306KOPenVfGi8AABCWMjMzlZmZWeXvFixYUGns4osvrjQ96S0aLwAAYEwgt5MIBqzxAgAAMITECwAAmGX/Pq62IfECAAAwhMQLAAAYE+5rvGi8AACAOefIdhJ2YaoRAADAEBIvAABgkOO/h7+vGRxIvAAAAAwh8QIAAOawxgsAAAAmkHgBAABzSLwAAABgwjnTeGVnZ8vhcGj06NF2lwIAAALFcgTmCBLnxFRjfn6+Zs+ercsuu8zuUgAAQABZ1snD39cMFrYnXkeOHFHfvn31/PPPq379+naXAwAAEDC2N17Dhw9Xt27ddMMNN/zsuaWlpSopKfE4AABAELECdAQJW6caX3rpJX344YfKz8+v1vnZ2dl65JFHAlwVAABAYNiWeBUVFenee+/VokWLFB0dXa3XZGVl6dChQxVHUVFRgKsEAAB+xeJ6exQUFKi4uFjp6ekVYy6XS+vXr9fMmTNVWlqqyMhIj9c4nU45nU7TpQIAAPiFbY3X9ddfr48//thj7K677tLFF1+sCRMmVGq6AABA8HNYJw9/XzNY2NZ4xcTEqFWrVh5jderUUcOGDSuNAwAAhAKv13i98MILev311yt+vu+++1SvXj21b99eu3fv9mtxAAAgxIT5txq9brymTJmiWrVqSZI2btyomTNnatq0aWrUqJHGjBnzi4p5++23NWPGjF90DQAAcA5jcb13ioqKdOGFF0qSXnnlFd122236wx/+oA4dOqhz587+rg8AACBkeJ141a1bVwcOHJAkvfnmmxUbn0ZHR+vYsWP+rQ4AAISWMJ9q9Drx6tKli4YMGaLWrVtrx44d6tatmyTp008/VYsWLfxdHwAAQMjwOvF65pln1K5dO33//fdavny5GjZsKOnkvlx9+vTxe4EAACCEkHh5p169epo5c2alcR7lAwAAcHbVary2bdumVq1aKSIiQtu2bTvruZdddplfCgMAACEoEAlVqCVeaWlp2rdvn+Lj45WWliaHwyHL+r93eepnh8Mhl8sVsGIBAACCWbUar127dqlx48YV/xsAAMAngdh3K9T28UpOTq7yf5/uf1MwAAAAePL6W439+vXTkSNHKo1//fXXuuaaa/xSFAAACE2nHpLt7yNYeN14bd++XZdeeqnee++9irEXXnhBl19+uRISEvxaHAAACDFsJ+Gdf//733rggQd03XXXady4cfriiy+0du1a/fWvf9WgQYMCUSMAAEBI8LrxqlGjhh5//HE5nU5NnjxZNWrU0DvvvKN27doFoj4AAICQ4fVU44kTJzRu3DhNnTpVWVlZateunX73u99pzZo1gagPAAAgZHideLVp00Y//fST3n77bV199dWyLEvTpk3TLbfcokGDBiknJycQdQIAgBDgkP8XwwfPZhI+Nl5/+9vfVKdOHUknN0+dMGGCfvOb3+jOO+/0e4HVsfjzAsXGeB3e2eq26++wuwSfuL4I3n3c+n+9xe4SfLJoYFu7S/DJ85e/aHcJPjtqRdldgk+ee6Sz3SX4pHTGhXaX4LM69+yxuwSvlB8t1W67iwhzXjdec+fOrXI8LS1NBQUFv7ggAAAQwthA1XfHjh3TiRMnPMacTucvKggAACBUeT0/d/ToUY0YMULx8fGqW7eu6tev73EAAACcUZjv4+V143Xfffdp3bp1ysnJkdPp1Jw5c/TII4+oWbNmWrhwYSBqBAAAoSLMGy+vpxpfffVVLVy4UJ07d9agQYPUqVMnXXjhhUpOTtbixYvVt2/fQNQJAAAQ9LxOvH744QelpKRIkmJjY/XDDz9Ikjp27Kj169f7tzoAABBSeFajl84//3x9/fXXkqRLLrlEL7/8sqSTSVi9evX8WRsAAEBI8brxuuuuu7R161ZJUlZWVsVarzFjxmj8+PF+LxAAAIQQ1nh5Z8yYMRX/+9prr9Xnn3+uDz74QBdccIEuv/xyvxYHAAAQSn7RPl6SlJSUpKSkJH/UAgAAQl0gEqogSryC6zk7AAAAQewXJ14AAADVFYhvIYbktxr37AmuB4ECAIBz0KlnNfr7CBLVbrxatWqlF198MZC1AAAAhLRqN15TpkzR8OHDdeutt+rAgQOBrAkAAISqMN9OotqNV2ZmprZu3aqDBw+qZcuWWr16dSDrAgAACDleLa5PSUnRunXrNHPmTN16661KTU1VjRqel/jwww/9WiAAAAgd4b643utvNe7evVvLly9XgwYN1LNnz0qNFwAAAKrmVdf0/PPPa9y4cbrhhhv0ySefqHHjxoGqCwAAhKIw30C12o3XjTfeqM2bN2vmzJnq379/IGsCAAAISdVuvFwul7Zt26bmzZsHsh4AABDKArDGKyQTr7y8vEDWAQAAwkGYTzXyrEYAAABD+EoiAAAwh8QLAAAAJpB4AQAAY8J9A1USLwAAAENovAAAAAyh8QIAADCENV4AAMCcMP9WI40XAAAwhsX1AAAAMILECwAAmBVECZW/kXgBAAAYQuIFAADMCfPF9SReAAAAhpB4AQAAY/hWIwAAAIwg8QIAAOaE+RovGi8AAGAMU40AAAAwgsQLAACYE+ZTjSReAAAAhpB4AQAAc0i8AAAAYAKNFwAAMObUtxr9ffgiJydHKSkpio6OVnp6ujZs2FCt17333nuqUaOG0tLSvL5nSEw19r3kStVw1LS7DK/c9dlbdpfgk9/HHLS7BJ/9tmUPu0vwydFHY+wuwSd3Hhtsdwk+c+932l2CTyJKg/O/pS/8+rDdJfjM9VAju0vwirv8uN0lnDNyc3M1evRo5eTkqEOHDnruuefUtWtXbd++XUlJSWd83aFDh9S/f39df/31+u6777y+b3D+KQUAAMHJCtDhpenTp2vw4MEaMmSIUlNTNWPGDCUmJmrWrFlnfd3QoUN1xx13qF27dt7fVDReAADApAA2XiUlJR5HaWlplSWUlZWpoKBAGRkZHuMZGRl6//33z1j6/Pnz9dVXX+mhhx7y5Z1LovECAAAhIjExUXFxcRVHdnZ2left379fLpdLCQkJHuMJCQnat29fla/54osvNHHiRC1evFg1avi+Uisk1ngBAIDgEMhHBhUVFSk2NrZi3Ok8+3pNh8Ph8bNlWZXGJMnlcumOO+7QI488ol/96le/qFYaLwAAEBJiY2M9Gq8zadSokSIjIyulW8XFxZVSMEk6fPiwPvjgA23ZskUjRoyQJLndblmWpRo1aujNN9/UddddV60aabwAAIA558AGqlFRUUpPT1deXp5+97vfVYzn5eWpZ8+elc6PjY3Vxx9/7DGWk5OjdevW6e9//7tSUlKqfW8aLwAAEHbGjh2rfv36qU2bNmrXrp1mz56twsJCDRs2TJKUlZWlb775RgsXLlRERIRatWrl8fr4+HhFR0dXGv85NF4AAMCYQK7x8kbv3r114MABTZo0SXv37lWrVq20Zs0aJScnS5L27t2rwsJC/xYqGi8AABCmMjMzlZmZWeXvFixYcNbXPvzww3r44Ye9vieNFwAAMOccWONlJxovAABgTpg3XmygCgAAYAiJFwAAMMbx38Pf1wwWJF4AAACGkHgBAABzWOMFAAAAE0i8AACAMefKBqp2IfECAAAwxPbG65tvvtGdd96phg0bqnbt2kpLS1NBQYHdZQEAgECwAnQECVunGg8ePKgOHTro2muv1T/+8Q/Fx8frq6++Ur169ewsCwAABFIQNUr+ZmvjNXXqVCUmJmr+/PkVYy1atLCvIAAAgACydapx9erVatOmjW6//XbFx8erdevWev755894fmlpqUpKSjwOAAAQPE4trvf3ESxsbbx27typWbNm6aKLLtIbb7yhYcOGadSoUVq4cGGV52dnZysuLq7iSExMNFwxAACA72xtvNxut6644gpNmTJFrVu31tChQ3X33Xdr1qxZVZ6flZWlQ4cOVRxFRUWGKwYAAL9ImC+ut7Xxatq0qS655BKPsdTUVBUWFlZ5vtPpVGxsrMcBAAAQLGxdXN+hQwf95z//8RjbsWOHkpOTbaoIAAAEEhuo2mjMmDHatGmTpkyZoi+//FJLlizR7NmzNXz4cDvLAgAACAhbG6+2bdtq5cqVWrp0qVq1aqXJkydrxowZ6tu3r51lAQCAQAnzNV62P6uxe/fu6t69u91lAAAABJztjRcAAAgf4b7Gi8YLAACYE4ipwSBqvGx/SDYAAEC4IPECAADmkHgBAADABBIvAABgTLgvrifxAgAAMITECwAAmMMaLwAAAJhA4gUAAIxxWJYcln8jKn9fL5BovAAAgDlMNQIAAMAEEi8AAGAM20kAAADACBIvAABgDmu8AAAAYEJIJF4/dW+tGjWj7S7DK399pK3dJfjkiT4/2F2Cz5rUPmJ3CT658cqtdpfgk6+GX2R3CT7bc32k3SX45HiC2+4SfOL45Eu7S/BZzaTz7C7BKw5Xqd0lsMbL7gIAAADCRUgkXgAAIEiE+RovGi8AAGAMU40AAAAwgsQLAACYE+ZTjSReAAAAhpB4AQAAo4JpTZa/kXgBAAAYQuIFAADMsayTh7+vGSRIvAAAAAwh8QIAAMaE+z5eNF4AAMActpMAAACACSReAADAGIf75OHvawYLEi8AAABDSLwAAIA5rPECAACACSReAADAmHDfToLECwAAwBASLwAAYE6YPzKIxgsAABjDVCMAAACMIPECAADmsJ0EAAAATCDxAgAAxrDGCwAAAEaQeAEAAHPCfDsJEi8AAABDSLwAAIAx4b7Gi8YLAACYw3YSAAAAMIHECwAAGBPuU40kXgAAAIaQeAEAAHPc1snD39cMEiReAAAAhpB4AQAAc/hWIwAAAEwg8QIAAMY4FIBvNfr3cgFF4wUAAMzhWY0AAAAwgcQLAAAYwwaqAAAAMILECwAAmMN2EgAAADCBxAsAABjjsCw5/PwtRH9fL5BCovFqO75Azro17S7DK9tvTbK7BN9stLsA3313Y7LdJfhk5+st7C7BJymff2J3CT4rG32+3SX4xGEF025G/+e7l4Pzz6YkPXvpIrtL8MrRw269dandVYS3kGi8AABAkHD/9/D3NYMEjRcAADAm3KcaWVwPAADCUk5OjlJSUhQdHa309HRt2LDhjOeuWLFCXbp0UePGjRUbG6t27drpjTfe8PqeNF4AAMAcK0CHl3JzczV69Gjdf//92rJlizp16qSuXbuqsLCwyvPXr1+vLl26aM2aNSooKNC1116rHj16aMuWLV7dl8YLAACEnenTp2vw4MEaMmSIUlNTNWPGDCUmJmrWrFlVnj9jxgzdd999atu2rS666CJNmTJFF110kV599VWv7kvjBQAAzDn1kGx/H5JKSko8jtLS0ipLKCsrU0FBgTIyMjzGMzIy9P7771frbbjdbh0+fFgNGjTw6u3TeAEAgJCQmJiouLi4iiM7O7vK8/bv3y+Xy6WEhASP8YSEBO3bt69a9/rLX/6io0ePqlevXl7VyLcaAQCAMYF8SHZRUZFiY2Mrxp1O59lf5/Dc+86yrEpjVVm6dKkefvhhrVq1SvHx8V7VSuMFAABCQmxsrEfjdSaNGjVSZGRkpXSruLi4Ugp2utzcXA0ePFjLli3TDTfc4HWNTDUCAABzArjGq7qioqKUnp6uvLw8j/G8vDy1b9/+jK9bunSpBg4cqCVLlqhbt24+vX0SLwAAEHbGjh2rfv36qU2bNmrXrp1mz56twsJCDRs2TJKUlZWlb775RgsXLpR0sunq37+//vrXv+rqq6+uSMtq1aqluLi4at+XxgsAABjjcJ88/H1Nb/Xu3VsHDhzQpEmTtHfvXrVq1Upr1qxRcvLJZ4fu3bvXY0+v5557TuXl5Ro+fLiGDx9eMT5gwAAtWLCg2vel8QIAAOb4MDVYrWv6IDMzU5mZmVX+7vRm6u233/bpHqdjjRcAAIAhJF4AAMAcHx/x87PXDBIkXgAAAIaQeAEAAGMcliWHn9d4+ft6gUTiBQAAYAiJFwAAMOcc+lajHWxNvMrLy/XAAw8oJSVFtWrV0vnnn69JkybJ7fbzBh8AAADnAFsTr6lTp+rZZ5/VCy+8oJYtW+qDDz7QXXfdpbi4ON177712lgYAAALBkuTvfCV4Ai97G6+NGzeqZ8+eFc87atGihZYuXaoPPvigyvNLS0tVWlpa8XNJSYmROgEAgH+wuN5GHTt21FtvvaUdO3ZIkrZu3ap3331Xv/3tb6s8Pzs7W3FxcRVHYmKiyXIBAAB+EVsTrwkTJujQoUO6+OKLFRkZKZfLpccee0x9+vSp8vysrCyNHTu24ueSkhKaLwAAgomlACyu9+/lAsnWxis3N1eLFi3SkiVL1LJlS3300UcaPXq0mjVrpgEDBlQ63+l0yul02lApAADAL2dr4zV+/HhNnDhRv//97yVJl156qXbv3q3s7OwqGy8AABDk2E7CPj/99JMiIjxLiIyMZDsJAAAQkmxNvHr06KHHHntMSUlJatmypbZs2aLp06dr0KBBdpYFAAACxS3JEYBrBglbG6+nn35af/7zn5WZmani4mI1a9ZMQ4cO1YMPPmhnWQAAAAFha+MVExOjGTNmaMaMGXaWAQAADAn3fbx4ViMAADCHxfUAAAAwgcQLAACYQ+IFAAAAE0i8AACAOSReAAAAMIHECwAAmBPmG6iSeAEAABhC4gUAAIxhA1UAAABTWFwPAAAAE0i8AACAOW5Lcvg5oXKTeAEAAOA0JF4AAMAc1ngBAADABBIvAABgUAASLwVP4hUSjdc7C9oqMira7jK8UjI8eP4h+V99u2ywuwSfLf0szu4SfNJwVW27S/DJHwo+tLsEn81u77S7BJ8cvTrF7hJ88s3/q293CT67x9HX7hK84vqpVNJf7C4jrIVE4wUAAIJEmK/xovECAADmuC35fWqQ7SQAAABwOhIvAABgjuU+efj7mkGCxAsAAMAQEi8AAGBOmC+uJ/ECAAAwhMQLAACYw7caAQAAYAKJFwAAMCfM13jReAEAAHMsBaDx8u/lAompRgAAAENIvAAAgDlhPtVI4gUAAGAIiRcAADDH7Zbk50f8uHlkEAAAAE5D4gUAAMxhjRcAAABMIPECAADmhHniReMFAADM4VmNAAAAMIHECwAAGGNZblmWf7d/8Pf1AonECwAAwBASLwAAYI5l+X9NVhAtrifxAgAAMITECwAAmGMF4FuNJF4AAAA4HYkXAAAwx+2WHH7+FmIQfauRxgsAAJjDVCMAAABMIPECAADGWG63LD9PNbKBKgAAACoh8QIAAOawxgsAAAAmkHgBAABz3JbkIPECAABAgJF4AQAAcyxLkr83UCXxAgAAwGlIvAAAgDGW25Ll5zVeVhAlXjReAADAHMst/081soEqAAAATkPiBQAAjAn3qUYSLwAAAENIvAAAgDlhvsYrqBuvU9Giq+y4zZV4z308eGLR/1V65ITdJfjM/VPw/XMiSa4TwRlM/3TYZXcJPit3l9ldgk/KTwTnP+Pu48H5z7gkuX4qtbsEr5yq186puXKd8PujGssVPH83Oaxgmhg9zZ49e5SYmGh3GQAABJWioiI1b97c6D2PHz+ulJQU7du3LyDXb9KkiXbt2qXo6OiAXN9fgrrxcrvd+vbbbxUTEyOHw+HXa5eUlCgxMVFFRUWKjY3167VRNT5zs/i8zeLzNo/PvDLLsnT48GE1a9ZMERHmk8bjx4+rrCwwiXJUVNQ533RJQT7VGBEREfCOPTY2lj+whvGZm8XnbRaft3l85p7i4uJsu3d0dHRQNEeBFLwT6wAAAEGGxgsAAMAQGq8zcDqdeuihh+R0Ou0uJWzwmZvF520Wn7d5fOY4FwX14noAAIBgQuIFAABgCI0XAACAITReAAAAhtB4AQAAGELjdQY5OTlKSUlRdHS00tPTtWHDBrtLCknZ2dlq27atYmJiFB8fr5tvvln/+c9/7C4rbGRnZ8vhcGj06NF2lxLSvvnmG915551q2LChateurbS0NBUUFNhdVkgqLy/XAw88oJSUFNWqVUvnn3++Jk2aJLc7eB6ijNBG41WF3NxcjR49Wvfff7+2bNmiTp06qWvXriosLLS7tJDzzjvvaPjw4dq0aZPy8vJUXl6ujIwMHT161O7SQl5+fr5mz56tyy67zO5SQtrBgwfVoUMH1axZU//4xz+0fft2/eUvf1G9evXsLi0kTZ06Vc8++6xmzpypzz77TNOmTdMTTzyhp59+2u7SAElsJ1Glq666SldccYVmzZpVMZaamqqbb75Z2dnZNlYW+r7//nvFx8frnXfe0TXXXGN3OSHryJEjuuKKK5STk6NHH31UaWlpmjFjht1lhaSJEyfqvffeIzU3pHv37kpISNDcuXMrxm699VbVrl1bL774oo2VASeReJ2mrKxMBQUFysjI8BjPyMjQ+++/b1NV4ePQoUOSpAYNGthcSWgbPny4unXrphtuuMHuUkLe6tWr1aZNG91+++2Kj49X69at9fzzz9tdVsjq2LGj3nrrLe3YsUOStHXrVr377rv67W9/a3NlwElB/ZDsQNi/f79cLpcSEhI8xhMSErRv3z6bqgoPlmVp7Nix6tixo1q1amV3OSHrpZde0ocffqj8/Hy7SwkLO3fu1KxZszR27Fj96U9/0ubNmzVq1Cg5nU7179/f7vJCzoQJE3To0CFdfPHFioyMlMvl0mOPPaY+ffrYXRogicbrjBwOh8fPlmVVGoN/jRgxQtu2bdO7775rdykhq6ioSPfee6/efPNNRUdH211OWHC73WrTpo2mTJkiSWrdurU+/fRTzZo1i8YrAHJzc7Vo0SItWbJELVu21EcffaTRo0erWbNmGjBggN3lATRep2vUqJEiIyMrpVvFxcWVUjD4z8iRI7V69WqtX79ezZs3t7uckFVQUKDi4mKlp6dXjLlcLq1fv14zZ85UaWmpIiMjbaww9DRt2lSXXHKJx1hqaqqWL19uU0Whbfz48Zo4caJ+//vfS5IuvfRS7d69W9nZ2TReOCewxus0UVFRSk9PV15ensd4Xl6e2rdvb1NVocuyLI0YMUIrVqzQunXrlJKSYndJIe3666/Xxx9/rI8++qjiaNOmjfr27auPPvqIpisAOnToUGmLlB07dig5OdmmikLbTz/9pIgIz7/aIiMj2U4C5wwSryqMHTtW/fr1U5s2bdSuXTvNnj1bhYWFGjZsmN2lhZzhw4dryZIlWrVqlWJiYiqSxri4ONWqVcvm6kJPTExMpfVzderUUcOGDVlXFyBjxoxR+/btNWXKFPXq1UubN2/W7NmzNXv2bLtLC0k9evTQY489pqSkJLVs2VJbtmzR9OnTNWjQILtLAySxncQZ5eTkaNq0adq7d69atWqlp556iu0NAuBM6+bmz5+vgQMHmi0mTHXu3JntJALstddeU1ZWlr744gulpKRo7Nixuvvuu+0uKyQdPnxYf/7zn7Vy5UoVFxerWbNm6tOnjx588EFFRUXZXR5A4wUAAGAKa7wAAAAMofECAAAwhMYLAADAEBovAAAAQ2i8AAAADKHxAgAAMITGCwAAwBAaLwAAAENovADYzuFw6JVXXrG7DAAIOBovAHK5XGrfvr1uvfVWj/FDhw4pMTFRDzzwQEDvv3fvXnXt2jWg9wCAcwGPDAIgSfriiy+Ulpam2bNnq2/fvpKk/v37a+vWrcrPz+c5dwDgByReACRJF110kbKzszVy5Eh9++23WrVqlV566SW98MILZ226Fi1apDZt2igmJkZNmjTRHXfcoeLi4orfT5o0Sc2aNdOBAwcqxm666SZdc801crvdkjynGsvKyjRixAg1bdpU0dHRatGihbKzswPzpgHAMBIvABUsy9J1112nyMhIffzxxxo5cuTPTjPOmzdPTZs21a9//WsVFxdrzJgxql+/vtasWSPp5DRmp06dlJCQoJUrV+rZZ5/VxIkTtXXrViUnJ0s62XitXLlSN998s5588kn97W9/0+LFi5WUlKSioiIVFRWpT58+AX//ABBoNF4APHz++edKTU3VpZdeqg8//FA1atTw6vX5+fm68sordfjwYdWtW1eStHPnTqWlpSkzM1NPP/20x3Sm5Nl4jRo1Sp9++qn++c9/yuFw+PW9AYDdmGoE4GHevHmqXbu2du3apT179vzs+Vu2bFHPnj2VnJysmJgYde7cWZJUWFhYcc7555+vJ598UlOnTlWPHj08mq7TDRw4UB999JF+/etfa9SoUXrzzTd/8XsCgHMFjReAChs3btRTTz2lVatWqV27dho8eLDOFoofPXpUGRkZqlu3rhYtWqT8/HytXLlS0sm1Wv9r/fr1ioyM1Ndff63y8vIzXvOKK67Qrl27NHnyZB07dky9evXSbbfd5p83CAA2o/ECIEk6duyYBgwYoKFDh+qGG27QnDlzlJ+fr+eee+6Mr/n888+1f/9+Pf744+rUqZMuvvhij4X1p+Tm5mrFihV6++23VVRUpMmTJ5+1ltjYWPXu3VvPP/+8cnNztXz5cv3www+/+D0CgN1ovABIkiZOnCi3262pU6dKkpKSkvSXv/xF48eP19dff13la5KSkhQVFaWnn35aO3fu1OrVqys1VXv27NE999yjqVOnqmPHjlqwYIGys7O1adOmKq/51FNP6aWXXtLnn3+uHTt2aNmyZWrSpInq1avnz7cLALag8QKgd955R88884wWLFigOnXqVIzffffdat++/RmnHBs3bqwFCxZo2bJluuSSS/T444/rySefrPi9ZVkaOHCgrrzySo0YMUKS1KVLF40YMUJ33nmnjhw5UumadevW1dSpU9WmTRu1bdtWX3/9tdasWaOICP51BSD48a1GAAAAQ/hPSAAAAENovAAAAAyh8QIAADCExgsAAMAQGi8AAABDaLwAAAAMofECAAAwhMYLAADAEBovAAAAQ2i8AAAADKHxAgAAMOT/Az8TaVc5Mxp0AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        self.conv_depthwise = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                        kernel_size=self.receptive_size, \n",
    "                                        stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.conv_depthwise.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.conv_depthwise.bias, 0)\n",
    "\n",
    "        # membrane potential 초기화\n",
    "        self.v = torch.full((self.in_channel, self.in_spike_size, self.in_spike_size), fill_value=self.v_init, requires_grad=False)\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "            recurrent_current = self.conv_depthwise(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*v + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NO_RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1):\n",
    "        super(NO_RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight)\n",
    "        self.linear = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight)\n",
    "        self.linear = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE, TIME, dvs_duration, dvs_clipping):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    elif (which_data == 'DVS_GESTURE'):\n",
    "        data_dir = data_path + '/gesture'\n",
    "        transform = None\n",
    "\n",
    "        # # spikingjelly.datasets.dvs128_gesture.DVS128Gesture(root: str, train: bool, use_frame=True, frames_num=10, split_by='number', normalization='max')\n",
    "       \n",
    "        #https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based_en/neuromorphic_datasets.html\n",
    "        # 10ms마다 1개의 timestep하고 싶으면 위의 주소 참고. 근데 timestep이 각각 좀 다를 거임.\n",
    "\n",
    "        \n",
    "        if dvs_duration > 0:\n",
    "            resize_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(\n",
    "                data_dir, train=False, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "\n",
    "        else:\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(data_dir, train=False,\n",
    "                                            data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        ## 11번째 클래스 배제 ########################################################################\n",
    "        exclude_class = 10\n",
    "        if dvs_duration > 0:\n",
    "            train_file_name = f'modules/dvs_gesture_class_index/train_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            test_file_name = f'modules/dvs_gesture_class_index/test_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            if (os.path.isfile(train_file_name) and os.path.isfile(test_file_name)):\n",
    "                print('\\ndvsgestrue 10th exclude class indices exist\\n')\n",
    "                with open(train_file_name, 'rb') as f:\n",
    "                    train_indices = pickle.load(f)\n",
    "                with open(test_file_name, 'rb') as f:\n",
    "                    test_indices = pickle.load(f)\n",
    "            else:\n",
    "                print('\\ndvsgestrue 10th exclude class indices doesn\\'t exist\\n')\n",
    "                train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "                test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "                with open(train_file_name, 'wb') as f:\n",
    "                    pickle.dump(train_indices, f)\n",
    "                with open(test_file_name, 'wb') as f:\n",
    "                    pickle.dump(test_indices, f)\n",
    "        else:\n",
    "            train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "            test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "        ################################################################################################\n",
    "            \n",
    "        # SubsetRandomSampler 생성\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "        # ([B, T, 2, 128, 128]) \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, sampler=train_sampler, collate_fn=pad_sequence_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, sampler=test_sampler, collate_fn=pad_sequence_collate)\n",
    "        synapse_conv_in_channels = 2\n",
    "        CLASS_NUM = 10\n",
    "        # mapping = { 0 :'Hand Clapping'  1 :'Right Hand Wave'2 :'Left Hand Wave' 3 :'Right Arm CW'   4 :'Right Arm CCW'  5 :'Left Arm CW'    6 :'Left Arm CCW'   7 :'Arm Roll'       8 :'Air Drums'      9 :'Air Guitar'     10:'Other'}\n",
    "\n",
    "\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterator = enumerate(train_loader, 0)\n",
    "    for i, data in iterator:\n",
    "    # for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if len(data) == 2:\n",
    "            inputs, labels = data\n",
    "            # 처리 로직 작성\n",
    "        elif len(data) == 3:\n",
    "            inputs, labels, x_len = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # if rate_coding == True:\n",
    "        #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        # else:\n",
    "        #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        \n",
    "\n",
    "        ###########################################################################################################################        \n",
    "        if (which_data == 'n_tidigits'):\n",
    "            inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "            labels = labels[:, 0, :]\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        elif (which_data == 'heidelberg'):\n",
    "            inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "            print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "        # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "        # print(labels)\n",
    "            \n",
    "        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        elif rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "        ####################################################################################################################### \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        # if i % 100 == 99:\n",
    "        # print(f\"[{i+1}] loss: {running_loss / 100:.3f}\")\n",
    "        # running_loss = 0.0\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "    tr_accuracy = 100 * correct / total         \n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    iterator = enumerate(test_loader, 0)\n",
    "    with torch.no_grad():\n",
    "        for i, data in iterator:\n",
    "        # for inputs, labels in test_loader:\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # if rate_coding == True:\n",
    "            #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            # else:\n",
    "            #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "\n",
    "        \n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST', gpu = '3',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "    # run = wandb.init(project=f'reservoir')\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "    wandb.run.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME=TIME_STEP, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if no_reservoir == True:\n",
    "        net = NO_RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight)\n",
    "    else:\n",
    "        net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP, which_data)\n",
    "        # torch.save(net.state_dict(), 'net_save/reservoir_net.pth')\n",
    "        # artifact = wandb.Artifact('model', type='model')\n",
    "        # artifact.add_file('net_save/reservoir_net.pth')\n",
    "        # run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하기 싫을 때\n",
    "# wandb.init(project=f'reservoir')\n",
    "# main(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: n3xkztes\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/n3xkztes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p0avikjt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 48\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.3921067525146739\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.021135257068920065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 5.203754526789743\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_122728-p0avikjt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/p0avikjt' target=\"_blank\">effortless-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/n3xkztes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/n3xkztes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/n3xkztes' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/n3xkztes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/p0avikjt' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/p0avikjt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [/data2/gesture/duration_100000] already exists.\n",
      "The directory [/data2/gesture/duration_100000] already exists.\n",
      "\n",
      "dvsgestrue 10th exclude class indices exist\n",
      "\n",
      "RESERVOIR_NET(\n",
      "  (reservoir): RESERVOIR(\n",
      "    (conv_depthwise): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)\n",
      "  )\n",
      "  (linear): Linear(in_features=4608, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "Train Accuracy: 33.58%\n",
      "Test loss: 3.512, Val Accuracy: 33.33%\n",
      "Epoch 2\n"
     ]
    }
   ],
   "source": [
    "# sweep하고싶을 때\n",
    "def sweep_cover(data_path='/data2', which_data='CIFAR10', gpu = '4', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False):\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.batch_size\n",
    "    TIME_STEP  =  wandb.config.time_step\n",
    "    v_decay  =  wandb.config.decay\n",
    "    pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "    which_data  =  wandb.config.which_data\n",
    "    data_path  =  wandb.config.data_path\n",
    "    rate_coding  =  wandb.config.rate_coding\n",
    "    EPOCH  =  wandb.config.EPOCH\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    no_reservoir  =  wandb.config.no_reservoir\n",
    "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping, no_reservoir = no_reservoir)\n",
    "\n",
    "\n",
    "\n",
    "which_data_hyper = 'DVS_GESTURE' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST', 'DVS_GESTURE'\n",
    "data_path_hyper = '/data2'\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': which_data_hyper,\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"min\": 0.00001, \"max\": 0.1},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64, 128, 256]},\n",
    "        \"time_step\": {\"values\": [4,5,6,7,8,9,10,11,12]},\n",
    "        \"decay\": {\"min\": 0.25, \"max\": 1.0},\n",
    "        \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "        \"which_data\": {\"values\": [which_data_hyper]},\n",
    "        \"data_path\": {\"values\": [data_path_hyper]},\n",
    "        \"rate_coding\": {\"values\": [True, False]},\n",
    "        \"EPOCH\": {\"values\": [20]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [16,32,48,128]},\n",
    "        \"dvs_duration\": {\"values\": [100000]},\n",
    "        \"dvs_clipping\": {\"values\": [True]},\n",
    "        \"no_reservoir\": {\"values\": [True, False]},\n",
    "     }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "wandb.agent(sweep_id, function=sweep_cover, count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE하기\n",
    "\n",
    "# # Import\n",
    "# import wandb\n",
    "# # Save your model.\n",
    "# torch.save(model.state_dict(), 'save/to/path/model.pth')\n",
    "# # Save as artifact for version control.\n",
    "# run = wandb.init(project='your-project-name')\n",
    "# artifact = wandb.Artifact('model', type='model')\n",
    "# artifact.add_file('save/to/path/model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()\n",
    "\n",
    "\n",
    "# # LOAD 하기\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "\n",
    "\n",
    "# artifact = run.use_artifact('entity/your-project-name/model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
