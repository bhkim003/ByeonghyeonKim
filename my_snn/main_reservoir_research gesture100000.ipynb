{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.7834769413661389\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:32\n",
    "# learning_rate:0.007176761798504128\n",
    "# pre_spike_weight:5.165214142219577\n",
    "# rate_coding:true\n",
    "# TIME_STEP:9\n",
    "# time_step:9\n",
    "# v_decay:0.7834769413661389\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"CIFAR10\"\n",
    "\n",
    "\n",
    "# BATCH:256\n",
    "# batch_size:256\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.38993471232202725\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.06285718352377828\n",
    "# pre_spike_weight:6.21970124592063\n",
    "# rate_coding:true\n",
    "# TIME_STEP:16\n",
    "# time_step:16\n",
    "# v_decay:0.38993471232202725\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"MNIST\"\n",
    "\n",
    "# BATCH:64\n",
    "# batch_size:64\n",
    "# data_path:\"/data2\"\n",
    "# decay:0.9266077968579136\n",
    "# EPOCH:20\n",
    "# hard_reset:true\n",
    "# IMAGE_SIZE:28\n",
    "# learning_rate:0.07732456724854177\n",
    "# pre_spike_weight:1.5377416716615555\n",
    "# rate_coding:true\n",
    "# TIME_STEP:7\n",
    "# time_step:7\n",
    "# v_decay:0.9266077968579136\n",
    "# v_reset:0\n",
    "# v_threshold:1\n",
    "# which_data:\"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Byeonghyeon Kim \n",
    "# github site: https://github.com/bhkim003/ByeonghyeonKim\n",
    "# email: bhkim003@snu.ac.kr\n",
    " \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to\n",
    "# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "# the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    " \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    " \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    " \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# my module import\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class RESERVOIR(nn.Module):\n",
    "    def __init__ (self, TIME_STEP=8, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1):\n",
    "        super(RESERVOIR, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.in_spike_size = in_spike_size\n",
    "        self.in_channel = in_channel\n",
    "        self.receptive_size = receptive_size #3\n",
    "        self.v_init = v_init\n",
    "        self.v_decay = v_decay\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.hard_reset = hard_reset\n",
    "        self.pre_spike_weight = pre_spike_weight\n",
    "\n",
    "        self.out_channel = 1\n",
    "\n",
    "        # 파라미터 \n",
    "        self.conv_depthwise = nn.Conv2d(in_channels=self.in_channel, out_channels=self.in_channel, \n",
    "                                        kernel_size=self.receptive_size, \n",
    "                                        stride=1, padding=1, groups=self.in_channel)\n",
    "\n",
    "        # kaiming 초기화\n",
    "        nn.init.kaiming_normal_(self.conv_depthwise.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.conv_depthwise.bias, 0)\n",
    "\n",
    "        # membrane potential 초기화\n",
    "        self.v = torch.full((self.in_channel, self.in_spike_size, self.in_spike_size), fill_value=self.v_init, requires_grad=False)\n",
    "\n",
    "        \n",
    "    def forward(self, pre_spike):    \n",
    "        # pre_spike [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        v = torch.full_like(pre_spike[0], fill_value=self.v_init, requires_grad=False)\n",
    "        post_spike = torch.zeros_like(pre_spike[0], requires_grad=False)\n",
    "        # v [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "        # recurrent [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # timestep 안 맞으면 종료\n",
    "        assert pre_spike.size(0) == self.TIME_STEP, f\"Time step mismatch: {pre_spike.size(0)} vs {self.TIME_STEP}\"\n",
    "\n",
    "        output = []\n",
    "        for t in range (self.TIME_STEP):\n",
    "            # pre_spike[t] [batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "            input_current = self.pre_spike_weight * pre_spike[t]\n",
    "            recurrent_current = self.conv_depthwise(post_spike)\n",
    "            current = input_current + recurrent_current\n",
    "            # current [batch_size, in_channel, in_spike_size, in_spike_size] # kernel size 3이니까 사이즈 유지\n",
    "            \n",
    "            # decay and itegrate\n",
    "            v = v*self.v_decay + current\n",
    "\n",
    "            # post spike\n",
    "            post_spike = (v >= self.v_threshold).float()\n",
    "\n",
    "            output.append(post_spike)\n",
    "            \n",
    "            #reset\n",
    "            if self.hard_reset: # hard reset\n",
    "                v = (1 - post_spike)*v + post_spike*self.v_reset \n",
    "            else: # soft reset\n",
    "                v = v - post_spike*self.v_threshold\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NO_RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1):\n",
    "        super(NO_RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight)\n",
    "        self.linear = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESERVOIR_NET(nn.Module):\n",
    "    def __init__(self, TIME_STEP=8, CLASS_NUM=10, in_spike_size=28, in_channel=1, receptive_size=3, v_init=0, v_decay=0.6, v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1):\n",
    "        super(RESERVOIR_NET, self).__init__()\n",
    "        self.TIME_STEP = TIME_STEP\n",
    "        self.reservoir = RESERVOIR(TIME_STEP = self.TIME_STEP, in_spike_size=in_spike_size, in_channel=in_channel, receptive_size=receptive_size, v_init=v_init, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight)\n",
    "        self.linear = nn.Linear(in_features=in_channel*in_spike_size*in_spike_size, out_features=CLASS_NUM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x size [batch_size, TIME_STEP, in_channel, in_spike_size, in_spike_size]\n",
    "        x = x.permute(1,0,2,3,4)\n",
    "        # x size [TIME_STEP, batch_size, in_channel, in_spike_size, in_spike_size]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.reservoir(x) # reservoir weight는 학습 안함\n",
    "\n",
    "        T, B, *spatial_dims = x.shape\n",
    "        x = x.reshape(T * B, -1) # time,batch 축은 합쳐서 FC에 삽입\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        x = x.view(T , B, -1).contiguous() \n",
    "        \n",
    "        x = x.mean(dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(which_data, data_path, rate_coding, BATCH, IMAGE_SIZE, TIME, dvs_duration, dvs_clipping):\n",
    "    if which_data == 'MNIST':\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0,), (1,))])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    elif (which_data == 'CIFAR10'):\n",
    "\n",
    "        if rate_coding :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor()])\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor()])\n",
    "            \n",
    "            transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor()])\n",
    "                                            # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                                transforms.ToTensor()])\n",
    "        \n",
    "        else :\n",
    "            # transform_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.RandomHorizontalFlip(),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "            # transform_test = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            #                                     transforms.ToTensor(),\n",
    "            #                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),])\n",
    "            #                                 # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            \n",
    "            # assert IMAGE_SIZE == 32, 'OTTT랑 맞짱뜰 때는 32로 ㄱ'\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(IMAGE_SIZE, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                    (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform_test)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        \n",
    "        synapse_conv_in_channels = 3\n",
    "        CLASS_NUM = 10\n",
    "        '''\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "        '''\n",
    "\n",
    "\n",
    "    elif (which_data == 'FASHION_MNIST'):\n",
    "\n",
    "        if rate_coding :\n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor()])\n",
    "        else : \n",
    "            transform = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=True,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "\n",
    "        testset = torchvision.datasets.FashionMNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "\n",
    "        train_loader = DataLoader(trainset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = True,\n",
    "                                num_workers =2)\n",
    "        test_loader = DataLoader(testset,\n",
    "                                batch_size =BATCH,\n",
    "                                shuffle = False,\n",
    "                                num_workers =2)\n",
    "        synapse_conv_in_channels = 1\n",
    "        CLASS_NUM = 10\n",
    "    elif (which_data == 'DVS_GESTURE'):\n",
    "        data_dir = data_path + '/gesture'\n",
    "        transform = None\n",
    "\n",
    "        # # spikingjelly.datasets.dvs128_gesture.DVS128Gesture(root: str, train: bool, use_frame=True, frames_num=10, split_by='number', normalization='max')\n",
    "       \n",
    "        #https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based_en/neuromorphic_datasets.html\n",
    "        # 10ms마다 1개의 timestep하고 싶으면 위의 주소 참고. 근데 timestep이 각각 좀 다를 거임.\n",
    "\n",
    "        \n",
    "        if dvs_duration > 0:\n",
    "            resize_shape = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(\n",
    "                data_dir, train=False, data_type='frame',  split_by='time',  duration=dvs_duration, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "\n",
    "        else:\n",
    "            train_data = CustomDVS128Gesture(\n",
    "                data_dir, train=True, data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "            test_data = CustomDVS128Gesture(data_dir, train=False,\n",
    "                                            data_type='frame', split_by='number', frames_number=TIME, resize_shape=resize_shape, dvs_clipping=dvs_clipping, dvs_duration_copy=dvs_duration, TIME=TIME)\n",
    "        ## 11번째 클래스 배제 ########################################################################\n",
    "        exclude_class = 10\n",
    "        if dvs_duration > 0:\n",
    "            train_file_name = f'modules/dvs_gesture_class_index/train_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            test_file_name = f'modules/dvs_gesture_class_index/test_indices_dvsgesture_duration_{dvs_duration}'\n",
    "            if (os.path.isfile(train_file_name) and os.path.isfile(test_file_name)):\n",
    "                print('\\ndvsgestrue 10th exclude class indices exist\\n')\n",
    "                with open(train_file_name, 'rb') as f:\n",
    "                    train_indices = pickle.load(f)\n",
    "                with open(test_file_name, 'rb') as f:\n",
    "                    test_indices = pickle.load(f)\n",
    "            else:\n",
    "                print('\\ndvsgestrue 10th exclude class indices doesn\\'t exist\\n')\n",
    "                train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "                test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "                with open(train_file_name, 'wb') as f:\n",
    "                    pickle.dump(train_indices, f)\n",
    "                with open(test_file_name, 'wb') as f:\n",
    "                    pickle.dump(test_indices, f)\n",
    "        else:\n",
    "            train_indices = [i for i, (_, target) in enumerate(train_data) if target != exclude_class]\n",
    "            test_indices = [i for i, (_, target) in enumerate(test_data) if target != exclude_class]\n",
    "        ################################################################################################\n",
    "            \n",
    "        # SubsetRandomSampler 생성\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "        # ([B, T, 2, 128, 128]) \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH, num_workers=2, sampler=train_sampler, collate_fn=pad_sequence_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH, num_workers=2, sampler=test_sampler, collate_fn=pad_sequence_collate)\n",
    "        synapse_conv_in_channels = 2\n",
    "        CLASS_NUM = 10\n",
    "        # mapping = { 0 :'Hand Clapping'  1 :'Right Hand Wave'2 :'Left Hand Wave' 3 :'Right Arm CW'   4 :'Right Arm CCW'  5 :'Left Arm CW'    6 :'Left Arm CCW'   7 :'Arm Roll'       8 :'Air Drums'      9 :'Air Guitar'     10:'Other'}\n",
    "\n",
    "\n",
    "    else:\n",
    "        assert False, 'wrong dataset name'\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iterator = enumerate(train_loader, 0)\n",
    "    for i, data in iterator:\n",
    "    # for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if len(data) == 2:\n",
    "            inputs, labels = data\n",
    "            # 처리 로직 작성\n",
    "        elif len(data) == 3:\n",
    "            inputs, labels, x_len = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # if rate_coding == True:\n",
    "        #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        # else:\n",
    "        #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        \n",
    "\n",
    "        ###########################################################################################################################        \n",
    "        if (which_data == 'n_tidigits'):\n",
    "            inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "            labels = labels[:, 0, :]\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        elif (which_data == 'heidelberg'):\n",
    "            inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "            print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "        # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "        # print(labels)\n",
    "            \n",
    "        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        elif rate_coding == True :\n",
    "            inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "        else :\n",
    "            inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "        # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "        ####################################################################################################################### \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        iter_correct = (predicted == labels).sum().item()\n",
    "        correct += iter_correct\n",
    "        # if i % 100 == 99:\n",
    "        # print(f\"[{i+1}] loss: {running_loss / 100:.3f}\")\n",
    "        # running_loss = 0.0\n",
    "        iter_accuracy = 100 * iter_correct / labels.size(0)\n",
    "        wandb.log({\"iter_accuracy\": iter_accuracy})\n",
    "    tr_accuracy = 100 * correct / total         \n",
    "    wandb.log({\"tr_accuracy\": tr_accuracy})\n",
    "    print(f\"Train Accuracy: {tr_accuracy:.2f}%\")\n",
    "    \n",
    "def test(model, test_loader, criterion, device, rate_coding, TIME_STEP, which_data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    iterator = enumerate(test_loader, 0)\n",
    "    with torch.no_grad():\n",
    "        for i, data in iterator:\n",
    "        # for inputs, labels in test_loader:\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # 처리 로직 작성\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "                \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # if rate_coding == True:\n",
    "            #     inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            # else:\n",
    "            #     inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "\n",
    "        \n",
    "\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'n_tidigits'):\n",
    "                inputs = inputs.permute(0, 1, 3, 2, 4)\n",
    "                labels = labels[:, 0, :]\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            elif (which_data == 'heidelberg'):\n",
    "                inputs = inputs.view(5, 1000, 1, 700, 1)\n",
    "                print(\"\\n\\n\\n경고!!!! heidelberg 이거 타임스텝이랑 채널 잘 바꿔줘라!!!\\n\\n\\n\\n\")\n",
    "            # print('inputs',inputs.size(),'\\nlabels',labels.size())\n",
    "            # print(labels)\n",
    "                \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME_STEP)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME_STEP, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_accuracy\": val_accuracy})\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.3f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_path='/data2', which_data='MNIST', gpu = '3',learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=10, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu\n",
    "    # run = wandb.init(project=f'reservoir')\n",
    "\n",
    "    hyperparameters = locals()\n",
    "\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'{which_data}_sweeprun_epoch{EPOCH}'\n",
    "    wandb.run.log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "\n",
    "    train_loader, test_loader, in_channel, CLASS_NUM = data_loader(\n",
    "        which_data=which_data, data_path=data_path, rate_coding=rate_coding, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME=TIME_STEP, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if no_reservoir == True:\n",
    "        net = NO_RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight)\n",
    "    else:\n",
    "        net = RESERVOIR_NET(TIME_STEP=TIME_STEP, CLASS_NUM=CLASS_NUM, in_spike_size=IMAGE_SIZE, in_channel=in_channel, receptive_size=3, v_init=0, v_decay=v_decay, v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight)\n",
    "    net = net.to(device)\n",
    "    wandb.watch(net, log=\"all\", log_freq = 1) #gradient, parameter logging해줌\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        train(net, train_loader, criterion, optimizer, device, rate_coding, TIME_STEP, which_data)\n",
    "        test(net, test_loader, criterion, device, rate_coding, TIME_STEP, which_data)\n",
    "        # torch.save(net.state_dict(), 'net_save/reservoir_net.pth')\n",
    "        # artifact = wandb.Artifact('model', type='model')\n",
    "        # artifact.add_file('net_save/reservoir_net.pth')\n",
    "        # run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep 하기 싫을 때\n",
    "# wandb.init(project=f'reservoir')\n",
    "# main(data_path='/data2', which_data='CIFAR10', gpu = '3', learning_rate = 0.0072, BATCH=256, IMAGE_SIZE=32, TIME_STEP=9, EPOCH=50, rate_coding=True, v_decay= 0.78,\n",
    "# v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=5.0, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: zgscy6la\n",
      "Sweep URL: https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/zgscy6la\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lq6lg45h with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCH: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.8511779343460462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.08335487265715981\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_reservoir: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_spike_weight: 1.7820289797029505\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_step: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/home/bhkim003/github_folder/ByeonghyeonKim/my_snn/wandb/run-20240726_121430-lq6lg45h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/lq6lg45h' target=\"_blank\">magic-sweep-1</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/zgscy6la' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/zgscy6la</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/zgscy6la' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/sweeps/zgscy6la</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/lq6lg45h' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/reservoir/runs/lq6lg45h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'EPOCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_spike_weight' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_reservoir' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [/data2/gesture/duration_100000] already exists.\n",
      "The directory [/data2/gesture/duration_100000] already exists.\n",
      "\n",
      "dvsgestrue 10th exclude class indices exist\n",
      "\n",
      "RESERVOIR_NET(\n",
      "  (reservoir): RESERVOIR(\n",
      "    (conv_depthwise): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "Train Accuracy: 40.26%\n",
      "Test loss: 1.389, Val Accuracy: 50.38%\n",
      "Epoch 2\n",
      "Train Accuracy: 64.66%\n",
      "Test loss: 1.118, Val Accuracy: 61.74%\n",
      "Epoch 3\n",
      "Train Accuracy: 73.10%\n",
      "Test loss: 1.010, Val Accuracy: 60.61%\n",
      "Epoch 4\n"
     ]
    }
   ],
   "source": [
    "# sweep하고싶을 때\n",
    "def sweep_cover(data_path='/data2', which_data='CIFAR10', gpu = '4', learning_rate = 0.0001, BATCH=5, IMAGE_SIZE=28, TIME_STEP=8, EPOCH=3, rate_coding=True, v_decay= 0.6,\n",
    "v_threshold=1, v_reset=0, hard_reset=True, pre_spike_weight=1, dvs_duration=1000000, dvs_clipping=True, no_reservoir = False):\n",
    "    \n",
    "    wandb.init(save_code = True)\n",
    "\n",
    "    learning_rate  =  wandb.config.learning_rate\n",
    "    BATCH  =  wandb.config.batch_size\n",
    "    TIME_STEP  =  wandb.config.time_step\n",
    "    v_decay  =  wandb.config.decay\n",
    "    pre_spike_weight  =  wandb.config.pre_spike_weight\n",
    "    which_data  =  wandb.config.which_data\n",
    "    data_path  =  wandb.config.data_path\n",
    "    rate_coding  =  wandb.config.rate_coding\n",
    "    EPOCH  =  wandb.config.EPOCH\n",
    "    IMAGE_SIZE  =  wandb.config.IMAGE_SIZE\n",
    "    dvs_duration  =  wandb.config.dvs_duration\n",
    "    dvs_clipping  =  wandb.config.dvs_clipping\n",
    "    no_reservoir  =  wandb.config.no_reservoir\n",
    "    main(data_path=data_path, which_data=which_data, gpu = gpu, learning_rate = learning_rate, BATCH=BATCH, IMAGE_SIZE=IMAGE_SIZE, TIME_STEP=TIME_STEP, EPOCH=EPOCH, rate_coding=rate_coding, v_decay= v_decay,\n",
    "v_threshold=v_threshold, v_reset=v_reset, hard_reset=hard_reset, pre_spike_weight=pre_spike_weight, dvs_duration=dvs_duration, dvs_clipping=dvs_clipping, no_reservoir = no_reservoir)\n",
    "\n",
    "\n",
    "\n",
    "which_data_hyper = 'DVS_GESTURE' # 'MNIST', 'CIFAR10' ', 'FASHION_MNIST', 'DVS_GESTURE'\n",
    "data_path_hyper = '/data2'\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': which_data_hyper,\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
    "    'parameters': \n",
    "    {\n",
    "        \"learning_rate\": {\"min\": 0.00001, \"max\": 0.1},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64, 128, 256]},\n",
    "        \"time_step\": {\"values\": [4,5,6,7,8,9,10,11,12]},\n",
    "        \"decay\": {\"min\": 0.25, \"max\": 1.0},\n",
    "        \"pre_spike_weight\": {\"min\": 0.5, \"max\": 10.0},\n",
    "        \"which_data\": {\"values\": [which_data_hyper]},\n",
    "        \"data_path\": {\"values\": [data_path_hyper]},\n",
    "        \"rate_coding\": {\"values\": [True, False]},\n",
    "        \"EPOCH\": {\"values\": [50]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [16,32,48,128]},\n",
    "        \"dvs_duration\": {\"values\": [100000]},\n",
    "        \"dvs_clipping\": {\"values\": [True]},\n",
    "        \"no_reservoir\": {\"values\": [True, False]},\n",
    "     }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'reservoir')\n",
    "wandb.agent(sweep_id, function=sweep_cover, count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE하기\n",
    "\n",
    "# # Import\n",
    "# import wandb\n",
    "# # Save your model.\n",
    "# torch.save(model.state_dict(), 'save/to/path/model.pth')\n",
    "# # Save as artifact for version control.\n",
    "# run = wandb.init(project='your-project-name')\n",
    "# artifact = wandb.Artifact('model', type='model')\n",
    "# artifact.add_file('save/to/path/model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()\n",
    "\n",
    "\n",
    "# # LOAD 하기\n",
    "\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "\n",
    "\n",
    "# artifact = run.use_artifact('entity/your-project-name/model:v0', type='model')\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
