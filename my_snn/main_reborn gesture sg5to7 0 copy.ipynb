{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36470/3748606120.py:46: DeprecationWarning: The module snntorch.spikevision is deprecated. For loading neuromorphic datasets, we recommend using the Tonic project: https://github.com/neuromorphs/tonic\n",
      "  from snntorch.spikevision import spikedata\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from snntorch import spikegen\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apex.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "''' Î†àÌçºÎü∞Ïä§\n",
    "https://spikingjelly.readthedocs.io/zh-cn/0.0.0.0.4/spikingjelly.datasets.html#module-spikingjelly.datasets\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/datasets.py\n",
    "https://github.com/GorkaAbad/Sneaky-Spikes/blob/main/how_to.md\n",
    "https://github.com/nmi-lab/torchneuromorphic\n",
    "https://snntorch.readthedocs.io/en/latest/snntorch.spikevision.spikedata.html#shd\n",
    "'''\n",
    "\n",
    "import snntorch\n",
    "from snntorch.spikevision import spikedata\n",
    "\n",
    "import modules.spikingjelly;\n",
    "from modules.spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from modules.spikingjelly.datasets.cifar10_dvs import CIFAR10DVS\n",
    "from modules.spikingjelly.datasets.n_mnist import NMNIST\n",
    "# from modules.spikingjelly.datasets.es_imagenet import ESImageNet\n",
    "from modules.spikingjelly.datasets import split_to_train_test_set\n",
    "from modules.spikingjelly.datasets.n_caltech101 import NCaltech101\n",
    "from modules.spikingjelly.datasets import pad_sequence_collate, padded_sequence_mask\n",
    "\n",
    "import modules.torchneuromorphic as torchneuromorphic\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torchviz import make_dot\n",
    "import graphviz\n",
    "from turtle import shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my module import\n",
    "from modules import *\n",
    "\n",
    "# modules Ìè¥ÎçîÏóê ÏÉàÎ™®Îìà.py ÎßåÎì§Î©¥\n",
    "# modules/__init__py ÌååÏùºÏóê form .ÏÉàÎ™®Îìà import * ÌïòÏÖà\n",
    "# Í∑∏Î¶¨Í≥† ÏÉàÎ™®Îìà.pyÏóêÏÑú from modules.ÏÉàÎ™®Îìà import * ÌïòÏÖà\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ft2font import EXTERNAL_STREAM\n",
    "\n",
    "\n",
    "def my_snn_system(devices = \"0,1,2,3\",\n",
    "                    single_step = False, # True # False\n",
    "                    unique_name = 'main',\n",
    "                    my_seed = 42,\n",
    "                    TIME = 10,\n",
    "                    BATCH = 256,\n",
    "                    IMAGE_SIZE = 32,\n",
    "                    which_data = 'CIFAR10',\n",
    "                    # CLASS_NUM = 10,\n",
    "                    data_path = '/data2',\n",
    "                    rate_coding = True,\n",
    "    \n",
    "                    lif_layer_v_init = 0.0,\n",
    "                    lif_layer_v_decay = 0.6,\n",
    "                    lif_layer_v_threshold = 1.2,\n",
    "                    lif_layer_v_reset = 0.0,\n",
    "                    lif_layer_sg_width = 1,\n",
    "\n",
    "                    # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "                    synapse_conv_kernel_size = 3,\n",
    "                    synapse_conv_stride = 1,\n",
    "                    synapse_conv_padding = 1,\n",
    "\n",
    "                    synapse_trace_const1 = 1,\n",
    "                    synapse_trace_const2 = 0.6,\n",
    "\n",
    "                    # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "                    pre_trained = False,\n",
    "                    convTrue_fcFalse = True,\n",
    "\n",
    "                    cfg = [64, 64],\n",
    "                    net_print = False, # True # False\n",
    "                    \n",
    "                    pre_trained_path = \"net_save/save_now_net.pth\",\n",
    "                    learning_rate = 0.0001,\n",
    "                    epoch_num = 200,\n",
    "                    tdBN_on = False,\n",
    "                    BN_on = False,\n",
    "\n",
    "                    surrogate = 'sigmoid',\n",
    "\n",
    "                    BPTT_on = False,\n",
    "\n",
    "                    optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "                    scheduler_name = 'no',\n",
    "                    \n",
    "                    ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "                    dvs_clipping = 1, \n",
    "                    dvs_duration = 25_000,\n",
    "\n",
    "\n",
    "                    DFA_on = False, # True # False\n",
    "                    trace_on = False, \n",
    "                    OTTT_input_trace_on = False, # True # False\n",
    "                    \n",
    "                    exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "                    merge_polarities = False, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "                    denoise_on = True, \n",
    "\n",
    "                    extra_train_dataset = 0, # DECREPATED # data_loaderÏóêÏÑú train datasetÏùÑ Î™áÍ∞ú Îçî Ïì∏Í±¥ÏßÄ \n",
    "\n",
    "                    num_workers = 2,\n",
    "                    chaching_on = True,\n",
    "                    pin_memory = True, # True # False\n",
    "                    \n",
    "                    UDA_on = False,  # DECREPATED # uda\n",
    "                    alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "                    bias = True,\n",
    "\n",
    "                    last_lif = False,\n",
    "                        \n",
    "                    temporal_filter = 1, \n",
    "                    initial_pooling = 1,\n",
    "\n",
    "                    temporal_filter_accumulation = False,\n",
    "\n",
    "                    quantize_bit_list=[],\n",
    "                    scale_exp=[],\n",
    "                    ):\n",
    "    ## Ìï®Ïàò ÎÇ¥ Î™®Îì† Î°úÏª¨ Î≥ÄÏàò Ï†ÄÏû• ########################################################\n",
    "    hyperparameters = locals()\n",
    "    print('param', hyperparameters,'\\n')\n",
    "    hyperparameters['current epoch'] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "    ## hyperparameter check #############################################################\n",
    "    if single_step == True:\n",
    "        assert BPTT_on == False and tdBN_on == False \n",
    "    if tdBN_on == True:\n",
    "        assert BPTT_on == True\n",
    "    if pre_trained == True:\n",
    "        print('\\n\\n')\n",
    "        print(\"Caution! pre_trained is True\\n\\n\"*3)    \n",
    "    if DFA_on == True:\n",
    "        assert single_step == True and BPTT_on == False \n",
    "    # assert single_step == DFA_on, 'DFAÎûë single_stepÍ≥µÏ°¥ÌïòÍ≤åÌï¥Îùº'\n",
    "    if trace_on:\n",
    "        assert BPTT_on == False and single_step == True\n",
    "    if OTTT_input_trace_on == True:\n",
    "        assert BPTT_on == False and single_step == True #and trace_on == True\n",
    "    if temporal_filter > 1:\n",
    "        assert convTrue_fcFalse == False\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## wandb ÏÑ∏ÌåÖ ###################################################################\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.run.name = f'lr_{learning_rate}_{unique_name}_{which_data}_tstep{TIME}'\n",
    "    wandb.define_metric(\"summary_val_acc\", summary=\"max\")\n",
    "    # wandb.run.log_code(\".\", \n",
    "    #                     include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #                     exclude_fn=lambda path: 'logs/' in path or 'net_save/' in path or 'result_save/' in path or 'trying/' in path or 'wandb/' in path or 'private/' in path or '.git/' in path or 'tonic' in path or 'torchneuromorphic' in path or 'spikingjelly' in path \n",
    "    #                     )\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    ## gpu setting ##################################################################################################################\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]= devices\n",
    "    ###################################################################################################################################\n",
    "\n",
    "\n",
    "    ## seed setting ##################################################################################################################\n",
    "    seed_assign(my_seed)\n",
    "    ###################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## data_loader Í∞ÄÏ†∏Ïò§Í∏∞ ##################################################################################################################\n",
    "    # data loader, pixel channel, class num\n",
    "    train_data_split_indices = []\n",
    "    train_loader, test_loader, synapse_conv_in_channels, CLASS_NUM, train_data_count = data_loader(\n",
    "            which_data,\n",
    "            data_path, \n",
    "            rate_coding, \n",
    "            BATCH, \n",
    "            IMAGE_SIZE,\n",
    "            ddp_on,\n",
    "            TIME*temporal_filter, \n",
    "            dvs_clipping,\n",
    "            dvs_duration,\n",
    "            exclude_class,\n",
    "            merge_polarities,\n",
    "            denoise_on,\n",
    "            my_seed,\n",
    "            extra_train_dataset,\n",
    "            num_workers,\n",
    "            chaching_on,\n",
    "            pin_memory,\n",
    "            train_data_split_indices,) \n",
    "    synapse_fc_out_features = CLASS_NUM\n",
    "\n",
    "    print('\\nlen(train_loader):', len(train_loader), 'BATCH:', BATCH, 'train_data_count:', train_data_count) \n",
    "    print('len(test_loader):', len(test_loader), 'BATCH:', BATCH)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\ndevice ==> {device}\\n\")\n",
    "    if device == \"cpu\":\n",
    "        print(\"=\"*50,\"\\n[WARNING]\\n[WARNING]\\n[WARNING]\\n: cpu mode\\n\\n\",\"=\"*50)\n",
    "\n",
    "    ### network setting #######################################################################################################################\n",
    "    if (convTrue_fcFalse == False):\n",
    "        net = REBORN_MY_SNN_FC(cfg, synapse_conv_in_channels*temporal_filter, IMAGE_SIZE//initial_pooling, synapse_fc_out_features,\n",
    "                    synapse_trace_const1, synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "    else:\n",
    "        net = REBORN_MY_SNN_CONV(cfg, synapse_conv_in_channels, IMAGE_SIZE//initial_pooling,\n",
    "                    synapse_conv_kernel_size, synapse_conv_stride, \n",
    "                    synapse_conv_padding, synapse_trace_const1, \n",
    "                    synapse_trace_const2, \n",
    "                    lif_layer_v_init, lif_layer_v_decay, \n",
    "                    lif_layer_v_threshold, lif_layer_v_reset,\n",
    "                    lif_layer_sg_width,\n",
    "                    synapse_fc_out_features, \n",
    "                    tdBN_on,\n",
    "                    BN_on, TIME,\n",
    "                    surrogate,\n",
    "                    BPTT_on,\n",
    "                    DFA_on,\n",
    "                    bias,\n",
    "                    single_step,\n",
    "                    last_lif,\n",
    "                    trace_on,\n",
    "                    quantize_bit_list,\n",
    "                    scale_exp).to(device)\n",
    "\n",
    "    net = torch.nn.DataParallel(net) \n",
    "    \n",
    "    if pre_trained == True:\n",
    "        # 1. Ï†ÑÏ≤¥ state_dict Î°úÎìú\n",
    "        checkpoint = torch.load(pre_trained_path)\n",
    "\n",
    "        # 2. ÌòÑÏû¨ Î™®Îç∏Ïùò state_dict Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        model_dict = net.state_dict()\n",
    "\n",
    "        # 3. 'SYNAPSE'Í∞Ä Ìè¨Ìï®Îêú keyÎßå ÌïÑÌÑ∞ÎßÅ (ÌòÑÏû¨ Î™®Îç∏ÏóêÎèÑ Ï°¥Ïû¨ÌïòÎäî keyÎßå)\n",
    "        filtered_dict = {k: v for k, v in checkpoint.items() if ('weight' in k or 'bias' in k) and k in model_dict}\n",
    "\n",
    "        # 4. ÏóÖÎç∞Ïù¥Ìä∏Îêú ÌÇ§ Ï∂úÎ†•\n",
    "        print(\"üîÑ ÏóÖÎç∞Ïù¥Ìä∏Îêú SYNAPSE Í¥ÄÎ†® Î†àÏù¥Ïñ¥Îì§:\")\n",
    "        for k in filtered_dict.keys():\n",
    "            print(f\" - {k}\")\n",
    "\n",
    "        # 5. Î™®Îç∏ dict ÏóÖÎç∞Ïù¥Ìä∏ Î∞è Î°úÎî©\n",
    "        model_dict.update(filtered_dict)\n",
    "        net.load_state_dict(model_dict)\n",
    "    \n",
    "    net = net.to(device)\n",
    "    if (net_print == True):\n",
    "        print(net)    \n",
    "\n",
    "    print(f\"\\n========================================================\\nTrainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\\n========================================================\\n\")\n",
    "    ####################################################################################################################################\n",
    "    \n",
    "\n",
    "    ## wandb logging ###########################################\n",
    "    # wandb.watch(net, log=\"all\", log_freq = 10) #gradient, parameter loggingÌï¥Ï§å\n",
    "    ############################################################\n",
    "\n",
    "    ## criterion ########################################## # loss Íµ¨Ìï¥Ï£ºÎäî ÏπúÍµ¨\n",
    "    def my_cross_entropy_loss(logits, targets):\n",
    "        # logits: (batch_size, num_classes)\n",
    "        # targets: (batch_size,) -> ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # log(p_i)\n",
    "        loss = F.nll_loss(log_probs, targets)\n",
    "        # print(loss.shape)\n",
    "        return loss\n",
    "    \n",
    "    class CustomLossFunction(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, target):\n",
    "            ctx.save_for_backward(input, target)\n",
    "            return F.cross_entropy(input, target)\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            # MAE Ïä§ÌÉÄÏùºÏùò gradientÎ•º ÌùâÎÇ¥ÎÉÑ\n",
    "            input, target = ctx.saved_tensors\n",
    "            input_argmax = input.argmax(dim=1)\n",
    "            input_one_hot = torch.zeros_like(input).scatter_(1, input_argmax.unsqueeze(1), 1.0)\n",
    "            target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1.0)\n",
    "\n",
    "            # print('grad_output', grad_output) # Ïù¥Í±∞ Í±ç 1.0ÏûÑ\n",
    "            return input_one_hot - target_one_hot, None  # targetÏóêÎäî gradient ÏóÜÏùå\n",
    "\n",
    "    # Wrapper module\n",
    "    class CustomCriterion(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            return CustomLossFunction.apply(input, target)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = CustomCriterion().to(device)\n",
    "    \n",
    "    # if (OTTT_sWS_on == True):\n",
    "    #     # criterion = nn.CrossEntropyLoss().to(device)\n",
    "        # criterion = lambda y_t, target_t: ((1 - 0.05) * F.cross_entropy(y_t, target_t) + 0.05 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    #     if which_data == 'DVS_GESTURE':\n",
    "    #         criterion = lambda y_t, target_t: ((1 - 0.001) * F.cross_entropy(y_t, target_t) + 0.001 * F.mse_loss(y_t, F.one_hot(target_t, CLASS_NUM).float())) / TIME \n",
    "    ####################################################\n",
    "\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "    class MySGD(torch.optim.Optimizer):\n",
    "        def __init__(self, params, lr=0.01, momentum=0.0, quantize_bit_list=[], scale_exp=[], net=None):\n",
    "            if momentum < 0.0 or momentum >= 1.0:\n",
    "                raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "            \n",
    "            defaults = {'lr': lr, 'momentum': momentum}\n",
    "            super(MySGD, self).__init__(params, defaults)\n",
    "            self.step_count = 0\n",
    "            self.quantize_bit_list = quantize_bit_list\n",
    "            # self.quantize_bit_list = []\n",
    "            self.scale_exp = scale_exp\n",
    "            self.param_to_name = {param: name for name, param in net.module.named_parameters()} if net else {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def step(self):\n",
    "            \"\"\"Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Ïóê ÎåÄÌï¥ gradient descent ÏàòÌñâ\"\"\"\n",
    "            loss = None\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                momentum = group['momentum']\n",
    "                for param in group['params']:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    name = self.param_to_name.get(param, 'unknown')\n",
    "                    # gradientÎ•º Ïù¥Ïö©Ìï¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "                    d_p = param.grad\n",
    "\n",
    "                    if momentum > 0.0:\n",
    "                        param_state = self.state[param]\n",
    "                        if 'momentum_buffer' not in param_state:\n",
    "                            # momentum buffer Ï¥àÍ∏∞Ìôî\n",
    "                            buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                        else:\n",
    "                            buf = param_state['momentum_buffer']\n",
    "                            buf.mul_(momentum).add_(d_p)\n",
    "                            # buf *= momentum \n",
    "                            # buf += d_p\n",
    "                        d_p = buf\n",
    "\n",
    "                    dw = -lr*d_p\n",
    "                                        \n",
    "                    # if 'layers.7.fc.weight' in name or 'layers.7.fc.bias' in name:\n",
    "                    #     dw = dw * 0.5\n",
    "\n",
    "                    if len(self.quantize_bit_list) != 0:\n",
    "                        if 'layers.1.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.1.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[0]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[0][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.4.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[1]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[1][1]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.weight' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][0]\n",
    "                                scale_dw = 2**exp\n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        elif 'layers.7.fc.bias' in name:\n",
    "                            dw_bit = self.quantize_bit_list[2]\n",
    "                            if self.scale_exp != []:\n",
    "                                exp = self.scale_exp[2][1]\n",
    "                                scale_dw = 2**exp\n",
    "                                \n",
    "                            else:\n",
    "                                max_dw = dw.abs().max().item()\n",
    "                                assert max_dw > 0, f\"max_dw is zero for parameter {param.name if hasattr(param, 'name') else 'unknown'}\"\n",
    "                                scale_dw = 2**math.ceil(math.log2(max_dw / (2**(dw_bit-1) -1)))\n",
    "                        else:\n",
    "                            assert False, f\"Unknown parameter name: {name}\"\n",
    "\n",
    "\n",
    "                        # print(f'dw_bit{dw_bit}, exp{exp}')\n",
    "                        # print(f'name {name}, d_p: {d_p.shape}, unique elements: {d_p.unique().numel()}, values: {d_p.unique().tolist()}')\n",
    "                        # print(f'name {name}, dw: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "                        # dw = torch.clamp((dw / scale_dw + 0).round(), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        dw = torch.clamp(round_away_from_zero(dw / scale_dw + 0), -2**(dw_bit-1) + 1, 2**(dw_bit-1) - 1) * scale_dw\n",
    "                        # print(f'name {name}, dw_post: {dw.shape}, unique elements: {dw.unique().numel()}, values: {dw.unique().tolist()}')\n",
    "\n",
    "                    if 'layers.1.fc.weight' in name:\n",
    "                        ooo_fifo = 2\n",
    "                    elif 'layers.4.fc.weight' in name:\n",
    "                        ooo_fifo = 1\n",
    "                    elif 'layers.7.fc.weight' in name:\n",
    "                        ooo_fifo = 0\n",
    "                    else:\n",
    "                        assert False\n",
    "                        \n",
    "                    if ooo_fifo > 0:\n",
    "                        # ====== FIFO Ï≤òÎ¶¨ ======\n",
    "                        param_state = self.state[param]\n",
    "                        if 'fifo_buffer' not in param_state:\n",
    "                            param_state['fifo_buffer'] = []\n",
    "\n",
    "                        fifo = param_state['fifo_buffer']\n",
    "                        fifo.append(dw.clone())  # clone() to detach from current graph\n",
    "\n",
    "                        if len(fifo) == ooo_fifo+1:\n",
    "                            oldest_dw = fifo.pop(0)\n",
    "                            param.add_(oldest_dw)\n",
    "                    else: \n",
    "                        param.add_(dw)\n",
    "                        # param -= dw ÏúÑ Ïó∞ÏÇ∞Ïù¥Îûë Îã§Î¶Ñ. inmemoryÏó∞ÏÇ∞Ïù¥Îùº Ï¢Ä Îã§Î•∏ ÎìØ\n",
    "            return loss\n",
    "    \n",
    "    if(optimizer_what == 'SGD'):\n",
    "        optimizer = MySGD(net.parameters(), lr=learning_rate, momentum=0.0, quantize_bit_list=quantize_bit_list, scale_exp=scale_exp, net=net)\n",
    "        # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "        print(optimizer)\n",
    "    elif(optimizer_what == 'Adam'):\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "        # optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate/256 * BATCH, weight_decay=1e-4)\n",
    "        # optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0, betas=(0.9, 0.999))\n",
    "    elif(optimizer_what == 'RMSprop'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    if (scheduler_name == 'StepLR'):\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    elif (scheduler_name == 'ExponentialLR'):\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif (scheduler_name == 'ReduceLROnPlateau'):\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "    elif (scheduler_name == 'CosineAnnealingLR'):\n",
    "        # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=50)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=0, T_max=epoch_num)\n",
    "    elif (scheduler_name == 'OneCycleLR'):\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=epoch_num)\n",
    "    else:\n",
    "        pass # 'no' scheduler\n",
    "    ## optimizer, scheduler ########################################################################\n",
    "\n",
    "\n",
    "    tr_acc = 0\n",
    "    tr_correct = 0\n",
    "    tr_total = 0\n",
    "    tr_acc_best = 0\n",
    "    tr_epoch_loss_temp = 0\n",
    "    tr_epoch_loss = 0\n",
    "    val_acc_best = 0\n",
    "    val_acc_now = 0\n",
    "    val_loss = 0\n",
    "    iter_of_val = False\n",
    "    total_backward_count = 0\n",
    "    real_backward_count = 0\n",
    "    #======== EPOCH START ==========================================================================================\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start_time = time.time()\n",
    "        print('total_backward_count', total_backward_count, 'real_backward_count',real_backward_count, f'{100*real_backward_count/(total_backward_count+0.00000001):7.3f}%')\n",
    "        if epoch == 1:\n",
    "            for name, module in net.named_modules():\n",
    "                if isinstance(module, Feedback_Receiver):\n",
    "                    print(f\"[{name}] weight_fb parameter count: {module.weight_fb.numel():,}\")\n",
    "\n",
    "        max_val_box = []\n",
    "        max_val_scale_exp_8bit_box = []\n",
    "        max_val_scale_exp_16bit_box = []\n",
    "        perc_95_box = []\n",
    "        perc_95_scale_exp_8bit_box = []\n",
    "        perc_95_scale_exp_16bit_box = []\n",
    "        perc_99_box = []\n",
    "        perc_99_scale_exp_8bit_box = []\n",
    "        perc_99_scale_exp_16bit_box = []\n",
    "        perc_999_box = []\n",
    "        perc_999_scale_exp_8bit_box = []\n",
    "        perc_999_scale_exp_16bit_box = []\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "        for name, param in net.module.named_parameters():\n",
    "            if ('weight' in name or 'bias' in name) and ('1' in name or '4' in name or '7' in name):\n",
    "                \n",
    "                data = param.detach().cpu().numpy().flatten()\n",
    "                abs_data = np.abs(data)\n",
    "\n",
    "                # ÌÜµÍ≥ÑÎüâ Í≥ÑÏÇ∞\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "                abs_mean = np.mean(abs_data)\n",
    "                abs_std = np.std(abs_data)\n",
    "                eps = 1e-15\n",
    "\n",
    "                # Ï†àÎåÄÍ∞í Í∏∞Î∞ò max, percentiles\n",
    "                max_val = abs_data.max()\n",
    "                max_val_scale_exp_8bit = math.ceil(math.log2((eps+max_val)/ (2**(8-1) -1)))\n",
    "                max_val_scale_exp_16bit = math.ceil(math.log2((eps+max_val)/ (2**(16-1) -1)))\n",
    "                perc_95 = np.percentile(abs_data, 95)\n",
    "                perc_95_scale_exp_8bit = math.ceil(math.log2((eps+perc_95)/ (2**(8-1) -1)))\n",
    "                perc_95_scale_exp_16bit = math.ceil(math.log2((eps+perc_95)/ (2**(16-1) -1)))\n",
    "                perc_99 = np.percentile(abs_data, 99)\n",
    "                perc_99_scale_exp_8bit = math.ceil(math.log2((eps+perc_99)/ (2**(8-1) -1)))\n",
    "                perc_99_scale_exp_16bit = math.ceil(math.log2((eps+perc_99)/ (2**(16-1) -1)))\n",
    "                perc_999 = np.percentile(abs_data, 99.9)\n",
    "                perc_999_scale_exp_8bit = math.ceil(math.log2((eps+perc_999)/ (2**(8-1) -1)))\n",
    "                perc_999_scale_exp_16bit = math.ceil(math.log2((eps+perc_999)/ (2**(16-1) -1)))\n",
    "                \n",
    "                max_val_box.append(max_val)\n",
    "                max_val_scale_exp_8bit_box.append(max_val_scale_exp_8bit)\n",
    "                max_val_scale_exp_16bit_box.append(max_val_scale_exp_16bit)\n",
    "                perc_95_box.append(perc_95)\n",
    "                perc_95_scale_exp_8bit_box.append(perc_95_scale_exp_8bit)\n",
    "                perc_95_scale_exp_16bit_box.append(perc_95_scale_exp_16bit)\n",
    "                perc_99_box.append(perc_99)\n",
    "                perc_99_scale_exp_8bit_box.append(perc_99_scale_exp_8bit)\n",
    "                perc_99_scale_exp_16bit_box.append(perc_99_scale_exp_16bit)\n",
    "                perc_999_box.append(perc_999)\n",
    "                perc_999_scale_exp_8bit_box.append(perc_999_scale_exp_8bit)\n",
    "                perc_999_scale_exp_16bit_box.append(perc_999_scale_exp_16bit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # if epoch % 5 == 0 or epoch < 3:\n",
    "                #     print(\"=> Plotting weight and bias distributions...\")\n",
    "                #     # Í∑∏ÎûòÌîÑ Í∑∏Î¶¨Í∏∞\n",
    "                #     plt.figure(figsize=(6, 4))\n",
    "                #     plt.hist(data, bins=100, alpha=0.7, color='skyblue')\n",
    "                #     plt.axvline(x=max_val, color='red', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "                #     plt.axvline(x=-max_val, color='red', linestyle='--')\n",
    "                #     plt.axvline(x=perc_95, color='green', linestyle='--', label=f'95%: {perc_95:.4f}')\n",
    "                #     plt.axvline(x=-perc_95, color='green', linestyle='--')\n",
    "                #     plt.axvline(x=perc_99, color='orange', linestyle='--', label=f'99%: {perc_99:.4f}')\n",
    "                #     plt.axvline(x=-perc_99, color='orange', linestyle='--')\n",
    "                #     plt.axvline(x=perc_999, color='purple', linestyle='--', label=f'99.9%: {perc_999:.4f}')\n",
    "                #     plt.axvline(x=-perc_999, color='purple', linestyle='--')\n",
    "                    \n",
    "                #     # Ï†úÎ™©Ïóê ÌÜµÍ≥ÑÍ∞í Ìè¨Ìï®\n",
    "                #     title = (\n",
    "                #         f\"{name}, Epoch {epoch}\\n\"\n",
    "                #         f\"mean={mean:.4f}, std={std:.4f}, \"\n",
    "                #         f\"|mean|={abs_mean:.4f}, |std|={abs_std:.4f}\\n\"\n",
    "                #         f\"Scale 8bit max = { max_val_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit max = {max_val_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p999 = {perc_999_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p999 = {perc_999_scale_exp_16bit }\\n\"\n",
    "                #         f\"Scale 8bit p99 = {perc_99_scale_exp_8bit }, \"\n",
    "                #         f\"Scale 16bit p99 = { perc_99_scale_exp_16bit}\\n\"\n",
    "                #         f\"Scale 8bit p95 = { perc_95_scale_exp_8bit}, \"\n",
    "                #         f\"Scale 16bit p95 = { perc_95_scale_exp_16bit}\"\n",
    "                #     )\n",
    "                #     plt.title(title)\n",
    "                #     plt.xlabel('Value')\n",
    "                #     plt.ylabel('Frequency')\n",
    "                #     plt.grid(True)\n",
    "                #     plt.legend()\n",
    "                #     plt.tight_layout()\n",
    "                #     plt.show()\n",
    "        ##### weight ÌîÑÎ¶∞Ìä∏ ######################################################################\n",
    "\n",
    "        ####### iterator : input_loading & tqdmÏùÑ ÌÜµÌïú progress_bar ÏÉùÏÑ±###################\n",
    "        iterator = enumerate(train_loader, 0)\n",
    "        # iterator = tqdm(iterator, total=len(train_loader), desc='train', dynamic_ncols=True, position=0, leave=True)\n",
    "        ##################################################################################   \n",
    "\n",
    "        ###### ITERATION START ##########################################################################################################\n",
    "        for i, data in iterator:\n",
    "            net.train() # train Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "            ### data loading & semi-pre-processing ################################################################################\n",
    "            if len(data) == 2:\n",
    "                inputs, labels = data\n",
    "                # Ï≤òÎ¶¨ Î°úÏßÅ ÏûëÏÑ±\n",
    "            elif len(data) == 3:\n",
    "                inputs, labels, x_len = data\n",
    "            else:\n",
    "                assert False, 'data length is not 2 or 3'\n",
    "            #######################################################################################################################\n",
    "                \n",
    "            ## batch ÌÅ¨Í∏∞ ######################################\n",
    "            real_batch = labels.size(0)\n",
    "            ###########################################################\n",
    "\n",
    "            # Ï∞®Ïõê Ï†ÑÏ≤òÎ¶¨\n",
    "            ###########################################################################################################################        \n",
    "            if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4)\n",
    "            elif rate_coding == True :\n",
    "                inputs = spikegen.rate(inputs, num_steps=TIME)\n",
    "            else :\n",
    "                inputs = inputs.repeat(TIME, 1, 1, 1, 1)\n",
    "            # inputs: [Time, Batch, Channel, Height, Width]  \n",
    "            ####################################################################################################################### \n",
    "                \n",
    "            # if i % 1000 == 999:\n",
    "            #     # SYNAPSE_FCÏóê ÏûàÎäî sparsity_print_and_reset() Ïã§Ìñâ\n",
    "            #     for name, module in net.module.named_modules():\n",
    "            #         if isinstance(module, SYNAPSE_FC):\n",
    "            #             module.sparsity_print_and_reset()\n",
    "\n",
    "                            \n",
    "            ## initial pooling #######################################################################\n",
    "            if (initial_pooling > 1):\n",
    "                pool = nn.MaxPool2d(kernel_size=2)\n",
    "                num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                shape_temp = inputs.shape\n",
    "                inputs = inputs.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                for _ in range(num_pooling_layers):\n",
    "                    inputs = pool(inputs)\n",
    "                inputs = inputs.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "            ## initial pooling #######################################################################\n",
    "            ## temporal filtering ####################################################################\n",
    "            shape_temp = inputs.shape\n",
    "            if (temporal_filter > 1):\n",
    "                slice_bucket = []\n",
    "                for t_temp in range(TIME):\n",
    "                    start = t_temp * temporal_filter\n",
    "                    end = start + temporal_filter\n",
    "                    slice_concat = torch.movedim(inputs[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                    \n",
    "                    if temporal_filter_accumulation == True:\n",
    "                        if t_temp == 0:\n",
    "                            slice_bucket.append(slice_concat)\n",
    "                        else:\n",
    "                            slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                    else:\n",
    "                        slice_bucket.append(slice_concat)\n",
    "\n",
    "                inputs = torch.stack(slice_bucket, dim=0)\n",
    "                if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                    inputs = (inputs != 0.0).float()\n",
    "            ## temporal filtering ####################################################################\n",
    "            ####################################################################################################################### \n",
    "                \n",
    "\n",
    "            # # dvs Îç∞Ïù¥ÌÑ∞ ÏãúÍ∞ÅÌôî ÏΩîÎìú (ÌôïÏù∏ ÌïÑÏöîÌï† Ïãú Ïç®Îùº)\n",
    "            # ##############################################################################################\n",
    "            # dvs_visualization(inputs, labels, TIME, BATCH, my_seed)\n",
    "            # #####################################################################################################\n",
    "\n",
    "            ## to (device) #######################################\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            ###########################################################\n",
    "\n",
    "            # ## gradient Ï¥àÍ∏∞Ìôî #######################################\n",
    "            # optimizer.zero_grad()\n",
    "            # ###########################################################\n",
    "                            \n",
    "            if merge_polarities == True:\n",
    "                inputs = inputs[:,:,0:1,:,:]\n",
    "\n",
    "            if single_step == False:\n",
    "                # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§##############################\n",
    "                # inputs: [Time, Batch, Channel, Height, Width]   \n",
    "                inputs = inputs.permute(1, 0, 2, 3, 4) # netÏóê ÎÑ£Ïñ¥Ï§ÑÎïåÎäî batchÍ∞Ä Ï†§ Ïïû Ï∞®ÏõêÏúºÎ°ú ÏôÄÏïºÌï®. # dataparallelÎïåÎß§\n",
    "                # inputs: [Batch, Time, Channel, Height, Width] \n",
    "                #################################################################################################\n",
    "            else:\n",
    "                labels = labels.repeat(TIME, 1)\n",
    "                ## first inputÎèÑ ottt trace Ï†ÅÏö©ÌïòÍ∏∞ ÏúÑÌïú ÏΩîÎìú (validation ÏãúÏóêÎäî ÌïÑÏöîX) ##########################\n",
    "                if trace_on == True and OTTT_input_trace_on == True:\n",
    "                    spike = inputs\n",
    "                    trace = torch.full_like(spike, fill_value = 0.0, dtype = torch.float, requires_grad=False)\n",
    "                    inputs = []\n",
    "                    for t in range(TIME):\n",
    "                        trace[t] = trace[t-1]*synapse_trace_const2 + spike[t]*synapse_trace_const1\n",
    "                        inputs += [[spike[t], trace[t]]]\n",
    "                ##################################################################################################\n",
    "\n",
    "\n",
    "            if single_step == False:\n",
    "                ### input --> net --> output #####################################################\n",
    "                outputs = net(inputs)\n",
    "                ##################################################################################\n",
    "                ## loss, backward ##########################################\n",
    "                iter_loss = criterion(outputs, labels)\n",
    "                iter_loss.backward()\n",
    "                ############################################################\n",
    "                ## weight ÏóÖÎç∞Ïù¥Ìä∏!! ##################################\n",
    "                optimizer.step()\n",
    "                ################################################################\n",
    "            else:\n",
    "                outputs_all = []\n",
    "                iter_loss = 0.0\n",
    "                for t in range(TIME):\n",
    "                    optimizer.step() # full step time update\n",
    "                    optimizer.zero_grad()\n",
    "                    ### input[t] --> net --> output_one_time #########################################\n",
    "                    outputs_one_time = net(inputs[t])\n",
    "                    ##################################################################################\n",
    "                    one_time_loss = criterion(outputs_one_time, labels[t].contiguous())\n",
    "                    one_time_loss.backward() # one_time backward\n",
    "                    iter_loss += one_time_loss.data\n",
    "                    outputs_all.append(outputs_one_time.detach())\n",
    "\n",
    "                    total_backward_count = total_backward_count + 1\n",
    "                    outputs_one_time_argmax = (outputs_one_time.detach()).argmax(dim=1)\n",
    "                    real_backward_count = real_backward_count + (outputs_one_time_argmax != labels[t]).sum().item()\n",
    "\n",
    "\n",
    "                outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                outputs = outputs_all.mean(1) # otttÍ∫º Ïì∏Îïå\n",
    "                labels = labels[0]\n",
    "                iter_loss /= TIME\n",
    "\n",
    "            tr_epoch_loss_temp += iter_loss.data/len(train_loader)\n",
    "\n",
    "            ## net Í∑∏Î¶º Ï∂úÎ†•Ìï¥Î≥¥Í∏∞ #################################################################\n",
    "            # print('ÏãúÍ∞ÅÌôî')\n",
    "            # make_dot(outputs, params=dict(list(net.named_parameters()))).render(\"net_torchviz\", format=\"png\")\n",
    "            # return 0\n",
    "            ##################################################################################\n",
    "\n",
    "            #### batch Ïñ¥Í∏ãÎÇ® Î∞©ÏßÄ ###############################################\n",
    "            assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "            #######################################################################\n",
    "            \n",
    "\n",
    "            ####### training accruacy save for print ###############################\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total = real_batch\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            iter_acc = correct / total\n",
    "            tr_total += total\n",
    "            tr_correct += correct\n",
    "            iter_acc_string = f'epoch-{epoch:<3} iter_acc:{100 * iter_acc:7.2f}%, lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            iter_acc_string2 = f'epoch-{epoch:<3} lr={[f\"{lr:9.7f}\" for lr in (param_group[\"lr\"] for param_group in optimizer.param_groups)]}'\n",
    "            ################################################################\n",
    "            \n",
    "\n",
    "            ##### validation ##################################################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                iter_of_val = True\n",
    "\n",
    "                tr_acc = tr_correct/tr_total\n",
    "                tr_correct = 0\n",
    "                tr_total = 0\n",
    "\n",
    "                val_loss = 0\n",
    "                correct_val = 0\n",
    "                total_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    net.eval() # eval Î™®ÎìúÎ°ú Î∞îÍøîÏ§òÏïºÌï® \n",
    "                    for data_val in test_loader:\n",
    "                        ## data_val loading & semi-pre-processing ##########################################################\n",
    "                        if len(data_val) == 2:\n",
    "                            inputs_val, labels_val = data_val\n",
    "                        elif len(data_val) == 3:\n",
    "                            inputs_val, labels_val, x_len = data_val\n",
    "                        else:\n",
    "                            assert False, 'data_val length is not 2 or 3'\n",
    "\n",
    "                        if (which_data == 'DVS_CIFAR10' or which_data == 'DVS_GESTURE' or which_data == 'DVS_GESTURE_TONIC' or which_data == 'DVS_CIFAR10_2' or which_data == 'NMNIST' or which_data == 'NMNIST_TONIC' or which_data == 'N_CALTECH101' or which_data == 'n_tidigits' or which_data == 'heidelberg'):\n",
    "                            inputs_val = inputs_val.permute(1, 0, 2, 3, 4)\n",
    "                        elif rate_coding == True :\n",
    "                            inputs_val = spikegen.rate(inputs_val, num_steps=TIME)\n",
    "                        else :\n",
    "                            inputs_val = inputs_val.repeat(TIME, 1, 1, 1, 1)\n",
    "                        # inputs_val: [Time, Batch, Channel, Height, Width]  \n",
    "                        ###################################################################################################\n",
    "\n",
    "                        \n",
    "                        ## initial pooling #######################################################################\n",
    "                        if (initial_pooling > 1):\n",
    "                            pool = nn.MaxPool2d(kernel_size=2)\n",
    "                            num_pooling_layers = int(math.log2(initial_pooling))\n",
    "                            # Time, Batch, Channel Ï∞®ÏõêÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Height, Width Ï∞®ÏõêÏóê ÎåÄÌï¥ÏÑúÎßå pooling Ï†ÅÏö©\n",
    "                            shape_temp = inputs_val.shape\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0]*shape_temp[1], shape_temp[2], shape_temp[3], shape_temp[4])\n",
    "                            for _ in range(num_pooling_layers):\n",
    "                                inputs_val = pool(inputs_val)\n",
    "                            inputs_val = inputs_val.reshape(shape_temp[0], shape_temp[1], shape_temp[2], shape_temp[3]//initial_pooling, shape_temp[4]//initial_pooling)\n",
    "                        ## initial pooling #######################################################################\n",
    "\n",
    "                        ## temporal filtering ####################################################################\n",
    "                        shape_temp = inputs_val.shape\n",
    "                        if (temporal_filter > 1):\n",
    "                            slice_bucket = []\n",
    "                            for t_temp in range(TIME):\n",
    "                                start = t_temp * temporal_filter\n",
    "                                end = start + temporal_filter\n",
    "                                slice_concat = torch.movedim(inputs_val[start:end], 0, -2).reshape(shape_temp[1],shape_temp[2],shape_temp[3],-1)\n",
    "                                \n",
    "                                if temporal_filter_accumulation == True:\n",
    "                                    if t_temp == 0:\n",
    "                                        slice_bucket.append(slice_concat)\n",
    "                                    else:\n",
    "                                        slice_bucket.append(slice_concat+slice_bucket[t_temp-1])\n",
    "                                else:\n",
    "                                    slice_bucket.append(slice_concat)\n",
    "\n",
    "                            inputs_val = torch.stack(slice_bucket, dim=0)\n",
    "                            if temporal_filter_accumulation == True and dvs_clipping > 0:\n",
    "                                inputs = (inputs != 0.0).float()\n",
    "                        ## temporal filtering ####################################################################\n",
    "                            \n",
    "                        inputs_val = inputs_val.to(device)\n",
    "                        labels_val = labels_val.to(device)\n",
    "                        real_batch = labels_val.size(0)\n",
    "                        \n",
    "                        if merge_polarities == True:\n",
    "                            inputs_val = inputs_val[:,:,0:1,:,:]\n",
    "\n",
    "                        ## network Ïó∞ÏÇ∞ ÏãúÏûë ############################################################################################################\n",
    "                        if single_step == False:\n",
    "                            outputs = net(inputs_val.permute(1, 0, 2, 3, 4)) #inputs_val: [Batch, Time, Channel, Height, Width]  \n",
    "                            val_loss += criterion(outputs, labels_val)/len(test_loader)\n",
    "                        else:\n",
    "                            outputs_all = []\n",
    "                            for t in range(TIME):\n",
    "                                outputs = net(inputs_val[t])\n",
    "                                val_loss_temp = criterion(outputs, labels_val)\n",
    "                                outputs_all.append(outputs.detach())\n",
    "                                val_loss += (val_loss_temp.data/TIME)/len(test_loader)\n",
    "                            outputs_all = torch.stack(outputs_all, dim=1)\n",
    "                            outputs = outputs_all.mean(1)\n",
    "                        #################################################################################################################################\n",
    "\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total_val += real_batch\n",
    "                        assert real_batch == outputs.size(0), f'batch size is not same. real_batch: {real_batch}, outputs.size(0): {outputs.size(0)}'\n",
    "                        correct_val += (predicted == labels_val).sum().item()\n",
    "\n",
    "                    val_acc_now = correct_val / total_val\n",
    "\n",
    "                if val_acc_best < val_acc_now:\n",
    "                    val_acc_best = val_acc_now\n",
    "                    # wandb ÌÇ§Î©¥ state_dictÏïÑÎãåÍ±∞Îäî Ï†ÄÏû• ÏïàÎê®\n",
    "                    # network save\n",
    "                    torch.save(net.state_dict(), f\"net_save/save_now_net_weights_{unique_name}.pth\")\n",
    "\n",
    "                if tr_acc_best < tr_acc:\n",
    "                    tr_acc_best = tr_acc\n",
    "\n",
    "                tr_epoch_loss = tr_epoch_loss_temp\n",
    "                tr_epoch_loss_temp = 0\n",
    "\n",
    "            ####################################################################################################################################################\n",
    "            \n",
    "            ## progress bar update ############################################################################################################\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time = epoch_end_time - epoch_start_time\n",
    "            if iter_of_val == False:\n",
    "                # iterator.set_description(f\"{iter_acc_string}, iter_loss:{iter_loss:10.6f}\") \n",
    "                pass \n",
    "            else:\n",
    "                # iterator.set_description(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%\")  \n",
    "                print(f\"{iter_acc_string2}, tr/val_loss:{tr_epoch_loss:10.6f}/{val_loss:10.6f}, val:{100 * val_acc_now:7.2f}%, val_best:{100 * val_acc_best:7.2f}%, tr:{100 * tr_acc:7.2f}%, tr_best:{100 * tr_acc_best:7.2f}%, epoch time: {epoch_time:.2f} seconds, {epoch_time/60:.2f} minutes\")\n",
    "                iter_of_val = False\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "            ## wandb logging ############################################################################################################\n",
    "            if i == len(train_loader)-1 :\n",
    "                wandb.log({\"iter_acc\": iter_acc})\n",
    "                wandb.log({\"tr_acc\": tr_acc})\n",
    "                wandb.log({\"val_acc_now\": val_acc_now})\n",
    "                wandb.log({\"val_acc_best\": val_acc_best})\n",
    "                wandb.log({\"summary_val_acc\": val_acc_now})\n",
    "                wandb.log({\"epoch\": epoch})\n",
    "                wandb.log({\"val_loss\": val_loss}) \n",
    "                wandb.log({\"tr_epoch_loss\": tr_epoch_loss}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1w\": max_val_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_1b\": max_val_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2w\": max_val_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_2b\": max_val_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3w\": max_val_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"max_val_scale_exp_8bit_3b\": max_val_scale_exp_8bit_box[5]})\n",
    "\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1w\": perc_999_scale_exp_8bit_box[0]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_1b\": perc_999_scale_exp_8bit_box[1]})\n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2w\": perc_999_scale_exp_8bit_box[2]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_2b\": perc_999_scale_exp_8bit_box[3]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3w\": perc_999_scale_exp_8bit_box[4]}) \n",
    "                # wandb.log({\"perc_999_scale_exp_8bit_3b\": perc_999_scale_exp_8bit_box[5]}) \n",
    "\n",
    "            ####################################################################################################################################\n",
    "            \n",
    "        ###### ITERATION END ##########################################################################################################\n",
    "\n",
    "        ## scheduler update #############################################################################\n",
    "        if (scheduler_name != 'no'):\n",
    "            if (scheduler_name == 'ReduceLROnPlateau'):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        #################################################################################################\n",
    "        \n",
    "    #======== EPOCH END ==========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_name = 'main' ## Ïù¥Í±∞ ÏÑ§Ï†ïÌïòÎ©¥ ÏÉàÎ°úÏö¥ Í≤ΩÎ°úÏóê Î™®Îëê save\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "# ## wandb Í≥ºÍ±∞ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏ÏôÄÏÑú Î∂ôÏó¨ÎÑ£Í∏∞ (devices unique_nameÏùÄ ÎãàÍ∞Ä Ìï†ÎãπÌï¥Îùº)#################################\n",
    "# param = {'devices': '3', 'single_step': True, 'unique_name': 'main', 'my_seed': 42, 'TIME': 10, 'BATCH': 16, 'IMAGE_SIZE': 128, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.25, 'lif_layer_v_threshold': 0.75, 'lif_layer_v_reset': 0, 'lif_layer_sg_width': 4, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': 'net_save/save_now_net_weights_{unique_name}.pth', 'learning_rate': 0.001, 'epoch_num': 100, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 2, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': True, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': False, 'denoise_on': True, 'extra_train_dataset': 0, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': True, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 8}\n",
    "# my_snn_system(devices = '0',single_step = param['single_step'],unique_name = unique_name,my_seed = param['my_seed'],TIME = param['TIME'],BATCH = param['BATCH'],IMAGE_SIZE = param['IMAGE_SIZE'],which_data = param['which_data'],data_path = param['data_path'],rate_coding = param['rate_coding'],lif_layer_v_init = param['lif_layer_v_init'],lif_layer_v_decay = param['lif_layer_v_decay'],lif_layer_v_threshold = param['lif_layer_v_threshold'],lif_layer_v_reset = param['lif_layer_v_reset'],lif_layer_sg_width = param['lif_layer_sg_width'],synapse_conv_kernel_size = param['synapse_conv_kernel_size'],synapse_conv_stride = param['synapse_conv_stride'],synapse_conv_padding = param['synapse_conv_padding'],synapse_trace_const1 = param['synapse_trace_const1'],synapse_trace_const2 = param['synapse_trace_const2'],pre_trained = param['pre_trained'],convTrue_fcFalse = param['convTrue_fcFalse'],cfg = param['cfg'],net_print = param['net_print'],pre_trained_path = param['pre_trained_path'],learning_rate = param['learning_rate'],epoch_num = param['epoch_num'],tdBN_on = param['tdBN_on'],BN_on = param['BN_on'],surrogate = param['surrogate'],BPTT_on = param['BPTT_on'],optimizer_what = param['optimizer_what'],scheduler_name = param['scheduler_name'],ddp_on = param['ddp_on'],dvs_clipping = param['dvs_clipping'],dvs_duration = param['dvs_duration'],DFA_on = param['DFA_on'],trace_on = param['trace_on'],OTTT_input_trace_on = param['OTTT_input_trace_on'],exclude_class = param['exclude_class'],merge_polarities = param['merge_polarities'],denoise_on = param['denoise_on'],extra_train_dataset = param['extra_train_dataset'],num_workers = param['num_workers'],chaching_on = param['chaching_on'],pin_memory = param['pin_memory'],UDA_on = param['UDA_on'],alpha_uda = param['alpha_uda'],bias = param['bias'],last_lif = param['last_lif'],temporal_filter = param['temporal_filter'],initial_pooling = param['initial_pooling'],temporal_filter_accumulation= param['temporal_filter_accumulation'])\n",
    "# #############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### my_snn control board (Gesture) ########################\n",
    "# decay = 0.5 # 0.0 # 0.875 0.25 0.125 0.75 0.5\n",
    "# # nda 0.25 # ottt 0.5\n",
    "\n",
    "# unique_name = 'main'\n",
    "# run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\"\n",
    "\n",
    "\n",
    "# wandb.init(project= f'my_snn {unique_name}',save_code=False, dir='/data2/bh_wandb', tags=[\"common\"])\n",
    "\n",
    "# my_snn_system(  devices = \"5\",\n",
    "#                 single_step = True, # True # False # DFA_onÏù¥Îûë Í∞ôÏù¥ Í∞ÄÎùº\n",
    "#                 unique_name = run_name,\n",
    "#                 my_seed = 20664,\n",
    "#                 TIME = 10, # dvscifar 10 # ottt 6 or 10 # nda 10  # Ï†úÏûëÌïòÎäî dvsÏóêÏÑú TIMEÎÑòÍ±∞ÎÇò Ï†ÅÏúºÎ©¥ ÏûêÎ•¥Í±∞ÎÇò PADDINGÌï®\n",
    "#                 BATCH = 1, # batch norm Ìï†Í±∞Î©¥ 2Ïù¥ÏÉÅÏúºÎ°ú Ìï¥ÏïºÌï®   # nda 256   #  ottt 128\n",
    "#                 IMAGE_SIZE = 14, # dvscifar 48 # MNIST 28 # CIFAR10 32 # PMNIST 28 #NMNIST 34 # GESTURE 128\n",
    "#                 # dvsgesture 128, dvs_cifar2 128, nmnist 34, n_caltech101 180,240, n_tidigits 64, heidelberg 700, \n",
    "\n",
    "#                 # DVS_CIFAR10 Ìï†Í±∞Î©¥ time 10ÏúºÎ°ú Ìï¥Îùº\n",
    "#                 which_data = 'DVS_GESTURE_TONIC',\n",
    "# # 'CIFAR100' 'CIFAR10' 'MNIST' 'FASHION_MNIST' 'DVS_CIFAR10' 'PMNIST'ÏïÑÏßÅ\n",
    "# # 'DVS_GESTURE', 'DVS_GESTURE_TONIC','DVS_CIFAR10_2','NMNIST','NMNIST_TONIC','CIFAR10','N_CALTECH101','n_tidigits','heidelberg'\n",
    "#                 # CLASS_NUM = 10,\n",
    "#                 data_path = '/data2', # YOU NEED TO CHANGE THIS\n",
    "#                 rate_coding = False, # True # False\n",
    "\n",
    "#                 lif_layer_v_init = 0.0,\n",
    "#                 lif_layer_v_decay = decay,\n",
    "#                 lif_layer_v_threshold = 0.5,   #nda 0.5  #ottt 1.0\n",
    "#                 lif_layer_v_reset = 10000.0, # 10000Ïù¥ÏÉÅÏùÄ hardreset (ÎÇ¥ LIFÏì∞Í∏∞Îäî Ìï® „Öá„Öá)\n",
    "#                 lif_layer_sg_width = 6.0, # 2.570969004857107 # sigmoidÎ•òÏóêÏÑúÎäî alphaÍ∞í 4.0, rectangleÎ•òÏóêÏÑúÎäî widthÍ∞í 0.5\n",
    "\n",
    "#                 # synapse_conv_in_channels = IMAGE_PIXEL_CHANNEL,\n",
    "#                 synapse_conv_kernel_size = 3,\n",
    "#                 synapse_conv_stride = 1,\n",
    "#                 synapse_conv_padding = 1,\n",
    "\n",
    "#                 synapse_trace_const1 = 1, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÌòÑÏû¨ spikeÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. Í±ç 1Î°ú ÎëêÏÖà.\n",
    "#                 synapse_trace_const2 = decay, # ÌòÑÏû¨ traceÍµ¨Ìï† Îïå ÏßÅÏ†Ñ traceÏóê Í≥±Ìï¥ÏßÄÎäî ÏÉÅÏàò. lif_layer_v_decayÏôÄ Í∞ôÍ≤å Ìï† Í≤ÉÏùÑ Ï∂îÏ≤ú\n",
    "\n",
    "#                 # synapse_fc_out_features = CLASS_NUM,\n",
    "\n",
    "#                 pre_trained = False, # True # False\n",
    "#                 convTrue_fcFalse = False, # True # False\n",
    "\n",
    "#                 # 'P' for average pooling, 'D' for (1,1) aver pooling, 'M' for maxpooling, 'L' for linear classifier, [  ] for residual block\n",
    "#                 # convÏóêÏÑú 10000 Ïù¥ÏÉÅÏùÄ depth-wise separable (BPTTÎßå ÏßÄÏõê), 20000Ïù¥ÏÉÅÏùÄ depth-wise (BPTTÎßå ÏßÄÏõê)\n",
    "#                 # cfg = ['M', 'M', 32, 'P', 32, 'P', 32, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'P', 64, 'P', 64, 'P'], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'M', 128, 'M'], \n",
    "#                 cfg = [200, 200], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96], \n",
    "#                 # cfg = ['M', 'M', 64, 'M', 96, 'L', 512, 512], \n",
    "#                 # cfg = ['M', 'M', 64], \n",
    "#                 # cfg = [64, 124, 64, 124],\n",
    "#                 # cfg = ['M','M',512], \n",
    "#                 # cfg = [512], \n",
    "#                 # cfg = ['M', 'M', 64, 128, 'P', 128, 'P'], \n",
    "#                 # cfg = ['M','M',512],\n",
    "#                 # cfg = ['M',200],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = ['M','M',200,200],\n",
    "#                 # cfg = ([200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = (['M','M',200],[200],[200],[2]), # (feature extractor, classifier, domain adapter, # of domain)\n",
    "#                 # cfg = ['M',200,200],\n",
    "#                 # cfg = ['M','M',1024,512,256,128,64],\n",
    "#                 # cfg = [200,200],\n",
    "#                 # cfg = [12], #fc\n",
    "#                 # cfg = [12, 'M', 48, 'M', 12], \n",
    "#                 # cfg = [64,[64,64],64], # ÎÅùÏóê linear classifier ÌïòÎÇò ÏûêÎèôÏúºÎ°ú Î∂ôÏäµÎãàÎã§\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512, 'D'], #ottt\n",
    "#                 # cfg = [64, 128, 'P', 256, 256, 'P', 512, 512, 'P', 512, 512], \n",
    "#                 # cfg = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512], \n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'D'], # nda\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512], # nda 128pixel\n",
    "#                 # cfg = [64, 'P', 128, 'P', 256, 256, 'P', 512, 512, 512, 512, 'L', 4096, 4096],\n",
    "#                 # cfg = [20001,10001], # depthwise, separable\n",
    "#                 # cfg = [64,20064,10001], # vanilla conv, depthwise, separable\n",
    "#                 # cfg = [8, 'P', 8, 'P', 8, 'P', 8,'P', 8, 'P'],\n",
    "#                 # cfg = [],        \n",
    "                \n",
    "#                 net_print = True, # True # False # TrueÎ°ú ÌïòÍ∏∏ Ï∂îÏ≤ú\n",
    "                \n",
    "#                 pre_trained_path = f\"net_save/save_now_net_weights_{unique_name}.pth\",\n",
    "#                 # learning_rate = 0.001, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 learning_rate = 1/512, #0.1 bptt, #0.01 ottt, # default 0.001  # ottt 0.1 # nda 0.001 # 0.00936191669529645\n",
    "#                 epoch_num = 300,\n",
    "#                 tdBN_on = False,  # True # False\n",
    "#                 BN_on = False,  # True # False\n",
    "                \n",
    "#                 surrogate = 'hard_sigmoid', # 'sigmoid' 'rectangle' 'rough_rectangle' 'hard_sigmoid'\n",
    "                \n",
    "#                 BPTT_on = False,  # True # False # TrueÏù¥Î©¥ BPTT, FalseÏù¥Î©¥ OTTT  # depthwise, separableÏùÄ BPTTÎßå Í∞ÄÎä•\n",
    "                \n",
    "#                 optimizer_what = 'SGD', # 'SGD' 'Adam', 'RMSprop'\n",
    "#                 scheduler_name = 'no', # 'no' 'StepLR' 'ExponentialLR' 'ReduceLROnPlateau' 'CosineAnnealingLR' 'OneCycleLR'\n",
    "                \n",
    "#                 ddp_on = False, # DECREPATED # fALSE\n",
    "\n",
    "#                 dvs_clipping = 14, #ÏùºÎ∞òÏ†ÅÏúºÎ°ú 1 ÎòêÎäî 2 # 100msÎïåÎäî 5 # Ïà´ÏûêÎßåÌÅº ÌÅ¨Î©¥ spike ÏïÑÎãàÎ©¥ Í±ç 0\n",
    "#                 # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # gesture: 100_000c1-5, 25_000c5, 10_000c5, 1_000c5, 1_000_000c5\n",
    "\n",
    "#                 dvs_duration = 25_000, # 0 ÏïÑÎãàÎ©¥ time sampling # dvs number sampling OR time sampling # gesture, cifar-dvs2, nmnist, ncaltech101\n",
    "#                 # ÏûàÎäî Îç∞Ïù¥ÌÑ∞Îì§ #gesture 100_000 25_000 10_000 1_000 1_000_000 #nmnist 10000 #nmnist_tonic 10_000 25_000\n",
    "#                 # Ìïú Ïà´ÏûêÍ∞Ä 1usÏù∏ÎìØ (spikingjellyÏΩîÎìúÏóêÏÑú)\n",
    "#                 # Ìïú Ïû•Ïóê 50 timestepÎßå ÏÉùÏÇ∞Ìï®. Ïã´ÏúºÎ©¥ my_snn/trying/spikingjelly_dvsgestureÏùò__init__.py Î•º Ï∞∏Í≥†Ìï¥Î¥ê\n",
    "#                 # nmnist 5_000us, gestureÎäî 100_000us, 25_000us\n",
    "\n",
    "#                 DFA_on = True, # True # False # single_stepÏù¥Îûë Í∞ôÏù¥ ÏºúÏïº Îê®.\n",
    "\n",
    "#                 trace_on = False,   # True # False\n",
    "#                 OTTT_input_trace_on = False, # True # False # Îß® Ï≤òÏùå inputÏóê trace Ï†ÅÏö© # trace_on FalseÎ©¥ ÏùòÎØ∏ÏóÜÏùå.\n",
    "\n",
    "#                 exclude_class = True, # True # False # gestureÏóêÏÑú 10Î≤àÏß∏ ÌÅ¥ÎûòÏä§ Ï†úÏô∏\n",
    "\n",
    "#                 merge_polarities = True, # True # False # tonic dvs dataset ÏóêÏÑú polarities Ìï©ÏπòÍ∏∞\n",
    "#                 denoise_on = False, # True # False # &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
    "\n",
    "#                 extra_train_dataset = 9, \n",
    "\n",
    "#                 num_workers = 2, # local wslÏóêÏÑúÎäî 2Í∞Ä ÎßûÍ≥†, ÏÑúÎ≤ÑÏóêÏÑúÎäî 4Í∞Ä Ï¢ãÎçîÎùº.\n",
    "#                 chaching_on = True, # True # False # only for certain datasets (gesture_tonic, nmnist_tonic)\n",
    "#                 pin_memory = True, # True # False \n",
    "\n",
    "#                 UDA_on = False,  # DECREPATED # uda\n",
    "#                 alpha_uda = 1.0, # DECREPATED # uda\n",
    "\n",
    "#                 bias = False, # True # False \n",
    "\n",
    "#                 last_lif = False, # True # False \n",
    "\n",
    "#                 temporal_filter = 5, \n",
    "#                 initial_pooling = 1,\n",
    "\n",
    "#                 temporal_filter_accumulation = False, # True # False \n",
    "\n",
    "#                 quantize_bit_list=[8,8,8],\n",
    "#                 scale_exp=[[-10,-10],[-10,-10],[-9,-9]], \n",
    "# # 1w -11~-9\n",
    "# # 1b -11~ -7\n",
    "# # 2w -10~-8\n",
    "# # 2b -10~-8\n",
    "# # 3w -10\n",
    "# # 3b -10\n",
    "#                 ) \n",
    "\n",
    "# # num_workers = 4 * num_GPU (or 8, 16, 2 * num_GPU)\n",
    "# # entry * batch_size * num_worker = num_GPU * GPU_throughtput\n",
    "# # num_workers = batch_size / num_GPU\n",
    "# # num_workers = batch_size / num_CPU\n",
    "\n",
    "# # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "# # average pooling  \n",
    "# # Ïù¥ ÎÇ´Îã§. \n",
    "\n",
    "# # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "# ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yptiex3s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 17138\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhkim003\u001b[0m (\u001b[33mbhkim003-seoul-national-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251008_210043-yptiex3s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yptiex3s' target=\"_blank\">trim-sweep-2</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yptiex3s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yptiex3s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251008_210050_188', 'my_seed': 17138, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 7, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=7, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=7, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 650.0\n",
      "lif layer 1 self.abs_max_v: 650.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 1 self.abs_max_out: 847.0\n",
      "lif layer 1 self.abs_max_v: 988.0\n",
      "fc layer 2 self.abs_max_out: 287.0\n",
      "lif layer 2 self.abs_max_v: 287.0\n",
      "fc layer 2 self.abs_max_out: 291.0\n",
      "lif layer 2 self.abs_max_v: 387.5\n",
      "fc layer 2 self.abs_max_out: 421.0\n",
      "lif layer 2 self.abs_max_v: 473.5\n",
      "lif layer 1 self.abs_max_v: 1015.5\n",
      "fc layer 1 self.abs_max_out: 1079.0\n",
      "lif layer 1 self.abs_max_v: 1079.0\n",
      "lif layer 2 self.abs_max_v: 527.0\n",
      "fc layer 3 self.abs_max_out: 32.0\n",
      "fc layer 1 self.abs_max_out: 1184.0\n",
      "lif layer 1 self.abs_max_v: 1184.0\n",
      "lif layer 2 self.abs_max_v: 663.5\n",
      "fc layer 2 self.abs_max_out: 443.0\n",
      "fc layer 3 self.abs_max_out: 36.0\n",
      "lif layer 1 self.abs_max_v: 1307.5\n",
      "fc layer 2 self.abs_max_out: 561.0\n",
      "lif layer 2 self.abs_max_v: 684.5\n",
      "fc layer 3 self.abs_max_out: 105.0\n",
      "lif layer 2 self.abs_max_v: 817.0\n",
      "fc layer 2 self.abs_max_out: 724.0\n",
      "fc layer 3 self.abs_max_out: 135.0\n",
      "lif layer 2 self.abs_max_v: 863.5\n",
      "fc layer 1 self.abs_max_out: 1234.0\n",
      "lif layer 1 self.abs_max_v: 1349.0\n",
      "fc layer 1 self.abs_max_out: 1589.0\n",
      "lif layer 1 self.abs_max_v: 1589.0\n",
      "lif layer 2 self.abs_max_v: 899.0\n",
      "fc layer 1 self.abs_max_out: 1724.0\n",
      "lif layer 1 self.abs_max_v: 1724.0\n",
      "lif layer 2 self.abs_max_v: 945.5\n",
      "fc layer 1 self.abs_max_out: 1741.0\n",
      "lif layer 1 self.abs_max_v: 1741.0\n",
      "lif layer 2 self.abs_max_v: 959.5\n",
      "lif layer 2 self.abs_max_v: 1016.0\n",
      "lif layer 2 self.abs_max_v: 1025.0\n",
      "fc layer 2 self.abs_max_out: 725.0\n",
      "lif layer 2 self.abs_max_v: 1060.5\n",
      "lif layer 2 self.abs_max_v: 1113.5\n",
      "fc layer 2 self.abs_max_out: 779.0\n",
      "fc layer 2 self.abs_max_out: 801.0\n",
      "fc layer 3 self.abs_max_out: 172.0\n",
      "lif layer 2 self.abs_max_v: 1115.0\n",
      "lif layer 2 self.abs_max_v: 1181.5\n",
      "fc layer 3 self.abs_max_out: 179.0\n",
      "fc layer 1 self.abs_max_out: 1746.0\n",
      "lif layer 1 self.abs_max_v: 1746.0\n",
      "fc layer 2 self.abs_max_out: 847.0\n",
      "fc layer 3 self.abs_max_out: 202.0\n",
      "fc layer 2 self.abs_max_out: 917.0\n",
      "fc layer 3 self.abs_max_out: 225.0\n",
      "fc layer 2 self.abs_max_out: 1035.0\n",
      "fc layer 3 self.abs_max_out: 257.0\n",
      "fc layer 2 self.abs_max_out: 1039.0\n",
      "fc layer 2 self.abs_max_out: 1053.0\n",
      "fc layer 2 self.abs_max_out: 1087.0\n",
      "fc layer 1 self.abs_max_out: 1755.0\n",
      "lif layer 1 self.abs_max_v: 1755.0\n",
      "lif layer 2 self.abs_max_v: 1219.0\n",
      "lif layer 2 self.abs_max_v: 1387.0\n",
      "fc layer 1 self.abs_max_out: 1915.0\n",
      "lif layer 1 self.abs_max_v: 1915.0\n",
      "fc layer 2 self.abs_max_out: 1207.0\n",
      "fc layer 2 self.abs_max_out: 1375.0\n",
      "lif layer 2 self.abs_max_v: 1436.0\n",
      "lif layer 2 self.abs_max_v: 1564.0\n",
      "fc layer 1 self.abs_max_out: 2037.0\n",
      "lif layer 1 self.abs_max_v: 2037.0\n",
      "fc layer 1 self.abs_max_out: 2112.0\n",
      "lif layer 1 self.abs_max_v: 2112.0\n",
      "fc layer 2 self.abs_max_out: 1377.0\n",
      "lif layer 2 self.abs_max_v: 1607.5\n",
      "fc layer 2 self.abs_max_out: 1389.0\n",
      "fc layer 2 self.abs_max_out: 1461.0\n",
      "fc layer 3 self.abs_max_out: 263.0\n",
      "fc layer 3 self.abs_max_out: 273.0\n",
      "fc layer 1 self.abs_max_out: 2227.0\n",
      "lif layer 1 self.abs_max_v: 2227.0\n",
      "lif layer 2 self.abs_max_v: 1645.5\n",
      "fc layer 3 self.abs_max_out: 304.0\n",
      "lif layer 2 self.abs_max_v: 1658.5\n",
      "lif layer 2 self.abs_max_v: 1771.5\n",
      "fc layer 1 self.abs_max_out: 2420.0\n",
      "lif layer 1 self.abs_max_v: 2420.0\n",
      "fc layer 1 self.abs_max_out: 2847.0\n",
      "lif layer 1 self.abs_max_v: 2847.0\n",
      "lif layer 2 self.abs_max_v: 1823.5\n",
      "lif layer 2 self.abs_max_v: 1997.0\n",
      "fc layer 2 self.abs_max_out: 1511.0\n",
      "fc layer 3 self.abs_max_out: 330.0\n",
      "fc layer 2 self.abs_max_out: 1672.0\n",
      "fc layer 1 self.abs_max_out: 3067.0\n",
      "lif layer 1 self.abs_max_v: 3067.0\n",
      "fc layer 2 self.abs_max_out: 1756.0\n",
      "fc layer 2 self.abs_max_out: 1793.0\n",
      "fc layer 2 self.abs_max_out: 1820.0\n",
      "fc layer 3 self.abs_max_out: 333.0\n",
      "fc layer 3 self.abs_max_out: 352.0\n",
      "fc layer 1 self.abs_max_out: 3077.0\n",
      "lif layer 1 self.abs_max_v: 3077.0\n",
      "fc layer 3 self.abs_max_out: 360.0\n",
      "fc layer 3 self.abs_max_out: 364.0\n",
      "fc layer 1 self.abs_max_out: 3177.0\n",
      "lif layer 1 self.abs_max_v: 3177.0\n",
      "fc layer 1 self.abs_max_out: 3337.0\n",
      "lif layer 1 self.abs_max_v: 3337.0\n",
      "lif layer 1 self.abs_max_v: 3388.0\n",
      "fc layer 1 self.abs_max_out: 3690.0\n",
      "lif layer 1 self.abs_max_v: 3690.0\n",
      "fc layer 1 self.abs_max_out: 3766.0\n",
      "lif layer 1 self.abs_max_v: 3766.0\n",
      "fc layer 2 self.abs_max_out: 1854.0\n",
      "fc layer 2 self.abs_max_out: 1960.0\n",
      "fc layer 2 self.abs_max_out: 2068.0\n",
      "lif layer 2 self.abs_max_v: 2068.0\n",
      "fc layer 1 self.abs_max_out: 3990.0\n",
      "lif layer 1 self.abs_max_v: 3990.0\n",
      "lif layer 2 self.abs_max_v: 2250.0\n",
      "lif layer 1 self.abs_max_v: 3992.5\n",
      "fc layer 1 self.abs_max_out: 4066.0\n",
      "lif layer 1 self.abs_max_v: 4066.0\n",
      "fc layer 1 self.abs_max_out: 4170.0\n",
      "lif layer 1 self.abs_max_v: 4170.0\n",
      "lif layer 2 self.abs_max_v: 2270.5\n",
      "lif layer 1 self.abs_max_v: 4494.0\n",
      "lif layer 2 self.abs_max_v: 2334.0\n",
      "fc layer 1 self.abs_max_out: 4241.0\n",
      "fc layer 1 self.abs_max_out: 4246.0\n",
      "fc layer 1 self.abs_max_out: 4294.0\n",
      "fc layer 1 self.abs_max_out: 4512.0\n",
      "lif layer 1 self.abs_max_v: 4512.0\n",
      "lif layer 1 self.abs_max_v: 4866.5\n",
      "fc layer 1 self.abs_max_out: 4560.0\n",
      "fc layer 1 self.abs_max_out: 5056.0\n",
      "lif layer 1 self.abs_max_v: 5056.0\n",
      "fc layer 2 self.abs_max_out: 2154.0\n",
      "fc layer 1 self.abs_max_out: 5145.0\n",
      "lif layer 1 self.abs_max_v: 5145.0\n",
      "fc layer 3 self.abs_max_out: 374.0\n",
      "fc layer 2 self.abs_max_out: 2169.0\n",
      "fc layer 2 self.abs_max_out: 2208.0\n",
      "lif layer 2 self.abs_max_v: 2403.0\n",
      "lif layer 2 self.abs_max_v: 2512.5\n",
      "fc layer 2 self.abs_max_out: 2261.0\n",
      "fc layer 1 self.abs_max_out: 5304.0\n",
      "lif layer 1 self.abs_max_v: 5304.0\n",
      "fc layer 2 self.abs_max_out: 2380.0\n",
      "fc layer 2 self.abs_max_out: 2425.0\n",
      "fc layer 1 self.abs_max_out: 5451.0\n",
      "lif layer 1 self.abs_max_v: 5451.0\n",
      "fc layer 2 self.abs_max_out: 2514.0\n",
      "lif layer 2 self.abs_max_v: 2514.0\n",
      "lif layer 2 self.abs_max_v: 2525.5\n",
      "fc layer 1 self.abs_max_out: 5924.0\n",
      "lif layer 1 self.abs_max_v: 5924.0\n",
      "lif layer 2 self.abs_max_v: 2538.5\n",
      "lif layer 2 self.abs_max_v: 2795.0\n",
      "lif layer 1 self.abs_max_v: 6351.5\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.105321/  2.156669, val:  44.58%, val_best:  44.58%, tr:  93.37%, tr_best:  93.37%, epoch time: 267.76 seconds, 4.46 minutes\n",
      "total_backward_count 44360 real_backward_count 11783  26.562%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 3 self.abs_max_out: 380.0\n",
      "fc layer 1 self.abs_max_out: 6333.0\n",
      "lif layer 2 self.abs_max_v: 2843.0\n",
      "fc layer 2 self.abs_max_out: 2594.0\n",
      "fc layer 1 self.abs_max_out: 6342.0\n",
      "fc layer 1 self.abs_max_out: 6459.0\n",
      "lif layer 1 self.abs_max_v: 6459.0\n",
      "lif layer 2 self.abs_max_v: 2856.0\n",
      "fc layer 1 self.abs_max_out: 6478.0\n",
      "lif layer 1 self.abs_max_v: 6478.0\n",
      "fc layer 2 self.abs_max_out: 2670.0\n",
      "fc layer 2 self.abs_max_out: 2683.0\n",
      "lif layer 2 self.abs_max_v: 2866.0\n",
      "lif layer 2 self.abs_max_v: 3234.0\n",
      "fc layer 1 self.abs_max_out: 6604.0\n",
      "lif layer 1 self.abs_max_v: 6604.0\n",
      "fc layer 2 self.abs_max_out: 2796.0\n",
      "fc layer 2 self.abs_max_out: 2822.0\n",
      "fc layer 1 self.abs_max_out: 6659.0\n",
      "lif layer 1 self.abs_max_v: 6659.0\n",
      "fc layer 1 self.abs_max_out: 6867.0\n",
      "lif layer 1 self.abs_max_v: 6867.0\n",
      "lif layer 1 self.abs_max_v: 6918.5\n",
      "lif layer 1 self.abs_max_v: 7591.5\n",
      "lif layer 1 self.abs_max_v: 7631.0\n",
      "fc layer 2 self.abs_max_out: 2915.0\n",
      "fc layer 2 self.abs_max_out: 3026.0\n",
      "lif layer 1 self.abs_max_v: 8088.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.095738/  2.164081, val:  62.50%, val_best:  62.50%, tr:  99.26%, tr_best:  99.26%, epoch time: 268.35 seconds, 4.47 minutes\n",
      "total_backward_count 88720 real_backward_count 19766  22.279%\n",
      "fc layer 1 self.abs_max_out: 6935.0\n",
      "fc layer 1 self.abs_max_out: 6994.0\n",
      "fc layer 1 self.abs_max_out: 7308.0\n",
      "fc layer 2 self.abs_max_out: 3092.0\n",
      "lif layer 2 self.abs_max_v: 3288.0\n",
      "lif layer 2 self.abs_max_v: 3361.0\n",
      "fc layer 1 self.abs_max_out: 7527.0\n",
      "lif layer 2 self.abs_max_v: 3445.0\n",
      "lif layer 2 self.abs_max_v: 3570.0\n",
      "lif layer 2 self.abs_max_v: 3602.5\n",
      "fc layer 2 self.abs_max_out: 3100.0\n",
      "lif layer 1 self.abs_max_v: 8451.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.095268/  2.144948, val:  60.83%, val_best:  62.50%, tr:  99.37%, tr_best:  99.37%, epoch time: 268.31 seconds, 4.47 minutes\n",
      "total_backward_count 133080 real_backward_count 26928  20.234%\n",
      "fc layer 1 self.abs_max_out: 7686.0\n",
      "fc layer 2 self.abs_max_out: 3128.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.079469/  2.140355, val:  54.58%, val_best:  62.50%, tr:  99.53%, tr_best:  99.53%, epoch time: 267.46 seconds, 4.46 minutes\n",
      "total_backward_count 177440 real_backward_count 33908  19.110%\n",
      "fc layer 1 self.abs_max_out: 7783.0\n",
      "lif layer 2 self.abs_max_v: 3767.0\n",
      "fc layer 2 self.abs_max_out: 3139.0\n",
      "lif layer 1 self.abs_max_v: 8558.0\n",
      "fc layer 1 self.abs_max_out: 7845.0\n",
      "fc layer 2 self.abs_max_out: 3206.0\n",
      "lif layer 1 self.abs_max_v: 9319.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.074787/  2.134470, val:  62.92%, val_best:  62.92%, tr:  99.59%, tr_best:  99.59%, epoch time: 270.53 seconds, 4.51 minutes\n",
      "total_backward_count 221800 real_backward_count 40595  18.303%\n",
      "fc layer 1 self.abs_max_out: 7980.0\n",
      "fc layer 1 self.abs_max_out: 8069.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.065656/  2.125569, val:  57.50%, val_best:  62.92%, tr:  99.62%, tr_best:  99.62%, epoch time: 265.95 seconds, 4.43 minutes\n",
      "total_backward_count 266160 real_backward_count 46979  17.651%\n",
      "fc layer 1 self.abs_max_out: 8389.0\n",
      "fc layer 1 self.abs_max_out: 8450.0\n",
      "lif layer 1 self.abs_max_v: 9695.0\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.050823/  2.128003, val:  68.75%, val_best:  68.75%, tr:  99.84%, tr_best:  99.84%, epoch time: 266.07 seconds, 4.43 minutes\n",
      "total_backward_count 310520 real_backward_count 52783  16.998%\n",
      "fc layer 2 self.abs_max_out: 3289.0\n",
      "lif layer 1 self.abs_max_v: 10039.5\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  2.057316/  2.116292, val:  55.42%, val_best:  68.75%, tr:  99.75%, tr_best:  99.84%, epoch time: 264.17 seconds, 4.40 minutes\n",
      "total_backward_count 354880 real_backward_count 58585  16.508%\n",
      "fc layer 1 self.abs_max_out: 8464.0\n",
      "fc layer 2 self.abs_max_out: 3331.0\n",
      "fc layer 2 self.abs_max_out: 3353.0\n",
      "lif layer 1 self.abs_max_v: 10409.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  2.042868/  2.099437, val:  69.17%, val_best:  69.17%, tr:  99.75%, tr_best:  99.84%, epoch time: 266.65 seconds, 4.44 minutes\n",
      "total_backward_count 399240 real_backward_count 64139  16.065%\n",
      "fc layer 1 self.abs_max_out: 8604.0\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  2.042425/  2.099694, val:  80.00%, val_best:  80.00%, tr:  99.86%, tr_best:  99.86%, epoch time: 268.19 seconds, 4.47 minutes\n",
      "total_backward_count 443600 real_backward_count 69438  15.653%\n",
      "fc layer 1 self.abs_max_out: 8731.0\n",
      "lif layer 1 self.abs_max_v: 10537.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  2.040603/  2.098788, val:  84.17%, val_best:  84.17%, tr:  99.91%, tr_best:  99.91%, epoch time: 263.44 seconds, 4.39 minutes\n",
      "total_backward_count 487960 real_backward_count 74602  15.289%\n",
      "fc layer 1 self.abs_max_out: 8757.0\n",
      "fc layer 1 self.abs_max_out: 8853.0\n",
      "lif layer 1 self.abs_max_v: 10578.5\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  2.036076/  2.092068, val:  70.00%, val_best:  84.17%, tr:  99.89%, tr_best:  99.91%, epoch time: 262.79 seconds, 4.38 minutes\n",
      "total_backward_count 532320 real_backward_count 79477  14.930%\n",
      "fc layer 1 self.abs_max_out: 8896.0\n",
      "lif layer 1 self.abs_max_v: 10602.5\n",
      "lif layer 2 self.abs_max_v: 3780.5\n",
      "fc layer 2 self.abs_max_out: 3354.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  2.037122/  2.099385, val:  68.33%, val_best:  84.17%, tr:  99.86%, tr_best:  99.91%, epoch time: 264.85 seconds, 4.41 minutes\n",
      "total_backward_count 576680 real_backward_count 84199  14.601%\n",
      "lif layer 2 self.abs_max_v: 3865.5\n",
      "fc layer 1 self.abs_max_out: 8922.0\n",
      "fc layer 2 self.abs_max_out: 3393.0\n",
      "lif layer 2 self.abs_max_v: 3876.0\n",
      "fc layer 2 self.abs_max_out: 3514.0\n",
      "fc layer 1 self.abs_max_out: 8954.0\n",
      "lif layer 1 self.abs_max_v: 10687.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  2.025530/  2.097376, val:  82.50%, val_best:  84.17%, tr:  99.93%, tr_best:  99.93%, epoch time: 265.21 seconds, 4.42 minutes\n",
      "total_backward_count 621040 real_backward_count 88626  14.271%\n",
      "lif layer 1 self.abs_max_v: 11099.0\n",
      "fc layer 1 self.abs_max_out: 9064.0\n",
      "lif layer 1 self.abs_max_v: 11113.5\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  2.016075/  2.077910, val:  75.42%, val_best:  84.17%, tr:  99.95%, tr_best:  99.95%, epoch time: 260.97 seconds, 4.35 minutes\n",
      "total_backward_count 665400 real_backward_count 93045  13.983%\n",
      "lif layer 1 self.abs_max_v: 11304.5\n",
      "fc layer 1 self.abs_max_out: 9108.0\n",
      "fc layer 3 self.abs_max_out: 383.0\n",
      "lif layer 2 self.abs_max_v: 3920.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  2.009289/  2.060932, val:  77.50%, val_best:  84.17%, tr:  99.89%, tr_best:  99.95%, epoch time: 264.47 seconds, 4.41 minutes\n",
      "total_backward_count 709760 real_backward_count 97326  13.713%\n",
      "fc layer 1 self.abs_max_out: 9243.0\n",
      "lif layer 1 self.abs_max_v: 11346.5\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  2.012961/  2.083447, val:  78.33%, val_best:  84.17%, tr:  99.93%, tr_best:  99.95%, epoch time: 265.35 seconds, 4.42 minutes\n",
      "total_backward_count 754120 real_backward_count 101412  13.448%\n",
      "lif layer 1 self.abs_max_v: 11575.5\n",
      "lif layer 1 self.abs_max_v: 12071.0\n",
      "fc layer 1 self.abs_max_out: 9325.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  2.011100/  2.064587, val:  85.00%, val_best:  85.00%, tr:  99.93%, tr_best:  99.95%, epoch time: 264.62 seconds, 4.41 minutes\n",
      "total_backward_count 798480 real_backward_count 105502  13.213%\n",
      "lif layer 2 self.abs_max_v: 3946.5\n",
      "fc layer 1 self.abs_max_out: 9359.0\n",
      "lif layer 2 self.abs_max_v: 4054.5\n",
      "lif layer 2 self.abs_max_v: 4102.5\n",
      "lif layer 2 self.abs_max_v: 4134.0\n",
      "lif layer 2 self.abs_max_v: 4261.0\n",
      "lif layer 2 self.abs_max_v: 4434.5\n",
      "lif layer 2 self.abs_max_v: 4662.5\n",
      "lif layer 1 self.abs_max_v: 12146.5\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  2.009377/  2.064689, val:  74.17%, val_best:  85.00%, tr:  99.95%, tr_best:  99.95%, epoch time: 265.82 seconds, 4.43 minutes\n",
      "total_backward_count 842840 real_backward_count 109484  12.990%\n",
      "fc layer 1 self.abs_max_out: 9434.0\n",
      "lif layer 2 self.abs_max_v: 4763.5\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  2.001145/  2.061048, val:  77.92%, val_best:  85.00%, tr:  99.98%, tr_best:  99.98%, epoch time: 265.60 seconds, 4.43 minutes\n",
      "total_backward_count 887200 real_backward_count 113313  12.772%\n",
      "fc layer 1 self.abs_max_out: 9552.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.998862/  2.063101, val:  80.83%, val_best:  85.00%, tr:  99.98%, tr_best:  99.98%, epoch time: 266.48 seconds, 4.44 minutes\n",
      "total_backward_count 931560 real_backward_count 116966  12.556%\n",
      "fc layer 1 self.abs_max_out: 9559.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.996212/  2.060487, val:  78.75%, val_best:  85.00%, tr:  99.98%, tr_best:  99.98%, epoch time: 265.52 seconds, 4.43 minutes\n",
      "total_backward_count 975920 real_backward_count 120663  12.364%\n",
      "fc layer 2 self.abs_max_out: 3646.0\n",
      "fc layer 1 self.abs_max_out: 9560.0\n",
      "lif layer 1 self.abs_max_v: 12486.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.994486/  2.066948, val:  83.33%, val_best:  85.00%, tr:  99.98%, tr_best:  99.98%, epoch time: 261.71 seconds, 4.36 minutes\n",
      "total_backward_count 1020280 real_backward_count 124177  12.171%\n",
      "fc layer 1 self.abs_max_out: 9657.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  2.000046/  2.060814, val:  76.67%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.15 seconds, 4.44 minutes\n",
      "total_backward_count 1064640 real_backward_count 127670  11.992%\n",
      "fc layer 1 self.abs_max_out: 9694.0\n",
      "lif layer 1 self.abs_max_v: 12847.0\n",
      "lif layer 2 self.abs_max_v: 4872.5\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.999279/  2.060609, val:  76.25%, val_best:  85.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.94 seconds, 4.45 minutes\n",
      "total_backward_count 1109000 real_backward_count 131050  11.817%\n",
      "lif layer 1 self.abs_max_v: 13196.5\n",
      "lif layer 2 self.abs_max_v: 4938.0\n",
      "fc layer 1 self.abs_max_out: 9746.0\n",
      "fc layer 1 self.abs_max_out: 9802.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.998393/  2.049205, val:  82.50%, val_best:  85.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 262.83 seconds, 4.38 minutes\n",
      "total_backward_count 1153360 real_backward_count 134306  11.645%\n",
      "fc layer 1 self.abs_max_out: 9806.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.985564/  2.048757, val:  86.25%, val_best:  86.25%, tr:  99.98%, tr_best: 100.00%, epoch time: 262.73 seconds, 4.38 minutes\n",
      "total_backward_count 1197720 real_backward_count 137511  11.481%\n",
      "fc layer 1 self.abs_max_out: 9836.0\n",
      "lif layer 2 self.abs_max_v: 5092.5\n",
      "lif layer 2 self.abs_max_v: 5173.5\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.984182/  2.044431, val:  79.58%, val_best:  86.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.48 seconds, 4.44 minutes\n",
      "total_backward_count 1242080 real_backward_count 140655  11.324%\n",
      "fc layer 1 self.abs_max_out: 9914.0\n",
      "fc layer 3 self.abs_max_out: 386.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.981304/  2.048667, val:  86.67%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 263.09 seconds, 4.38 minutes\n",
      "total_backward_count 1286440 real_backward_count 143716  11.172%\n",
      "lif layer 1 self.abs_max_v: 13298.5\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.980204/  2.046591, val:  84.17%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.97 seconds, 4.47 minutes\n",
      "total_backward_count 1330800 real_backward_count 146737  11.026%\n",
      "lif layer 1 self.abs_max_v: 13686.5\n",
      "fc layer 1 self.abs_max_out: 9915.0\n",
      "lif layer 2 self.abs_max_v: 5214.5\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.978341/  2.039608, val:  85.42%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 264.88 seconds, 4.41 minutes\n",
      "total_backward_count 1375160 real_backward_count 149678  10.884%\n",
      "lif layer 2 self.abs_max_v: 5219.5\n",
      "fc layer 1 self.abs_max_out: 9942.0\n",
      "lif layer 2 self.abs_max_v: 5286.5\n",
      "lif layer 1 self.abs_max_v: 13801.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.975438/  2.032908, val:  85.00%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.67 seconds, 4.44 minutes\n",
      "total_backward_count 1419520 real_backward_count 152655  10.754%\n",
      "fc layer 1 self.abs_max_out: 9962.0\n",
      "fc layer 1 self.abs_max_out: 10039.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.975253/  2.031752, val:  87.08%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.08 seconds, 4.42 minutes\n",
      "total_backward_count 1463880 real_backward_count 155521  10.624%\n",
      "fc layer 1 self.abs_max_out: 10084.0\n",
      "lif layer 1 self.abs_max_v: 13809.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.961942/  2.045644, val:  80.00%, val_best:  87.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.93 seconds, 4.37 minutes\n",
      "total_backward_count 1508240 real_backward_count 158158  10.486%\n",
      "lif layer 1 self.abs_max_v: 13912.0\n",
      "fc layer 1 self.abs_max_out: 10144.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.974981/  2.049237, val:  87.50%, val_best:  87.50%, tr:  99.95%, tr_best: 100.00%, epoch time: 263.38 seconds, 4.39 minutes\n",
      "total_backward_count 1552600 real_backward_count 160758  10.354%\n",
      "fc layer 1 self.abs_max_out: 10173.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.971685/  2.048272, val:  82.50%, val_best:  87.50%, tr:  99.95%, tr_best: 100.00%, epoch time: 265.91 seconds, 4.43 minutes\n",
      "total_backward_count 1596960 real_backward_count 163373  10.230%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.972015/  2.045345, val:  87.08%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.15 seconds, 4.44 minutes\n",
      "total_backward_count 1641320 real_backward_count 165871  10.106%\n",
      "lif layer 2 self.abs_max_v: 5302.5\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.966895/  2.028768, val:  77.92%, val_best:  87.50%, tr:  99.98%, tr_best: 100.00%, epoch time: 261.31 seconds, 4.36 minutes\n",
      "total_backward_count 1685680 real_backward_count 168348   9.987%\n",
      "lif layer 1 self.abs_max_v: 14024.0\n",
      "fc layer 1 self.abs_max_out: 10191.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.950482/  2.025481, val:  84.17%, val_best:  87.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.87 seconds, 4.45 minutes\n",
      "total_backward_count 1730040 real_backward_count 170794   9.872%\n",
      "fc layer 1 self.abs_max_out: 10200.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.958330/  2.019465, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.90 seconds, 4.42 minutes\n",
      "total_backward_count 1774400 real_backward_count 173283   9.766%\n",
      "fc layer 1 self.abs_max_out: 10219.0\n",
      "lif layer 1 self.abs_max_v: 14155.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.953214/  2.034067, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.39 seconds, 4.44 minutes\n",
      "total_backward_count 1818760 real_backward_count 175595   9.655%\n",
      "fc layer 1 self.abs_max_out: 10307.0\n",
      "fc layer 2 self.abs_max_out: 3770.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.950406/  2.014459, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.17 seconds, 4.44 minutes\n",
      "total_backward_count 1863120 real_backward_count 177979   9.553%\n",
      "fc layer 1 self.abs_max_out: 10317.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.951281/  2.033896, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.77 seconds, 4.43 minutes\n",
      "total_backward_count 1907480 real_backward_count 180287   9.452%\n",
      "fc layer 1 self.abs_max_out: 10323.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.959408/  2.031219, val:  89.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.34 seconds, 4.44 minutes\n",
      "total_backward_count 1951840 real_backward_count 182524   9.351%\n",
      "fc layer 1 self.abs_max_out: 10347.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.955658/  2.026526, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.27 seconds, 4.39 minutes\n",
      "total_backward_count 1996200 real_backward_count 184730   9.254%\n",
      "fc layer 1 self.abs_max_out: 10387.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.947619/  2.019129, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.88 seconds, 4.41 minutes\n",
      "total_backward_count 2040560 real_backward_count 186879   9.158%\n",
      "fc layer 1 self.abs_max_out: 10433.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.946633/  2.031374, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.01 seconds, 4.42 minutes\n",
      "total_backward_count 2084920 real_backward_count 188911   9.061%\n",
      "fc layer 1 self.abs_max_out: 10463.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.943260/  2.022550, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.19 seconds, 4.44 minutes\n",
      "total_backward_count 2129280 real_backward_count 190933   8.967%\n",
      "fc layer 1 self.abs_max_out: 10510.0\n",
      "fc layer 3 self.abs_max_out: 391.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.933374/  1.996010, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.19 seconds, 4.39 minutes\n",
      "total_backward_count 2173640 real_backward_count 192937   8.876%\n",
      "fc layer 1 self.abs_max_out: 10523.0\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.932129/  2.005054, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.53 seconds, 4.43 minutes\n",
      "total_backward_count 2218000 real_backward_count 194931   8.789%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.921941/  2.004656, val:  84.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.74 seconds, 4.43 minutes\n",
      "total_backward_count 2262360 real_backward_count 196819   8.700%\n",
      "fc layer 3 self.abs_max_out: 399.0\n",
      "fc layer 1 self.abs_max_out: 10544.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.917900/  1.992050, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.11 seconds, 4.44 minutes\n",
      "total_backward_count 2306720 real_backward_count 198831   8.620%\n",
      "fc layer 3 self.abs_max_out: 406.0\n",
      "fc layer 1 self.abs_max_out: 10573.0\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.908265/  1.993245, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.86 seconds, 4.45 minutes\n",
      "total_backward_count 2351080 real_backward_count 200722   8.537%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.911730/  2.001616, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.72 seconds, 4.41 minutes\n",
      "total_backward_count 2395440 real_backward_count 202570   8.456%\n",
      "fc layer 1 self.abs_max_out: 10593.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.921287/  2.007510, val:  85.42%, val_best:  91.25%, tr:  99.98%, tr_best: 100.00%, epoch time: 266.83 seconds, 4.45 minutes\n",
      "total_backward_count 2439800 real_backward_count 204444   8.380%\n",
      "fc layer 1 self.abs_max_out: 10636.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.918905/  2.000912, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.21 seconds, 4.44 minutes\n",
      "total_backward_count 2484160 real_backward_count 206253   8.303%\n",
      "fc layer 1 self.abs_max_out: 10695.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.909869/  1.982332, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.62 seconds, 4.34 minutes\n",
      "total_backward_count 2528520 real_backward_count 208059   8.228%\n",
      "lif layer 1 self.abs_max_v: 14173.5\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.907447/  1.988709, val:  91.25%, val_best:  91.25%, tr:  99.98%, tr_best: 100.00%, epoch time: 265.49 seconds, 4.42 minutes\n",
      "total_backward_count 2572880 real_backward_count 209834   8.156%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.903399/  1.986926, val:  82.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.09 seconds, 4.43 minutes\n",
      "total_backward_count 2617240 real_backward_count 211594   8.085%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.886738/  1.971963, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.46 seconds, 4.42 minutes\n",
      "total_backward_count 2661600 real_backward_count 213250   8.012%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.890295/  1.985030, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.56 seconds, 4.36 minutes\n",
      "total_backward_count 2705960 real_backward_count 214905   7.942%\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.885207/  1.967567, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.43 seconds, 4.46 minutes\n",
      "total_backward_count 2750320 real_backward_count 216529   7.873%\n",
      "lif layer 1 self.abs_max_v: 14233.5\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.884923/  1.961031, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.16 seconds, 4.42 minutes\n",
      "total_backward_count 2794680 real_backward_count 218137   7.805%\n",
      "fc layer 3 self.abs_max_out: 411.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.877706/  1.975255, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.19 seconds, 4.42 minutes\n",
      "total_backward_count 2839040 real_backward_count 219683   7.738%\n",
      "fc layer 3 self.abs_max_out: 423.0\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.889849/  1.962744, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.16 seconds, 4.44 minutes\n",
      "total_backward_count 2883400 real_backward_count 221269   7.674%\n",
      "fc layer 3 self.abs_max_out: 427.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.883650/  1.974698, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.06 seconds, 4.43 minutes\n",
      "total_backward_count 2927760 real_backward_count 222824   7.611%\n",
      "fc layer 3 self.abs_max_out: 430.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.875639/  1.968621, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.38 seconds, 4.44 minutes\n",
      "total_backward_count 2972120 real_backward_count 224301   7.547%\n",
      "fc layer 3 self.abs_max_out: 431.0\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.871472/  1.952813, val:  91.25%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.46 seconds, 4.37 minutes\n",
      "total_backward_count 3016480 real_backward_count 225935   7.490%\n",
      "fc layer 3 self.abs_max_out: 445.0\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.871144/  1.962538, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.91 seconds, 4.43 minutes\n",
      "total_backward_count 3060840 real_backward_count 227402   7.429%\n",
      "fc layer 2 self.abs_max_out: 3798.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.869289/  1.954994, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.48 seconds, 4.42 minutes\n",
      "total_backward_count 3105200 real_backward_count 228871   7.371%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.861631/  1.948029, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.87 seconds, 4.40 minutes\n",
      "total_backward_count 3149560 real_backward_count 230304   7.312%\n",
      "fc layer 2 self.abs_max_out: 3814.0\n",
      "fc layer 3 self.abs_max_out: 462.0\n",
      "fc layer 1 self.abs_max_out: 10739.0\n",
      "lif layer 1 self.abs_max_v: 14409.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.860261/  1.951894, val:  87.08%, val_best:  91.25%, tr:  99.98%, tr_best: 100.00%, epoch time: 263.71 seconds, 4.40 minutes\n",
      "total_backward_count 3193920 real_backward_count 231742   7.256%\n",
      "fc layer 1 self.abs_max_out: 10750.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.862499/  1.961407, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.60 seconds, 4.44 minutes\n",
      "total_backward_count 3238280 real_backward_count 233109   7.199%\n",
      "fc layer 3 self.abs_max_out: 468.0\n",
      "fc layer 1 self.abs_max_out: 10761.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.866923/  1.970502, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.79 seconds, 4.40 minutes\n",
      "total_backward_count 3282640 real_backward_count 234470   7.143%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.864771/  1.955768, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.51 seconds, 4.43 minutes\n",
      "total_backward_count 3327000 real_backward_count 235862   7.089%\n",
      "lif layer 1 self.abs_max_v: 14477.5\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.859652/  1.965697, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.24 seconds, 4.44 minutes\n",
      "total_backward_count 3371360 real_backward_count 237219   7.036%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.861600/  1.952178, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.11 seconds, 4.44 minutes\n",
      "total_backward_count 3415720 real_backward_count 238527   6.983%\n",
      "fc layer 2 self.abs_max_out: 3883.0\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.851437/  1.954598, val:  85.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.73 seconds, 4.46 minutes\n",
      "total_backward_count 3460080 real_backward_count 239837   6.932%\n",
      "fc layer 2 self.abs_max_out: 3930.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.853671/  1.959172, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.93 seconds, 4.40 minutes\n",
      "total_backward_count 3504440 real_backward_count 241098   6.880%\n",
      "lif layer 1 self.abs_max_v: 14524.5\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.848426/  1.944260, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.78 seconds, 4.40 minutes\n",
      "total_backward_count 3548800 real_backward_count 242361   6.829%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.850843/  1.949336, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.02 seconds, 4.43 minutes\n",
      "total_backward_count 3593160 real_backward_count 243570   6.779%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.854378/  1.966781, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.57 seconds, 4.44 minutes\n",
      "total_backward_count 3637520 real_backward_count 244884   6.732%\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.852350/  1.944414, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.01 seconds, 4.40 minutes\n",
      "total_backward_count 3681880 real_backward_count 246073   6.683%\n",
      "fc layer 1 self.abs_max_out: 10792.0\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.841971/  1.940569, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.35 seconds, 4.39 minutes\n",
      "total_backward_count 3726240 real_backward_count 247275   6.636%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.841012/  1.943517, val:  87.50%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.19 seconds, 4.42 minutes\n",
      "total_backward_count 3770600 real_backward_count 248472   6.590%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.841425/  1.936023, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.19 seconds, 4.40 minutes\n",
      "total_backward_count 3814960 real_backward_count 249656   6.544%\n",
      "fc layer 1 self.abs_max_out: 10798.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.843511/  1.948512, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.00 seconds, 4.43 minutes\n",
      "total_backward_count 3859320 real_backward_count 250778   6.498%\n",
      "lif layer 1 self.abs_max_v: 14569.0\n",
      "fc layer 1 self.abs_max_out: 10811.0\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.844409/  1.946206, val:  83.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.16 seconds, 4.44 minutes\n",
      "total_backward_count 3903680 real_backward_count 251963   6.454%\n",
      "fc layer 1 self.abs_max_out: 10846.0\n",
      "fc layer 3 self.abs_max_out: 472.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.834824/  1.932228, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.88 seconds, 4.41 minutes\n",
      "total_backward_count 3948040 real_backward_count 253089   6.410%\n",
      "fc layer 3 self.abs_max_out: 478.0\n",
      "lif layer 1 self.abs_max_v: 14730.5\n",
      "fc layer 1 self.abs_max_out: 10855.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.829748/  1.932507, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.43 seconds, 4.46 minutes\n",
      "total_backward_count 3992400 real_backward_count 254190   6.367%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.829729/  1.938747, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.10 seconds, 4.37 minutes\n",
      "total_backward_count 4036760 real_backward_count 255318   6.325%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.828428/  1.932056, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.86 seconds, 4.41 minutes\n",
      "total_backward_count 4081120 real_backward_count 256413   6.283%\n",
      "fc layer 1 self.abs_max_out: 10869.0\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.837756/  1.942770, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.26 seconds, 4.44 minutes\n",
      "total_backward_count 4125480 real_backward_count 257528   6.242%\n",
      "fc layer 2 self.abs_max_out: 3931.0\n",
      "fc layer 3 self.abs_max_out: 481.0\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.827505/  1.936726, val:  90.83%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.34 seconds, 4.46 minutes\n",
      "total_backward_count 4169840 real_backward_count 258602   6.202%\n",
      "fc layer 3 self.abs_max_out: 507.0\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.817093/  1.919888, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.65 seconds, 4.34 minutes\n",
      "total_backward_count 4214200 real_backward_count 259687   6.162%\n",
      "fc layer 1 self.abs_max_out: 10902.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.821141/  1.929639, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.01 seconds, 4.45 minutes\n",
      "total_backward_count 4258560 real_backward_count 260799   6.124%\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.824029/  1.933353, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.64 seconds, 4.41 minutes\n",
      "total_backward_count 4302920 real_backward_count 261854   6.085%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.822382/  1.926957, val:  88.75%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.86 seconds, 4.46 minutes\n",
      "total_backward_count 4347280 real_backward_count 262965   6.049%\n",
      "fc layer 1 self.abs_max_out: 10914.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.818940/  1.931725, val:  85.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.33 seconds, 4.44 minutes\n",
      "total_backward_count 4391640 real_backward_count 264016   6.012%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.810084/  1.922071, val:  86.67%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.98 seconds, 4.47 minutes\n",
      "total_backward_count 4436000 real_backward_count 265062   5.975%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.817360/  1.924842, val:  89.17%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.24 seconds, 4.45 minutes\n",
      "total_backward_count 4480360 real_backward_count 266010   5.937%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.812560/  1.921284, val:  87.92%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.58 seconds, 4.38 minutes\n",
      "total_backward_count 4524720 real_backward_count 267025   5.901%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.809119/  1.902012, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.67 seconds, 4.44 minutes\n",
      "total_backward_count 4569080 real_backward_count 267996   5.865%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.801260/  1.903505, val:  87.08%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.51 seconds, 4.49 minutes\n",
      "total_backward_count 4613440 real_backward_count 268913   5.829%\n",
      "fc layer 1 self.abs_max_out: 10951.0\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.803109/  1.914690, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.09 seconds, 4.47 minutes\n",
      "total_backward_count 4657800 real_backward_count 269887   5.794%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.800516/  1.905991, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.03 seconds, 4.37 minutes\n",
      "total_backward_count 4702160 real_backward_count 270844   5.760%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.804564/  1.917177, val:  88.33%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.45 seconds, 4.42 minutes\n",
      "total_backward_count 4746520 real_backward_count 271825   5.727%\n",
      "fc layer 1 self.abs_max_out: 10962.0\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.806557/  1.908604, val:  89.58%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.29 seconds, 4.40 minutes\n",
      "total_backward_count 4790880 real_backward_count 272777   5.694%\n",
      "fc layer 1 self.abs_max_out: 10968.0\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.800176/  1.910028, val:  90.42%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.36 seconds, 4.44 minutes\n",
      "total_backward_count 4835240 real_backward_count 273683   5.660%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.801728/  1.916818, val:  90.00%, val_best:  91.25%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.97 seconds, 4.43 minutes\n",
      "total_backward_count 4879600 real_backward_count 274545   5.626%\n",
      "fc layer 1 self.abs_max_out: 10998.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.794415/  1.906887, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.10 seconds, 4.44 minutes\n",
      "total_backward_count 4923960 real_backward_count 275503   5.595%\n",
      "fc layer 1 self.abs_max_out: 11016.0\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.791713/  1.911662, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.57 seconds, 4.46 minutes\n",
      "total_backward_count 4968320 real_backward_count 276453   5.564%\n",
      "fc layer 1 self.abs_max_out: 11026.0\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.790473/  1.898607, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.82 seconds, 4.40 minutes\n",
      "total_backward_count 5012680 real_backward_count 277327   5.533%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.785283/  1.901756, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.85 seconds, 4.40 minutes\n",
      "total_backward_count 5057040 real_backward_count 278179   5.501%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.787354/  1.910712, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.89 seconds, 4.43 minutes\n",
      "total_backward_count 5101400 real_backward_count 279073   5.471%\n",
      "fc layer 1 self.abs_max_out: 11045.0\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.794422/  1.912427, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.39 seconds, 4.47 minutes\n",
      "total_backward_count 5145760 real_backward_count 279961   5.441%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.787905/  1.902524, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.47 seconds, 4.47 minutes\n",
      "total_backward_count 5190120 real_backward_count 280824   5.411%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.787843/  1.907384, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.89 seconds, 4.36 minutes\n",
      "total_backward_count 5234480 real_backward_count 281693   5.381%\n",
      "fc layer 2 self.abs_max_out: 4008.0\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.780941/  1.890720, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.11 seconds, 4.40 minutes\n",
      "total_backward_count 5278840 real_backward_count 282570   5.353%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.784797/  1.896145, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.31 seconds, 4.42 minutes\n",
      "total_backward_count 5323200 real_backward_count 283456   5.325%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.771902/  1.881422, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.89 seconds, 4.45 minutes\n",
      "total_backward_count 5367560 real_backward_count 284311   5.297%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.772387/  1.889121, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.54 seconds, 4.48 minutes\n",
      "total_backward_count 5411920 real_backward_count 285073   5.268%\n",
      "fc layer 3 self.abs_max_out: 514.0\n",
      "fc layer 1 self.abs_max_out: 11054.0\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.780686/  1.899360, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.18 seconds, 4.45 minutes\n",
      "total_backward_count 5456280 real_backward_count 285876   5.239%\n",
      "fc layer 1 self.abs_max_out: 11068.0\n",
      "fc layer 3 self.abs_max_out: 517.0\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.775108/  1.891856, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.01 seconds, 4.42 minutes\n",
      "total_backward_count 5500640 real_backward_count 286666   5.212%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.763379/  1.889792, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.91 seconds, 4.37 minutes\n",
      "total_backward_count 5545000 real_backward_count 287528   5.185%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.758760/  1.879493, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.02 seconds, 4.42 minutes\n",
      "total_backward_count 5589360 real_backward_count 288360   5.159%\n",
      "fc layer 1 self.abs_max_out: 11071.0\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.766733/  1.883359, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.77 seconds, 4.45 minutes\n",
      "total_backward_count 5633720 real_backward_count 289167   5.133%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.772524/  1.882035, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.16 seconds, 4.42 minutes\n",
      "total_backward_count 5678080 real_backward_count 289943   5.106%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.762328/  1.871594, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.51 seconds, 4.39 minutes\n",
      "total_backward_count 5722440 real_backward_count 290745   5.081%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.761825/  1.886417, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.20 seconds, 4.40 minutes\n",
      "total_backward_count 5766800 real_backward_count 291532   5.055%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.766331/  1.879148, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.83 seconds, 4.41 minutes\n",
      "total_backward_count 5811160 real_backward_count 292360   5.031%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.771552/  1.892601, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.92 seconds, 4.47 minutes\n",
      "total_backward_count 5855520 real_backward_count 293128   5.006%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.766111/  1.877494, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.19 seconds, 4.47 minutes\n",
      "total_backward_count 5899880 real_backward_count 293844   4.981%\n",
      "fc layer 3 self.abs_max_out: 525.0\n",
      "fc layer 1 self.abs_max_out: 11087.0\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.772178/  1.882487, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.51 seconds, 4.43 minutes\n",
      "total_backward_count 5944240 real_backward_count 294553   4.955%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.771281/  1.887634, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.32 seconds, 4.42 minutes\n",
      "total_backward_count 5988600 real_backward_count 295273   4.931%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.766412/  1.877031, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.82 seconds, 4.36 minutes\n",
      "total_backward_count 6032960 real_backward_count 295953   4.906%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.763505/  1.886445, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.80 seconds, 4.45 minutes\n",
      "total_backward_count 6077320 real_backward_count 296682   4.882%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.763430/  1.875532, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.49 seconds, 4.44 minutes\n",
      "total_backward_count 6121680 real_backward_count 297395   4.858%\n",
      "fc layer 2 self.abs_max_out: 4076.0\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.760858/  1.873850, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.88 seconds, 4.43 minutes\n",
      "total_backward_count 6166040 real_backward_count 298090   4.834%\n",
      "fc layer 3 self.abs_max_out: 526.0\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.767376/  1.884859, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.87 seconds, 4.45 minutes\n",
      "total_backward_count 6210400 real_backward_count 298800   4.811%\n",
      "fc layer 1 self.abs_max_out: 11095.0\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.767144/  1.868738, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.97 seconds, 4.42 minutes\n",
      "total_backward_count 6254760 real_backward_count 299487   4.788%\n",
      "fc layer 1 self.abs_max_out: 11097.0\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.767150/  1.877649, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.95 seconds, 4.40 minutes\n",
      "total_backward_count 6299120 real_backward_count 300157   4.765%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.759488/  1.872872, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.64 seconds, 4.43 minutes\n",
      "total_backward_count 6343480 real_backward_count 300871   4.743%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.759481/  1.882093, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.75 seconds, 4.45 minutes\n",
      "total_backward_count 6387840 real_backward_count 301577   4.721%\n",
      "fc layer 3 self.abs_max_out: 545.0\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.765235/  1.875803, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.77 seconds, 4.43 minutes\n",
      "total_backward_count 6432200 real_backward_count 302272   4.699%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.752327/  1.875214, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.27 seconds, 4.42 minutes\n",
      "total_backward_count 6476560 real_backward_count 302908   4.677%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.759452/  1.869250, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.30 seconds, 4.39 minutes\n",
      "total_backward_count 6520920 real_backward_count 303545   4.655%\n",
      "fc layer 3 self.abs_max_out: 561.0\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.761277/  1.883600, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.31 seconds, 4.41 minutes\n",
      "total_backward_count 6565280 real_backward_count 304225   4.634%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.758604/  1.871662, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.22 seconds, 4.42 minutes\n",
      "total_backward_count 6609640 real_backward_count 304864   4.612%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.762102/  1.878327, val:  87.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.80 seconds, 4.45 minutes\n",
      "total_backward_count 6654000 real_backward_count 305534   4.592%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.752932/  1.872811, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.58 seconds, 4.46 minutes\n",
      "total_backward_count 6698360 real_backward_count 306188   4.571%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.750425/  1.861725, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.85 seconds, 4.35 minutes\n",
      "total_backward_count 6742720 real_backward_count 306818   4.550%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.751382/  1.872349, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.77 seconds, 4.41 minutes\n",
      "total_backward_count 6787080 real_backward_count 307468   4.530%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.761512/  1.882583, val:  86.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.63 seconds, 4.41 minutes\n",
      "total_backward_count 6831440 real_backward_count 308140   4.511%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.760581/  1.880161, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.63 seconds, 4.41 minutes\n",
      "total_backward_count 6875800 real_backward_count 308800   4.491%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.765385/  1.881650, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.97 seconds, 4.45 minutes\n",
      "total_backward_count 6920160 real_backward_count 309457   4.472%\n",
      "fc layer 1 self.abs_max_out: 11111.0\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.764108/  1.883371, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.72 seconds, 4.43 minutes\n",
      "total_backward_count 6964520 real_backward_count 310123   4.453%\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.758444/  1.868038, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.65 seconds, 4.43 minutes\n",
      "total_backward_count 7008880 real_backward_count 310720   4.433%\n",
      "fc layer 1 self.abs_max_out: 11129.0\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.756898/  1.871004, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.03 seconds, 4.35 minutes\n",
      "total_backward_count 7053240 real_backward_count 311414   4.415%\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.758326/  1.879600, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.44 seconds, 4.42 minutes\n",
      "total_backward_count 7097600 real_backward_count 312039   4.396%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.762811/  1.869985, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.67 seconds, 4.44 minutes\n",
      "total_backward_count 7141960 real_backward_count 312671   4.378%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.765128/  1.880827, val:  87.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.48 seconds, 4.41 minutes\n",
      "total_backward_count 7186320 real_backward_count 313238   4.359%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.760462/  1.879537, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.23 seconds, 4.39 minutes\n",
      "total_backward_count 7230680 real_backward_count 313879   4.341%\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.753147/  1.877612, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.14 seconds, 4.39 minutes\n",
      "total_backward_count 7275040 real_backward_count 314492   4.323%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.755672/  1.870054, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.57 seconds, 4.39 minutes\n",
      "total_backward_count 7319400 real_backward_count 315109   4.305%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.754210/  1.873527, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.93 seconds, 4.42 minutes\n",
      "total_backward_count 7363760 real_backward_count 315753   4.288%\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.757491/  1.878856, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.39 seconds, 4.42 minutes\n",
      "total_backward_count 7408120 real_backward_count 316372   4.271%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.751368/  1.866844, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.28 seconds, 4.45 minutes\n",
      "total_backward_count 7452480 real_backward_count 316996   4.254%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.758177/  1.872529, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.34 seconds, 4.41 minutes\n",
      "total_backward_count 7496840 real_backward_count 317563   4.236%\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.756934/  1.871245, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.17 seconds, 4.39 minutes\n",
      "total_backward_count 7541200 real_backward_count 318174   4.219%\n",
      "fc layer 1 self.abs_max_out: 11137.0\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.765784/  1.877101, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.45 seconds, 4.39 minutes\n",
      "total_backward_count 7585560 real_backward_count 318802   4.203%\n",
      "fc layer 1 self.abs_max_out: 11138.0\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.758958/  1.875753, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.63 seconds, 4.41 minutes\n",
      "total_backward_count 7629920 real_backward_count 319392   4.186%\n",
      "fc layer 1 self.abs_max_out: 11150.0\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.754597/  1.859314, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.88 seconds, 4.41 minutes\n",
      "total_backward_count 7674280 real_backward_count 320042   4.170%\n",
      "fc layer 1 self.abs_max_out: 11161.0\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.748749/  1.862288, val:  92.50%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.82 seconds, 4.48 minutes\n",
      "total_backward_count 7718640 real_backward_count 320584   4.153%\n",
      "fc layer 1 self.abs_max_out: 11163.0\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.747873/  1.879594, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.96 seconds, 4.35 minutes\n",
      "total_backward_count 7763000 real_backward_count 321150   4.137%\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.749087/  1.866323, val:  88.75%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.50 seconds, 4.41 minutes\n",
      "total_backward_count 7807360 real_backward_count 321720   4.121%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.753688/  1.860855, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.98 seconds, 4.42 minutes\n",
      "total_backward_count 7851720 real_backward_count 322253   4.104%\n",
      "fc layer 1 self.abs_max_out: 11164.0\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.742848/  1.864520, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.82 seconds, 4.41 minutes\n",
      "total_backward_count 7896080 real_backward_count 322816   4.088%\n",
      "fc layer 1 self.abs_max_out: 11189.0\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.740677/  1.871208, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.74 seconds, 4.45 minutes\n",
      "total_backward_count 7940440 real_backward_count 323377   4.073%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.745436/  1.868111, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.15 seconds, 4.45 minutes\n",
      "total_backward_count 7984800 real_backward_count 323932   4.057%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.741015/  1.867371, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.13 seconds, 4.40 minutes\n",
      "total_backward_count 8029160 real_backward_count 324452   4.041%\n",
      "fc layer 2 self.abs_max_out: 4099.0\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.738545/  1.860535, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.64 seconds, 4.38 minutes\n",
      "total_backward_count 8073520 real_backward_count 325018   4.026%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.735901/  1.860766, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.51 seconds, 4.44 minutes\n",
      "total_backward_count 8117880 real_backward_count 325585   4.011%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.736245/  1.854982, val:  91.25%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.53 seconds, 4.48 minutes\n",
      "total_backward_count 8162240 real_backward_count 326137   3.996%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.736285/  1.857387, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.21 seconds, 4.47 minutes\n",
      "total_backward_count 8206600 real_backward_count 326664   3.981%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.729655/  1.855765, val:  90.00%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.51 seconds, 4.44 minutes\n",
      "total_backward_count 8250960 real_backward_count 327212   3.966%\n",
      "fc layer 2 self.abs_max_out: 4109.0\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.729823/  1.860639, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.73 seconds, 4.43 minutes\n",
      "total_backward_count 8295320 real_backward_count 327756   3.951%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.736305/  1.872769, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.05 seconds, 4.47 minutes\n",
      "total_backward_count 8339680 real_backward_count 328267   3.936%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.739130/  1.863165, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.21 seconds, 4.49 minutes\n",
      "total_backward_count 8384040 real_backward_count 328772   3.921%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.736526/  1.859941, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 270.03 seconds, 4.50 minutes\n",
      "total_backward_count 8428400 real_backward_count 329267   3.907%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.730709/  1.846596, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.72 seconds, 4.48 minutes\n",
      "total_backward_count 8472760 real_backward_count 329757   3.892%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.729864/  1.853769, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.57 seconds, 4.46 minutes\n",
      "total_backward_count 8517120 real_backward_count 330284   3.878%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.727588/  1.859771, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.45 seconds, 4.39 minutes\n",
      "total_backward_count 8561480 real_backward_count 330782   3.864%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.725589/  1.864232, val:  91.67%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.21 seconds, 4.45 minutes\n",
      "total_backward_count 8605840 real_backward_count 331338   3.850%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.729573/  1.858913, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.40 seconds, 4.47 minutes\n",
      "total_backward_count 8650200 real_backward_count 331829   3.836%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.732101/  1.860144, val:  89.58%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.32 seconds, 4.47 minutes\n",
      "total_backward_count 8694560 real_backward_count 332345   3.822%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.724966/  1.852907, val:  90.83%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.32 seconds, 4.47 minutes\n",
      "total_backward_count 8738920 real_backward_count 332851   3.809%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.728074/  1.855623, val:  90.42%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.52 seconds, 4.38 minutes\n",
      "total_backward_count 8783280 real_backward_count 333334   3.795%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.728560/  1.859170, val:  89.17%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.01 seconds, 4.47 minutes\n",
      "total_backward_count 8827640 real_backward_count 333813   3.781%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.733591/  1.859015, val:  92.08%, val_best:  92.50%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.10 seconds, 4.48 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1384d8464c9744d48716b58482a637ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.73359</td></tr><tr><td>val_acc_best</td><td>0.925</td></tr><tr><td>val_acc_now</td><td>0.92083</td></tr><tr><td>val_loss</td><td>1.85901</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trim-sweep-2</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yptiex3s' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/yptiex3s</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251008_210043-yptiex3s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z15mbjf3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 41538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251009_114728-z15mbjf3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z15mbjf3' target=\"_blank\">daily-sweep-3</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z15mbjf3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z15mbjf3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251009_114736_131', 'my_seed': 41538, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6.5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 568.0\n",
      "lif layer 1 self.abs_max_v: 568.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 621.0\n",
      "lif layer 2 self.abs_max_v: 103.0\n",
      "lif layer 1 self.abs_max_v: 733.0\n",
      "lif layer 2 self.abs_max_v: 123.5\n",
      "lif layer 2 self.abs_max_v: 126.5\n",
      "lif layer 1 self.abs_max_v: 798.0\n",
      "fc layer 2 self.abs_max_out: 204.0\n",
      "lif layer 2 self.abs_max_v: 210.5\n",
      "fc layer 1 self.abs_max_out: 598.0\n",
      "lif layer 1 self.abs_max_v: 960.5\n",
      "fc layer 2 self.abs_max_out: 302.0\n",
      "lif layer 2 self.abs_max_v: 342.5\n",
      "fc layer 1 self.abs_max_out: 702.0\n",
      "lif layer 2 self.abs_max_v: 410.5\n",
      "fc layer 1 self.abs_max_out: 761.0\n",
      "fc layer 2 self.abs_max_out: 401.0\n",
      "lif layer 2 self.abs_max_v: 593.5\n",
      "fc layer 3 self.abs_max_out: 52.0\n",
      "fc layer 1 self.abs_max_out: 850.0\n",
      "fc layer 1 self.abs_max_out: 877.0\n",
      "lif layer 1 self.abs_max_v: 1005.5\n",
      "fc layer 1 self.abs_max_out: 1103.0\n",
      "lif layer 1 self.abs_max_v: 1103.0\n",
      "fc layer 2 self.abs_max_out: 443.0\n",
      "lif layer 2 self.abs_max_v: 639.5\n",
      "fc layer 1 self.abs_max_out: 1143.0\n",
      "lif layer 1 self.abs_max_v: 1202.5\n",
      "fc layer 3 self.abs_max_out: 77.0\n",
      "fc layer 1 self.abs_max_out: 1360.0\n",
      "lif layer 1 self.abs_max_v: 1430.5\n",
      "fc layer 2 self.abs_max_out: 467.0\n",
      "lif layer 2 self.abs_max_v: 650.5\n",
      "fc layer 3 self.abs_max_out: 80.0\n",
      "lif layer 2 self.abs_max_v: 702.5\n",
      "fc layer 3 self.abs_max_out: 88.0\n",
      "fc layer 2 self.abs_max_out: 532.0\n",
      "lif layer 2 self.abs_max_v: 710.0\n",
      "lif layer 2 self.abs_max_v: 726.5\n",
      "lif layer 2 self.abs_max_v: 767.0\n",
      "lif layer 2 self.abs_max_v: 772.5\n",
      "fc layer 3 self.abs_max_out: 156.0\n",
      "fc layer 2 self.abs_max_out: 553.0\n",
      "lif layer 2 self.abs_max_v: 860.0\n",
      "fc layer 2 self.abs_max_out: 707.0\n",
      "lif layer 2 self.abs_max_v: 917.5\n",
      "lif layer 1 self.abs_max_v: 1435.5\n",
      "lif layer 2 self.abs_max_v: 990.5\n",
      "fc layer 1 self.abs_max_out: 1493.0\n",
      "lif layer 1 self.abs_max_v: 1493.0\n",
      "fc layer 2 self.abs_max_out: 781.0\n",
      "fc layer 3 self.abs_max_out: 157.0\n",
      "fc layer 1 self.abs_max_out: 1580.0\n",
      "lif layer 1 self.abs_max_v: 1580.0\n",
      "lif layer 1 self.abs_max_v: 1634.0\n",
      "fc layer 2 self.abs_max_out: 851.0\n",
      "fc layer 1 self.abs_max_out: 1872.0\n",
      "lif layer 1 self.abs_max_v: 1872.0\n",
      "lif layer 2 self.abs_max_v: 1023.0\n",
      "fc layer 2 self.abs_max_out: 1028.0\n",
      "lif layer 2 self.abs_max_v: 1028.0\n",
      "fc layer 2 self.abs_max_out: 1042.0\n",
      "lif layer 2 self.abs_max_v: 1042.0\n",
      "lif layer 2 self.abs_max_v: 1184.5\n",
      "fc layer 3 self.abs_max_out: 177.0\n",
      "fc layer 3 self.abs_max_out: 193.0\n",
      "lif layer 2 self.abs_max_v: 1356.0\n",
      "fc layer 1 self.abs_max_out: 2023.0\n",
      "lif layer 1 self.abs_max_v: 2023.0\n",
      "fc layer 3 self.abs_max_out: 212.0\n",
      "fc layer 3 self.abs_max_out: 220.0\n",
      "fc layer 2 self.abs_max_out: 1074.0\n",
      "fc layer 2 self.abs_max_out: 1214.0\n",
      "fc layer 3 self.abs_max_out: 247.0\n",
      "lif layer 2 self.abs_max_v: 1394.5\n",
      "fc layer 3 self.abs_max_out: 275.0\n",
      "fc layer 2 self.abs_max_out: 1217.0\n",
      "fc layer 2 self.abs_max_out: 1312.0\n",
      "fc layer 3 self.abs_max_out: 312.0\n",
      "fc layer 2 self.abs_max_out: 1342.0\n",
      "fc layer 2 self.abs_max_out: 1457.0\n",
      "lif layer 2 self.abs_max_v: 1457.0\n",
      "fc layer 3 self.abs_max_out: 316.0\n",
      "lif layer 2 self.abs_max_v: 1658.0\n",
      "fc layer 1 self.abs_max_out: 2057.0\n",
      "lif layer 1 self.abs_max_v: 2057.0\n",
      "fc layer 2 self.abs_max_out: 1475.0\n",
      "fc layer 3 self.abs_max_out: 331.0\n",
      "fc layer 1 self.abs_max_out: 2469.0\n",
      "lif layer 1 self.abs_max_v: 2469.0\n",
      "fc layer 1 self.abs_max_out: 2497.0\n",
      "lif layer 1 self.abs_max_v: 2497.0\n",
      "fc layer 2 self.abs_max_out: 1646.0\n",
      "fc layer 2 self.abs_max_out: 1679.0\n",
      "lif layer 2 self.abs_max_v: 1679.0\n",
      "fc layer 1 self.abs_max_out: 2613.0\n",
      "lif layer 1 self.abs_max_v: 2613.0\n",
      "lif layer 2 self.abs_max_v: 1756.0\n",
      "fc layer 1 self.abs_max_out: 2774.0\n",
      "lif layer 1 self.abs_max_v: 2774.0\n",
      "fc layer 2 self.abs_max_out: 1692.0\n",
      "fc layer 1 self.abs_max_out: 2995.0\n",
      "lif layer 1 self.abs_max_v: 2995.0\n",
      "fc layer 2 self.abs_max_out: 1723.0\n",
      "lif layer 2 self.abs_max_v: 1757.5\n",
      "lif layer 2 self.abs_max_v: 1847.5\n",
      "fc layer 2 self.abs_max_out: 1748.0\n",
      "fc layer 3 self.abs_max_out: 361.0\n",
      "fc layer 2 self.abs_max_out: 1828.0\n",
      "lif layer 2 self.abs_max_v: 1939.0\n",
      "lif layer 2 self.abs_max_v: 2015.5\n",
      "fc layer 2 self.abs_max_out: 1893.0\n",
      "fc layer 1 self.abs_max_out: 3114.0\n",
      "lif layer 1 self.abs_max_v: 3114.0\n",
      "fc layer 3 self.abs_max_out: 389.0\n",
      "fc layer 3 self.abs_max_out: 447.0\n",
      "fc layer 1 self.abs_max_out: 3600.0\n",
      "lif layer 1 self.abs_max_v: 3600.0\n",
      "lif layer 2 self.abs_max_v: 2084.0\n",
      "fc layer 2 self.abs_max_out: 1902.0\n",
      "fc layer 2 self.abs_max_out: 1907.0\n",
      "fc layer 3 self.abs_max_out: 473.0\n",
      "fc layer 2 self.abs_max_out: 1943.0\n",
      "fc layer 2 self.abs_max_out: 2131.0\n",
      "lif layer 2 self.abs_max_v: 2131.0\n",
      "fc layer 1 self.abs_max_out: 3694.0\n",
      "lif layer 1 self.abs_max_v: 3694.0\n",
      "fc layer 1 self.abs_max_out: 3785.0\n",
      "lif layer 1 self.abs_max_v: 3785.0\n",
      "fc layer 3 self.abs_max_out: 496.0\n",
      "lif layer 2 self.abs_max_v: 2243.5\n",
      "lif layer 2 self.abs_max_v: 2401.0\n",
      "lif layer 2 self.abs_max_v: 2409.5\n",
      "fc layer 1 self.abs_max_out: 4196.0\n",
      "lif layer 1 self.abs_max_v: 4196.0\n",
      "fc layer 3 self.abs_max_out: 501.0\n",
      "fc layer 2 self.abs_max_out: 2141.0\n",
      "fc layer 2 self.abs_max_out: 2148.0\n",
      "fc layer 2 self.abs_max_out: 2240.0\n",
      "fc layer 1 self.abs_max_out: 4325.0\n",
      "lif layer 1 self.abs_max_v: 4325.0\n",
      "fc layer 2 self.abs_max_out: 2287.0\n",
      "fc layer 2 self.abs_max_out: 2299.0\n",
      "fc layer 2 self.abs_max_out: 2348.0\n",
      "fc layer 2 self.abs_max_out: 2454.0\n",
      "lif layer 2 self.abs_max_v: 2454.0\n",
      "fc layer 2 self.abs_max_out: 2474.0\n",
      "lif layer 2 self.abs_max_v: 2474.0\n",
      "fc layer 2 self.abs_max_out: 2512.0\n",
      "lif layer 2 self.abs_max_v: 2512.0\n",
      "fc layer 1 self.abs_max_out: 4665.0\n",
      "lif layer 1 self.abs_max_v: 4665.0\n",
      "fc layer 2 self.abs_max_out: 2529.0\n",
      "lif layer 2 self.abs_max_v: 2529.0\n",
      "fc layer 2 self.abs_max_out: 2584.0\n",
      "lif layer 2 self.abs_max_v: 2584.0\n",
      "fc layer 1 self.abs_max_out: 4730.0\n",
      "lif layer 1 self.abs_max_v: 4730.0\n",
      "fc layer 1 self.abs_max_out: 4753.0\n",
      "lif layer 1 self.abs_max_v: 4753.0\n",
      "fc layer 2 self.abs_max_out: 2685.0\n",
      "lif layer 2 self.abs_max_v: 2685.0\n",
      "fc layer 1 self.abs_max_out: 4947.0\n",
      "lif layer 1 self.abs_max_v: 4947.0\n",
      "lif layer 1 self.abs_max_v: 4957.5\n",
      "fc layer 1 self.abs_max_out: 4948.0\n",
      "fc layer 1 self.abs_max_out: 5463.0\n",
      "lif layer 1 self.abs_max_v: 5463.0\n",
      "fc layer 1 self.abs_max_out: 5531.0\n",
      "lif layer 1 self.abs_max_v: 5531.0\n",
      "fc layer 1 self.abs_max_out: 5794.0\n",
      "lif layer 1 self.abs_max_v: 5794.0\n",
      "lif layer 1 self.abs_max_v: 6134.0\n",
      "fc layer 1 self.abs_max_out: 5866.0\n",
      "fc layer 2 self.abs_max_out: 2774.0\n",
      "lif layer 2 self.abs_max_v: 2774.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.054221/  2.153636, val:  34.17%, val_best:  34.17%, tr:  95.65%, tr_best:  95.65%, epoch time: 268.85 seconds, 4.48 minutes\n",
      "total_backward_count 44360 real_backward_count 10822  24.396%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 5874.0\n",
      "fc layer 1 self.abs_max_out: 6203.0\n",
      "lif layer 1 self.abs_max_v: 6203.0\n",
      "fc layer 2 self.abs_max_out: 2788.0\n",
      "lif layer 2 self.abs_max_v: 2788.0\n",
      "lif layer 2 self.abs_max_v: 2827.5\n",
      "lif layer 2 self.abs_max_v: 2909.0\n",
      "fc layer 1 self.abs_max_out: 6235.0\n",
      "lif layer 1 self.abs_max_v: 6235.0\n",
      "lif layer 1 self.abs_max_v: 6690.5\n",
      "lif layer 1 self.abs_max_v: 6965.5\n",
      "lif layer 2 self.abs_max_v: 3146.0\n",
      "lif layer 2 self.abs_max_v: 3324.0\n",
      "lif layer 2 self.abs_max_v: 3342.5\n",
      "lif layer 2 self.abs_max_v: 3501.5\n",
      "lif layer 1 self.abs_max_v: 7008.5\n",
      "lif layer 1 self.abs_max_v: 8020.5\n",
      "fc layer 1 self.abs_max_out: 6319.0\n",
      "fc layer 2 self.abs_max_out: 2807.0\n",
      "fc layer 2 self.abs_max_out: 2815.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.074839/  2.147666, val:  62.92%, val_best:  62.92%, tr:  99.30%, tr_best:  99.30%, epoch time: 268.34 seconds, 4.47 minutes\n",
      "total_backward_count 88720 real_backward_count 18219  20.535%\n",
      "fc layer 2 self.abs_max_out: 2944.0\n",
      "fc layer 1 self.abs_max_out: 6530.0\n",
      "fc layer 1 self.abs_max_out: 6687.0\n",
      "fc layer 1 self.abs_max_out: 6940.0\n",
      "fc layer 1 self.abs_max_out: 6977.0\n",
      "fc layer 1 self.abs_max_out: 7332.0\n",
      "lif layer 1 self.abs_max_v: 8322.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.072098/  2.138808, val:  55.83%, val_best:  62.92%, tr:  99.46%, tr_best:  99.46%, epoch time: 269.13 seconds, 4.49 minutes\n",
      "total_backward_count 133080 real_backward_count 25208  18.942%\n",
      "lif layer 2 self.abs_max_v: 3527.0\n",
      "lif layer 2 self.abs_max_v: 3561.0\n",
      "lif layer 2 self.abs_max_v: 3672.0\n",
      "fc layer 1 self.abs_max_out: 7368.0\n",
      "fc layer 1 self.abs_max_out: 7403.0\n",
      "lif layer 1 self.abs_max_v: 8396.0\n",
      "lif layer 1 self.abs_max_v: 8810.0\n",
      "fc layer 2 self.abs_max_out: 2946.0\n",
      "fc layer 2 self.abs_max_out: 3009.0\n",
      "lif layer 2 self.abs_max_v: 3679.0\n",
      "lif layer 2 self.abs_max_v: 3890.5\n",
      "lif layer 1 self.abs_max_v: 8929.0\n",
      "lif layer 1 self.abs_max_v: 9328.0\n",
      "fc layer 2 self.abs_max_out: 3034.0\n",
      "lif layer 2 self.abs_max_v: 3917.0\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.064615/  2.129344, val:  51.25%, val_best:  62.92%, tr:  99.64%, tr_best:  99.64%, epoch time: 265.34 seconds, 4.42 minutes\n",
      "total_backward_count 177440 real_backward_count 31925  17.992%\n",
      "fc layer 1 self.abs_max_out: 7504.0\n",
      "lif layer 2 self.abs_max_v: 3944.0\n",
      "lif layer 2 self.abs_max_v: 3973.5\n",
      "lif layer 2 self.abs_max_v: 4049.0\n",
      "lif layer 2 self.abs_max_v: 4180.5\n",
      "fc layer 2 self.abs_max_out: 3101.0\n",
      "lif layer 1 self.abs_max_v: 9517.5\n",
      "fc layer 1 self.abs_max_out: 7623.0\n",
      "lif layer 1 self.abs_max_v: 10107.0\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.064713/  2.135676, val:  60.00%, val_best:  62.92%, tr:  99.80%, tr_best:  99.80%, epoch time: 269.00 seconds, 4.48 minutes\n",
      "total_backward_count 221800 real_backward_count 38291  17.264%\n",
      "fc layer 2 self.abs_max_out: 3121.0\n",
      "lif layer 2 self.abs_max_v: 4229.0\n",
      "fc layer 1 self.abs_max_out: 7750.0\n",
      "fc layer 1 self.abs_max_out: 7786.0\n",
      "fc layer 1 self.abs_max_out: 7832.0\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.061529/  2.112301, val:  55.00%, val_best:  62.92%, tr:  99.84%, tr_best:  99.84%, epoch time: 261.28 seconds, 4.35 minutes\n",
      "total_backward_count 266160 real_backward_count 44515  16.725%\n",
      "lif layer 2 self.abs_max_v: 4463.5\n",
      "lif layer 2 self.abs_max_v: 4547.0\n",
      "fc layer 1 self.abs_max_out: 7853.0\n",
      "fc layer 2 self.abs_max_out: 3188.0\n",
      "fc layer 1 self.abs_max_out: 7997.0\n",
      "lif layer 1 self.abs_max_v: 10623.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  2.052559/  2.117473, val:  60.83%, val_best:  62.92%, tr:  99.73%, tr_best:  99.84%, epoch time: 267.94 seconds, 4.47 minutes\n",
      "total_backward_count 310520 real_backward_count 50576  16.288%\n",
      "fc layer 1 self.abs_max_out: 8125.0\n",
      "fc layer 1 self.abs_max_out: 8175.0\n",
      "fc layer 2 self.abs_max_out: 3224.0\n",
      "lif layer 1 self.abs_max_v: 10780.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  2.053114/  2.118994, val:  65.00%, val_best:  65.00%, tr:  99.71%, tr_best:  99.84%, epoch time: 268.94 seconds, 4.48 minutes\n",
      "total_backward_count 354880 real_backward_count 56313  15.868%\n",
      "lif layer 2 self.abs_max_v: 4550.0\n",
      "fc layer 1 self.abs_max_out: 8274.0\n",
      "lif layer 2 self.abs_max_v: 4640.0\n",
      "lif layer 2 self.abs_max_v: 4670.0\n",
      "lif layer 2 self.abs_max_v: 4756.0\n",
      "lif layer 2 self.abs_max_v: 4820.0\n",
      "fc layer 1 self.abs_max_out: 8351.0\n",
      "lif layer 2 self.abs_max_v: 4859.5\n",
      "lif layer 1 self.abs_max_v: 11154.0\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  2.054200/  2.109413, val:  76.67%, val_best:  76.67%, tr:  99.80%, tr_best:  99.84%, epoch time: 262.97 seconds, 4.38 minutes\n",
      "total_backward_count 399240 real_backward_count 61932  15.512%\n",
      "fc layer 2 self.abs_max_out: 3258.0\n",
      "fc layer 2 self.abs_max_out: 3357.0\n",
      "fc layer 1 self.abs_max_out: 8463.0\n",
      "lif layer 2 self.abs_max_v: 5011.5\n",
      "lif layer 2 self.abs_max_v: 5014.0\n",
      "lif layer 2 self.abs_max_v: 5215.0\n",
      "lif layer 1 self.abs_max_v: 11448.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  2.051270/  2.106982, val:  78.33%, val_best:  78.33%, tr:  99.93%, tr_best:  99.93%, epoch time: 268.10 seconds, 4.47 minutes\n",
      "total_backward_count 443600 real_backward_count 67479  15.212%\n",
      "fc layer 1 self.abs_max_out: 8471.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  2.049240/  2.100359, val:  72.50%, val_best:  78.33%, tr:  99.84%, tr_best:  99.93%, epoch time: 268.28 seconds, 4.47 minutes\n",
      "total_backward_count 487960 real_backward_count 72740  14.907%\n",
      "fc layer 1 self.abs_max_out: 8483.0\n",
      "lif layer 1 self.abs_max_v: 11510.0\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  2.046832/  2.104041, val:  80.00%, val_best:  80.00%, tr:  99.93%, tr_best:  99.93%, epoch time: 268.63 seconds, 4.48 minutes\n",
      "total_backward_count 532320 real_backward_count 77776  14.611%\n",
      "fc layer 1 self.abs_max_out: 8499.0\n",
      "fc layer 1 self.abs_max_out: 8545.0\n",
      "lif layer 1 self.abs_max_v: 11914.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  2.045045/  2.089818, val:  77.08%, val_best:  80.00%, tr:  99.91%, tr_best:  99.93%, epoch time: 269.47 seconds, 4.49 minutes\n",
      "total_backward_count 576680 real_backward_count 82772  14.353%\n",
      "lif layer 1 self.abs_max_v: 12543.5\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  2.040183/  2.103058, val:  63.33%, val_best:  80.00%, tr:  99.86%, tr_best:  99.93%, epoch time: 267.72 seconds, 4.46 minutes\n",
      "total_backward_count 621040 real_backward_count 87670  14.117%\n",
      "fc layer 2 self.abs_max_out: 3401.0\n",
      "lif layer 2 self.abs_max_v: 5229.5\n",
      "lif layer 2 self.abs_max_v: 5397.0\n",
      "lif layer 1 self.abs_max_v: 12701.5\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  2.036801/  2.089564, val:  80.00%, val_best:  80.00%, tr:  99.86%, tr_best:  99.93%, epoch time: 265.52 seconds, 4.43 minutes\n",
      "total_backward_count 665400 real_backward_count 92351  13.879%\n",
      "fc layer 1 self.abs_max_out: 8567.0\n",
      "fc layer 1 self.abs_max_out: 8699.0\n",
      "lif layer 1 self.abs_max_v: 13264.5\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  2.034912/  2.096360, val:  75.42%, val_best:  80.00%, tr:  99.89%, tr_best:  99.93%, epoch time: 266.27 seconds, 4.44 minutes\n",
      "total_backward_count 709760 real_backward_count 96883  13.650%\n",
      "fc layer 1 self.abs_max_out: 8719.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  2.034262/  2.089144, val:  84.17%, val_best:  84.17%, tr:  99.89%, tr_best:  99.93%, epoch time: 267.33 seconds, 4.46 minutes\n",
      "total_backward_count 754120 real_backward_count 101273  13.429%\n",
      "fc layer 1 self.abs_max_out: 8841.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  2.025209/  2.084523, val:  73.75%, val_best:  84.17%, tr:  99.91%, tr_best:  99.93%, epoch time: 267.83 seconds, 4.46 minutes\n",
      "total_backward_count 798480 real_backward_count 105594  13.224%\n",
      "fc layer 2 self.abs_max_out: 3406.0\n",
      "fc layer 1 self.abs_max_out: 8882.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  2.019513/  2.071444, val:  77.92%, val_best:  84.17%, tr:  99.93%, tr_best:  99.93%, epoch time: 268.73 seconds, 4.48 minutes\n",
      "total_backward_count 842840 real_backward_count 109770  13.024%\n",
      "fc layer 2 self.abs_max_out: 3443.0\n",
      "fc layer 2 self.abs_max_out: 3462.0\n",
      "fc layer 2 self.abs_max_out: 3564.0\n",
      "lif layer 1 self.abs_max_v: 13448.5\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  2.021003/  2.080306, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.49 seconds, 4.44 minutes\n",
      "total_backward_count 887200 real_backward_count 113766  12.823%\n",
      "fc layer 1 self.abs_max_out: 8925.0\n",
      "fc layer 2 self.abs_max_out: 3572.0\n",
      "fc layer 2 self.abs_max_out: 3578.0\n",
      "lif layer 1 self.abs_max_v: 13547.5\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  2.012257/  2.071913, val:  65.00%, val_best:  85.83%, tr:  99.95%, tr_best: 100.00%, epoch time: 265.93 seconds, 4.43 minutes\n",
      "total_backward_count 931560 real_backward_count 117692  12.634%\n",
      "fc layer 1 self.abs_max_out: 8984.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  2.002404/  2.073092, val:  79.17%, val_best:  85.83%, tr:  99.95%, tr_best: 100.00%, epoch time: 270.24 seconds, 4.50 minutes\n",
      "total_backward_count 975920 real_backward_count 121507  12.451%\n",
      "fc layer 1 self.abs_max_out: 9031.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  2.009434/  2.068280, val:  77.08%, val_best:  85.83%, tr:  99.98%, tr_best: 100.00%, epoch time: 269.09 seconds, 4.48 minutes\n",
      "total_backward_count 1020280 real_backward_count 125171  12.268%\n",
      "fc layer 1 self.abs_max_out: 9062.0\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  2.008770/  2.067814, val:  80.00%, val_best:  85.83%, tr:  99.98%, tr_best: 100.00%, epoch time: 267.56 seconds, 4.46 minutes\n",
      "total_backward_count 1064640 real_backward_count 128729  12.091%\n",
      "fc layer 1 self.abs_max_out: 9071.0\n",
      "fc layer 2 self.abs_max_out: 3596.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  2.001375/  2.055795, val:  81.67%, val_best:  85.83%, tr:  99.98%, tr_best: 100.00%, epoch time: 267.02 seconds, 4.45 minutes\n",
      "total_backward_count 1109000 real_backward_count 132275  11.927%\n",
      "fc layer 1 self.abs_max_out: 9157.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  2.000401/  2.051997, val:  77.50%, val_best:  85.83%, tr:  99.95%, tr_best: 100.00%, epoch time: 264.35 seconds, 4.41 minutes\n",
      "total_backward_count 1153360 real_backward_count 135687  11.764%\n",
      "fc layer 1 self.abs_max_out: 9187.0\n",
      "fc layer 2 self.abs_max_out: 3820.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.998472/  2.060474, val:  80.42%, val_best:  85.83%, tr:  99.93%, tr_best: 100.00%, epoch time: 262.96 seconds, 4.38 minutes\n",
      "total_backward_count 1197720 real_backward_count 139091  11.613%\n",
      "fc layer 1 self.abs_max_out: 9199.0\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  2.004452/  2.067222, val:  77.50%, val_best:  85.83%, tr:  99.98%, tr_best: 100.00%, epoch time: 265.19 seconds, 4.42 minutes\n",
      "total_backward_count 1242080 real_backward_count 142359  11.461%\n",
      "fc layer 1 self.abs_max_out: 9246.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  2.006749/  2.062505, val:  85.83%, val_best:  85.83%, tr:  99.98%, tr_best: 100.00%, epoch time: 266.61 seconds, 4.44 minutes\n",
      "total_backward_count 1286440 real_backward_count 145567  11.315%\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.998622/  2.060725, val:  82.50%, val_best:  85.83%, tr:  99.98%, tr_best: 100.00%, epoch time: 265.47 seconds, 4.42 minutes\n",
      "total_backward_count 1330800 real_backward_count 148610  11.167%\n",
      "lif layer 2 self.abs_max_v: 5437.0\n",
      "lif layer 2 self.abs_max_v: 5442.5\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.992123/  2.052182, val:  82.92%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.17 seconds, 4.42 minutes\n",
      "total_backward_count 1375160 real_backward_count 151745  11.035%\n",
      "lif layer 2 self.abs_max_v: 5476.5\n",
      "fc layer 1 self.abs_max_out: 9399.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.982158/  2.053081, val:  85.83%, val_best:  85.83%, tr: 100.00%, tr_best: 100.00%, epoch time: 259.59 seconds, 4.33 minutes\n",
      "total_backward_count 1419520 real_backward_count 154723  10.900%\n",
      "lif layer 2 self.abs_max_v: 5701.5\n",
      "lif layer 2 self.abs_max_v: 5880.0\n",
      "fc layer 1 self.abs_max_out: 9447.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.984127/  2.040205, val:  86.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.68 seconds, 4.43 minutes\n",
      "total_backward_count 1463880 real_backward_count 157575  10.764%\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.981267/  2.052216, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.98 seconds, 4.43 minutes\n",
      "total_backward_count 1508240 real_backward_count 160447  10.638%\n",
      "lif layer 2 self.abs_max_v: 5883.5\n",
      "lif layer 2 self.abs_max_v: 5935.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.981860/  2.039271, val:  84.17%, val_best:  86.67%, tr:  99.95%, tr_best: 100.00%, epoch time: 265.46 seconds, 4.42 minutes\n",
      "total_backward_count 1552600 real_backward_count 163259  10.515%\n",
      "lif layer 2 self.abs_max_v: 6014.5\n",
      "lif layer 2 self.abs_max_v: 6046.5\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.980314/  2.050712, val:  82.08%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 265.52 seconds, 4.43 minutes\n",
      "total_backward_count 1596960 real_backward_count 166100  10.401%\n",
      "fc layer 1 self.abs_max_out: 9480.0\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.980398/  2.046101, val:  87.92%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.52 seconds, 4.43 minutes\n",
      "total_backward_count 1641320 real_backward_count 168852  10.288%\n",
      "fc layer 1 self.abs_max_out: 9494.0\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.973072/  2.028497, val:  86.25%, val_best:  87.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.04 seconds, 4.37 minutes\n",
      "total_backward_count 1685680 real_backward_count 171485  10.173%\n",
      "fc layer 1 self.abs_max_out: 9540.0\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.968850/  2.030416, val:  90.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.13 seconds, 4.42 minutes\n",
      "total_backward_count 1730040 real_backward_count 174048  10.060%\n",
      "fc layer 1 self.abs_max_out: 9573.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.964189/  2.033993, val:  87.92%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 265.60 seconds, 4.43 minutes\n",
      "total_backward_count 1774400 real_backward_count 176612   9.953%\n",
      "fc layer 1 self.abs_max_out: 9594.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.955916/  2.029296, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.64 seconds, 4.43 minutes\n",
      "total_backward_count 1818760 real_backward_count 179126   9.849%\n",
      "fc layer 1 self.abs_max_out: 9627.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.950997/  2.028220, val:  81.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.60 seconds, 4.44 minutes\n",
      "total_backward_count 1863120 real_backward_count 181534   9.744%\n",
      "fc layer 1 self.abs_max_out: 9647.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.952318/  2.024104, val:  87.50%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.50 seconds, 4.34 minutes\n",
      "total_backward_count 1907480 real_backward_count 183933   9.643%\n",
      "fc layer 1 self.abs_max_out: 9667.0\n",
      "lif layer 1 self.abs_max_v: 13629.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.947801/  2.025727, val:  77.08%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 264.12 seconds, 4.40 minutes\n",
      "total_backward_count 1951840 real_backward_count 186248   9.542%\n",
      "fc layer 1 self.abs_max_out: 9746.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.941323/  2.008124, val:  84.17%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 265.53 seconds, 4.43 minutes\n",
      "total_backward_count 1996200 real_backward_count 188558   9.446%\n",
      "fc layer 1 self.abs_max_out: 9788.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.940143/  2.017769, val:  87.92%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.95 seconds, 4.43 minutes\n",
      "total_backward_count 2040560 real_backward_count 190843   9.352%\n",
      "fc layer 1 self.abs_max_out: 9790.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.937197/  2.009204, val:  83.75%, val_best:  90.00%, tr:  99.98%, tr_best: 100.00%, epoch time: 264.85 seconds, 4.41 minutes\n",
      "total_backward_count 2084920 real_backward_count 193087   9.261%\n",
      "fc layer 1 self.abs_max_out: 9821.0\n",
      "lif layer 1 self.abs_max_v: 13744.5\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.936982/  2.008518, val:  86.25%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.07 seconds, 4.43 minutes\n",
      "total_backward_count 2129280 real_backward_count 195274   9.171%\n",
      "lif layer 1 self.abs_max_v: 13837.0\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.943604/  2.013932, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.08 seconds, 4.37 minutes\n",
      "total_backward_count 2173640 real_backward_count 197429   9.083%\n",
      "lif layer 1 self.abs_max_v: 13847.5\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.944273/  2.015620, val:  85.83%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.59 seconds, 4.39 minutes\n",
      "total_backward_count 2218000 real_backward_count 199622   9.000%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.939007/  2.014510, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.45 seconds, 4.42 minutes\n",
      "total_backward_count 2262360 real_backward_count 201681   8.915%\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.949079/  2.011537, val:  85.42%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.88 seconds, 4.41 minutes\n",
      "total_backward_count 2306720 real_backward_count 203772   8.834%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.928783/  1.997807, val:  86.67%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.69 seconds, 4.43 minutes\n",
      "total_backward_count 2351080 real_backward_count 205804   8.754%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.931507/  2.011682, val:  84.17%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.38 seconds, 4.39 minutes\n",
      "total_backward_count 2395440 real_backward_count 207731   8.672%\n",
      "fc layer 1 self.abs_max_out: 9873.0\n",
      "fc layer 2 self.abs_max_out: 3891.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.926022/  1.991290, val:  85.00%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.80 seconds, 4.38 minutes\n",
      "total_backward_count 2439800 real_backward_count 209722   8.596%\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.927328/  2.010878, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.45 seconds, 4.42 minutes\n",
      "total_backward_count 2484160 real_backward_count 211676   8.521%\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.921712/  1.989704, val:  84.58%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.98 seconds, 4.42 minutes\n",
      "total_backward_count 2528520 real_backward_count 213587   8.447%\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.918348/  1.995489, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.28 seconds, 4.44 minutes\n",
      "total_backward_count 2572880 real_backward_count 215453   8.374%\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.912193/  1.989569, val:  88.33%, val_best:  90.00%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.91 seconds, 4.42 minutes\n",
      "total_backward_count 2617240 real_backward_count 217272   8.302%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.907972/  1.987791, val:  90.42%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.00 seconds, 4.40 minutes\n",
      "total_backward_count 2661600 real_backward_count 219136   8.233%\n",
      "fc layer 1 self.abs_max_out: 9898.0\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.900741/  1.975267, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.13 seconds, 4.39 minutes\n",
      "total_backward_count 2705960 real_backward_count 220916   8.164%\n",
      "fc layer 1 self.abs_max_out: 9899.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.901000/  1.977938, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.65 seconds, 4.46 minutes\n",
      "total_backward_count 2750320 real_backward_count 222722   8.098%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.905654/  1.987191, val:  89.17%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.47 seconds, 4.44 minutes\n",
      "total_backward_count 2794680 real_backward_count 224396   8.029%\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.906301/  1.984369, val:  85.83%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.12 seconds, 4.44 minutes\n",
      "total_backward_count 2839040 real_backward_count 226115   7.964%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.896610/  1.983616, val:  82.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.59 seconds, 4.43 minutes\n",
      "total_backward_count 2883400 real_backward_count 227919   7.905%\n",
      "fc layer 1 self.abs_max_out: 9911.0\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.894034/  1.973402, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.86 seconds, 4.35 minutes\n",
      "total_backward_count 2927760 real_backward_count 229584   7.842%\n",
      "fc layer 1 self.abs_max_out: 9921.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.884713/  1.966396, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.83 seconds, 4.43 minutes\n",
      "total_backward_count 2972120 real_backward_count 231234   7.780%\n",
      "fc layer 1 self.abs_max_out: 9931.0\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.890432/  1.967756, val:  86.25%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.87 seconds, 4.41 minutes\n",
      "total_backward_count 3016480 real_backward_count 232900   7.721%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.884161/  1.969584, val:  87.92%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.66 seconds, 4.43 minutes\n",
      "total_backward_count 3060840 real_backward_count 234514   7.662%\n",
      "fc layer 1 self.abs_max_out: 9953.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.877008/  1.963886, val:  84.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.16 seconds, 4.42 minutes\n",
      "total_backward_count 3105200 real_backward_count 236080   7.603%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.879069/  1.959522, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.73 seconds, 4.43 minutes\n",
      "total_backward_count 3149560 real_backward_count 237636   7.545%\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.871000/  1.952399, val:  87.08%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.48 seconds, 4.37 minutes\n",
      "total_backward_count 3193920 real_backward_count 239096   7.486%\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.871591/  1.963160, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.17 seconds, 4.44 minutes\n",
      "total_backward_count 3238280 real_backward_count 240565   7.429%\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.874584/  1.961881, val:  88.33%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.00 seconds, 4.43 minutes\n",
      "total_backward_count 3282640 real_backward_count 241984   7.372%\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.880833/  1.964038, val:  89.58%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.33 seconds, 4.44 minutes\n",
      "total_backward_count 3327000 real_backward_count 243479   7.318%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.879817/  1.956265, val:  88.75%, val_best:  90.42%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.02 seconds, 4.43 minutes\n",
      "total_backward_count 3371360 real_backward_count 244968   7.266%\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.880069/  1.957869, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.40 seconds, 4.36 minutes\n",
      "total_backward_count 3415720 real_backward_count 246441   7.215%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.878517/  1.962345, val:  85.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.52 seconds, 4.39 minutes\n",
      "total_backward_count 3460080 real_backward_count 247867   7.164%\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.879327/  1.967679, val:  85.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.40 seconds, 4.44 minutes\n",
      "total_backward_count 3504440 real_backward_count 249257   7.113%\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.883577/  1.955875, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.03 seconds, 4.42 minutes\n",
      "total_backward_count 3548800 real_backward_count 250594   7.061%\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.874320/  1.961919, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.79 seconds, 4.43 minutes\n",
      "total_backward_count 3593160 real_backward_count 251948   7.012%\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.867006/  1.952436, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.76 seconds, 4.45 minutes\n",
      "total_backward_count 3637520 real_backward_count 253251   6.962%\n",
      "fc layer 2 self.abs_max_out: 3894.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.856077/  1.944314, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.53 seconds, 4.36 minutes\n",
      "total_backward_count 3681880 real_backward_count 254476   6.912%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.855556/  1.948366, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.89 seconds, 4.40 minutes\n",
      "total_backward_count 3726240 real_backward_count 255766   6.864%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.863159/  1.955980, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.00 seconds, 4.42 minutes\n",
      "total_backward_count 3770600 real_backward_count 257039   6.817%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.860024/  1.947995, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.90 seconds, 4.43 minutes\n",
      "total_backward_count 3814960 real_backward_count 258302   6.771%\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.855960/  1.943306, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.33 seconds, 4.44 minutes\n",
      "total_backward_count 3859320 real_backward_count 259539   6.725%\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.852355/  1.948040, val:  87.50%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.01 seconds, 4.40 minutes\n",
      "total_backward_count 3903680 real_backward_count 260755   6.680%\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.858523/  1.958471, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.72 seconds, 4.36 minutes\n",
      "total_backward_count 3948040 real_backward_count 261947   6.635%\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.855610/  1.951407, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.49 seconds, 4.46 minutes\n",
      "total_backward_count 3992400 real_backward_count 263150   6.591%\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.861324/  1.947667, val:  84.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.80 seconds, 4.41 minutes\n",
      "total_backward_count 4036760 real_backward_count 264318   6.548%\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.850240/  1.939890, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.10 seconds, 4.44 minutes\n",
      "total_backward_count 4081120 real_backward_count 265550   6.507%\n",
      "fc layer 1 self.abs_max_out: 9969.0\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.852495/  1.954012, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.09 seconds, 4.40 minutes\n",
      "total_backward_count 4125480 real_backward_count 266702   6.465%\n",
      "lif layer 1 self.abs_max_v: 13947.5\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.855673/  1.940395, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.68 seconds, 4.41 minutes\n",
      "total_backward_count 4169840 real_backward_count 267908   6.425%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.838856/  1.927334, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.65 seconds, 4.38 minutes\n",
      "total_backward_count 4214200 real_backward_count 269037   6.384%\n",
      "fc layer 1 self.abs_max_out: 9979.0\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.836486/  1.932529, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.78 seconds, 4.41 minutes\n",
      "total_backward_count 4258560 real_backward_count 270135   6.343%\n",
      "lif layer 1 self.abs_max_v: 14138.0\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.839250/  1.937782, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.89 seconds, 4.45 minutes\n",
      "total_backward_count 4302920 real_backward_count 271217   6.303%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.840534/  1.937437, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.99 seconds, 4.42 minutes\n",
      "total_backward_count 4347280 real_backward_count 272319   6.264%\n",
      "fc layer 1 self.abs_max_out: 9993.0\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.842826/  1.940002, val:  91.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.46 seconds, 4.42 minutes\n",
      "total_backward_count 4391640 real_backward_count 273473   6.227%\n",
      "fc layer 1 self.abs_max_out: 10003.0\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.841385/  1.931054, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.83 seconds, 4.36 minutes\n",
      "total_backward_count 4436000 real_backward_count 274548   6.189%\n",
      "fc layer 1 self.abs_max_out: 10019.0\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.839416/  1.934080, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.14 seconds, 4.42 minutes\n",
      "total_backward_count 4480360 real_backward_count 275671   6.153%\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.838466/  1.937392, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.74 seconds, 4.41 minutes\n",
      "total_backward_count 4524720 real_backward_count 276754   6.116%\n",
      "fc layer 1 self.abs_max_out: 10028.0\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.844755/  1.937491, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.12 seconds, 4.44 minutes\n",
      "total_backward_count 4569080 real_backward_count 277830   6.081%\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.836805/  1.929443, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.33 seconds, 4.44 minutes\n",
      "total_backward_count 4613440 real_backward_count 278878   6.045%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.840538/  1.932033, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.64 seconds, 4.41 minutes\n",
      "total_backward_count 4657800 real_backward_count 279864   6.009%\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.842619/  1.943282, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.49 seconds, 4.41 minutes\n",
      "total_backward_count 4702160 real_backward_count 280807   5.972%\n",
      "lif layer 1 self.abs_max_v: 14280.0\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.846202/  1.932719, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.20 seconds, 4.47 minutes\n",
      "total_backward_count 4746520 real_backward_count 281852   5.938%\n",
      "fc layer 2 self.abs_max_out: 3900.0\n",
      "lif layer 1 self.abs_max_v: 14315.5\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.850946/  1.949583, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.32 seconds, 4.47 minutes\n",
      "total_backward_count 4790880 real_backward_count 282858   5.904%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.842629/  1.934914, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.62 seconds, 4.46 minutes\n",
      "total_backward_count 4835240 real_backward_count 283895   5.871%\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.837376/  1.922223, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.48 seconds, 4.47 minutes\n",
      "total_backward_count 4879600 real_backward_count 284951   5.840%\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.826997/  1.923701, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.74 seconds, 4.43 minutes\n",
      "total_backward_count 4923960 real_backward_count 285946   5.807%\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.824851/  1.917522, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.42 seconds, 4.41 minutes\n",
      "total_backward_count 4968320 real_backward_count 286894   5.774%\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.817713/  1.924595, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.70 seconds, 4.48 minutes\n",
      "total_backward_count 5012680 real_backward_count 287845   5.742%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.825134/  1.922379, val:  88.33%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.00 seconds, 4.48 minutes\n",
      "total_backward_count 5057040 real_backward_count 288759   5.710%\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.826971/  1.930303, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.78 seconds, 4.45 minutes\n",
      "total_backward_count 5101400 real_backward_count 289743   5.680%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.832527/  1.925708, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.65 seconds, 4.49 minutes\n",
      "total_backward_count 5145760 real_backward_count 290623   5.648%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.828105/  1.924263, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.31 seconds, 4.42 minutes\n",
      "total_backward_count 5190120 real_backward_count 291556   5.618%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.822965/  1.917248, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.04 seconds, 4.45 minutes\n",
      "total_backward_count 5234480 real_backward_count 292449   5.587%\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.807894/  1.904413, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.87 seconds, 4.50 minutes\n",
      "total_backward_count 5278840 real_backward_count 293365   5.557%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.804312/  1.909595, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.83 seconds, 4.48 minutes\n",
      "total_backward_count 5323200 real_backward_count 294350   5.530%\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.811732/  1.917630, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.77 seconds, 4.46 minutes\n",
      "total_backward_count 5367560 real_backward_count 295277   5.501%\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.824800/  1.919067, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.77 seconds, 4.46 minutes\n",
      "total_backward_count 5411920 real_backward_count 296216   5.473%\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.818529/  1.903254, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.79 seconds, 4.40 minutes\n",
      "total_backward_count 5456280 real_backward_count 297090   5.445%\n",
      "fc layer 1 self.abs_max_out: 10032.0\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.811817/  1.906690, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.41 seconds, 4.49 minutes\n",
      "total_backward_count 5500640 real_backward_count 297944   5.417%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.808474/  1.921305, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.33 seconds, 4.46 minutes\n",
      "total_backward_count 5545000 real_backward_count 298788   5.388%\n",
      "fc layer 1 self.abs_max_out: 10038.0\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.816393/  1.912753, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.12 seconds, 4.49 minutes\n",
      "total_backward_count 5589360 real_backward_count 299674   5.362%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.808698/  1.906410, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.19 seconds, 4.47 minutes\n",
      "total_backward_count 5633720 real_backward_count 300572   5.335%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.802419/  1.899552, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.37 seconds, 4.46 minutes\n",
      "total_backward_count 5678080 real_backward_count 301428   5.309%\n",
      "fc layer 2 self.abs_max_out: 3941.0\n",
      "lif layer 1 self.abs_max_v: 14429.5\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.805634/  1.915823, val:  86.67%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.57 seconds, 4.43 minutes\n",
      "total_backward_count 5722440 real_backward_count 302260   5.282%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.814210/  1.911586, val:  91.25%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.87 seconds, 4.46 minutes\n",
      "total_backward_count 5766800 real_backward_count 303113   5.256%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.808774/  1.909575, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.93 seconds, 4.47 minutes\n",
      "total_backward_count 5811160 real_backward_count 303929   5.230%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.805664/  1.910396, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.29 seconds, 4.47 minutes\n",
      "total_backward_count 5855520 real_backward_count 304763   5.205%\n",
      "fc layer 3 self.abs_max_out: 514.0\n",
      "lif layer 1 self.abs_max_v: 14694.0\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.799018/  1.905459, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.38 seconds, 4.44 minutes\n",
      "total_backward_count 5899880 real_backward_count 305577   5.179%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.795930/  1.899518, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.11 seconds, 4.47 minutes\n",
      "total_backward_count 5944240 real_backward_count 306288   5.153%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.795642/  1.911516, val:  87.08%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.43 seconds, 4.42 minutes\n",
      "total_backward_count 5988600 real_backward_count 307102   5.128%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.799741/  1.908626, val:  90.83%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.87 seconds, 4.46 minutes\n",
      "total_backward_count 6032960 real_backward_count 307961   5.105%\n",
      "fc layer 2 self.abs_max_out: 3953.0\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.803761/  1.912688, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.98 seconds, 4.47 minutes\n",
      "total_backward_count 6077320 real_backward_count 308782   5.081%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.808146/  1.913496, val:  88.75%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.75 seconds, 4.46 minutes\n",
      "total_backward_count 6121680 real_backward_count 309593   5.057%\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.792844/  1.907564, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 269.46 seconds, 4.49 minutes\n",
      "total_backward_count 6166040 real_backward_count 310336   5.033%\n",
      "lif layer 1 self.abs_max_v: 14900.5\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.802075/  1.903038, val:  90.00%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.59 seconds, 4.39 minutes\n",
      "total_backward_count 6210400 real_backward_count 311166   5.010%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.800105/  1.907815, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.34 seconds, 4.47 minutes\n",
      "total_backward_count 6254760 real_backward_count 312013   4.988%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.800717/  1.903376, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.81 seconds, 4.45 minutes\n",
      "total_backward_count 6299120 real_backward_count 312806   4.966%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.796932/  1.902509, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.48 seconds, 4.46 minutes\n",
      "total_backward_count 6343480 real_backward_count 313570   4.943%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.804150/  1.905344, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 268.07 seconds, 4.47 minutes\n",
      "total_backward_count 6387840 real_backward_count 314298   4.920%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.806397/  1.916393, val:  87.92%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.31 seconds, 4.46 minutes\n",
      "total_backward_count 6432200 real_backward_count 315037   4.898%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.801776/  1.905892, val:  90.42%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.51 seconds, 4.38 minutes\n",
      "total_backward_count 6476560 real_backward_count 315771   4.876%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.803864/  1.905720, val:  89.17%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.16 seconds, 4.45 minutes\n",
      "total_backward_count 6520920 real_backward_count 316566   4.855%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.797334/  1.900545, val:  89.58%, val_best:  91.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.97 seconds, 4.45 minutes\n",
      "total_backward_count 6565280 real_backward_count 317322   4.833%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.797627/  1.900316, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.72 seconds, 4.43 minutes\n",
      "total_backward_count 6609640 real_backward_count 318031   4.812%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.803986/  1.902686, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.35 seconds, 4.44 minutes\n",
      "total_backward_count 6654000 real_backward_count 318745   4.790%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.794654/  1.902995, val:  86.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.16 seconds, 4.35 minutes\n",
      "total_backward_count 6698360 real_backward_count 319478   4.769%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.796128/  1.905691, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.40 seconds, 4.41 minutes\n",
      "total_backward_count 6742720 real_backward_count 320228   4.749%\n",
      "epoch-152 lr=['0.0019531'], tr/val_loss:  1.795362/  1.896014, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.76 seconds, 4.40 minutes\n",
      "total_backward_count 6787080 real_backward_count 320978   4.729%\n",
      "epoch-153 lr=['0.0019531'], tr/val_loss:  1.788613/  1.892654, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.22 seconds, 4.40 minutes\n",
      "total_backward_count 6831440 real_backward_count 321674   4.709%\n",
      "epoch-154 lr=['0.0019531'], tr/val_loss:  1.789453/  1.905613, val:  85.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.08 seconds, 4.40 minutes\n",
      "total_backward_count 6875800 real_backward_count 322410   4.689%\n",
      "epoch-155 lr=['0.0019531'], tr/val_loss:  1.790893/  1.902470, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.80 seconds, 4.38 minutes\n",
      "total_backward_count 6920160 real_backward_count 323097   4.669%\n",
      "epoch-156 lr=['0.0019531'], tr/val_loss:  1.795774/  1.909283, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.90 seconds, 4.37 minutes\n",
      "total_backward_count 6964520 real_backward_count 323782   4.649%\n",
      "fc layer 1 self.abs_max_out: 10043.0\n",
      "epoch-157 lr=['0.0019531'], tr/val_loss:  1.792402/  1.900547, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.02 seconds, 4.40 minutes\n",
      "total_backward_count 7008880 real_backward_count 324506   4.630%\n",
      "epoch-158 lr=['0.0019531'], tr/val_loss:  1.791197/  1.896291, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.18 seconds, 4.40 minutes\n",
      "total_backward_count 7053240 real_backward_count 325191   4.611%\n",
      "fc layer 1 self.abs_max_out: 10067.0\n",
      "epoch-159 lr=['0.0019531'], tr/val_loss:  1.789678/  1.897517, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.59 seconds, 4.44 minutes\n",
      "total_backward_count 7097600 real_backward_count 325921   4.592%\n",
      "epoch-160 lr=['0.0019531'], tr/val_loss:  1.783723/  1.900933, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.69 seconds, 4.43 minutes\n",
      "total_backward_count 7141960 real_backward_count 326582   4.573%\n",
      "epoch-161 lr=['0.0019531'], tr/val_loss:  1.788650/  1.899011, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.04 seconds, 4.40 minutes\n",
      "total_backward_count 7186320 real_backward_count 327275   4.554%\n",
      "epoch-162 lr=['0.0019531'], tr/val_loss:  1.785316/  1.895048, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.08 seconds, 4.38 minutes\n",
      "total_backward_count 7230680 real_backward_count 328010   4.536%\n",
      "fc layer 3 self.abs_max_out: 524.0\n",
      "epoch-163 lr=['0.0019531'], tr/val_loss:  1.780320/  1.886564, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.77 seconds, 4.45 minutes\n",
      "total_backward_count 7275040 real_backward_count 328719   4.518%\n",
      "epoch-164 lr=['0.0019531'], tr/val_loss:  1.777674/  1.890541, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.14 seconds, 4.44 minutes\n",
      "total_backward_count 7319400 real_backward_count 329331   4.499%\n",
      "epoch-165 lr=['0.0019531'], tr/val_loss:  1.782266/  1.894276, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.60 seconds, 4.43 minutes\n",
      "total_backward_count 7363760 real_backward_count 330004   4.481%\n",
      "fc layer 1 self.abs_max_out: 10072.0\n",
      "epoch-166 lr=['0.0019531'], tr/val_loss:  1.778760/  1.884084, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.36 seconds, 4.41 minutes\n",
      "total_backward_count 7408120 real_backward_count 330633   4.463%\n",
      "epoch-167 lr=['0.0019531'], tr/val_loss:  1.776624/  1.896864, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.81 seconds, 4.41 minutes\n",
      "total_backward_count 7452480 real_backward_count 331316   4.446%\n",
      "epoch-168 lr=['0.0019531'], tr/val_loss:  1.785515/  1.885784, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.66 seconds, 4.36 minutes\n",
      "total_backward_count 7496840 real_backward_count 331968   4.428%\n",
      "fc layer 1 self.abs_max_out: 10089.0\n",
      "fc layer 2 self.abs_max_out: 3959.0\n",
      "epoch-169 lr=['0.0019531'], tr/val_loss:  1.778095/  1.893520, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.83 seconds, 4.45 minutes\n",
      "total_backward_count 7541200 real_backward_count 332597   4.410%\n",
      "epoch-170 lr=['0.0019531'], tr/val_loss:  1.779986/  1.886243, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.76 seconds, 4.43 minutes\n",
      "total_backward_count 7585560 real_backward_count 333259   4.393%\n",
      "epoch-171 lr=['0.0019531'], tr/val_loss:  1.774926/  1.884883, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.28 seconds, 4.42 minutes\n",
      "total_backward_count 7629920 real_backward_count 333901   4.376%\n",
      "epoch-172 lr=['0.0019531'], tr/val_loss:  1.774798/  1.887040, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.35 seconds, 4.44 minutes\n",
      "total_backward_count 7674280 real_backward_count 334556   4.359%\n",
      "epoch-173 lr=['0.0019531'], tr/val_loss:  1.779518/  1.893198, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.76 seconds, 4.36 minutes\n",
      "total_backward_count 7718640 real_backward_count 335194   4.343%\n",
      "epoch-174 lr=['0.0019531'], tr/val_loss:  1.778606/  1.888525, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.64 seconds, 4.44 minutes\n",
      "total_backward_count 7763000 real_backward_count 335872   4.327%\n",
      "fc layer 2 self.abs_max_out: 3982.0\n",
      "epoch-175 lr=['0.0019531'], tr/val_loss:  1.775998/  1.887588, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.64 seconds, 4.41 minutes\n",
      "total_backward_count 7807360 real_backward_count 336488   4.310%\n",
      "epoch-176 lr=['0.0019531'], tr/val_loss:  1.779312/  1.890865, val:  87.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.22 seconds, 4.40 minutes\n",
      "total_backward_count 7851720 real_backward_count 337119   4.294%\n",
      "fc layer 3 self.abs_max_out: 533.0\n",
      "epoch-177 lr=['0.0019531'], tr/val_loss:  1.777092/  1.893465, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.24 seconds, 4.44 minutes\n",
      "total_backward_count 7896080 real_backward_count 337750   4.277%\n",
      "epoch-178 lr=['0.0019531'], tr/val_loss:  1.771699/  1.881417, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.91 seconds, 4.40 minutes\n",
      "total_backward_count 7940440 real_backward_count 338368   4.261%\n",
      "epoch-179 lr=['0.0019531'], tr/val_loss:  1.774314/  1.889596, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.39 seconds, 4.37 minutes\n",
      "total_backward_count 7984800 real_backward_count 338995   4.246%\n",
      "epoch-180 lr=['0.0019531'], tr/val_loss:  1.774407/  1.885256, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.28 seconds, 4.42 minutes\n",
      "total_backward_count 8029160 real_backward_count 339596   4.230%\n",
      "epoch-181 lr=['0.0019531'], tr/val_loss:  1.771971/  1.892847, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.29 seconds, 4.42 minutes\n",
      "total_backward_count 8073520 real_backward_count 340147   4.213%\n",
      "epoch-182 lr=['0.0019531'], tr/val_loss:  1.775630/  1.895884, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.35 seconds, 4.42 minutes\n",
      "total_backward_count 8117880 real_backward_count 340756   4.198%\n",
      "epoch-183 lr=['0.0019531'], tr/val_loss:  1.774857/  1.883343, val:  84.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.42 seconds, 4.42 minutes\n",
      "total_backward_count 8162240 real_backward_count 341363   4.182%\n",
      "epoch-184 lr=['0.0019531'], tr/val_loss:  1.774164/  1.886215, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.06 seconds, 4.37 minutes\n",
      "total_backward_count 8206600 real_backward_count 341918   4.166%\n",
      "epoch-185 lr=['0.0019531'], tr/val_loss:  1.773532/  1.887235, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.77 seconds, 4.40 minutes\n",
      "total_backward_count 8250960 real_backward_count 342530   4.151%\n",
      "epoch-186 lr=['0.0019531'], tr/val_loss:  1.786664/  1.895834, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.48 seconds, 4.42 minutes\n",
      "total_backward_count 8295320 real_backward_count 343132   4.136%\n",
      "epoch-187 lr=['0.0019531'], tr/val_loss:  1.784303/  1.899499, val:  87.92%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.77 seconds, 4.40 minutes\n",
      "total_backward_count 8339680 real_backward_count 343678   4.121%\n",
      "epoch-188 lr=['0.0019531'], tr/val_loss:  1.785128/  1.894606, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.86 seconds, 4.41 minutes\n",
      "total_backward_count 8384040 real_backward_count 344225   4.106%\n",
      "epoch-189 lr=['0.0019531'], tr/val_loss:  1.773397/  1.886178, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.70 seconds, 4.39 minutes\n",
      "total_backward_count 8428400 real_backward_count 344832   4.091%\n",
      "epoch-190 lr=['0.0019531'], tr/val_loss:  1.769825/  1.889356, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.79 seconds, 4.40 minutes\n",
      "total_backward_count 8472760 real_backward_count 345403   4.077%\n",
      "epoch-191 lr=['0.0019531'], tr/val_loss:  1.772507/  1.888273, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.95 seconds, 4.40 minutes\n",
      "total_backward_count 8517120 real_backward_count 345967   4.062%\n",
      "epoch-192 lr=['0.0019531'], tr/val_loss:  1.766805/  1.875621, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.38 seconds, 4.41 minutes\n",
      "total_backward_count 8561480 real_backward_count 346536   4.048%\n",
      "epoch-193 lr=['0.0019531'], tr/val_loss:  1.756521/  1.865706, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.52 seconds, 4.43 minutes\n",
      "total_backward_count 8605840 real_backward_count 347141   4.034%\n",
      "epoch-194 lr=['0.0019531'], tr/val_loss:  1.747194/  1.862709, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.57 seconds, 4.43 minutes\n",
      "total_backward_count 8650200 real_backward_count 347645   4.019%\n",
      "epoch-195 lr=['0.0019531'], tr/val_loss:  1.749312/  1.873253, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.61 seconds, 4.38 minutes\n",
      "total_backward_count 8694560 real_backward_count 348157   4.004%\n",
      "epoch-196 lr=['0.0019531'], tr/val_loss:  1.759356/  1.880724, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.25 seconds, 4.39 minutes\n",
      "total_backward_count 8738920 real_backward_count 348686   3.990%\n",
      "epoch-197 lr=['0.0019531'], tr/val_loss:  1.757107/  1.873021, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.09 seconds, 4.43 minutes\n",
      "total_backward_count 8783280 real_backward_count 349230   3.976%\n",
      "epoch-198 lr=['0.0019531'], tr/val_loss:  1.756320/  1.874370, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.68 seconds, 4.41 minutes\n",
      "total_backward_count 8827640 real_backward_count 349757   3.962%\n",
      "epoch-199 lr=['0.0019531'], tr/val_loss:  1.757530/  1.874264, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.32 seconds, 4.44 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c54cdce7e14a93b9492ed255a6d2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>iter_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>summary_val_acc</td><td>‚ñÉ‚ñÅ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>tr_acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>tr_epoch_loss</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>val_acc_best</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_acc_now</td><td>‚ñÉ‚ñÅ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>iter_acc</td><td>1.0</td></tr><tr><td>tr_acc</td><td>1.0</td></tr><tr><td>tr_epoch_loss</td><td>1.75753</td></tr><tr><td>val_acc_best</td><td>0.92083</td></tr><tr><td>val_acc_now</td><td>0.91667</td></tr><tr><td>val_loss</td><td>1.87426</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">daily-sweep-3</strong> at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z15mbjf3' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/z15mbjf3</a><br/> View project at: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/data2/bh_wandb/wandb/run-20251009_114728-z15mbjf3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ih21w9lk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBPTT_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tDFA_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tIMAGE_SIZE: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tOTTT_input_trace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTIME: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tUDA_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha_uda: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbias: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcfg: [200, 200]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchaching_on: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconvTrue_fcFalse: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /data2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tddp_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdenoise_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_clipping: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdvs_duration: 25000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch_num: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \texclude_class: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \textra_train_dataset: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitial_pooling: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlast_lif: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001953125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_sg_width: 6.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_decay: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_init: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_reset: 10000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlif_layer_v_threshold: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmerge_polarities: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmy_seed: 9816\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnet_print: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_what: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpin_memory: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_trained_path: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_0: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_1: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tquantize_bit_list_2: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trate_coding: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_1w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_2w: -10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale_exp_3w: -9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler_name: no\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_step: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsurrogate: hard_sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_padding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_conv_stride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const1: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsynapse_trace_const2: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttdBN_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_filter_accumulation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrace_on: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_data: DVS_GESTURE_TONIC\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.22.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/bh_wandb/wandb/run-20251010_023419-ih21w9lk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ih21w9lk' target=\"_blank\">still-sweep-5</a></strong> to <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/sweeps/t0h80lho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ih21w9lk' target=\"_blank\">https://wandb.ai/bhkim003-seoul-national-university/my_snn%20main/runs/ih21w9lk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'single_step' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'my_seed' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'TIME' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'IMAGE_SIZE' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'which_data' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'data_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'rate_coding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_init' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_threshold' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_v_reset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lif_layer_sg_width' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_kernel_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_stride' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_conv_padding' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const1' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'synapse_trace_const2' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'convTrue_fcFalse' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'cfg' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'net_print' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pre_trained_path' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'epoch_num' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'tdBN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BN_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'surrogate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'BPTT_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'optimizer_what' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'scheduler_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'ddp_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_clipping' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dvs_duration' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'DFA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'OTTT_input_trace_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'exclude_class' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'merge_polarities' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'denoise_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'extra_train_dataset' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_workers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'chaching_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'pin_memory' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'UDA_on' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'alpha_uda' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bias' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'last_lif' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'initial_pooling' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'temporal_filter_accumulation' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param {'devices': '0', 'single_step': True, 'unique_name': '20251010_023427_327', 'my_seed': 9816, 'TIME': 10, 'BATCH': 1, 'IMAGE_SIZE': 14, 'which_data': 'DVS_GESTURE_TONIC', 'data_path': '/data2', 'rate_coding': False, 'lif_layer_v_init': 0, 'lif_layer_v_decay': 0.5, 'lif_layer_v_threshold': 0.5, 'lif_layer_v_reset': 10000, 'lif_layer_sg_width': 6.5, 'synapse_conv_kernel_size': 3, 'synapse_conv_stride': 1, 'synapse_conv_padding': 1, 'synapse_trace_const1': 1, 'synapse_trace_const2': 0.5, 'pre_trained': False, 'convTrue_fcFalse': False, 'cfg': [200, 200], 'net_print': True, 'pre_trained_path': '', 'learning_rate': 0.001953125, 'epoch_num': 200, 'tdBN_on': False, 'BN_on': False, 'surrogate': 'hard_sigmoid', 'BPTT_on': False, 'optimizer_what': 'SGD', 'scheduler_name': 'no', 'ddp_on': False, 'dvs_clipping': 14, 'dvs_duration': 25000, 'DFA_on': True, 'trace_on': False, 'OTTT_input_trace_on': False, 'exclude_class': True, 'merge_polarities': True, 'denoise_on': False, 'extra_train_dataset': 9, 'num_workers': 2, 'chaching_on': True, 'pin_memory': True, 'UDA_on': False, 'alpha_uda': 1, 'bias': False, 'last_lif': False, 'temporal_filter': 5, 'initial_pooling': 1, 'temporal_filter_accumulation': False, 'quantize_bit_list': [8, 8, 8], 'scale_exp': [[-10, -10], [-10, -10], [-9, -9]]} \n",
      "\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 979 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 0df5ce43f802d21fe74cde54437db10b\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 977 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = f205136b2771111650a88c4e480cfe73\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 963 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 391e4997dc3a746988cd0e9dceb2d42e\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 816 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = bb0ac3251c9e44bfe72bcb8b2e969f0d\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 448 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = c796a451486ae8cd6d0dd9bd02a9e235\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 149 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = a6e81fbc907b11cedc166a7f5b843582\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 61 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = d4ded3e2b3703cdb1192f3d689158f82\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 26 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 602987c624e8b98603f8b906841eadb1\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 13 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 2d3185edb0c7b53adc6375ce1392ad59\n",
      "cache path exists\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 4 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Îç∞Ïù¥ÌÑ∞ Í∞úÏàòÎäî 240 ÏûÖÎãàÎã§. (test setÏùÄ ÏïàÎ∞îÎÄåÍ≤å Ìï¥ÎÜ®Îã§ ÏïåÏ†ú)\n",
      "dataset_hash = 9e9960951042c2f18fd3576739597330\n",
      "cache path exists\n",
      "\n",
      "len(train_loader): 4436 BATCH: 1 train_data_count: 4436\n",
      "len(test_loader): 240 BATCH: 1\n",
      "\n",
      "device ==> cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " layer_count 1\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 1 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 1 v_bit: 17, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 2\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -10 -10\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "\n",
      "\n",
      "\n",
      "LIF 2 sg_bit 4\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++\n",
      "\n",
      " lif layer 2 v_bit: 16, v_exp: -10\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      " layer_count 3\n",
      "weight bias bit 8\n",
      "weight exp, bias exp -9 -9\n",
      "bit_for_output 0 exp_for_output None \n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "bit 8 percentile 0.999\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "======================================================================================\n",
      "DataParallel(\n",
      "  (module): REBORN_MY_SNN_FC(\n",
      "    (layers): REBORN_MY_Sequential(\n",
      "      (0): DimChanger_for_FC()\n",
      "      (1): SYNAPSE_FC(in_features=980, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=1, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (2): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=1, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (3): Feedback_Receiver()\n",
      "      (4): SYNAPSE_FC(in_features=200, out_features=200, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=2, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (5): LIF_layer(v_init=0, v_decay=0.5, v_threshold=0.5, v_reset=10000, sg_width=6.5, surrogate=hard_sigmoid, BPTT_on=False, trace_const1=1, trace_const2=0.5, TIME=10, sstep=True, trace_on=False, layer_count=2, scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (6): Feedback_Receiver()\n",
      "      (7): SYNAPSE_FC(in_features=200, out_features=10, TIME=10, bias=False, sstep=True, time_different_weight=False, layer_count=3, quantize_bit_list=[8, 8, 8], scale_exp=[[-10, -10], [-10, -10], [-9, -9]])\n",
      "      (DFA_top): Top_Gradient()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "========================================================\n",
      "Trainable parameters: 238,000\n",
      "========================================================\n",
      "\n",
      "MySGD (\n",
      "Parameter Group 0\n",
      "    lr: 0.001953125\n",
      "    momentum: 0.0\n",
      ")\n",
      "total_backward_count 0 real_backward_count 0   0.000%\n",
      "fc layer 1 self.abs_max_out: 558.0\n",
      "lif layer 1 self.abs_max_v: 558.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "fc layer 2 self.abs_max_out: 72.0\n",
      "lif layer 2 self.abs_max_v: 72.0\n",
      "new_weights[i] tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "new_weights[i] tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "lif layer 1 self.abs_max_v: 746.5\n",
      "fc layer 2 self.abs_max_out: 305.0\n",
      "lif layer 2 self.abs_max_v: 340.0\n",
      "fc layer 1 self.abs_max_out: 604.0\n",
      "lif layer 1 self.abs_max_v: 777.5\n",
      "lif layer 1 self.abs_max_v: 818.5\n",
      "lif layer 2 self.abs_max_v: 372.5\n",
      "lif layer 2 self.abs_max_v: 373.0\n",
      "fc layer 1 self.abs_max_out: 695.0\n",
      "lif layer 1 self.abs_max_v: 872.0\n",
      "fc layer 1 self.abs_max_out: 892.0\n",
      "lif layer 1 self.abs_max_v: 892.0\n",
      "fc layer 2 self.abs_max_out: 415.0\n",
      "lif layer 2 self.abs_max_v: 545.0\n",
      "fc layer 3 self.abs_max_out: 33.0\n",
      "lif layer 1 self.abs_max_v: 901.0\n",
      "lif layer 1 self.abs_max_v: 962.0\n",
      "fc layer 2 self.abs_max_out: 440.0\n",
      "lif layer 1 self.abs_max_v: 1068.0\n",
      "fc layer 2 self.abs_max_out: 499.0\n",
      "lif layer 2 self.abs_max_v: 576.0\n",
      "fc layer 3 self.abs_max_out: 40.0\n",
      "lif layer 1 self.abs_max_v: 1153.0\n",
      "fc layer 2 self.abs_max_out: 563.0\n",
      "lif layer 2 self.abs_max_v: 673.0\n",
      "fc layer 3 self.abs_max_out: 91.0\n",
      "lif layer 2 self.abs_max_v: 723.0\n",
      "fc layer 1 self.abs_max_out: 1066.0\n",
      "fc layer 2 self.abs_max_out: 576.0\n",
      "fc layer 1 self.abs_max_out: 1212.0\n",
      "lif layer 1 self.abs_max_v: 1212.0\n",
      "fc layer 2 self.abs_max_out: 595.0\n",
      "lif layer 2 self.abs_max_v: 738.0\n",
      "fc layer 2 self.abs_max_out: 951.0\n",
      "lif layer 2 self.abs_max_v: 1046.0\n",
      "fc layer 3 self.abs_max_out: 138.0\n",
      "lif layer 2 self.abs_max_v: 1048.0\n",
      "fc layer 3 self.abs_max_out: 172.0\n",
      "fc layer 1 self.abs_max_out: 1278.0\n",
      "lif layer 1 self.abs_max_v: 1278.0\n",
      "lif layer 2 self.abs_max_v: 1150.5\n",
      "fc layer 3 self.abs_max_out: 175.0\n",
      "fc layer 1 self.abs_max_out: 1359.0\n",
      "lif layer 1 self.abs_max_v: 1359.0\n",
      "fc layer 2 self.abs_max_out: 1199.0\n",
      "lif layer 2 self.abs_max_v: 1255.0\n",
      "fc layer 1 self.abs_max_out: 1444.0\n",
      "lif layer 1 self.abs_max_v: 1444.0\n",
      "fc layer 2 self.abs_max_out: 1258.0\n",
      "lif layer 2 self.abs_max_v: 1258.0\n",
      "fc layer 1 self.abs_max_out: 1491.0\n",
      "lif layer 1 self.abs_max_v: 1746.5\n",
      "fc layer 3 self.abs_max_out: 235.0\n",
      "fc layer 1 self.abs_max_out: 1537.0\n",
      "fc layer 3 self.abs_max_out: 271.0\n",
      "lif layer 2 self.abs_max_v: 1440.0\n",
      "fc layer 1 self.abs_max_out: 1635.0\n",
      "fc layer 1 self.abs_max_out: 1733.0\n",
      "fc layer 1 self.abs_max_out: 2083.0\n",
      "lif layer 1 self.abs_max_v: 2083.0\n",
      "fc layer 2 self.abs_max_out: 1327.0\n",
      "lif layer 2 self.abs_max_v: 1496.0\n",
      "fc layer 2 self.abs_max_out: 1331.0\n",
      "fc layer 2 self.abs_max_out: 1455.0\n",
      "lif layer 2 self.abs_max_v: 1668.5\n",
      "lif layer 2 self.abs_max_v: 1695.5\n",
      "lif layer 2 self.abs_max_v: 1826.0\n",
      "lif layer 2 self.abs_max_v: 1861.5\n",
      "lif layer 2 self.abs_max_v: 1882.0\n",
      "fc layer 1 self.abs_max_out: 2112.0\n",
      "lif layer 1 self.abs_max_v: 2112.0\n",
      "fc layer 2 self.abs_max_out: 1504.0\n",
      "fc layer 1 self.abs_max_out: 2157.0\n",
      "lif layer 1 self.abs_max_v: 2157.0\n",
      "fc layer 1 self.abs_max_out: 2177.0\n",
      "lif layer 1 self.abs_max_v: 2177.0\n",
      "fc layer 3 self.abs_max_out: 338.0\n",
      "fc layer 2 self.abs_max_out: 1517.0\n",
      "fc layer 1 self.abs_max_out: 2321.0\n",
      "lif layer 1 self.abs_max_v: 2321.0\n",
      "fc layer 2 self.abs_max_out: 1519.0\n",
      "fc layer 1 self.abs_max_out: 2362.0\n",
      "lif layer 1 self.abs_max_v: 2362.0\n",
      "fc layer 1 self.abs_max_out: 2516.0\n",
      "lif layer 1 self.abs_max_v: 2516.0\n",
      "lif layer 2 self.abs_max_v: 2018.0\n",
      "fc layer 2 self.abs_max_out: 1556.0\n",
      "fc layer 2 self.abs_max_out: 1586.0\n",
      "fc layer 2 self.abs_max_out: 1676.0\n",
      "lif layer 2 self.abs_max_v: 2043.0\n",
      "fc layer 2 self.abs_max_out: 1755.0\n",
      "fc layer 1 self.abs_max_out: 2693.0\n",
      "lif layer 1 self.abs_max_v: 2693.0\n",
      "fc layer 1 self.abs_max_out: 2767.0\n",
      "lif layer 1 self.abs_max_v: 2767.0\n",
      "fc layer 2 self.abs_max_out: 1767.0\n",
      "fc layer 3 self.abs_max_out: 343.0\n",
      "fc layer 3 self.abs_max_out: 364.0\n",
      "lif layer 2 self.abs_max_v: 2128.0\n",
      "fc layer 2 self.abs_max_out: 1789.0\n",
      "fc layer 1 self.abs_max_out: 2963.0\n",
      "lif layer 1 self.abs_max_v: 2963.0\n",
      "fc layer 2 self.abs_max_out: 1797.0\n",
      "fc layer 1 self.abs_max_out: 2981.0\n",
      "lif layer 1 self.abs_max_v: 2981.0\n",
      "fc layer 2 self.abs_max_out: 1844.0\n",
      "fc layer 2 self.abs_max_out: 1966.0\n",
      "lif layer 2 self.abs_max_v: 2247.5\n",
      "fc layer 1 self.abs_max_out: 3078.0\n",
      "lif layer 1 self.abs_max_v: 3078.0\n",
      "fc layer 1 self.abs_max_out: 3101.0\n",
      "lif layer 1 self.abs_max_v: 3101.0\n",
      "fc layer 1 self.abs_max_out: 3142.0\n",
      "lif layer 1 self.abs_max_v: 3142.0\n",
      "fc layer 3 self.abs_max_out: 368.0\n",
      "lif layer 1 self.abs_max_v: 3143.5\n",
      "fc layer 1 self.abs_max_out: 3649.0\n",
      "lif layer 1 self.abs_max_v: 3649.0\n",
      "fc layer 3 self.abs_max_out: 406.0\n",
      "lif layer 2 self.abs_max_v: 2481.0\n",
      "lif layer 1 self.abs_max_v: 3817.5\n",
      "fc layer 3 self.abs_max_out: 416.0\n",
      "fc layer 1 self.abs_max_out: 4190.0\n",
      "lif layer 1 self.abs_max_v: 4190.0\n",
      "fc layer 1 self.abs_max_out: 4200.0\n",
      "lif layer 1 self.abs_max_v: 4200.0\n",
      "fc layer 3 self.abs_max_out: 439.0\n",
      "fc layer 1 self.abs_max_out: 4291.0\n",
      "lif layer 1 self.abs_max_v: 4291.0\n",
      "fc layer 1 self.abs_max_out: 4370.0\n",
      "lif layer 1 self.abs_max_v: 4370.0\n",
      "fc layer 1 self.abs_max_out: 4745.0\n",
      "lif layer 1 self.abs_max_v: 4745.0\n",
      "fc layer 2 self.abs_max_out: 1972.0\n",
      "fc layer 2 self.abs_max_out: 2059.0\n",
      "fc layer 2 self.abs_max_out: 2224.0\n",
      "fc layer 2 self.abs_max_out: 2300.0\n",
      "fc layer 1 self.abs_max_out: 4846.0\n",
      "lif layer 1 self.abs_max_v: 4846.0\n",
      "fc layer 2 self.abs_max_out: 2436.0\n",
      "fc layer 2 self.abs_max_out: 2562.0\n",
      "lif layer 2 self.abs_max_v: 2562.0\n",
      "fc layer 1 self.abs_max_out: 4965.0\n",
      "lif layer 1 self.abs_max_v: 4965.0\n",
      "fc layer 2 self.abs_max_out: 2590.0\n",
      "lif layer 2 self.abs_max_v: 2590.0\n",
      "fc layer 2 self.abs_max_out: 2656.0\n",
      "lif layer 2 self.abs_max_v: 2656.0\n",
      "fc layer 3 self.abs_max_out: 448.0\n",
      "fc layer 3 self.abs_max_out: 471.0\n",
      "fc layer 2 self.abs_max_out: 2753.0\n",
      "lif layer 2 self.abs_max_v: 2753.0\n",
      "fc layer 1 self.abs_max_out: 5381.0\n",
      "lif layer 1 self.abs_max_v: 5381.0\n",
      "fc layer 2 self.abs_max_out: 2963.0\n",
      "lif layer 2 self.abs_max_v: 2963.0\n",
      "fc layer 2 self.abs_max_out: 3059.0\n",
      "lif layer 2 self.abs_max_v: 3059.0\n",
      "fc layer 2 self.abs_max_out: 3155.0\n",
      "lif layer 2 self.abs_max_v: 3155.0\n",
      "fc layer 2 self.abs_max_out: 3184.0\n",
      "lif layer 2 self.abs_max_v: 3184.0\n",
      "fc layer 1 self.abs_max_out: 5531.0\n",
      "lif layer 1 self.abs_max_v: 5531.0\n",
      "fc layer 2 self.abs_max_out: 3229.0\n",
      "lif layer 2 self.abs_max_v: 3229.0\n",
      "fc layer 2 self.abs_max_out: 3285.0\n",
      "lif layer 2 self.abs_max_v: 3285.0\n",
      "fc layer 1 self.abs_max_out: 5763.0\n",
      "lif layer 1 self.abs_max_v: 5763.0\n",
      "fc layer 1 self.abs_max_out: 5817.0\n",
      "lif layer 1 self.abs_max_v: 5817.0\n",
      "fc layer 1 self.abs_max_out: 5889.0\n",
      "lif layer 1 self.abs_max_v: 5889.0\n",
      "fc layer 2 self.abs_max_out: 3450.0\n",
      "lif layer 2 self.abs_max_v: 3450.0\n",
      "fc layer 2 self.abs_max_out: 3702.0\n",
      "lif layer 2 self.abs_max_v: 3702.0\n",
      "fc layer 1 self.abs_max_out: 5999.0\n",
      "lif layer 1 self.abs_max_v: 5999.0\n",
      "fc layer 1 self.abs_max_out: 6353.0\n",
      "lif layer 1 self.abs_max_v: 6353.0\n",
      "epoch-0   lr=['0.0019531'], tr/val_loss:  2.062671/  2.132765, val:  47.08%, val_best:  47.08%, tr:  94.86%, tr_best:  94.86%, epoch time: 264.82 seconds, 4.41 minutes\n",
      "total_backward_count 44360 real_backward_count 10905  24.583%\n",
      "[module.layers.3] weight_fb parameter count: 2,000\n",
      "[module.layers.6] weight_fb parameter count: 2,000\n",
      "fc layer 1 self.abs_max_out: 6453.0\n",
      "lif layer 1 self.abs_max_v: 6453.0\n",
      "fc layer 1 self.abs_max_out: 6553.0\n",
      "lif layer 1 self.abs_max_v: 6553.0\n",
      "fc layer 1 self.abs_max_out: 7051.0\n",
      "lif layer 1 self.abs_max_v: 7051.0\n",
      "fc layer 2 self.abs_max_out: 3780.0\n",
      "lif layer 2 self.abs_max_v: 3780.0\n",
      "lif layer 1 self.abs_max_v: 7481.5\n",
      "fc layer 2 self.abs_max_out: 3795.0\n",
      "lif layer 2 self.abs_max_v: 3795.0\n",
      "epoch-1   lr=['0.0019531'], tr/val_loss:  2.047520/  2.127144, val:  47.92%, val_best:  47.92%, tr:  99.28%, tr_best:  99.28%, epoch time: 263.78 seconds, 4.40 minutes\n",
      "total_backward_count 88720 real_backward_count 18403  20.743%\n",
      "fc layer 2 self.abs_max_out: 3843.0\n",
      "lif layer 2 self.abs_max_v: 3843.0\n",
      "fc layer 1 self.abs_max_out: 7358.0\n",
      "lif layer 1 self.abs_max_v: 7520.5\n",
      "fc layer 2 self.abs_max_out: 3925.0\n",
      "lif layer 2 self.abs_max_v: 3925.0\n",
      "lif layer 1 self.abs_max_v: 8398.5\n",
      "epoch-2   lr=['0.0019531'], tr/val_loss:  2.034276/  2.114984, val:  51.25%, val_best:  51.25%, tr:  99.39%, tr_best:  99.39%, epoch time: 262.29 seconds, 4.37 minutes\n",
      "total_backward_count 133080 real_backward_count 25384  19.074%\n",
      "lif layer 2 self.abs_max_v: 4046.0\n",
      "fc layer 1 self.abs_max_out: 7385.0\n",
      "fc layer 1 self.abs_max_out: 7730.0\n",
      "lif layer 1 self.abs_max_v: 8953.5\n",
      "epoch-3   lr=['0.0019531'], tr/val_loss:  2.023573/  2.111140, val:  50.42%, val_best:  51.25%, tr:  99.62%, tr_best:  99.62%, epoch time: 265.78 seconds, 4.43 minutes\n",
      "total_backward_count 177440 real_backward_count 31890  17.972%\n",
      "fc layer 1 self.abs_max_out: 7930.0\n",
      "lif layer 2 self.abs_max_v: 4172.5\n",
      "lif layer 2 self.abs_max_v: 4234.5\n",
      "lif layer 2 self.abs_max_v: 4367.5\n",
      "lif layer 2 self.abs_max_v: 4405.0\n",
      "lif layer 2 self.abs_max_v: 4409.0\n",
      "lif layer 2 self.abs_max_v: 4534.0\n",
      "lif layer 2 self.abs_max_v: 4755.0\n",
      "lif layer 2 self.abs_max_v: 4757.5\n",
      "epoch-4   lr=['0.0019531'], tr/val_loss:  2.017036/  2.107423, val:  46.67%, val_best:  51.25%, tr:  99.68%, tr_best:  99.68%, epoch time: 265.08 seconds, 4.42 minutes\n",
      "total_backward_count 221800 real_backward_count 38084  17.170%\n",
      "fc layer 1 self.abs_max_out: 8026.0\n",
      "lif layer 2 self.abs_max_v: 4786.5\n",
      "lif layer 2 self.abs_max_v: 4996.0\n",
      "lif layer 2 self.abs_max_v: 5039.0\n",
      "fc layer 2 self.abs_max_out: 3958.0\n",
      "lif layer 1 self.abs_max_v: 9002.0\n",
      "fc layer 1 self.abs_max_out: 8077.0\n",
      "fc layer 1 self.abs_max_out: 8477.0\n",
      "lif layer 1 self.abs_max_v: 9022.5\n",
      "epoch-5   lr=['0.0019531'], tr/val_loss:  2.011624/  2.087758, val:  61.67%, val_best:  61.67%, tr:  99.75%, tr_best:  99.75%, epoch time: 266.03 seconds, 4.43 minutes\n",
      "total_backward_count 266160 real_backward_count 44202  16.607%\n",
      "lif layer 1 self.abs_max_v: 9058.5\n",
      "fc layer 1 self.abs_max_out: 8614.0\n",
      "lif layer 1 self.abs_max_v: 9389.0\n",
      "lif layer 1 self.abs_max_v: 9532.5\n",
      "epoch-6   lr=['0.0019531'], tr/val_loss:  1.998476/  2.077596, val:  55.42%, val_best:  61.67%, tr:  99.86%, tr_best:  99.86%, epoch time: 264.95 seconds, 4.42 minutes\n",
      "total_backward_count 310520 real_backward_count 49986  16.098%\n",
      "lif layer 2 self.abs_max_v: 5126.5\n",
      "lif layer 2 self.abs_max_v: 5172.5\n",
      "fc layer 1 self.abs_max_out: 8933.0\n",
      "lif layer 1 self.abs_max_v: 9574.0\n",
      "epoch-7   lr=['0.0019531'], tr/val_loss:  1.993934/  2.074196, val:  61.25%, val_best:  61.67%, tr:  99.91%, tr_best:  99.91%, epoch time: 264.59 seconds, 4.41 minutes\n",
      "total_backward_count 354880 real_backward_count 55403  15.612%\n",
      "lif layer 2 self.abs_max_v: 5325.5\n",
      "lif layer 2 self.abs_max_v: 5445.0\n",
      "fc layer 1 self.abs_max_out: 8937.0\n",
      "fc layer 1 self.abs_max_out: 9225.0\n",
      "lif layer 1 self.abs_max_v: 9608.5\n",
      "epoch-8   lr=['0.0019531'], tr/val_loss:  1.986738/  2.067260, val:  73.75%, val_best:  73.75%, tr:  99.86%, tr_best:  99.91%, epoch time: 266.69 seconds, 4.44 minutes\n",
      "total_backward_count 399240 real_backward_count 60715  15.208%\n",
      "fc layer 1 self.abs_max_out: 9436.0\n",
      "lif layer 1 self.abs_max_v: 9936.5\n",
      "epoch-9   lr=['0.0019531'], tr/val_loss:  1.983606/  2.057926, val:  68.33%, val_best:  73.75%, tr:  99.91%, tr_best:  99.91%, epoch time: 265.51 seconds, 4.43 minutes\n",
      "total_backward_count 443600 real_backward_count 65834  14.841%\n",
      "fc layer 1 self.abs_max_out: 9453.0\n",
      "epoch-10  lr=['0.0019531'], tr/val_loss:  1.964556/  2.031572, val:  69.17%, val_best:  73.75%, tr:  99.95%, tr_best:  99.95%, epoch time: 258.08 seconds, 4.30 minutes\n",
      "total_backward_count 487960 real_backward_count 70730  14.495%\n",
      "fc layer 1 self.abs_max_out: 9552.0\n",
      "lif layer 1 self.abs_max_v: 10094.5\n",
      "epoch-11  lr=['0.0019531'], tr/val_loss:  1.961933/  2.031986, val:  80.00%, val_best:  80.00%, tr:  99.91%, tr_best:  99.95%, epoch time: 264.59 seconds, 4.41 minutes\n",
      "total_backward_count 532320 real_backward_count 75520  14.187%\n",
      "fc layer 1 self.abs_max_out: 9640.0\n",
      "fc layer 2 self.abs_max_out: 4074.0\n",
      "lif layer 1 self.abs_max_v: 10348.0\n",
      "epoch-12  lr=['0.0019531'], tr/val_loss:  1.965472/  2.039048, val:  69.58%, val_best:  80.00%, tr:  99.93%, tr_best:  99.95%, epoch time: 265.80 seconds, 4.43 minutes\n",
      "total_backward_count 576680 real_backward_count 80074  13.885%\n",
      "fc layer 1 self.abs_max_out: 9753.0\n",
      "lif layer 1 self.abs_max_v: 10393.0\n",
      "lif layer 1 self.abs_max_v: 10479.0\n",
      "epoch-13  lr=['0.0019531'], tr/val_loss:  1.951383/  2.040298, val:  69.17%, val_best:  80.00%, tr:  99.93%, tr_best:  99.95%, epoch time: 262.31 seconds, 4.37 minutes\n",
      "total_backward_count 621040 real_backward_count 84521  13.610%\n",
      "fc layer 1 self.abs_max_out: 9834.0\n",
      "lif layer 1 self.abs_max_v: 10835.0\n",
      "epoch-14  lr=['0.0019531'], tr/val_loss:  1.954319/  2.033428, val:  75.00%, val_best:  80.00%, tr:  99.98%, tr_best:  99.98%, epoch time: 266.60 seconds, 4.44 minutes\n",
      "total_backward_count 665400 real_backward_count 88877  13.357%\n",
      "fc layer 1 self.abs_max_out: 9857.0\n",
      "epoch-15  lr=['0.0019531'], tr/val_loss:  1.942896/  2.021526, val:  78.75%, val_best:  80.00%, tr:  99.98%, tr_best:  99.98%, epoch time: 267.30 seconds, 4.45 minutes\n",
      "total_backward_count 709760 real_backward_count 93094  13.116%\n",
      "fc layer 1 self.abs_max_out: 9881.0\n",
      "epoch-16  lr=['0.0019531'], tr/val_loss:  1.940169/  2.022523, val:  71.67%, val_best:  80.00%, tr:  99.98%, tr_best:  99.98%, epoch time: 265.61 seconds, 4.43 minutes\n",
      "total_backward_count 754120 real_backward_count 97009  12.864%\n",
      "lif layer 1 self.abs_max_v: 11044.0\n",
      "epoch-17  lr=['0.0019531'], tr/val_loss:  1.936897/  2.005512, val:  86.67%, val_best:  86.67%, tr:  99.98%, tr_best:  99.98%, epoch time: 265.54 seconds, 4.43 minutes\n",
      "total_backward_count 798480 real_backward_count 100884  12.635%\n",
      "fc layer 1 self.abs_max_out: 9943.0\n",
      "fc layer 3 self.abs_max_out: 480.0\n",
      "epoch-18  lr=['0.0019531'], tr/val_loss:  1.933781/  2.023033, val:  80.00%, val_best:  86.67%, tr:  99.95%, tr_best:  99.98%, epoch time: 261.55 seconds, 4.36 minutes\n",
      "total_backward_count 842840 real_backward_count 104663  12.418%\n",
      "fc layer 1 self.abs_max_out: 10000.0\n",
      "epoch-19  lr=['0.0019531'], tr/val_loss:  1.930619/  2.011265, val:  77.92%, val_best:  86.67%, tr:  99.95%, tr_best:  99.98%, epoch time: 265.57 seconds, 4.43 minutes\n",
      "total_backward_count 887200 real_backward_count 108329  12.210%\n",
      "fc layer 3 self.abs_max_out: 488.0\n",
      "epoch-20  lr=['0.0019531'], tr/val_loss:  1.928120/  2.000469, val:  86.67%, val_best:  86.67%, tr:  99.98%, tr_best:  99.98%, epoch time: 265.78 seconds, 4.43 minutes\n",
      "total_backward_count 931560 real_backward_count 111935  12.016%\n",
      "fc layer 1 self.abs_max_out: 10078.0\n",
      "epoch-21  lr=['0.0019531'], tr/val_loss:  1.925835/  2.004512, val:  76.67%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.78 seconds, 4.43 minutes\n",
      "total_backward_count 975920 real_backward_count 115362  11.821%\n",
      "fc layer 1 self.abs_max_out: 10222.0\n",
      "epoch-22  lr=['0.0019531'], tr/val_loss:  1.922645/  1.998579, val:  84.58%, val_best:  86.67%, tr:  99.98%, tr_best: 100.00%, epoch time: 264.77 seconds, 4.41 minutes\n",
      "total_backward_count 1020280 real_backward_count 118641  11.628%\n",
      "fc layer 2 self.abs_max_out: 4121.0\n",
      "fc layer 1 self.abs_max_out: 10224.0\n",
      "lif layer 1 self.abs_max_v: 11116.5\n",
      "lif layer 1 self.abs_max_v: 11232.5\n",
      "epoch-23  lr=['0.0019531'], tr/val_loss:  1.908600/  1.988761, val:  82.92%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.57 seconds, 4.41 minutes\n",
      "total_backward_count 1064640 real_backward_count 121965  11.456%\n",
      "fc layer 1 self.abs_max_out: 10260.0\n",
      "lif layer 1 self.abs_max_v: 11483.0\n",
      "epoch-24  lr=['0.0019531'], tr/val_loss:  1.900781/  1.976727, val:  86.25%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.33 seconds, 4.41 minutes\n",
      "total_backward_count 1109000 real_backward_count 125197  11.289%\n",
      "lif layer 1 self.abs_max_v: 11501.0\n",
      "fc layer 1 self.abs_max_out: 10286.0\n",
      "fc layer 2 self.abs_max_out: 4153.0\n",
      "fc layer 2 self.abs_max_out: 4226.0\n",
      "epoch-25  lr=['0.0019531'], tr/val_loss:  1.893613/  1.994316, val:  78.33%, val_best:  86.67%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.09 seconds, 4.38 minutes\n",
      "total_backward_count 1153360 real_backward_count 128400  11.133%\n",
      "lif layer 1 self.abs_max_v: 11789.0\n",
      "fc layer 1 self.abs_max_out: 10296.0\n",
      "epoch-26  lr=['0.0019531'], tr/val_loss:  1.902714/  1.986029, val:  87.92%, val_best:  87.92%, tr:  99.98%, tr_best: 100.00%, epoch time: 264.99 seconds, 4.42 minutes\n",
      "total_backward_count 1197720 real_backward_count 131452  10.975%\n",
      "epoch-27  lr=['0.0019531'], tr/val_loss:  1.901626/  1.982526, val:  88.33%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.79 seconds, 4.43 minutes\n",
      "total_backward_count 1242080 real_backward_count 134457  10.825%\n",
      "fc layer 3 self.abs_max_out: 494.0\n",
      "fc layer 1 self.abs_max_out: 10360.0\n",
      "fc layer 2 self.abs_max_out: 4228.0\n",
      "fc layer 2 self.abs_max_out: 4234.0\n",
      "epoch-28  lr=['0.0019531'], tr/val_loss:  1.895462/  1.979020, val:  87.50%, val_best:  88.33%, tr:  99.98%, tr_best: 100.00%, epoch time: 265.88 seconds, 4.43 minutes\n",
      "total_backward_count 1286440 real_backward_count 137349  10.677%\n",
      "fc layer 1 self.abs_max_out: 10449.0\n",
      "lif layer 1 self.abs_max_v: 11852.5\n",
      "epoch-29  lr=['0.0019531'], tr/val_loss:  1.888535/  1.975928, val:  87.50%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.87 seconds, 4.38 minutes\n",
      "total_backward_count 1330800 real_backward_count 140231  10.537%\n",
      "fc layer 2 self.abs_max_out: 4326.0\n",
      "fc layer 1 self.abs_max_out: 10453.0\n",
      "fc layer 2 self.abs_max_out: 4350.0\n",
      "lif layer 1 self.abs_max_v: 11877.0\n",
      "fc layer 3 self.abs_max_out: 496.0\n",
      "epoch-30  lr=['0.0019531'], tr/val_loss:  1.888850/  1.973809, val:  87.08%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.26 seconds, 4.42 minutes\n",
      "total_backward_count 1375160 real_backward_count 142980  10.397%\n",
      "lif layer 1 self.abs_max_v: 12081.0\n",
      "fc layer 1 self.abs_max_out: 10458.0\n",
      "epoch-31  lr=['0.0019531'], tr/val_loss:  1.884354/  1.972335, val:  79.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.21 seconds, 4.42 minutes\n",
      "total_backward_count 1419520 real_backward_count 145662  10.261%\n",
      "lif layer 1 self.abs_max_v: 12100.5\n",
      "fc layer 1 self.abs_max_out: 10481.0\n",
      "epoch-32  lr=['0.0019531'], tr/val_loss:  1.886332/  1.975361, val:  84.17%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.03 seconds, 4.43 minutes\n",
      "total_backward_count 1463880 real_backward_count 148266  10.128%\n",
      "fc layer 3 self.abs_max_out: 507.0\n",
      "fc layer 1 self.abs_max_out: 10483.0\n",
      "epoch-33  lr=['0.0019531'], tr/val_loss:  1.884450/  1.974778, val:  84.58%, val_best:  88.33%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.69 seconds, 4.43 minutes\n",
      "total_backward_count 1508240 real_backward_count 150827  10.000%\n",
      "fc layer 1 self.abs_max_out: 10526.0\n",
      "fc layer 2 self.abs_max_out: 4407.0\n",
      "epoch-34  lr=['0.0019531'], tr/val_loss:  1.890077/  1.973796, val:  88.75%, val_best:  88.75%, tr:  99.98%, tr_best: 100.00%, epoch time: 263.89 seconds, 4.40 minutes\n",
      "total_backward_count 1552600 real_backward_count 153312   9.875%\n",
      "fc layer 1 self.abs_max_out: 10547.0\n",
      "lif layer 1 self.abs_max_v: 12330.0\n",
      "epoch-35  lr=['0.0019531'], tr/val_loss:  1.879685/  1.968529, val:  89.58%, val_best:  89.58%, tr:  99.98%, tr_best: 100.00%, epoch time: 264.95 seconds, 4.42 minutes\n",
      "total_backward_count 1596960 real_backward_count 155719   9.751%\n",
      "epoch-36  lr=['0.0019531'], tr/val_loss:  1.877427/  1.972928, val:  80.00%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.45 seconds, 4.37 minutes\n",
      "total_backward_count 1641320 real_backward_count 158109   9.633%\n",
      "lif layer 1 self.abs_max_v: 12411.5\n",
      "epoch-37  lr=['0.0019531'], tr/val_loss:  1.875483/  1.965976, val:  84.17%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.52 seconds, 4.44 minutes\n",
      "total_backward_count 1685680 real_backward_count 160454   9.519%\n",
      "lif layer 1 self.abs_max_v: 12598.5\n",
      "epoch-38  lr=['0.0019531'], tr/val_loss:  1.872879/  1.968533, val:  85.42%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.56 seconds, 4.46 minutes\n",
      "total_backward_count 1730040 real_backward_count 162810   9.411%\n",
      "fc layer 1 self.abs_max_out: 10577.0\n",
      "epoch-39  lr=['0.0019531'], tr/val_loss:  1.867415/  1.948956, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.29 seconds, 4.44 minutes\n",
      "total_backward_count 1774400 real_backward_count 165000   9.299%\n",
      "lif layer 1 self.abs_max_v: 12749.5\n",
      "fc layer 1 self.abs_max_out: 10592.0\n",
      "epoch-40  lr=['0.0019531'], tr/val_loss:  1.860170/  1.950807, val:  86.67%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.91 seconds, 4.40 minutes\n",
      "total_backward_count 1818760 real_backward_count 167223   9.194%\n",
      "lif layer 1 self.abs_max_v: 13082.5\n",
      "fc layer 1 self.abs_max_out: 10609.0\n",
      "fc layer 2 self.abs_max_out: 4501.0\n",
      "epoch-41  lr=['0.0019531'], tr/val_loss:  1.861216/  1.950227, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.58 seconds, 4.39 minutes\n",
      "total_backward_count 1863120 real_backward_count 169395   9.092%\n",
      "fc layer 1 self.abs_max_out: 10628.0\n",
      "fc layer 2 self.abs_max_out: 4523.0\n",
      "epoch-42  lr=['0.0019531'], tr/val_loss:  1.850659/  1.940065, val:  85.83%, val_best:  89.58%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.13 seconds, 4.45 minutes\n",
      "total_backward_count 1907480 real_backward_count 171546   8.993%\n",
      "fc layer 1 self.abs_max_out: 10657.0\n",
      "fc layer 3 self.abs_max_out: 510.0\n",
      "epoch-43  lr=['0.0019531'], tr/val_loss:  1.852790/  1.937529, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.13 seconds, 4.42 minutes\n",
      "total_backward_count 1951840 real_backward_count 173691   8.899%\n",
      "fc layer 1 self.abs_max_out: 10718.0\n",
      "epoch-44  lr=['0.0019531'], tr/val_loss:  1.844594/  1.938010, val:  84.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.61 seconds, 4.43 minutes\n",
      "total_backward_count 1996200 real_backward_count 175740   8.804%\n",
      "fc layer 3 self.abs_max_out: 519.0\n",
      "fc layer 1 self.abs_max_out: 10735.0\n",
      "epoch-45  lr=['0.0019531'], tr/val_loss:  1.841078/  1.929962, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.42 seconds, 4.39 minutes\n",
      "total_backward_count 2040560 real_backward_count 177722   8.709%\n",
      "fc layer 3 self.abs_max_out: 527.0\n",
      "fc layer 1 self.abs_max_out: 10785.0\n",
      "epoch-46  lr=['0.0019531'], tr/val_loss:  1.843075/  1.944647, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.61 seconds, 4.44 minutes\n",
      "total_backward_count 2084920 real_backward_count 179648   8.617%\n",
      "fc layer 2 self.abs_max_out: 4589.0\n",
      "epoch-47  lr=['0.0019531'], tr/val_loss:  1.843890/  1.934701, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.32 seconds, 4.37 minutes\n",
      "total_backward_count 2129280 real_backward_count 181572   8.527%\n",
      "epoch-48  lr=['0.0019531'], tr/val_loss:  1.837751/  1.933427, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.14 seconds, 4.40 minutes\n",
      "total_backward_count 2173640 real_backward_count 183494   8.442%\n",
      "epoch-49  lr=['0.0019531'], tr/val_loss:  1.836239/  1.939682, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.23 seconds, 4.44 minutes\n",
      "total_backward_count 2218000 real_backward_count 185359   8.357%\n",
      "epoch-50  lr=['0.0019531'], tr/val_loss:  1.828077/  1.921435, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.68 seconds, 4.44 minutes\n",
      "total_backward_count 2262360 real_backward_count 187137   8.272%\n",
      "fc layer 1 self.abs_max_out: 10809.0\n",
      "fc layer 3 self.abs_max_out: 538.0\n",
      "epoch-51  lr=['0.0019531'], tr/val_loss:  1.822647/  1.930243, val:  85.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.11 seconds, 4.44 minutes\n",
      "total_backward_count 2306720 real_backward_count 188952   8.191%\n",
      "epoch-52  lr=['0.0019531'], tr/val_loss:  1.824080/  1.937050, val:  83.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.12 seconds, 4.35 minutes\n",
      "total_backward_count 2351080 real_backward_count 190713   8.112%\n",
      "epoch-53  lr=['0.0019531'], tr/val_loss:  1.817312/  1.916378, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.37 seconds, 4.44 minutes\n",
      "total_backward_count 2395440 real_backward_count 192462   8.035%\n",
      "fc layer 1 self.abs_max_out: 10816.0\n",
      "epoch-54  lr=['0.0019531'], tr/val_loss:  1.817376/  1.913552, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.69 seconds, 4.43 minutes\n",
      "total_backward_count 2439800 real_backward_count 194220   7.960%\n",
      "fc layer 1 self.abs_max_out: 10846.0\n",
      "epoch-55  lr=['0.0019531'], tr/val_loss:  1.809546/  1.912025, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.94 seconds, 4.43 minutes\n",
      "total_backward_count 2484160 real_backward_count 195946   7.888%\n",
      "fc layer 1 self.abs_max_out: 10880.0\n",
      "epoch-56  lr=['0.0019531'], tr/val_loss:  1.811551/  1.906125, val:  90.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.40 seconds, 4.41 minutes\n",
      "total_backward_count 2528520 real_backward_count 197681   7.818%\n",
      "fc layer 1 self.abs_max_out: 10900.0\n",
      "fc layer 3 self.abs_max_out: 554.0\n",
      "epoch-57  lr=['0.0019531'], tr/val_loss:  1.806615/  1.922190, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.62 seconds, 4.41 minutes\n",
      "total_backward_count 2572880 real_backward_count 199323   7.747%\n",
      "fc layer 1 self.abs_max_out: 10914.0\n",
      "epoch-58  lr=['0.0019531'], tr/val_loss:  1.811047/  1.917223, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.67 seconds, 4.41 minutes\n",
      "total_backward_count 2617240 real_backward_count 200855   7.674%\n",
      "epoch-59  lr=['0.0019531'], tr/val_loss:  1.800160/  1.915049, val:  86.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.57 seconds, 4.39 minutes\n",
      "total_backward_count 2661600 real_backward_count 202344   7.602%\n",
      "epoch-60  lr=['0.0019531'], tr/val_loss:  1.808084/  1.916194, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.52 seconds, 4.43 minutes\n",
      "total_backward_count 2705960 real_backward_count 203819   7.532%\n",
      "fc layer 3 self.abs_max_out: 560.0\n",
      "epoch-61  lr=['0.0019531'], tr/val_loss:  1.803522/  1.890316, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.72 seconds, 4.46 minutes\n",
      "total_backward_count 2750320 real_backward_count 205357   7.467%\n",
      "epoch-62  lr=['0.0019531'], tr/val_loss:  1.795287/  1.903206, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.30 seconds, 4.44 minutes\n",
      "total_backward_count 2794680 real_backward_count 206922   7.404%\n",
      "fc layer 3 self.abs_max_out: 563.0\n",
      "epoch-63  lr=['0.0019531'], tr/val_loss:  1.798643/  1.907598, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.31 seconds, 4.36 minutes\n",
      "total_backward_count 2839040 real_backward_count 208458   7.343%\n",
      "epoch-64  lr=['0.0019531'], tr/val_loss:  1.799734/  1.906871, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.53 seconds, 4.41 minutes\n",
      "total_backward_count 2883400 real_backward_count 209918   7.280%\n",
      "epoch-65  lr=['0.0019531'], tr/val_loss:  1.794083/  1.903579, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.18 seconds, 4.44 minutes\n",
      "total_backward_count 2927760 real_backward_count 211320   7.218%\n",
      "fc layer 3 self.abs_max_out: 568.0\n",
      "fc layer 3 self.abs_max_out: 589.0\n",
      "epoch-66  lr=['0.0019531'], tr/val_loss:  1.782889/  1.889427, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.04 seconds, 4.42 minutes\n",
      "total_backward_count 2972120 real_backward_count 212731   7.158%\n",
      "epoch-67  lr=['0.0019531'], tr/val_loss:  1.784070/  1.896644, val:  89.17%, val_best:  92.08%, tr:  99.98%, tr_best: 100.00%, epoch time: 265.98 seconds, 4.43 minutes\n",
      "total_backward_count 3016480 real_backward_count 214204   7.101%\n",
      "epoch-68  lr=['0.0019531'], tr/val_loss:  1.794809/  1.895134, val:  85.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.12 seconds, 4.40 minutes\n",
      "total_backward_count 3060840 real_backward_count 215645   7.045%\n",
      "fc layer 1 self.abs_max_out: 10917.0\n",
      "epoch-69  lr=['0.0019531'], tr/val_loss:  1.790662/  1.897761, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.47 seconds, 4.42 minutes\n",
      "total_backward_count 3105200 real_backward_count 217028   6.989%\n",
      "epoch-70  lr=['0.0019531'], tr/val_loss:  1.790593/  1.901817, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.80 seconds, 4.36 minutes\n",
      "total_backward_count 3149560 real_backward_count 218370   6.933%\n",
      "fc layer 1 self.abs_max_out: 10925.0\n",
      "epoch-71  lr=['0.0019531'], tr/val_loss:  1.791381/  1.892692, val:  86.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.97 seconds, 4.42 minutes\n",
      "total_backward_count 3193920 real_backward_count 219717   6.879%\n",
      "fc layer 1 self.abs_max_out: 10929.0\n",
      "epoch-72  lr=['0.0019531'], tr/val_loss:  1.785078/  1.880189, val:  85.83%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.48 seconds, 4.42 minutes\n",
      "total_backward_count 3238280 real_backward_count 221052   6.826%\n",
      "fc layer 1 self.abs_max_out: 10941.0\n",
      "epoch-73  lr=['0.0019531'], tr/val_loss:  1.769470/  1.882143, val:  90.00%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.69 seconds, 4.46 minutes\n",
      "total_backward_count 3282640 real_backward_count 222338   6.773%\n",
      "fc layer 3 self.abs_max_out: 614.0\n",
      "fc layer 1 self.abs_max_out: 10973.0\n",
      "epoch-74  lr=['0.0019531'], tr/val_loss:  1.769007/  1.878180, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.46 seconds, 4.41 minutes\n",
      "total_backward_count 3327000 real_backward_count 223643   6.722%\n",
      "epoch-75  lr=['0.0019531'], tr/val_loss:  1.762556/  1.876521, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.87 seconds, 4.40 minutes\n",
      "total_backward_count 3371360 real_backward_count 224860   6.670%\n",
      "fc layer 1 self.abs_max_out: 10980.0\n",
      "epoch-76  lr=['0.0019531'], tr/val_loss:  1.753987/  1.873415, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.67 seconds, 4.43 minutes\n",
      "total_backward_count 3415720 real_backward_count 226102   6.619%\n",
      "epoch-77  lr=['0.0019531'], tr/val_loss:  1.752497/  1.867575, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.84 seconds, 4.43 minutes\n",
      "total_backward_count 3460080 real_backward_count 227334   6.570%\n",
      "fc layer 1 self.abs_max_out: 10995.0\n",
      "epoch-78  lr=['0.0019531'], tr/val_loss:  1.753817/  1.876406, val:  89.17%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.64 seconds, 4.46 minutes\n",
      "total_backward_count 3504440 real_backward_count 228506   6.520%\n",
      "fc layer 1 self.abs_max_out: 10998.0\n",
      "epoch-79  lr=['0.0019531'], tr/val_loss:  1.759031/  1.870963, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.95 seconds, 4.40 minutes\n",
      "total_backward_count 3548800 real_backward_count 229713   6.473%\n",
      "fc layer 1 self.abs_max_out: 11030.0\n",
      "epoch-80  lr=['0.0019531'], tr/val_loss:  1.753021/  1.868484, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.45 seconds, 4.44 minutes\n",
      "total_backward_count 3593160 real_backward_count 230867   6.425%\n",
      "lif layer 1 self.abs_max_v: 13289.5\n",
      "epoch-81  lr=['0.0019531'], tr/val_loss:  1.751194/  1.859828, val:  89.58%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.70 seconds, 4.39 minutes\n",
      "total_backward_count 3637520 real_backward_count 232017   6.378%\n",
      "fc layer 1 self.abs_max_out: 11032.0\n",
      "epoch-82  lr=['0.0019531'], tr/val_loss:  1.746435/  1.868532, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.99 seconds, 4.40 minutes\n",
      "total_backward_count 3681880 real_backward_count 233164   6.333%\n",
      "epoch-83  lr=['0.0019531'], tr/val_loss:  1.743217/  1.862064, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.98 seconds, 4.45 minutes\n",
      "total_backward_count 3726240 real_backward_count 234338   6.289%\n",
      "epoch-84  lr=['0.0019531'], tr/val_loss:  1.750284/  1.869823, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.02 seconds, 4.43 minutes\n",
      "total_backward_count 3770600 real_backward_count 235399   6.243%\n",
      "epoch-85  lr=['0.0019531'], tr/val_loss:  1.751494/  1.875296, val:  90.42%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.27 seconds, 4.42 minutes\n",
      "total_backward_count 3814960 real_backward_count 236458   6.198%\n",
      "fc layer 1 self.abs_max_out: 11047.0\n",
      "epoch-86  lr=['0.0019531'], tr/val_loss:  1.752084/  1.871552, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.32 seconds, 4.37 minutes\n",
      "total_backward_count 3859320 real_backward_count 237576   6.156%\n",
      "fc layer 1 self.abs_max_out: 11089.0\n",
      "epoch-87  lr=['0.0019531'], tr/val_loss:  1.745179/  1.871416, val:  87.50%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.06 seconds, 4.42 minutes\n",
      "total_backward_count 3903680 real_backward_count 238605   6.112%\n",
      "fc layer 1 self.abs_max_out: 11105.0\n",
      "epoch-88  lr=['0.0019531'], tr/val_loss:  1.741674/  1.865077, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.51 seconds, 4.46 minutes\n",
      "total_backward_count 3948040 real_backward_count 239594   6.069%\n",
      "fc layer 1 self.abs_max_out: 11132.0\n",
      "epoch-89  lr=['0.0019531'], tr/val_loss:  1.740554/  1.863554, val:  88.75%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.48 seconds, 4.44 minutes\n",
      "total_backward_count 3992400 real_backward_count 240542   6.025%\n",
      "fc layer 1 self.abs_max_out: 11155.0\n",
      "epoch-90  lr=['0.0019531'], tr/val_loss:  1.740456/  1.860876, val:  88.33%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.34 seconds, 4.42 minutes\n",
      "total_backward_count 4036760 real_backward_count 241546   5.984%\n",
      "fc layer 1 self.abs_max_out: 11164.0\n",
      "epoch-91  lr=['0.0019531'], tr/val_loss:  1.743912/  1.864657, val:  92.08%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.15 seconds, 4.40 minutes\n",
      "total_backward_count 4081120 real_backward_count 242543   5.943%\n",
      "epoch-92  lr=['0.0019531'], tr/val_loss:  1.745100/  1.861594, val:  91.67%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.99 seconds, 4.43 minutes\n",
      "total_backward_count 4125480 real_backward_count 243508   5.903%\n",
      "epoch-93  lr=['0.0019531'], tr/val_loss:  1.735266/  1.854500, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.28 seconds, 4.39 minutes\n",
      "total_backward_count 4169840 real_backward_count 244476   5.863%\n",
      "epoch-94  lr=['0.0019531'], tr/val_loss:  1.732816/  1.853600, val:  91.25%, val_best:  92.08%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.65 seconds, 4.44 minutes\n",
      "total_backward_count 4214200 real_backward_count 245523   5.826%\n",
      "epoch-95  lr=['0.0019531'], tr/val_loss:  1.734667/  1.854832, val:  92.92%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.97 seconds, 4.43 minutes\n",
      "total_backward_count 4258560 real_backward_count 246490   5.788%\n",
      "fc layer 3 self.abs_max_out: 648.0\n",
      "epoch-96  lr=['0.0019531'], tr/val_loss:  1.737848/  1.850768, val:  89.58%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.73 seconds, 4.41 minutes\n",
      "total_backward_count 4302920 real_backward_count 247437   5.750%\n",
      "epoch-97  lr=['0.0019531'], tr/val_loss:  1.736210/  1.857680, val:  88.33%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.13 seconds, 4.37 minutes\n",
      "total_backward_count 4347280 real_backward_count 248449   5.715%\n",
      "epoch-98  lr=['0.0019531'], tr/val_loss:  1.734800/  1.855345, val:  92.08%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.76 seconds, 4.45 minutes\n",
      "total_backward_count 4391640 real_backward_count 249384   5.679%\n",
      "epoch-99  lr=['0.0019531'], tr/val_loss:  1.734801/  1.849583, val:  90.42%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.49 seconds, 4.41 minutes\n",
      "total_backward_count 4436000 real_backward_count 250301   5.642%\n",
      "epoch-100 lr=['0.0019531'], tr/val_loss:  1.724067/  1.847326, val:  91.67%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.67 seconds, 4.44 minutes\n",
      "total_backward_count 4480360 real_backward_count 251207   5.607%\n",
      "lif layer 1 self.abs_max_v: 13443.0\n",
      "epoch-101 lr=['0.0019531'], tr/val_loss:  1.722908/  1.850300, val:  89.17%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.49 seconds, 4.42 minutes\n",
      "total_backward_count 4524720 real_backward_count 252105   5.572%\n",
      "epoch-102 lr=['0.0019531'], tr/val_loss:  1.727150/  1.844027, val:  90.42%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.51 seconds, 4.43 minutes\n",
      "total_backward_count 4569080 real_backward_count 252979   5.537%\n",
      "lif layer 1 self.abs_max_v: 13447.0\n",
      "epoch-103 lr=['0.0019531'], tr/val_loss:  1.721434/  1.846238, val:  90.83%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.62 seconds, 4.44 minutes\n",
      "total_backward_count 4613440 real_backward_count 253858   5.503%\n",
      "epoch-104 lr=['0.0019531'], tr/val_loss:  1.716623/  1.850981, val:  90.83%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.45 seconds, 4.37 minutes\n",
      "total_backward_count 4657800 real_backward_count 254739   5.469%\n",
      "lif layer 1 self.abs_max_v: 13467.5\n",
      "epoch-105 lr=['0.0019531'], tr/val_loss:  1.725854/  1.840323, val:  92.50%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.89 seconds, 4.45 minutes\n",
      "total_backward_count 4702160 real_backward_count 255714   5.438%\n",
      "epoch-106 lr=['0.0019531'], tr/val_loss:  1.724635/  1.838964, val:  92.92%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.65 seconds, 4.41 minutes\n",
      "total_backward_count 4746520 real_backward_count 256533   5.405%\n",
      "epoch-107 lr=['0.0019531'], tr/val_loss:  1.719553/  1.838479, val:  88.75%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.93 seconds, 4.45 minutes\n",
      "total_backward_count 4790880 real_backward_count 257345   5.372%\n",
      "epoch-108 lr=['0.0019531'], tr/val_loss:  1.717993/  1.842049, val:  91.67%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.04 seconds, 4.38 minutes\n",
      "total_backward_count 4835240 real_backward_count 258162   5.339%\n",
      "lif layer 1 self.abs_max_v: 13579.5\n",
      "epoch-109 lr=['0.0019531'], tr/val_loss:  1.720253/  1.852942, val:  91.25%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.17 seconds, 4.42 minutes\n",
      "total_backward_count 4879600 real_backward_count 258993   5.308%\n",
      "fc layer 1 self.abs_max_out: 11166.0\n",
      "epoch-110 lr=['0.0019531'], tr/val_loss:  1.723540/  1.848449, val:  92.50%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.00 seconds, 4.43 minutes\n",
      "total_backward_count 4923960 real_backward_count 259843   5.277%\n",
      "fc layer 1 self.abs_max_out: 11174.0\n",
      "epoch-111 lr=['0.0019531'], tr/val_loss:  1.714919/  1.840134, val:  88.33%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.26 seconds, 4.44 minutes\n",
      "total_backward_count 4968320 real_backward_count 260607   5.245%\n",
      "lif layer 1 self.abs_max_v: 13598.0\n",
      "fc layer 1 self.abs_max_out: 11182.0\n",
      "epoch-112 lr=['0.0019531'], tr/val_loss:  1.712123/  1.835634, val:  90.83%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.07 seconds, 4.43 minutes\n",
      "total_backward_count 5012680 real_backward_count 261444   5.216%\n",
      "epoch-113 lr=['0.0019531'], tr/val_loss:  1.709030/  1.838876, val:  90.00%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.41 seconds, 4.39 minutes\n",
      "total_backward_count 5057040 real_backward_count 262254   5.186%\n",
      "lif layer 1 self.abs_max_v: 13666.5\n",
      "fc layer 3 self.abs_max_out: 649.0\n",
      "epoch-114 lr=['0.0019531'], tr/val_loss:  1.709407/  1.838953, val:  88.75%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.61 seconds, 4.43 minutes\n",
      "total_backward_count 5101400 real_backward_count 263042   5.156%\n",
      "epoch-115 lr=['0.0019531'], tr/val_loss:  1.706480/  1.841340, val:  89.58%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.54 seconds, 4.41 minutes\n",
      "total_backward_count 5145760 real_backward_count 263791   5.126%\n",
      "epoch-116 lr=['0.0019531'], tr/val_loss:  1.711409/  1.843663, val:  88.33%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 262.84 seconds, 4.38 minutes\n",
      "total_backward_count 5190120 real_backward_count 264592   5.098%\n",
      "epoch-117 lr=['0.0019531'], tr/val_loss:  1.709069/  1.833962, val:  91.67%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.83 seconds, 4.43 minutes\n",
      "total_backward_count 5234480 real_backward_count 265370   5.070%\n",
      "fc layer 1 self.abs_max_out: 11198.0\n",
      "epoch-118 lr=['0.0019531'], tr/val_loss:  1.711230/  1.836832, val:  90.83%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.59 seconds, 4.46 minutes\n",
      "total_backward_count 5278840 real_backward_count 266209   5.043%\n",
      "epoch-119 lr=['0.0019531'], tr/val_loss:  1.710733/  1.842682, val:  91.67%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.97 seconds, 4.40 minutes\n",
      "total_backward_count 5323200 real_backward_count 267050   5.017%\n",
      "fc layer 1 self.abs_max_out: 11212.0\n",
      "epoch-120 lr=['0.0019531'], tr/val_loss:  1.708559/  1.828352, val:  92.08%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.87 seconds, 4.36 minutes\n",
      "total_backward_count 5367560 real_backward_count 267800   4.989%\n",
      "lif layer 1 self.abs_max_v: 13733.0\n",
      "fc layer 1 self.abs_max_out: 11213.0\n",
      "epoch-121 lr=['0.0019531'], tr/val_loss:  1.697698/  1.827278, val:  88.33%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.06 seconds, 4.43 minutes\n",
      "total_backward_count 5411920 real_backward_count 268614   4.963%\n",
      "fc layer 1 self.abs_max_out: 11215.0\n",
      "epoch-122 lr=['0.0019531'], tr/val_loss:  1.702434/  1.833828, val:  91.25%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.57 seconds, 4.44 minutes\n",
      "total_backward_count 5456280 real_backward_count 269375   4.937%\n",
      "fc layer 1 self.abs_max_out: 11222.0\n",
      "epoch-123 lr=['0.0019531'], tr/val_loss:  1.703549/  1.826544, val:  90.00%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.28 seconds, 4.44 minutes\n",
      "total_backward_count 5500640 real_backward_count 270131   4.911%\n",
      "epoch-124 lr=['0.0019531'], tr/val_loss:  1.701747/  1.821667, val:  92.08%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.51 seconds, 4.41 minutes\n",
      "total_backward_count 5545000 real_backward_count 270912   4.886%\n",
      "epoch-125 lr=['0.0019531'], tr/val_loss:  1.704457/  1.838788, val:  89.17%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.49 seconds, 4.41 minutes\n",
      "total_backward_count 5589360 real_backward_count 271621   4.860%\n",
      "epoch-126 lr=['0.0019531'], tr/val_loss:  1.705356/  1.827865, val:  90.42%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.44 seconds, 4.44 minutes\n",
      "total_backward_count 5633720 real_backward_count 272312   4.834%\n",
      "epoch-127 lr=['0.0019531'], tr/val_loss:  1.697519/  1.822721, val:  90.83%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.41 seconds, 4.34 minutes\n",
      "total_backward_count 5678080 real_backward_count 272994   4.808%\n",
      "epoch-128 lr=['0.0019531'], tr/val_loss:  1.702358/  1.835118, val:  90.00%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.56 seconds, 4.44 minutes\n",
      "total_backward_count 5722440 real_backward_count 273726   4.783%\n",
      "epoch-129 lr=['0.0019531'], tr/val_loss:  1.704483/  1.830258, val:  90.83%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.43 seconds, 4.42 minutes\n",
      "total_backward_count 5766800 real_backward_count 274432   4.759%\n",
      "epoch-130 lr=['0.0019531'], tr/val_loss:  1.699614/  1.825990, val:  90.00%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.52 seconds, 4.43 minutes\n",
      "total_backward_count 5811160 real_backward_count 275188   4.736%\n",
      "epoch-131 lr=['0.0019531'], tr/val_loss:  1.701576/  1.832371, val:  90.83%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 260.54 seconds, 4.34 minutes\n",
      "total_backward_count 5855520 real_backward_count 275856   4.711%\n",
      "epoch-132 lr=['0.0019531'], tr/val_loss:  1.702716/  1.828536, val:  91.25%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.35 seconds, 4.42 minutes\n",
      "total_backward_count 5899880 real_backward_count 276593   4.688%\n",
      "epoch-133 lr=['0.0019531'], tr/val_loss:  1.695939/  1.825097, val:  92.08%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.45 seconds, 4.44 minutes\n",
      "total_backward_count 5944240 real_backward_count 277244   4.664%\n",
      "epoch-134 lr=['0.0019531'], tr/val_loss:  1.686064/  1.821214, val:  92.50%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 267.21 seconds, 4.45 minutes\n",
      "total_backward_count 5988600 real_backward_count 277922   4.641%\n",
      "epoch-135 lr=['0.0019531'], tr/val_loss:  1.691235/  1.816659, val:  88.75%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.65 seconds, 4.43 minutes\n",
      "total_backward_count 6032960 real_backward_count 278591   4.618%\n",
      "epoch-136 lr=['0.0019531'], tr/val_loss:  1.686485/  1.815195, val:  89.58%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 264.01 seconds, 4.40 minutes\n",
      "total_backward_count 6077320 real_backward_count 279241   4.595%\n",
      "epoch-137 lr=['0.0019531'], tr/val_loss:  1.682521/  1.816344, val:  92.08%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 265.67 seconds, 4.43 minutes\n",
      "total_backward_count 6121680 real_backward_count 279812   4.571%\n",
      "fc layer 3 self.abs_max_out: 655.0\n",
      "epoch-138 lr=['0.0019531'], tr/val_loss:  1.683805/  1.811363, val:  92.92%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.51 seconds, 4.39 minutes\n",
      "total_backward_count 6166040 real_backward_count 280458   4.548%\n",
      "epoch-139 lr=['0.0019531'], tr/val_loss:  1.684507/  1.823473, val:  88.33%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 263.74 seconds, 4.40 minutes\n",
      "total_backward_count 6210400 real_backward_count 281109   4.526%\n",
      "epoch-140 lr=['0.0019531'], tr/val_loss:  1.685511/  1.822126, val:  91.67%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.16 seconds, 4.44 minutes\n",
      "total_backward_count 6254760 real_backward_count 281770   4.505%\n",
      "epoch-141 lr=['0.0019531'], tr/val_loss:  1.691500/  1.830060, val:  89.17%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 266.77 seconds, 4.45 minutes\n",
      "total_backward_count 6299120 real_backward_count 282392   4.483%\n",
      "epoch-142 lr=['0.0019531'], tr/val_loss:  1.690398/  1.830966, val:  90.42%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 261.64 seconds, 4.36 minutes\n",
      "total_backward_count 6343480 real_backward_count 283040   4.462%\n",
      "epoch-143 lr=['0.0019531'], tr/val_loss:  1.688150/  1.819441, val:  91.67%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 257.98 seconds, 4.30 minutes\n",
      "total_backward_count 6387840 real_backward_count 283727   4.442%\n",
      "epoch-144 lr=['0.0019531'], tr/val_loss:  1.682477/  1.816887, val:  92.08%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 256.99 seconds, 4.28 minutes\n",
      "total_backward_count 6432200 real_backward_count 284384   4.421%\n",
      "epoch-145 lr=['0.0019531'], tr/val_loss:  1.688281/  1.831275, val:  90.83%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 253.55 seconds, 4.23 minutes\n",
      "total_backward_count 6476560 real_backward_count 285070   4.402%\n",
      "epoch-146 lr=['0.0019531'], tr/val_loss:  1.686075/  1.805347, val:  91.25%, val_best:  92.92%, tr: 100.00%, tr_best: 100.00%, epoch time: 244.54 seconds, 4.08 minutes\n",
      "total_backward_count 6520920 real_backward_count 285694   4.381%\n",
      "epoch-147 lr=['0.0019531'], tr/val_loss:  1.674917/  1.811457, val:  93.75%, val_best:  93.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 240.66 seconds, 4.01 minutes\n",
      "total_backward_count 6565280 real_backward_count 286248   4.360%\n",
      "epoch-148 lr=['0.0019531'], tr/val_loss:  1.676210/  1.807258, val:  87.50%, val_best:  93.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 241.10 seconds, 4.02 minutes\n",
      "total_backward_count 6609640 real_backward_count 286802   4.339%\n",
      "epoch-149 lr=['0.0019531'], tr/val_loss:  1.677924/  1.813303, val:  89.17%, val_best:  93.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 242.40 seconds, 4.04 minutes\n",
      "total_backward_count 6654000 real_backward_count 287432   4.320%\n",
      "epoch-150 lr=['0.0019531'], tr/val_loss:  1.678166/  1.830585, val:  87.50%, val_best:  93.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 243.99 seconds, 4.07 minutes\n",
      "total_backward_count 6698360 real_backward_count 288046   4.300%\n",
      "epoch-151 lr=['0.0019531'], tr/val_loss:  1.671994/  1.808460, val:  92.08%, val_best:  93.75%, tr: 100.00%, tr_best: 100.00%, epoch time: 248.32 seconds, 4.14 minutes\n",
      "total_backward_count 6742720 real_backward_count 288626   4.281%\n"
     ]
    }
   ],
   "source": [
    "# sweep ÌïòÎäî ÏΩîÎìú, ÏúÑ ÏÖÄ Ï£ºÏÑùÏ≤òÎ¶¨ Ìï¥Ïïº Îê®.\n",
    "\n",
    "# Ïù¥Îü∞ ÏõåÎãù Îú®Îäî Í±∞Îäî Í±ç ÎÑàÍ∞Ä main ÏïàÏóêÏÑú  wandb.config.update(hyperparameters)Ìï† Îïå Î¨ºÎ†§ÏÑúÏûÑ. Ïñ¥Ï∞®Ìîº Í∑ºÎç∞ sweepÏóêÏÑú ÏßÄÏ†ïÌïú Í±∏Î°ú ÎçÆÏñ¥Ïßê \n",
    "# wandb: WARNING Config item 'BATCH' was locked by 'sweep' (ignored update).\n",
    "\n",
    "unique_name_hyper = 'main'\n",
    "sweep_configuration = {\n",
    "    'method': 'random', # 'random', 'bayes', 'grid'\n",
    "    'name': f'my_snn_sweep{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc_best'},\n",
    "    'parameters': \n",
    "    {\n",
    "        # \"devices\": {\"values\": [\"1\"]},\n",
    "        \"single_step\": {\"values\": [True]},\n",
    "        # \"unique_name\": {\"values\": [unique_name_hyper]},\n",
    "        \"my_seed\": {\"min\": 1, \"max\": 42000},\n",
    "        # \"my_seed\": {\"values\": [42]},\n",
    "        \"TIME\": {\"values\": [10]},\n",
    "        \"BATCH\": {\"values\": [1]},\n",
    "        \"IMAGE_SIZE\": {\"values\": [14]},\n",
    "        \"which_data\": {\"values\": ['DVS_GESTURE_TONIC']},\n",
    "        \"data_path\": {\"values\": ['/data2']},\n",
    "        \"rate_coding\": {\"values\": [False]},\n",
    "        \"lif_layer_v_init\": {\"values\": [0.0]},\n",
    "        \"lif_layer_v_decay\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_threshold\": {\"values\": [0.5]},\n",
    "        \"lif_layer_v_reset\": {\"values\": [10000.0]},\n",
    "        \"lif_layer_sg_width\": {\"values\": [5.0, 5.5, 6.0, 6.5, 7.0]},\n",
    "        # \"lif_layer_sg_width\": {\"values\": [3.0, 6.0, 10.0, 15.0, 20.0]},\n",
    "\n",
    "        \"synapse_conv_kernel_size\": {\"values\": [3]},\n",
    "        \"synapse_conv_stride\": {\"values\": [1]},\n",
    "        \"synapse_conv_padding\": {\"values\": [1]},\n",
    "\n",
    "        \"synapse_trace_const1\": {\"values\": [1]},\n",
    "        \"synapse_trace_const2\": {\"values\": [0.5]},\n",
    "\n",
    "        \"pre_trained\": {\"values\": [False]},\n",
    "        \"convTrue_fcFalse\": {\"values\": [False]},\n",
    "\n",
    "        \"cfg\": {\"values\": [[200,200]]},\n",
    "\n",
    "        \"net_print\": {\"values\": [True]},\n",
    "\n",
    "        \"pre_trained_path\": {\"values\": [\"\"]},\n",
    "        \"learning_rate\": {\"values\": [1/512]}, \n",
    "        \"epoch_num\": {\"values\": [200]}, \n",
    "        \"tdBN_on\": {\"values\": [False]},\n",
    "        \"BN_on\": {\"values\": [False]},\n",
    "\n",
    "        \"surrogate\": {\"values\": ['hard_sigmoid']},\n",
    "\n",
    "        \"BPTT_on\": {\"values\": [False]},\n",
    "\n",
    "        \"optimizer_what\": {\"values\": ['SGD']},\n",
    "        \"scheduler_name\": {\"values\": ['no']},\n",
    "\n",
    "        \"ddp_on\": {\"values\": [False]},\n",
    "\n",
    "        \"dvs_clipping\": {\"values\": [14]}, \n",
    "\n",
    "        \"dvs_duration\": {\"values\": [25_000]}, \n",
    "\n",
    "        \"DFA_on\": {\"values\": [True]},\n",
    "\n",
    "        \"trace_on\": {\"values\": [False]},\n",
    "        \"OTTT_input_trace_on\": {\"values\": [False]},\n",
    "\n",
    "        \"exclude_class\": {\"values\": [True]},\n",
    "\n",
    "        \"merge_polarities\": {\"values\": [True]},\n",
    "        \"denoise_on\": {\"values\": [False]},\n",
    "\n",
    "        \"extra_train_dataset\": {\"values\": [9]},\n",
    "\n",
    "        \"num_workers\": {\"values\": [2]},\n",
    "        \"chaching_on\": {\"values\": [True]},\n",
    "        \"pin_memory\": {\"values\": [True]},\n",
    "\n",
    "        \"UDA_on\": {\"values\": [False]},\n",
    "        \"alpha_uda\": {\"values\": [1.0]},\n",
    "\n",
    "        \"bias\": {\"values\": [False]},\n",
    "\n",
    "        \"last_lif\": {\"values\": [False]},\n",
    "\n",
    "        \"temporal_filter\": {\"values\": [5]},\n",
    "        \"initial_pooling\": {\"values\": [1]},\n",
    "\n",
    "        \"temporal_filter_accumulation\": {\"values\": [False]},\n",
    "\n",
    "        \"quantize_bit_list_0\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_1\": {\"values\": [8]},\n",
    "        \"quantize_bit_list_2\": {\"values\": [8]},\n",
    "\n",
    "\n",
    "        \"scale_exp_1w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_1b\": {\"values\": [-11,-10,-9,-8,-7,-6]},\n",
    "        \"scale_exp_2w\": {\"values\": [-10]},\n",
    "        # \"scale_exp_2b\": {\"values\": [-10,-9,-8]},\n",
    "        \"scale_exp_3w\": {\"values\": [-9]},\n",
    "        # \"scale_exp_3b\": {\"values\": [-10,-9,-8,-7,-6]},\n",
    "     }\n",
    "}\n",
    "\n",
    "def hyper_iter():\n",
    "    ### my_snn control board ########################\n",
    "    wandb.init(save_code=False, dir='/data2/bh_wandb', tags=[\"sweep\"])\n",
    "\n",
    "    my_snn_system(  \n",
    "        devices  =  \"0\",\n",
    "        single_step  =  wandb.config.single_step,\n",
    "        unique_name  =  datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + f\"{datetime.datetime.now().microsecond // 1000:03d}\",\n",
    "        my_seed  =  wandb.config.my_seed,\n",
    "        TIME  =  wandb.config.TIME,\n",
    "        BATCH  =  wandb.config.BATCH,\n",
    "        IMAGE_SIZE  =  wandb.config.IMAGE_SIZE,\n",
    "        which_data  =  wandb.config.which_data,\n",
    "        data_path  =  wandb.config.data_path,\n",
    "        rate_coding  =  wandb.config.rate_coding,\n",
    "        lif_layer_v_init  =  wandb.config.lif_layer_v_init,\n",
    "        lif_layer_v_decay  =  wandb.config.lif_layer_v_decay,\n",
    "        lif_layer_v_threshold  =  wandb.config.lif_layer_v_threshold,\n",
    "        lif_layer_v_reset  =  wandb.config.lif_layer_v_reset,\n",
    "        lif_layer_sg_width  =  wandb.config.lif_layer_sg_width,\n",
    "        synapse_conv_kernel_size  =  wandb.config.synapse_conv_kernel_size,\n",
    "        synapse_conv_stride  =  wandb.config.synapse_conv_stride,\n",
    "        synapse_conv_padding  =  wandb.config.synapse_conv_padding,\n",
    "        synapse_trace_const1  =  wandb.config.synapse_trace_const1,\n",
    "        synapse_trace_const2  =  wandb.config.synapse_trace_const2,\n",
    "        pre_trained  =  wandb.config.pre_trained,\n",
    "        convTrue_fcFalse  =  wandb.config.convTrue_fcFalse,\n",
    "        cfg  =  wandb.config.cfg,\n",
    "        net_print  =  wandb.config.net_print,\n",
    "        pre_trained_path  =  wandb.config.pre_trained_path,\n",
    "        learning_rate  =  wandb.config.learning_rate,\n",
    "        epoch_num  =  wandb.config.epoch_num,\n",
    "        tdBN_on  =  wandb.config.tdBN_on,\n",
    "        BN_on  =  wandb.config.BN_on,\n",
    "        surrogate  =  wandb.config.surrogate,\n",
    "        BPTT_on  =  wandb.config.BPTT_on,\n",
    "        optimizer_what  =  wandb.config.optimizer_what,\n",
    "        scheduler_name  =  wandb.config.scheduler_name,\n",
    "        ddp_on  =  wandb.config.ddp_on,\n",
    "        dvs_clipping  =  wandb.config.dvs_clipping,\n",
    "        dvs_duration  =  wandb.config.dvs_duration,\n",
    "        DFA_on  =  wandb.config.DFA_on,\n",
    "        trace_on  =  wandb.config.trace_on,\n",
    "        OTTT_input_trace_on  =  wandb.config.OTTT_input_trace_on,\n",
    "        exclude_class  =  wandb.config.exclude_class,\n",
    "        merge_polarities  =  wandb.config.merge_polarities,\n",
    "        denoise_on  =  wandb.config.denoise_on,\n",
    "        extra_train_dataset  =  wandb.config.extra_train_dataset,\n",
    "        num_workers  =  wandb.config.num_workers,\n",
    "        chaching_on  =  wandb.config.chaching_on,\n",
    "        pin_memory  =  wandb.config.pin_memory,\n",
    "        UDA_on  =  wandb.config.UDA_on,\n",
    "        alpha_uda  =  wandb.config.alpha_uda,\n",
    "        bias  =  wandb.config.bias,\n",
    "        last_lif  =  wandb.config.last_lif,\n",
    "        temporal_filter  =  wandb.config.temporal_filter,\n",
    "        initial_pooling  =  wandb.config.initial_pooling,\n",
    "        temporal_filter_accumulation  =  wandb.config.temporal_filter_accumulation,\n",
    "        quantize_bit_list  =  [wandb.config.quantize_bit_list_0,wandb.config.quantize_bit_list_1,wandb.config.quantize_bit_list_2],\n",
    "        scale_exp = [[wandb.config.scale_exp_1w,wandb.config.scale_exp_1w],[wandb.config.scale_exp_2w,wandb.config.scale_exp_2w],[wandb.config.scale_exp_3w,wandb.config.scale_exp_3w]],\n",
    "                        ) \n",
    "    # sigmoidÏôÄ BNÏù¥ ÏûàÏñ¥Ïïº ÏûòÎêúÎã§.\n",
    "    # average pooling\n",
    "    # Ïù¥ ÎÇ´Îã§. \n",
    "    \n",
    "    # ndaÏóêÏÑúÎäî decay = 0.25, threshold = 0.5, width =1, surrogate = rectangle, batch = 256, tdBN = True\n",
    "    ## OTTT ÏóêÏÑúÎäî decay = 0.5, threshold = 1.0, surrogate = sigmoid, batch = 128, BN = True\n",
    "\n",
    "sweep_id = 't0h80lho'\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=f'my_snn {unique_name_hyper}')\n",
    "wandb.agent(sweep_id, function=hyper_iter, count=10000, project=f'my_snn {unique_name_hyper}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aedat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
